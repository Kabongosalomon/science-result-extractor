section: title
Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction
section: abstract
Many statistical learning problems in NLP call for local model search methods. But accuracy tends to suffer with current techniques, which often explore either too narrowly or too broadly: hill-climbers can get stuck in local optima, whereas samplers maybe inefficient. We propose to arrange individual local opti-mizers into organized networks. Our building blocks are operators of two types: (i) transform , which suggests new places to search, via non-random restarts from already-found local optima; and (ii) join, which merges candidate solutions to find better optima. Experiments on grammar induction show that pursuing different transforms (e.g., discarding parts of a learned model or ignoring portions of training data) results in improvements. Groups of locally-optimal solutions can be further perturbed jointly, by constructing mixtures. Using these tools, we designed several modular dependency grammar induction networks of increasing complexity. Our complete system achieves 48.6% accuracy (directed dependency macro-average overall 19 languages in the 2006/7 CoNLL data)-more than 5% higher than the previous state-of-the-art.
section: Introduction
Statistical methods for grammar induction often boil down to solving non-convex optimization problems. Early work attempted to locally maximize the likelihood of a corpus, using EM to estimate probabilities of dependency arcs between word bigrams). That parsing model has since been extended to make unsupervised learning more feasible (). But even the latest techniques can be quite error-prone and sensitive to initialization, because of approximate, local search.
In theory, global optima can be found by enumerating all parse forests that derive a corpus, though this is usually prohibitively expensive in practice. A
section: Abstract Operators
Let C be a collection of counts -the sufficient statistics from which a candidate solution to an optimization problem could be computed, e.g., by smoothing and normalizing to yield probabilities. The counts maybe fractional and solutions could take the form of multinomial distributions. A local optimizer L will convert C into C * = L D (C) -an updated collection of counts, resulting in a probabilistic model that is no less (and hopefully more) consistent with a data set D than the original C:
Unless C * is a global optimum, we should be able to make further improvements. But if L is idempotent (and ran to convergence) then L(L(C)) = L(C). Given only C and L D , the single-node optimization network above would be the minimal search pattern worth considering. However, if we had another optimizer L ′ -or afresh starting point C ′ -then more complicated networks could become useful.
section: Transforms (Unary)
New starts could be chosen by perturbing an existing solution, as in MCMC, or independently of previous results, as in random restarts. We focus on intermediate changes to C, without injecting randomness. All of our transforms involve selective forgetting or filtering. For example, if the probabilistic model that is being estimated decomposes into independent constituents (e.g., several multinomials) then a subset of them can be reset to uniform distributions, by discarding associated counts from C. In text classification, this could correspond to eliminating frequent or rare tokens from bags-of-words. We use circular shapes to represent such model ablation operators: C An orthogonal approach might separate out various counts in C by their provenance. For instance, if D consisted of several heterogeneous data sources, then the counts from some of them could be ignored: a classifier might be estimated from just news text. We will use squares to represent data-set filtering: C Finally, if C represents a mixture of possible interpretations over D -e.g., because it captures the output of a "soft" EM algorithm -contributions from less likely, noisier completions could also be suppressed (and their weights redistributed to the more likely ones), as in "hard" EM. Diamonds will represent plain (single) steps of Viterbi training:
section: Joins (Binary)
Starting from different initializers, say C 1 and C 2 , it maybe possible for L to arrive at distinct local optima, C * 1 񮽙 = C * 2 . The better of the two solutions, according to likelihood L D of D, could then be selected -as is standard practice when sampling.
Our joining technique could do better than either
, by entertaining also a third possibility, which combines the two candidates. We construct a mixture model by adding together all counts from
Original initializers C 1 , C 2 will, this way, have equal pull on the merged model, 1 regardless of nominal size (because C * 1 , C * 2 will have converged using a shared training set, D). We return the best of C * 1 , C * 2 and C * + = L(C + ). This approach may uncover more (and never returns less) likely solutions than choosing among
We will use a short-hand notation to represent the combiner network diagrammed above, less clutter:
section: The Task and Methodology
We apply transform and join paradigms to grammar induction, an important problem of computational linguistics that involves notoriously difficult objectives (. The goal is to induce grammars capable of parsing unseen text. Input, in both training and testing, is a sequence of tokens labeled as: (i) a lexical item and its category, (w, cw ); (ii) a punctuation mark; or (iii) a sentence boundary. Output is unlabeled dependency trees.
section: Models and Data
We constrain all parse structures to be projective, via dependency-and-boundary grammars (): DBMs 0-3 are head-outward generative parsing models) that distinguish complete sentences from incomplete fragments in a corpus D: D comp comprises inputs ending with punctuation; D frag = D − D comp is everything else. The "complete" subset is further partitioned into simple sentences, D simp ⊆ D comp , with no internal punctuation, and others, which maybe complex.
As an example, consider the beginning of an article from (simple) Wikipedia: (i) Linguistics (ii) Linguistics (sometimes called philology) is the science that studies language. (iii) Scientists who study language are called linguists. Since the title does not end with punctuation, it would be relegated to D frag . But two complete sentences would be in D comp , with the last also filed under D simp , as it has only a trailing punctuation mark. Spitkovsky et al. suggested two curriculum learning strategies: (i) one in which induction begins with clean, simple data, D simp , and a basic model, DBM-1 (2012b); and (ii) an alternative bootstrapping approach: starting with still more, simpler data -namely, short inter-punctuation fragments up to length l = 15, D l split ⊇ D l simp -and a bare-bones model, DBM-0 (2012a). In our example, D split would hold five text snippets: (i) Linguistics; (ii) Linguistics; (iii) sometimes called philology; (iv) is the science that studies language; and (v) Scientists who study language are called linguists. Only the last piece of text would still be considered complete, isolating its contribution to sentence root and boundary word distributions from those of incomplete fragments. The sparse model, DBM-0, assumes a uniform distribution for roots of incomplete inputs and reduces conditioning contexts of stopping probabilities, which works well with split data. We will exploit both DBM-0 and the full DBM, 2 drawing also on split, simple and raw views of input text.
All experiments prior to final multi-lingual evaluation will use the Penn English Treebank's Wall Street Journal (WSJ) portion as the underlying tokenized and sentence-broken corpus D. Instead of gold parts-of-speech, we plugged in 200 context-sensitive unsupervised tags, from Spitkovsky et al. (2011c), for the word categories.
section: Smoothing and Lexicalization
All unlexicalized instances of DBMs will be estimated with "add one" (a.k.a. Laplace) smoothing, using only the word category cw to represent a token. Fully-lexicalized grammars (L-DBM) are left unsmoothed, and represent each token as both a word and its category, i.e., the whole pair (w, cw ). To evaluate a lexicalized parsing model, we will always obtain a delexicalized-and-smoothed instance first.
section: Optimization and Viterbi Decoding
We use "early-switching lateen" EM ( to train unlexicalized models, alternating between the objectives of ordinary (soft) and hard EM algorithms, until neither can improve its own objective without harming the other's. This approach does not require tuning termination thresholds, allowing optimizers to run to numerical convergence if necessary, and handles only our shorter inputs (l ≤ 15), starting with soft EM (L = SL, for "soft lateen"). Lexicalized models will cover full data (l ≤ 45) and employ "early-stopping lateen" EM (2011a, §2.3), re-estimating via hard EM until soft EM's objective suffers. Alternating EMs would be expensive here, since updates take (at least) O(l 3 ) time, and hard EM's objective (L = H) is the one better suited to long inputs (.
Our decoders always force an inter-punctuation fragment to derive itself (Spitkovsky et al., 2011b, §2.2). In evaluation, such (loose) constraints may help attach sometimes and philology to called (and the science... to is). In training, stronger (strict) constraints also disallow attachment of fragments' heads by non-heads, to connect Linguistics, called and is (assuming each piece got parsed correctly).
section: Final Evaluation and Metrics
Evaluation is against held-out CoNLL shared task data (, spanning 19 languages. We compute performance as directed dependency accuracies (DDA), fractions of correct unlabeled arcs in parsed output (an extrinsic metric). For most WSJ experiments we include also sentence and parse tree cross-entropies (soft and hard EMs' intrinsic metrics), in bits per token (bpt).
section: Concrete Operators
We will now instantiate the operators sketched out in §2 specifically for the grammar induction task. Throughout, we repeatedly employ single steps of Viterbi training to transfer information between subnetworks in a model-independent way: when a module's output is a set of (Viterbi) parse trees, it necessarily contains sufficient information required to estimate an arbitrarily-factored model down-stream.
section: Transform #1: A Simple Filter
Given a model that was estimated from (and therefore parses) a data set D, the simple filter (F ) attempts to extract a cleaner model, based on the simpler complete sentences of D simp . It is implemented as a single (unlexicalized) step of Viterbi training:
The idea here is to focus on sentences that are not too complicated yet grammatical. This punctuationsensitive heuristic may steer a learner towards easy but representative training text and, we showed, aids grammar induction (Spitkovsky et al., 2012b, §7.1).
section: Transform #2: A Symmetrizer
The symmetrizer (S) reduces input models to sets of word association scores. It blurs all details of induced parses in a data set D, except the number of times each (ordered) word pair participates in a dependency relation. We implemented symmetrization also as a single unlexicalized Viterbi training step, but now with proposed parse trees' scores, fora sentence in D, proportional to a product over non-root dependency arcs of one plus how often the left and right tokens (are expected to) appear connected:
The idea behind the symmetrizer is to glean information from skeleton parses. Grammar inducers can sometimes make good progress in resolving undirected parse structures despite being wrong about the polarities of most arcs (Spitkovsky et al., 2009,: Uninformed). Symmetrization offers an extra chance to make heads or tails of syntactic relations, after learning which words tend to go together.
At each instance where a word a attaches z on (say) the right, our implementation attributes half its weight to the intended construction, a z , reserving the other half for the symmetric structure, z attaching a to its left: a z . For the desired effect, these aggregated counts are left unnormalized, while all other counts (of word fertilities and sentence roots) get discarded. To see why we don't turn word attachment scores into probabilities, consider sentences ), not because there is evidence in the data, but as a side-effect of a model's head-driven nature (i.e., factored with dependents conditioned on heads). Always branching right would be a mistake, however, for example if z is a noun, since either of a orc could be a determiner, with the other a verb.
section: Join: A Combiner
The combiner must admit arbitrary inputs, including models not estimated from D, unlike the transforms. Consequently, as a preliminary step, we convert each input Ci into parse trees of D, with counts
section: Basic Networks
We are ready to propose a non-trivial subnetwork for grammar induction, based on the transform and join operators, which we will reuse in larger networks.
section: Fork/Join (FJ)
Given a model that parses abase data set D 0 , the fork/join subnetwork will output an adaptation of that model for D. It could facilitate a grammar induction process, e.g., by advancing it from smaller to larger -or possibly more complex -data sets.
We first fork off two variations of the incoming model based on D 0 : (i) a filtered view, which focuses on cleaner, simpler data (transform #1); and (ii) a symmetrized view that backs off to word associations (transform #2). Next is grammar induction over D. We optimize a full DBM instance starting from the first fork, and bootstrap a reduced DBM 0 from the second. Finally, the two new induced sets of parse trees, for D, are merged (lexicalized join):
The idea here is to prepare for two scenarios: an incoming grammar that is either good or bad for D. If the model is good, DBM should be able to hang onto it and make improvements. But if it is bad, DBM could get stuck fitting noise, whereas DBM 0 might be more likely to ramp up to a good alternative. Since we can't know ahead of time which is the true case, we pursue both optimization paths simultaneously and let a combiner later decide for us.
Note that the forks start (and end) optimizing with soft EM. This is because soft EM integrates previously unseen tokens into new grammars better than hard EM, as evidenced by our failed attempt to reproduce the "baby steps" strategy with Viterbi training (. A combiner then executes hard EM, and since outputs of transforms are trees, the end-to-end process is a chain of lateen alternations that starts and ends with hard EM.
We will use a "grammar inductor" to represent subnetworks that transition from D l split to D l+1 split , by taking transformed parse trees of inter-punctuation fragments up to length l (base data set, D 0 ) to initialize training over fragments up to length l + 1:
The FJ network instantiates a grammar inductor with l = 14, thus training on inter-punctuation fragments up to length 15, as in previous work, starting from an empty set of counts, C = ∅. Smoothing causes initial parse trees to be chosen uniformly at random, as suggested by We diagrammed this system as not taking an input, since the first inductor's output is fully determined by unique parse trees of single-token strings. This iterative approach to optimization is akin to deterministic annealing, and is patterned after "baby steps" (Spitkovsky et al., 2009, §4.2).
Unlike the basic FJ, where symmetrization was a no-op (since there were no counts in C = ∅), IFJ makes use of symmetrizers -e.g., in the third inductor, whose input is based on strings with up to two tokens. Although it should be easy to learn words that go together from very short fragments, extracting correct polarities of their relations could be a challenge: to a large extent, outputs of early inductors maybe artifacts of how our generative models factor (see §4.2) or how ties are broken in optimization (. We therefore expect symmetrization to be crucial in earlier stages but to weaken any high quality grammars, nearer the end; it will be up to combiners to handle such phase transitions correctly (or gracefully).
section: Grounded Iterated Fork/Join (GIFJ)
So far, our networks have been either purely iterative (IFJ) or static (FJ). These two approaches can also be combined, by injecting FJ's solutions into IFJ's more dynamic stream. Our new transition subnetwork will join outputs of grammar inductors that either (i) continue a previous solution (as in IFJ); or (ii) start over from scratch ("grounding" to an FJ): H L·DBM
The full GIFJ network can then be obtained by unrolling the above template from l = 14 back to one.
section: Performance of Basic Networks
We compared our three networks' performance on their final training sets, WSJ 15 split (see, which also tabulates results fora cleaner subset, WSJ 15 simp ). The first network starts from C = ∅, helping us establish several straw-man baselines. Its empty initializer corresponds to guessing (projective) parse trees uniformly at random, which has 21.4% accuracy and sentence string cross-entropy of 8.76bpt.
section: Fork/Join (FJ)
FJ's symmetrizer yields random parses of WSJ 14 split , which initialize training of DBM 0 . This baseline (B) lowers cross-entropy to 6.18bpt and scores 57.0%. FJ's filter starts from parse trees of WSJ 14 simp only, and trains up a full DBM. This choice makes a stronger baseline (A), with 5.89bpt cross-entropy, at 62.2%.
The join operator uses counts from A and B, C 1 and C 2 , to obtain parse trees whose own counts C ′ 1 and C ′ 2 initialize lexicalized training. From each C ′ i , an optimizer arrives at C * i . Grammars corresponding to these counts have higher cross-entropies, because of vastly larger vocabularies, but also better accuracies: 59.2 and 62.3%. Their mixture C + is a simple sum of counts in C * 1 and C * 2 : it is not expected to bean improvement but happens to be a good move, resulting in a grammar with higher accuracy (64.0%), though not better Viterbi cross-entropy (7.27 falls between 7.08 and 7.30bpt) than both sources. The combiner's third alternative, a locally optimal C * + , is then obtained by re-optimizing from C + . This solution performs slightly better (64.2%) and will be the local optimum returned by FJ's join operator, because it attains the lowest cross-entropy (7.04bpt).
section: Iterated Fork/Join (IFJ)
IFJ's iterative approach results in an improvement: 70.5% accuracy and 6.96bpt cross-entropy. To test how much of this performance could be obtained by a simpler iterated network, we experimented with ablated systems that don't fork or join, i.e., our classic "baby steps" schema (chaining together 15 optimizers), using both DBM and DBM 0 , with and without a transform in-between. However, all such "linear" networks scored well below 50%. We conclude from these results that an ability to branch out into different promising regions of a solution space, and to merge solutions of varying quality into better models, are important properties of FJ subnetworks.
section: Grounded Iterated Fork/Join (GIFJ)
Grounding improves GIFJ's performance further, to 71.4% accuracy and 6.92bpt cross-entropy. This result shows that fresh perspectives from optimizers that start over can make search efforts more fruitful.
section: Enhanced Subnetworks
Modularity and abstraction allow for compact representations of complex systems. Another key benefit is that individual components can be understood and improved in isolation, as we will demonstrate next.
section: An Iterative Combiner (IC)
Our basic combiner introduced a third option, C * + , into a pool of candidate solutions, {C * 1 , C * 2 }. This new entry may not be a simple mixture of the originals, because of non-linear effects from applying L to C * 1 + C * 2 , but could most likely still be improved. Rather than stop at C * + , when it is better than both originals, we could recombine it with a next best solution, continuing until no further improvement is made. Iterating can't harm a given combiner's crossentropy (e.g., it lowers FJ's from 7.04 to 7.00bpt), and its advantages can be realized more fully in the larger networks (albeit without any end-to-end guarantees): upgrading all 15 combiners in IFJ would improve performance (slightly) more than grounding (71.5 vs. 71.4%), and lower cross-entropy (from 6.96 to 6.93bpt). But this approach is still a bit timid.
A more greedy way is to proceed so long as C * + is not worse than both predecessors. We shall now state our most general iterative combiner (IC) algorithm: Start with a solution pool
) top and removing the worst of n + 1 candidates in the new set. Finally, if p = p ′ , return the best of the solutions in p; otherwise, repeat from p := p ′ . At n = 2, one could think of taking L(C * 1 + C * 2 ) as performing a kind of bisection search in some (strange) space. With these new and improved combiners, the IFJ network performs better: 71.9% (up from 70.5 -see), lowering cross-entropy (down from 6.96 to 6.93bpt). We propose a distinguished notation for the ICs:
section: A Grammar Transformer (GT)
The levels of our systems' performance at grammar induction thus far suggest that the space of possible networks (say, with up to k components) may itself be worth exploring more thoroughly. We leave this exercise to future work, ending with two relatively straight-forward extensions for grounded systems.
Our static bootstrapping mechanism ("ground" of GIFJ) can be improved by pretraining with simple sentences first -as in the curriculum for learning DBM-1 (Spitkovsky et al., 2012b, §7.1), but now with a variable length cut-off l (much lower than the original 45) -instead of starting from ∅ directly:
The output of this subnetwork can then be refined, by reconciling it with a previous dynamic solution. We perform a mini-join of anew ground's counts with Cl , using the filter transform (single steps of lexicalized Viterbi training on clean, simple data), ahead of the main join (over more training data):
This template can be unrolled, as before, to obtain our last network (GT), which achieves 72.9% accuracy and 6.83bpt cross-entropy (slightly less accurate with basic combiners, at 72.3% -see).
section: Full Training and System Combination
All systems that we described so far stop training at
split . We will use a two-stage adaptor network to transition their grammars to a full data set, D 45 :
The first stage exposes grammar inducers to longer inputs (inter-punctuation fragments with up to 45 tokens); the second stage, at last, reassembles text snippets into actual sentences (also up to l = 45). After full training, our IFJ and GT systems parse Section 23 of WSJ at 62.7 and 63.4% accuracy, better than the previous state-of-the-art (61.2% -see). To test the generalized IC algorithm, we merged our implementations of these three strong grammar induction pipelines into a combined system (CS). It scored highest: 64.4%.
section: CS
The quality of bracketings corresponding to (nontrivial) spans derived by heads of our dependency structures is competitive with the state-of-the-art in unsupervised constituent parsing. On the WSJ sentences up to length 40 in Section 23, CS attains similar F 1 -measure (54.2 vs. 54.6, with higher recall) to System DDA (@10) ( 53.1 (64.3) ( 53.3 (64.3) ( 53.3 (71.5) 55.7 (67.7) ( 57.0 (71.4) ( 58.4 59.1: Directed dependency accuracies (DDA) on Section 23 of WSJ (all sentences and up to length ten) for recent systems, our full networks (IFJ and GT), and threeway combination (CS) with the previous state-of-the-art.
PRLG), which is the strongest system of which we are aware (see). 9
section: Multi-Lingual Evaluation
Last, we checked how our algorithms generalize outside English WSJ, by testing in 23 more set-ups: all 2006/7 CoNLL test sets (, spanning 19 languages. Most recent work evaluates against this multi-lingual data, with the unrealistic assumption of part-of-speech tags. But since inducing high quality word clusters for many languages would be beyond the scope of our paper, here we too plugged in gold tags for word categories (instead of unsupervised tags, as in §3-8).
We compared to the two strongest systems we knew: 10 MZ (Mareček andŽabokrtsk´yandˇandŽabokrtsk´andŽabokrtsk´y, 2012) and SAJ (, which report average accuracies of 40.0 and 42.9% for CoNLL data (see). Our fully-trained IFJ and GT systems score 40.0 and 47.6%. As before, combining these networks with our own implementation of the best previous state-of-the-art system (SAJ) yields a further improvement, increasing final accuracy to 48.6%. These numbers differ from Ponvert et al.'s (2011) for the full Section 23 because we restricted their eval-ps.py script to a maximum length of 40 words, in our evaluation, to match other previous work: Golland et al.'s (2012,) for CCM and LLCCM;) for the rest. During review, another strong system (Mareček and Straka, 2013, scoring 48.7%) of possible interest to the reader came out, exploiting prior knowledge of stopping probabilities (estimated from large POS-tagged corpora, via reducibility principles).
section: System
F1 Binary-Branching Upper Bound 85.7 Left-Branching Baseline 12.0 CCM (: Harmonic mean (F 1 ) of precision (P) and recall (R) for unlabeled constituent bracketings on Section 23 of WSJ (sentences up to length 40) for our combined system (CS), recent state-of-the-art and the baselines.
section: Discussion
CoNLL training sets were intended for comparing supervised systems, and aren't all suitable for unsupervised learning: 12 languages have under 10,000 sentences (with Arabic, Basque, Danish, Greek, Italian, Slovenian, Spanish and Turkish particularly small), compared to WSJ's nearly 50,000. In some treebanks sentences are very short (e.g., Chinese and Japanese, which appear to have been split on punctuation), and in others extremely long (e.g., Arabic). Even gold tags aren't always helpful, as their number is rarely ideal for grammar induction (e.g., 42 vs. 200 for English). These factors contribute to high variances of our (and previous) results (see). Nevertheless, if we look at the more stable average accuracies, we see a positive trend as we move from a simpler fully-trained system (IFJ, 40.0%), to a more complex system (GT, 47.6%), to system combination (CS, 48.6%). Grounding seems to be more important for the CoNLL sets, possibly because of data sparsity or availability of gold tags.
section: Related Work
The surest way to avoid local optima is to craft an objective that doesn't have them. For example, demonstrated a convex training method for semi-supervised dependency parsing; introduced a convex reformulation of likelihood functions for clustering tasks; and Corlett and Penn (2010) designed  a search algorithm for encoding decipherment problems that guarantees to quickly converge on optimal solutions. Convexity can be ideal for comparative analyses, by eliminating dependence on initial conditions. But for many NLP tasks, including grammar induction, the most relevant known objective functions are still riddled with local optima. Renewed efforts to find exact solutions) maybe a good fit for the smaller and simpler, earlier stages of our iterative networks.
Multi-start methods can recover certain global extrema almost surely (i.e., with probability approaching one). Moreover, random restarts via uniform probability measures can be optimal, in a worst-case-analysis sense, with parallel processing sometimes leading to exponential speed-ups (. This approach is rarely emphasized in NLP literature. For instance, demonstrated consistent, substantial gains from random restarts in statistical machine translation (but also suggested better and faster replacements -see below); §5,) found random restarts for EM to be crucial in parts-of-speech disambiguation. However, other reviews are few and generally negative. Iterated local search methods (, inter alia) escape local basins of attraction by perturbing candidate solutions, without undoing all previous work. "Largestep" moves can come from jittering, dithering, Ch. 2) or smoothing (. Nonimproving "sideways" moves offer substantial help with hard satisfiability problems; and injecting non-random noise, by introducing "uphill" moves via mixtures of random walks and greedy search strategies, does better than random noise alone or simulated annealing. In NLP, random walks from previous local optima were faster than uniform sampling and also increased BLEU scores; showed that local search can outperform greedy solutions for document clustering and chat disentanglement tasks; and incorporated tabu search 3) into HMM training for ASR.
Genetic algorithms area fusion of what's best in local search and multi-start methods, exploiting a problem's structure to combine valid parts of any partial solutions. Evolutionary heuristics proved useful in the induction of phonotactics), text planning (, factored modeling of morphologically-rich languages () and plot induction for story generation. Multi-objective genetic algorithms) can handle problems with equally important but conflicting criteria, using Pareto-optimal ensembles. They are especially well-suited to language, which evolves under pressures from competing (e.g., speaker, listener and learner) constraints, and have been used to model configurations of vowels and tone systems (. Our transform and join mechanisms also exhibit some features of genetic search, and make use of competing objec-tives: good sets of parse trees must make sense both lexicalized and with word categories, to rich and impoverished models of grammar, and for both long, complex sentences and short, simple text fragments.
This selection of text filters is a specialized case of more general "data perturbation" techniqueseven cycling over randomly chosen mini-batches that partition a data set helps avoid some local optima (. suggested how example-reweighing could cause "informed" changes, rather than arbitrary damage, to a hypothesis. Their (adversarial) training scheme guided learning toward improved generalizations, robust against input fluctuations. Language learning has a rich history of reweighing data via (cooperative) "starting small" strategies, beginning from simpler or more certain cases. This family of techniques has met with success in semisupervised named entity classification, parts-of-speech induction, and language modeling, in addition to unsupervised parsing ().
section: Conclusion
We proposed several simple algorithms for combining grammars and showed their usefulness in merging the outputs of iterative and static grammar induction systems. Unlike conventional system combination methods, e.g., in machine translation (), ours do not require incoming models to be of similar quality to make improvements. We exploited these properties of the combiners to reconcile grammars induced by different views of data. One such view retains just the simple sentences, making it easier to recognize root words. Another splits text into many inter-punctuation fragments, helping learn word associations. The induced dependency trees can themselves also be viewed not only as directed structures but also as skeleton parses, facilitating the recovery of correct polarities for unlabeled dependency arcs.
By reusing templates, as in dynamic Bayesian network (DBN) frameworks (Koller and Friedman, The so-called Yarowsky-cautious modification of the original algorithm for unsupervised word-sense disambiguation. 2009, §6.2.2), we managed to specify relatively "deep" learning architectures without sacrificing (too much) clarity or simplicity. On a still more speculative note, we see two (admittedly, tenuous) connections to human cognition. First, the benefits of not normalizing probabilities, when symmetrizing, might be related to human language processing through the base-rate fallacy and the availability heuristic, since people are notoriously bad at probability. And second, intermittent "unlearning" -though perhaps not of the kind that takes place inside of our transformsis an adaptation that can be essential to cognitive development in general, as evidenced by neuronal pruning in mammals). "Forgetful EM" strategies that reset subsets of parameters may thus, possibly, be no less relevant to unsupervised learning than is "partial EM," which only suppresses updates, other EM variants, or "dropout training", which is important in supervised settings.
Future parsing models, in grammar induction, may benefit by modeling head-dependent relations separately from direction. As frequently employed in tasks like semantic role labeling) and relation extraction), it maybe easier to first establish existence, before trying to understand its nature. Other key next steps may include exploring more intelligent ways of combining systems and automating the operator discovery process. Furthermore, we are optimistic that both count transforms and model recombination could be usefully incorporated into sampling methods: although symmetrized models may have higher cross-entropies, hence prone to rejection in vanilla MCMC, they could work well as seeds in multi-chain designs; existing algorithms, such as MCMCMC, which switch contents of adjacent chains running at different temperatures, may also benefit from introducing the option to combine solutions, in addition to just swapping them.
