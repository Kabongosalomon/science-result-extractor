section: title
Semantic Relation Classification via Hierarchical Recurrent Neural Network with Attention
section: abstract
Semantic relation classification remains a challenge in natural language processing. In this paper , we introduce a hierarchical recurrent neural network that is capable of extracting information from raw sentences for relation classification. Our model has several distinctive features: (1) Each sentence is divided into three context subsequences according to two annotated nom-inals, which allows the model to encode each context subsequence independently so as to selectively focus as on the important context information; (2) The hierarchical model consists of two recurrent neural networks (RNNs): the first one learns context representations of the three context subsequences respectively, and the second one computes semantic composition of these three representations and produces a sentence representation for the relationship classification of the two nominals. (3) The attention mechanism is adopted in both RNNs to encourage the model to concentrate on the important information when learning the sentence representations. Experimental results on the SemEval-2010 Task 8 dataset demonstrate that our model is comparable to the state-of-the-art without using any hand-crafted features.
section: Introduction
Semantic relation classification is an important task in natural language processing, which has attracted great attention in recent years. The goal is to identify the semantic relationship between a pair of nominals marked in a sentence. For instance, in the sentence "The software e 1 addressed the problem with thee 2 of a fix on Saturday", the marked nominals of company and publication are of relationship Product-Producer(e 2 , e 1 ). Most conventional models focus on machine learning and feature design, which have been shown to obtain performance improvements.
Recently, neural network approaches have been widely used for relation classification, which aim at reducing the need of hand-crafted features. These approaches are broadly divided into two categories: one explores the effectiveness of using dependency paths and its attached subtrees between two nominals, and various neural networks are adopted to model the shortest dependency paths and dependency subtrees (;; the other exploits deep neural networks to learn syntactic and semantic features from raw sentences (, which has been proved effective, but inevitably suffers from irrelevant parts. Our paper introduces an attentive neural network that selectively focuses on useful information on raw sentences.
Context information of the annotated nominals has been widely believed to be useful for relation classification (. In this work, we further explore the effectiveness of context information around the annotated nominals in a sentence. In our model, a sentence with two marked nominals is divided into three context subsequences according to two marked nominals: the left context subsequence, the middle context subsequence and the right context subsequence. This method is similar to and Thang, which have showed that contextual information is effectively obtained by deep learning techniques. Instead of combining the middle context subsequence with the left and right context subsequences, respectively, as in, we propose to learn context representations via recurrent neural networks that work on each context subsequence independently. For example, the sentence "The software e 1 addressed the problem with thee 2 of a fix on Saturday" is split into three subsequences: " The software", "addressed the problem with the" and "of a fix on Saturday". And the marked nominals of e 1 and [publication] e 2 are not included in any context subsequence. As a result, the sentence is divided into five parts: three context subsequences and two annotated nominals. Our sentence representations are leant hierarchically from context subsequences to sentences using a hierarchical recurrent neural network, which firstly learns the context representation of each context subsequence independently, and then encodes the semantics of context subsequences into a sentence representation for the relation classification. Furthermore, we introduce the attention mechanism () that encourages the model to focus on the important information. Experimental results demonstrate that our model is comparable to the state-of-the-art with a single model that works on the raw sentences.
In the rest of this paper, we review recurrent neural networks in Section 2. We provide details about our model in Section 3. Section 4 presents our experiments and their results. Finally, we make a conclusion in Section 5.
section: Recurrent Neural Networks
Recurrent neural networks (RNNs)) project a sequence of inputs x 1 , . . . , x T to a sequence of outputs y 1 , . . . , y T via an affine transformation followed by a non-linear function. At timestep t, a standard RNN computes the new hidden vector as
where Wis trained matrix transforming the current input x t into the current state linearly, U is also trained matrix connecting the previous state h t−1 with the current state, and b is a bias term, and f is a non-linear function (e.g., tanh). However, RNNs with the above form may suffer from gradient exploding or vanishing problem) during training when it is trained with the backpropagation through time algorithm. To address this problem, long short-term memory network (LSTM) was proposed in where the architecture of a standard RNN was modified to avoid vanishing or exploding gradients. Many LSTM variants have been proposed, and here we adopt the version of.
The LSTM model comprises a memory cell that can store information over along period of time, and three gates that allow it to control the flow of information into and out of the cell: input gate, forget gate, and output gate. Concretely, the LSTM unit at time step t encompasses a collection of vectors: an input gate it , a forget gate ft , an output gate o t , a memory cell ct , and a hidden state ht . The unit accepts an input vector x t , the previous hidden state h t−1 , and the memory cell c t−1 and computes the new vectors using the following equations:
where σ denotes the element-wise application of the logistic function, denotes the element-wise multiplication of two vectors, W and U are weight matrices, and bare bias vectors.
...
...
section: Model
In this section, we introduce the proposed neural model that learns distributed representations from raw sentences. These representations serving as features are further used for relation classification. An overview of our model is shown in.
Given a sentence with two annotated nominals, the sentence is firstly divided into five parts (three context subsequences and two annotated nominals) based on the two marked nominals (Section 3.1). Next, the model computes the distributed representations for the context subsequences using a bidirectional LSTM that works on word vectors (Section 3.2). Lastly, these distributed context representations are further encoded into a sentence representation via a bidirectional RNN (Section 3.3). Furthermore, we extend this model with a neural attention that encourages the model to focus on important information.
section: Context Subsequences
In most cases different contexts have different functions for the meaning of sentences. Some recent work fell into the idea that the middle context contains the most relevant information for relation classification, combining the middle context with the left and right context respectively (). We instead model each context part independently, which allows the model to automatically identify contexts that contain useful information. Given a sentence sand its annotated nominals e 1 and e 2 , the sentence first is split into five parts according to the two annotated nominals: the left context subsequence, entity e 1 , the middle context subsequence, entity e 2 and the right context subsequence. Preprocessing the sentence in such away allows the model to encode each context subsequence independently.
section: Context Subsequence Composition
section: Word Encoder
A bidirectional LSTM (Bi-LSTM) () is applied to independently encoding each of the three context subsequences. A bidirectional LSTM consists of two LSTMs: the forward and backward LSTMs. They are run in parallel: the forward LSTM inputs the words from x 1 to x T , and the backward LSTM inputs in an reverse order from x T back to x 1 . At time step t, we obtain the hidden state (denoted ash t ) of the bidirectional LSTM by concatenating the forward hidden state (denoted as − → ht ) and the backward one (denoted as
. Bi-LSTM can summarize the information from the whole context subsequence centered around words, which let the model understand the meaning of words comprehensively. Given a sentence s divided into the left context subsequence c 1 , the middle context subsequence c 2 and the right context subsequence c 3 , we assume that the sentence s contains L words and the context subsequence c i has Ti words, where i ∈. The input to Bi-LSTM is a context subsequence c i : [x i1 , . . . , x iT i ] where x it is the word vector for word wit . At time step t, the encoder produces a hidden state hit which gathers the information of the whole context subsequence c i centered around wit . The equations are following:
where concat is concatenation function, i.e.,
. Note that our model encodes the left, middle and right context subsequence independently but with one Bi-LSTM.
section: Word-level Attention
Due to the fact that raw sentences contain more information than the shortest dependency paths, there maybe some irrelevant information in raw sentences. For concentrating on these words that are important to predict the relationship of entities, it can be a good strategy to pay more attention on these words. To encourage such behavior, this paper introduces a word-level attention mechanism. The attention mechanism enables the model to differently attend over the hidden vectors of Bi-LSTM along a context subsequence, and produces a weighted representation mi of them as follows:
where W (w) and W (c) are weight matrices, b is a bias vector, v z is a weight vector and v z is its transformation, and r i is an external context vector that is randomly initialized and jointly optimized during training.
The attention representation zit corresponding to the t-th word wit in the context subsequence c i is computed via a non-linear combination of the hidden state hit and the external context vector r i . The attention weight α it for the t-th word wit in the context subsequence c i is a probability that is the normalized weight of zit (parameterized by v z ) through a softmax layer, reflecting the importance of the t-th word wit with respect to the meaning of the context subsequence c i in classifying the relationship of two entities. The external context vector r i not only represents the high-level meaning of the context subsequence c i , but also allows the model to identify that the word wit is in the context subsequence c i .
section: Sentence Composition
After establishing an attention-based Bi-LSTM (Section 3.2) to capture the meaning of three context subsequences, resulting in three context representations, there is one difficulty that how to further obtain the semantic composition of these context representations plus two representations of marked nominals. Note that there are five semantic representations. The most common approach is that a multilayer perceptron (MLP) is adopted to take these representations as input and compute semantic compositionality for them. In this work, we adopt a Bi-RNN to integrate syntactics and semantics of three context subsequences and two annotated nominals into sentence representation s, which is further fed into a classifier for relation classification. We propose to learn sentence representations via Bi-RNNs for two reasons: (1) a sentence containing two annotated nominals divided into three context subsequences that are ordered as in the sentence, can be treated as a short sequence that consists of five tokens; (2) recurrent neural networks are competent enough to model the semantics of these context subsequences and their inherent relations, which is important to obtain the semantic meaning of the sentence. The experimental results demonstrate that Bi-RNNs significantly outperform MLP.
Let Y be a matrix containing five column vectors [m 1 , me 1 , m 2 , me 2 , m 3 ], where mi (i ∈) is the representation of the context subsequence c i , and me 1 and me 2 are the representations of annotated nominals e 1 and e 2 . 2 To obtain compositional vector representations for sentences, we iterate the following sequence of equations:
where y j ∈ Y (j ∈) is the j-th column vector in Y.
Note that the sentence only contains five elements, our model do not make any assumptions about the type of RNNs used in this subsection. But as far as comparison goes, LSTMs performs better than the standard RNNs.
To selectively focus on the important context subsequences, it is an alternative solution to applying neural attention to the hidden vectors of the above Bi-RNNs, similar to Subsection 3.2.2. We also make further extensions such as average pooling and max pooling.
section: Training
A fully connected softmax layer is used as classifier for classification. It produces the probability distribution p over relation types conditioned on the sentence representation s:
The training objective is to minimize the cross-entropy error between the ground truth and predicted label. The parameters of our model are optimized using AdaGrad (Duchi et al., 2011) with a learning rate of 0.01, a mini-batch size of 5 and a L 2 regularization coefficient of 10 −6 . The details are described further in Section 4.2.
section: Experiments and Evaluation
section: Dataset
In our experiments, we evaluate our model on the SemEval-2010 Task 8 dataset (, which is one of the most widely used benchmarks for relation classification. The dataset contains 10,717 annotated sentences divided into 8,000 sentences for training and 2717 for testing. Each sentence is annotated with each of nine different relationship and an artificial relation Other, and each relationship has two direction except for the undirected relation Other. The nine directed relations are Cause-Effect, Instrument-Agency, Product-Producer, Content-Container, Entity-Origin, EntityDestination, Component-Whole, Member-Collection, and Message-Topic.
The official evaluation metric is the macro-averaged F1-score (excluding Other), and takes into consideration the directionality. We use the official scorer to test the model performance.
section: Implementation
We tune the hyperparameters for our model using 5-fold cross-validation. We pretrain 200-dimensional word embeddings using word2vec () on the English Wikipedia corpus, and randomly initialize other hyperparameters. We set the LSTM dimension to be 200. We apply dropout only on the word embeddings and outputs of LSTM as in (, and the dropout rate is 0.2.
To enable a direct comparison with the previous work, we use the same features: position features, WordNet hypernyms and NER. WordNet hypernyms and NER were obtained using the tool of Ciaramita and Al-tun (2006).
section: Results
section: Model features F1
Bi    Here we apply neural attention to the hidden states of RNNs (Bi-LSTM and Bi-RNN). To ensure the number of parameters comparable, we adopt a two-layer full-connected neural network with the hidden size of 600 dimension and a non-linear function of tanh to serve as MLP. And the hidden size of the standard RNN is 350-dimensional. From, we find that both the combinations of Bi-LSTM with Bi-RNN and Bi-LSTM outperform the combination of Bi-LSTM and MLP without any features. In particular, the combination of Bi-LSTM and Bi-LSTM achieves the best result 83.90% without any feature, and its F1-score is about 1.5% higher than the model of Bi-LSTM+MLP. The results indicate that the neural architecture of two Bi-LSTMs effectively captures semantic meanings of these context subsequences and their inherent relations, and obtains more robust sentence representations for relation classification. In this paper, we tackle the relation classification task using the combination of two BiLSTMs.
The comparison of different methods shows experiments for our model with various methods for the hidden vectors of Bi-LSTMs. We begin with the model using the concatenation of the final state of forward and backward LSTMs. And then we replace concatenation operation with average pooling, max-pooling and neural attention respectively. Not surprisingly, processing the hidden vectors of Bi-LSTMs via neural attention achieves the best result, which gives an improvement of 2.23 percentage points in F1-score over max-pooling. We suspect that this is due to the attention model being run in a more focused way that makes it easier to capture large important information from contexts. We also consider the impact of features for these methods. Results in show that by adding features the F1-scores of all methods improve, which hints that three features are useful for relation classification.: Experimental results of our model against other models.. And they also propose connectionist bi-directional recurrent neural networks (R-RNN) that adds a connection to the hidden states of bi-directional recurrent neural networks. We observe in that our model is comparable to the state-of-the-art (previous best result is 84.0% obtained by depLCNN + NS) without any features, whereas depLCNN works on the shortest dependency
section: Comparison with State-of-the-art Models
section: Model
Feature Set F1 84.9
Our model -84.1 +position features 84.5: Comparison of ranking models (no lexical features).
paths, which consist of most relevant information and avoid negative effect from irrelevant parts in the sentences. This result suggests that our model automatically focuses on important information related to determining the relationship of two entities. The F1-score is improved by adding three features but not as obvious as in (). We argue that this is due to BiLSTMs being able to learn position information on sequences and lexical features leading to overfitting as in ().
section: Comparison of ranking models
For fair comparison, we also replace the softmax layer with a ranking layer to train our model, as proposed in. We use training settings following Thang. More details about ranking layer are described in.
From, we observe that our model outperform the state-of-the-art without any feature, whereas previous work's best reported performance is 83.9% in ER-CNN using word embeddings of size 400. Combining ER-CNN and R-RNN using a voting scheme achieves a state-of-the-art result of 84.9 in F1-score, which is presented by). But our model reaches anew state-of-the-art result with a single model when position features are added, and outperforms the model of ER-CNN that learns context representations for two contexts of the combinations of the middle context with the left and right context respectively.
section: Conclusion
In this work, we introduce a hierarchical recurrent neural network model that learns useful features from raw sentences for relation classification. We further extend the model with neural attention at two different levels that provides significant improvements over the concatenation, average pooling and max-pooling. Our model shows comparable performance to the state-of-the-art on the SemEval-2010 Task 8 dataset without using any costly hand-crafted features. In addition, the models presented here are general hierarchical models, and are therefore suitable for hierarchical structures, such as paragraphs and documents.
