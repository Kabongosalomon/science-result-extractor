section: title
Exploring Machine Reading Comprehension with Explicit Knowledge
section: abstract
To apply general knowledge to machine reading comprehension (MRC), we propose an innovative MRC approach, which consists of a WordNet-based data enrichment method and an MRC model named as Knowledge Aided Reader (KAR). The data enrichment method uses the semantic relations of WordNet to extract semantic level inter-word connections from each passage-question pair in the MRC dataset, and allows us to control the amount of the extraction results by setting a hyper-parameter. KAR uses the extraction results of the data enrichment method as explicit knowledge to assist the prediction of answer spans. According to the experimental results, the single model of KAR achieves an Exact Match (EM) of 72.4 and an F1 Score of 81.1 on the development set of SQuAD, and more importantly , by applying different settings in the data enrichment method to change the amount of the extraction results, there is a 2% variation in the resulting performance of KAR, which implies that the explicit knowledge provided by the data enrichment method plays an effective role in the training of KAR.
section: Introduction
Machine reading comprehension (MRC) is a challenging task in artificial intelligence. As the name suggests, MRC requires a machine to read a passage and answer a relevant question. Since the answer to each question is supposed to stem from the corresponding passage, a common solution for MRC is to train an MRC model that predicts for each given passage-question pair an answer span (i.e. the answer start position and the answer end position) in the passage. To encourage the exploration of MRC models, many MRC datasets have been published, such as SQuAD ( and MS-MARCO (. In this paper, we focus on SQuAD. A lot of MRC models have been proposed for the challenge of SQuAD. Although the top models on the leader-board have achieved almost the same performance as human beings, we are firmly convinced that the way human beings conduct reading comprehension is still worth studying for us to make further innovations in MRC. Therefore, let us briefly review human reading comprehension before diving into MRC. Given a passage and a relevant question, we may wish to match the passage words with the question words, so that we could find the answer around the matched passage words. However, due to the complexity and diversity of natural languages, this naive method is often useless in practice. Instead, we must rely on our reasoning skills to deal with reading comprehension, which makes it necessary for us to obtain enough inter-word connections from each given passage-question pair. Inter-word connections have a wide coverage, they exist not only on the syntactic level (e.g. dependency), but also on the semantic level (e.g. synonymy). The examples provided in demonstrate how human reading comprehension could benefit from semantic level inter-word connections. By roughly analyzing the MRC models proposed for SQuAD, we find that leveraging neural attention mechanisms ( ) based on recurrent neural networks, such as LSTM) and GRU ( , is currently the dominant approach. Since neural network models are usually deemed as simulations of human brains, we may as well interpret the training of an MRC model as a process of teaching knowledge to it, where the knowledge comes from the training samples, and thus can be absorbed into the model parameters through gradient descent. However, neural network models are also known as black boxes, that is to say, by just updating model parameters according to training samples, we cannot understand the meaning of the knowledge taught to an
section: Passage
section: Question
Answer Teachers may use a lesson plan to facilitate student learning, providing a course of study which is called the curriculum.
What can a teacher use to help students learn? lesson plan
Manufacturing accounts fora significant but declining share of employment, although the city's garment industry is showing a resurgence in Brooklyn.
In what borough is the garment business prominent?
Brooklyn: Two examples about the effects of semantic level inter-word connections on human reading comprehension. In the first example, we can find the answer because we know "facilitate" and "help" are synonyms.
Similarly, in the second example, we can find the answer because we know "borough" is a hypernym of "Brooklyn", or "Brooklyn" is a hyponym of "borough". Both of the two examples are selected from SQuAD.
MRC model, neither can we control the amount of the knowledge taught to it, therefore we name such knowledge as implicit knowledge. So far, human beings have accumulated a tremendous amount of general knowledge. These general knowledge, despite being an essential component of human intelligence, has never been effectively applied to MRC, which we believe is the biggest gap between MRC and human reading comprehension. We intend to bridge this gap with the help of knowledge bases, which store general knowledge in structured forms. In recent years, many knowledge bases have been established, such as WordNet and Freebase, and they have made it convenient for machines to access and process the general knowledge of human beings. Therefore, it is both meaningful and feasible to integrate the general knowledge in a knowledge base with the training of an MRC model. However, rather than leveraging knowledge base embeddings, we would prefer our MRC model to use general knowledge in an understandable and controllable way, and we name the general knowledge used in this way as explicit knowledge. In this paper, by using WordNet as our knowledge base, we propose an innovative MRC approach, which consists of two components: a WordNetbased data enrichment method, which uses WordNet to extract semantic level inter-word connections from each passage-question pair in the MRC dataset, and an MRC model named as Knowledge Aided Reader (KAR), which uses the extraction results of the data enrichment method as explicit knowledge to assist the prediction of answer spans. There are two important features in our MRC approach: on the one hand, the data enrichment method allows us to control the amount of the extraction results; on the other hand, this amount in turn affects the performance of KAR. According to the experimental results, by applying different settings in the data enrichment method to change the amount of the extraction results, there is a 2% variation in the resulting performance of KAR, which implies that the explicit knowledge provided by the data enrichment method plays an effective role in the training of KAR.
section: Task Description
The MRC task considered in this paper is defined as the following prediction problem: given a passage P := [p 1 , . . . , p n ], which is a sequence of n words, and a relevant question Q := [q 1 , . . . , q m ], which is a sequence of m words, predict an answer start position a sand an answer end position a e , where 1 ≤ a s ≤ a e ≤ n, so that the fragment [p as , . . . , p ae ] in P is the answer to Q.
section: WordNet-based Data Enrichment
To provide our MRC model with explicit knowledge, we would like to enrich the content of the MRC dataset by extracting semantic level interword connections from each passage-question pair in it, therefore we propose a WordNet-based data enrichment method.
section: What and how to extract from each passage-question pair
WordNet is a lexical database for English. Words in WordNet are organized into synsets, which in turn are related to each other through semantic relations, such as "hypernym" and "hyponym".
In our data enrichment method, we use the semantic relations of WordNet to extract semantic level inter-word connections from each passagequestion pair in the MRC dataset. Considering the requirements of our MRC model, we need to represent the extraction results as positional information. Specifically, for each word win a passagequestion pair, we need to obtain a set Z w , which contains the positions of the passage words that w is semantically connected to. Besides, when w itself is a passage word, we also need to ensure that its position is excluded from Z w .
The key problem to obtain the above extraction results is to determine if a subject word is semantically connected to an object word. To solve this problem, we introduce two concepts: the directlyinvolved synsets and indirectly-involved synsets of a word. Given a word w, its directly-involved synsets Φ w represents the synsets that w belongs to, and its indirectly-involved synsets Φ w represents the synsets that the synsets in Φ ware related to through semantic relations. Based on the two concepts, we propose the following hypothesis: given a subject word w sand an object word w o , w sis semantically connected tow o if and only if (Φ ws ∪ Φ ws ) ∩ Φ wo = ∅. According to the hypothesis, Algorithm 1 describes the process of extracting semantic level inter-word connections from each passage-question pair.
section: How to obtain the indirectly-involved synsets of each word
The above hypothesis and process can work only if we know how to obtain the directly-involved synsets and indirectly-involved synsets of each word. Given a word w, we can easily obtain its directly-involved synsets Φ w from WordNet, but obtaining its indirectly-involved synsets Φ w is much more complicated, because in WordNet, the way synsets are related to each other is flexible and extensible. In some cases, a synset is related to another synset through a single semantic relation. For example, the synset "cold.a.01" is related to the synset "temperature.n.01" through the semantic relation "attribute". However, in more cases, a synset is related to another synset through a semantic relation chain. For example, first the synset "keratin.n.01" is related to the synset "feather.n.01" through the semantic relation "substance holonym", then the synset "feather.n.01" is related to the synset "bird.n.01" through the semantic relation "part holonym", and finally the synset "bird.n.01" is related to the synset "parrot.n.01" through the semantic relation "hyponym", thus we can say that the synset "keratin.n.01" is related to the synset "parrot.n.01" through the semantic relation chain "substance holonym → part holonym → hyponym". We name each semantic relation in a semantic relation chain as a hop, so that a semantic relation chain having k semantic relations is a k-hop semantic relation chain. Besides, each single semantic relation is a 1-hop semantic relation chain. Let us use Γ := {γ 1 , γ 2 , . . .} to represent the semantic relations of WordNet, and use Ω γ i φ to represent the synsets that a synset φ is related to through a single semantic relation γ i ∈ Γ. Since Ω γ i φ is easy to obtain from WordNet, we can further obtain the synsets that φ is related to through 1-hop semantic relation chains:
and by induction, the synsets that φ is related to through k-hop semantic relation chains:
. In theory, if we do not limit the hop counts of semantic relation chains, φ can be related to all other synsets in WordNet, which is meaningless in many cases. Therefore, we use a hyper-parameter χ ∈ N to represent the maximum hop count of semantic relation chains, and only consider the semantic relation chains that have no more than χ hops. Based on the above descriptions, given a word wand its directly-involved synsets Φ w , we can obtain its indirectly-involved synsets:
section: About controlling the amount of the extraction results
The hyper-parameter χ is crucial in controlling the amount of the extraction results. When we set χ to 0, the indirectly-involved synsets of each word contains no synset, so that semantic level interword connections only exist between synonyms.
As we increase χ, the indirectly-involved synsets of each word usually contains more synsets, so that semantic level inter-word connections are likely to exist between more words. As a result, by increasing χ within a certain range, we can extract more semantic level inter-word connections from the MRC dataset, and thus provide our MRC model with more explicit knowledge. However, due to the limitations of WordNet, only apart of the extraction results are useful explicit knowledge, while the rest are useless for the prediction of answer spans. According to our observation, the proportion of the useless explicit knowledge Algorithm 1 Extract semantic level inter-word connections from each passage-question pair procedure EXTRACT(P, Q) Given a passage P and a relevant question Q for pi in P do For each passage word
Obtain the extraction results Z q i end for end procedure
Return the extraction results on P and Q increases as χ gets larger. Therefore, there exists an optimal setting for χ, which can result in the best performance of our MRC model.
section: Knowledge Aided Reader
As depicted in, our MRC model, Knowledge Aided Reader (KAR), consists of five layers: given a passage-question pair, the lexical embedding layer encodes the lexical features of each word to generate the passage lexical embeddings and the question lexical embeddings; based on the lexical embeddings, the contextual embedding layer encodes the contextual clues about each word to generate the passage contextual embeddings and the question contextual embeddings; based on the contextual embeddings, the memory generation layer performs passage-to-question attention and question-to-passage attention to generate the preliminary memories over the passagequestion pair; based on the preliminary memories, the memory refining layer performs self-matching attention to generate the refined memories over the passage-question pair; based on the refined memories and the question contextual embeddings, the answer span prediction layer generates the answer start position distribution and the answer end position distribution. KAR is quite different from the existing MRC models in that it uses the semantic level inter-word connections, which are preextracted from the MRC dataset by the WordNetbased data enrichment method, as explicit knowledge to assist the prediction of answer spans. On the one hand, the memory generation layer uses the explicit knowledge to assist the passage-toquestion attention and the question-to-passage attention. On the other hand, the memory refining layer uses the explicit knowledge to assist the selfmatching attention. Besides, to better utilize the explicit knowledge, the lexical embedding layer encodes dependency and synonymy information into the lexical embedding of each word.
section: Lexical Embedding Layer
For each word, the lexical embedding layer generates its lexical embedding by merging the following four basic embeddings:
1. Word-level Embedding. We define our vocabulary as the intersection between the words in all training samples and those in the pre-trained 300-dimensional GloVe (). Given a word w, if it is in the vocabulary, we set its word-level embedding α w to its GloVe word vector, which is fixed during the training, otherwise we have α w = α o ∈ R 300 , where α o is a trainable parameter serving as the shared word vector of all out-of-vocabulary (OOV) words.
2. Character-level Embedding. We represent each character as a separate 150-dimensional vector, which is a trainable parameter. Given a word w consisting of a sequence of k characters, whose vectors are represented as U β ∈ R 150×k , we use a bidirectional FOFE () to process U β , concatenate the forward FOFE output (R 150×k ) and the backward FOFE output (R 150×k ) across rows to obtain F β ∈ R 300×k , and perform self attention on F β to obtain the character-level embedding of w:
where W β and v β are trainable parameters. Applying character-level embedding is helpful in representing OOV words.
3. Dependency Embedding. Inspired by, we use a dependency parser to obtain the dependent words of each word. Given a word w having k dependent words, whose wordlevel embeddings are represented as U η ∈ R 300×k , we perform self attention on U η to obtain the dependency embedding of w: where W η and v η are trainable parameters. By applying dependency embedding, we make use of syntactic level inter-word connections, which serve as a supplement to the pre-extracted semantic level inter-word connections. 4. Synonymy Embedding. In the scope of the vocabulary, we use WordNet to obtain the synonyms of each word. Given a word w having k synonyms, whose word-level embeddings are represented as U µ ∈ R 300×k , we perform self attention on U µ to obtain the synonymy embedding of w:
where W µ and v µ are trainable parameters. By applying synonymy embedding, we improve the vector-space similarity between synonyms, and thus promote the effects of the pre-extracted semantic level inter-word connections. Based on the above descriptions, given a word w, we obtain α w , β w , η w , and µ w , and concatenate them across rows to obtain π w ∈ R 1200 . In this way, for all passage words, we obtain Π P = [π p 1 , . . . , π pn ] ∈ R 1200×n , and for all question words, we obtain Π Q = [π q 1 , . . . , π qm ] ∈ R 1200×m . We put Π P through a 1-layer highway network () to obtain the passage lexical embeddings:
, and put Π Q through the same highway network to obtain the question lexical embeddings:
section: Contextual Embedding Layer
For each word, the contextual embedding layer fuses its lexical embedding with those of its surrounding words to generate its contextual embedding. Specifically, we use a bidirectional LSTM (BiLSTM), whose hidden state size is d, to process LP and L Q separately. For LP , we concatenate the forward LSTM output (R d×n ) and the backward LSTM output (R d×n ) across rows to obtain the passage contextual embeddings:
For L Q , we concatenate the forward LSTM output (R d×m ) and the backward LSTM output (R d×m ) across rows to obtain the question contextual embeddings:
section: Memory Generation Layer
For each passage word, the memory generation layer fuses its contextual embedding with both the passage contextual embeddings and the question contextual embeddings to generate its preliminary memory over the passage-question pair. Specifically, the task of this layer is decomposed into the following four steps:
1. Generating enhanced contextual embeddings. We enhance the contextual embedding of each word according to the pre-extracted semantic level inter-word connections. Given a word w, whose contextual embedding is cw ∈ R 2d , suppose we have obtained Z w through Algorithm 1, then we gather the columns in C P whose positions are contained in Z w , represent these columns as U τ ∈ R 2d×|Zw| , and perform attention on U τ to obtain the cw -attended contextual embedding:
where W τ and v τ are trainable parameters, and x X represents concatenating a vector x with each column in a matrix X across rows. Based on the above descriptions, given a word w, we concatenate cw and τ w across rows to obtain λ w ∈ R 4d . In this way, for all passage words, we ob- to obtain each element in A:
where v A ∈ R 12d is a trainable parameter, 1 represents concatenation across rows, and represents element-wise multiplication. Since the enhanced contextual embeddings are generated according to the pre-extracted semantic level inter-word connections, the alignment matrix A is named as knowledge aided alignment matrix.
3. Performing passage-to-question attention and question-to-passage attention.
On the one hand, following, we perform passage-to-question attention to obtain the passage-attended question representations:
where softmax r (X) represents normalizing each row in a matrix X by softmax. On the other hand, following, we perform question-to-passage attention to obtain the question-attended passage representations:
where softmax c (X) represents normalizing each column in a matrix X by softmax. 4. Generating preliminary memories. We concatenate C P , R Q , C PR Q , and C PR P across rows, put this concatenation (R 8d×n ) through a 1-layer highway network, use a BiLSTM, whose hidden state size is d, to process the output of the highway network (R 8d×n ), and concatenate the forward LSTM output (R d×n ) and the backward LSTM output (R d×n ) across rows to obtain the preliminary memories over the passage-question pair:
section: Memory Refining Layer
For each passage word, the memory refining layer fuses its preliminary memory with those of some other passage words to generate its refined memory over the passage-question pair. Inspired by, we perform self-matching attention on the preliminary memories. However, we are different from in that for each passage word, we only match its preliminary memory with those of a corresponding subset of other passage words, which are selected according to the pre-extracted semantic level inter-word connections, therefore our self-matching attention is named as knowledge aided self-matching attention. Specifically, given a passage word pi , whose preliminary memory is g pi ∈ R 2d , suppose we have obtained Z w through Algorithm 1, then we gather the columns in G whose positions are contained in Z w , represent these columns as U ζ ∈ R 2d×|Zw| , and perform attention on U ζ to obtain the g pi -attended preliminary memory:
where W ζ and v ζ are trainable parameters. Based on the above descriptions, given a passage word pi , we concatenate g pi and ζ pi across rows to obtain δ pi ∈ R 4d . In this way, for all passage words, we obtain ∆ = [δ p 1 , . . . , δ pn ] ∈ R 4d×n . We put ∆ through a 1-layer highway network, use a BiLSTM, whose hidden state size is d, to process the output of the highway network (R 4d×n ), and concatenate the forward LSTM output (R d×n ) and the backward LSTM output (R d×n ) across rows to obtain the refined memories over the passagequestion pair:
section: Answer Span Prediction Layer
In the answer span prediction layer, we first perform self attention on C Q to obtain a summary of the question:
where W and v are trainable parameters. Then with as the query, we perform attention on H to obtain the answer start position distribution:
where W sand v s are trainable parameters. Next we concatenate and Hd s ∈ R 2d across rows to obtain ξ ∈ R 4d . Finally with ξ as the query, we perform attention on H again to obtain the answer end position distribution:
where W e and v e are trainable parameters. Based on the above descriptions, for the training, we minimize the sum of the negative log probabilities of the ground truth answer start position and the ground truth answer end position by the predicted distributions, and for the inference, the answer start position a sand the answer end position a e are chosen such that the product of d s [a s ] and d e [a e ] is maximized and a s ≤ a e . (2017a) generate a syntactic tree for each sentence in the original passage-question pairs. However, the above works just enrich the original MRC dataset with the outputs of certain external models or systems, therefore their MRC models can only make use of machine generated data, but cannot utilize human knowledge explicitly. Attention mechanism has also been widely used in the existing MRC models. For example, use a coattention encoder and a dynamic pointer decoder to address the local maximum problem; use a bidirectional attention flow mechanism to obtain the questionaware passage representation; and use a self-matching attention mechanism to refine the question-aware passage representation. The passage-to-question attention, questionto-passage attention, and self-matching attention in KAR draw on the ideas of the above works, but are different from them in that we integrate explicit knowledge with these attentions.
section: Related Works
section: Experiments
section: MRC Dataset
The MRC dataset used in this paper is SQuAD, which contains over 100, 000 passage-question pairs and their answers. All questions and answers in SQuAD are human generated, and the answer to each question is a fragment in the corresponding passage. SQuAD has been randomly partitioned into three parts: the training set (80%), the development set (10%), and the test set (10%). Both the training set and the development set are publicly available, while the test set is confidential. Besides, SQuAD adopts both Exact Match (EM) and F1 Score as the evaluation metrics.
section: Implementation Details
To implement KAR, we first preprocess SQuAD. Specifically, we put each passage and question in SQuAD through a Stanford CoreNLP ( ) pipeline, which performs tokenization, sentence splitting, POS tagging, lemmatization, and dependency parsing in order. With the outputs of the pipeline, we use the WordNet interface provided by NLTK () to perform the WordNet-based data enrichment method, and thus obtain an enriched MRC dataset. Based on the data preprocessing, we implement KAR using TensorFlow (). For the character-level embedding, we set the forget-  ting factor of FOFE to 0.7. For each BiLSTM, we set its hidden state size d to 300. For the training, we use ADAM () as our optimizer, set the learning rate to 0.0005, and set the mini-batch size to 40. To avoid overfitting, we apply dropout () to the word vectors, the character vectors, the input to each BiLSTM, and the linear transformation before each softmax in the answer span prediction layer, with a dropout rate of 0.2, and apply early stopping with a patience of 5. To avoid the exploding gradient problem, we apply gradient clipping () with a cutoff threshold of 2. Besides, we also apply exponential moving average with a decay rate of 0.999.
section: Experimental Process and Results
In this paper, we only consider the single model performance of MRC models on the development set of SQuAD. On this premise, we perform the following two experiments:
1. Verifying the effects of explicit knowledge. We obtain six enriched MRC datasets by setting χ to 0, 1, 2, 3, 4, and 5 separately, and train a different KAR on each enriched MRC dataset. As shown in, the amount of the extraction results increases monotonically as we increase χ from 0 to 5, but during this process, the performance of KAR first rises by 2% until χ reaches 3, and then begins to drop gradually. Thus it can be seen that the explicit knowledge provided by the WordNet-based data enrichment method plays an effective role in the training of KAR.
2. Verifying the effects of dependency embedding and synonymy embedding. By applying the optimal setting for χ (i.e. 3), we perform ablation analysis on the dependency embedding and the synonymy embedding. As shown in
section: MRC Models
Performance (EM / F1) GDAN (  -/ 67.2 DCN ( 65.4 / 75.6 BiDAF ( 67.7 / 77.3 SEDT ( 68.1 / 77.5 DrQA ( 69.5 / 78.8 MEMEN ( 70.9 / 80.3 R-NET ( 72.3 / 80.6 KAR (ours) 72.4 / 81.1 QANet ( 75.1 / 83.8 SAN ( 76.2 / 84.0 both of the two basic embeddings contribute to the performance of KAR, but the synonymy embedding seems to be more important. Besides, we also compare the best performance of KAR with the published performance of the MRC models mentioned in the related works. As shown in, although KAR has achieved fairly good performance, there is still someway to go to catch up with the cutting-edge MRC models. This is because the scope of the general knowledge in WordNet is very limited, so that KAR cannot obtain enough useful explicit knowledge.
section: Conclusion
In this paper, we explore how to apply the general knowledge in WordNet as explicit knowledge to the training of an MRC model, and thereby propose the WordNet-based data enrichment method and KAR. Based on the explicit knowledge provided by the data enrichment method, KAR has achieved fairly good performance on SQuAD, and more importantly, the performance of KAR varies with the amount of the explicit knowledge. In the future work, we will use larger knowledge bases, such as Freebase, to improve the quality of the explicit knowledge provided to KAR.
