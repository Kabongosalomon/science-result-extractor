section: title
Neural Latent Extractive Document Summarization
section: abstract
Extractive summarization models require sentence-level labels, which are usually created heuristically (e.g., with rule-based methods) given that most summarization datasets only have document-summary pairs. Since these labels might be suboptimal, we propose a latent variable extractive model where sentences are viewed as latent variables and sentences with activated variables are used to infer gold summaries. During training the loss comes directly from gold summaries. Experiments on the CNN/Dailymail dataset show that our model improves over a strong extractive baseline trained on heuristically approximated labels and also performs competitively to several recent models.
section: Introduction
Document summarization aims to automatically rewrite a document into a shorter version while retaining its most important content. Of the many summarization paradigms that have been identified over the years (see for comprehensive overviews), two have consistently attracted attention: extractive approaches generate summaries by copying parts of the source document (usually whole sentences), while abstractive methods may generate new words or phrases which are not in the document.
A great deal of previous work has focused on extractive summarization which is usually modeled as a sentence ranking or binary classification problem (i.e., sentences which are top ranked or predicted as True are selected as summaries).
Early attempts mostly leverage human-engineered features) coupled with binary classifiers (, hidden Markov models (, graph based methods, and integer linear programming.
The successful application of neural network models to a variety of NLP tasks and the availability of large scale summarization datasets has provided strong impetus to develop data-driven approaches which take advantage of continuousspace representations. propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to learn context dependent sentence representations fora document and then use yet another LSTM decoder to predict a binary label for each sentence. adopt a similar approach, they differ in their neural architecture for sentence encoding and the features used during label prediction, while equip the same architecture with a training algorithm based on reinforcement learning. Abstractive models) are based on sequenceto-sequence learning, however, most of them underperform or are on par with the baseline of simply selecting the leading sentences in the document as summaries (but see for exceptions).
Although seemingly more successful than their abstractive counterparts, extractive models require sentence-level labels, which are not included inmost summarization datasets (only document and gold summary pairs are available). Sentence labels are usually obtained by rule-based methods or by maximizing the ROUGE score) between a subset of sentences and the human written summaries). These methods do not fully exploit the human summaries, they only create True/False labels which might be suboptimal. In this paper we propose a latent variable extractive model and view labels of sentences in a document as binary latent variables (i.e., zeros and ones). Instead of maximizing the likelihood of "gold" standard labels, the latent model directly maximizes the likelihood of human summaries given selected sentences. Experiments on the CNN/Dailymail dataset ( show that our latent extractive model improves upon a strong extractive baseline trained on rulebased labels and also performs competitively to several recent models.
section: Model
We first introduce the neural extractive summarization model upon which our latent model is based on. We then describe a sentence compression model which is used in our latent model and finally move onto present the latent model itself.
section: Neural Extractive Summarization
In extractive summarization, a subset of sentences in a document is selected as its summary. We model this problem as an instance of sequence labeling. Specifically, a document is viewed as a sequence of sentences and the model is expected to predict a True or False label for each sentence, where True indicates that the sentence should be included in the summary. It is assumed that during training sentences and their labels in each document are given (methods for obtaining these labels are discussed in Section 3). As shown in the lower part of, our extractive model has three parts: a sentence encoder to convert each sentence into a vector, a document encoder to learn sentence representations given surrounding sentences as context, and a document decoder to predict sentence labels based on representations learned by the document encoder. Let D = (S 1 , S 2 , . . . , S |D| ) denote a document and
. . , y |D| ) denote sentence labels. The sentence encoder first transforms Si into a list of hidden states (h i 1 , hi 2 , . . . , hi |S i | ) using a Bidirectional Long Short-Term Memory Network (Bi-LSTM; Hochreiter and Schmidhuber 1997;. Then, the sentence encoder yields vi , the representation of Si , by averaging these hidden states (also see):
In analogy to the sentence encoder, the document encoder is another Bi-LSTM but applies on the sentence level. After running the Bi-LSTM on a sequence of sentence representations
The document decoder is also an LSTM which predicts sentence labels. At each time step, it takes the context dependent sentence representation of Si produced by the document encoder as well as the prediction in the previous time step:
where W e ∈ R d×2 is the label embedding matrix (d is the hidden dimension for the document decoder LSTM) and y i−1 is the prediction at time step i − 1; the predicted label distribution for y i is:
where W o ∈ R 2×d . The model described above is usually trained by minimizing the negative log-likelihood of sentence labels in training documents; it is almost identical to except that we use a word-level long short-term memory network coupled with mean pooling to learn sentence representations, while they use convolutional neural network coupled with max pooling ().
section: Sentence Compression
We train a sentence compression model to map a sentence selected by the extractive model to a sentence in the summary. The model can be used to evaluate the quality of a selected sentence with respect to the summary (i.e., the degree to which it is similar) or rewrite an extracted sentence according to the style of the summary.
For our compression model we adopt a standard attention-based sequence-to-sequence architecture (). The training set for this model is generated from the same summarization dataset used to train the exractive model. Let D = (S 1 , S 2 , . . . , S |D| ) denote a document and H = (H 1 , H 2 , . . . , H |H| ) its summary. We view each sentence H i in the summary as a target sentence and assume that its corresponding source is a sentence in D most similar to it. We measure the similarity between source sentences and candidate targets using ROUGE, i.e., S j = argmax S j ROUGE(S j , H i ) and S j , H i is a training instance for the compression model. The probability of a sentencê H i being the compression ofˆSofˆ ofˆS j (i.e., p s2s ( ˆ H i | ˆ S j )) can be estimated with a trained compression model.
section: Latent Extractive Summarization
Training the extractive model described in Section 2.1 requires sentence-level labels which are obtained heuristically. Our latent variable model views sentences in a document as binary variables (i.e., zeros and ones) and uses sentences with activated latent variables (i.e., ones) to infer gold summaries. The latent variables are predicted with an extractive model and the loss during training comes from gold summaries directly.
Let D = (S 1 , S 2 , . . . , S |D| ) denote a document and H = (H 1 , H 2 , . . . , H |H| ) its human summary (H k is a sentence in H). We assume that there is a latent variable z i ∈ {0, 1} for each sentence Si indicating whether Si should be selected, and z i = 1 entails it should. We use the extractive model from Section 2.1 to produce probability distributions for latent variables (see Equation) and obtain them by sampling z i ∼ p(z i |z 1:i−1 , h D i−1 ) (see). C = {S i |z i = 1}, the set of sentences whose latent variables equal to one, are our current extractive summaries. Without loss of generality, we denote C = (C 1 , . . . , C |C| ). Then, we estimate how likely it is to infer the human summary H from C. We estimate the likelihood of summary sentence H l given document sentence C k with the compression model introduced in Section 2.2 and calculate the normalized 1 probability s kl :
The score R p measures the extent to which H can be inferred from C:
For simplicity, we assume one document sentence can only find one summary sentence to explain it. Therefore, for all H l , we only retain the most evident s kl . R p (C, H) can be viewed as the "precision" of document sentences with regard to summary sentences. Analogously, we also define R r , which indicates the extent to which H can be covered by C:
R r (C, H) can be viewed as the "recall" of document sentences with regard to summary sentences. The final score R(C, H) is the weighted sum of the two:
Our use of the terms "precision" and "recall" is reminiscent of relevance and coverage in other summarization work. We train the model by minimizing the negative expected R(C, H):
where p(·|D) is the distribution produced by the neural extractive model (see Equation). Unfortunately, computing the expectation term is prohibitive, since the possible latent variable combinations are exponential. In practice, we approximate this expectation with a single sample from the distribution of p(·|D). We use the REIN-FORCE algorithm to approximate the gradient of L(θ):
Note that the model described above can be viewed as a reinforcement learning model, where R(C, H) is the reward. To reduce the variance of gradients, we also introduce a baseline linear regression 2 model bi () to estimate the expected value of R(C, H). To avoid random label sequences during sampling, we use a pre-trained extractive model to initialize our latent model.
section: Experiments
Dataset and Evaluation We conducted experiments on the CNN/Dailymail dataset (. We followed the same pre-processing steps as in. The resulting dataset contains 287,226 document-summary pairs for training, 13,368 for validation and 11,490 for test. To create sentence level labels, we used a strategy similar to. We label the subset of sentences in a document that maximizes ROUGE (against the human summary) as True and all other sentences as False. Using the method described in Section 2.2, we created a compression dataset with 1,045,492 sentence pairs for training, 53,434 for validation and 43,382 for testing. We evaluated our models using full length F1 ROUGE) and the official ROUGE-1.5.5.pl script. We report ROUGE-1, ROUGE-2, and ROUGE-L.
Implementation We trained our extractive model on an Nvidia K80 GPU card with a batch size of 32. Model parameters were uniformly ini-
] (c is the number of columns in a weight matrix). We used Adam () to optimize our models with a learning rate of 0.001, β 1 = 0.9, and β 2 = 0.999. We trained our extractive model for 10 epochs and selected the model with the highest ROUGE on the validation set. We rescaled the gradient when its norm exceeded 5 ( and
40.34 17.70 36.57 LEAD3 (:
Results of different models on the CNN/Dailymail test set using full-length F1 ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L).
regularized all LSTMs with a dropout rate of 0.3 (). We also applied word dropout) at rate 0.2. We set the hidden unit size d = 300 for both word-level and sentence-level LSTMs and all LSTMs had one layer. We used 300 dimensional pre-trained FastText vectors ( to initialize our word embeddings. The latent model was initialized from the extractive model (thus both models have the same size) and we set the weight in Equation to α = 0.5. The latent model was trained with SGD, with learning rate 0.01 for 5 epochs. During inference, for both extractive and latent models, we rank sentences with p(y i = True|y 1:i−1 , D) and select the top three as summary (see also Equation).
Comparison Systems We compared our model against LEAD3, which selects the first three leading sentences in a document as the summary and a variety of abstractive and extractive models. Abstractive models include a sequence-tosequence architecture (; abstract), its pointer generator variant (, and two reinforcement learning-based models (; abstract-RL and abstract-ML+RL). We also compared our approach against an extractive model based on hierarchical recurrent neural networks (, the model described in Section 2.1 (EXTRACT) which encodes sentences using LSTMs, a variant which employs CNNs instead (Cheng and Lapata 2016; EXTRACT-CNN), as well as a similar system based on reinforcement learning (., EXTRACT, our extractive model outperforms LEAD3 by a wide margin. EXTRACT also outperforms previously published extractive models (i.e., SummaRuNNer, EXTRACT-CNN, and REFRESH). However, note that SummaRuNNer generates anonymized summaries () while our models generate non-anonymized ones, and therefore the results of EXTRACT and SummaRuNNer are not strictly comparable (also note that LEAD3 results are different in). Nevertheless, EXTRACT exceeds LEAD3 by +0.75 ROUGE-2 points and +0.57 in terms of ROUGE-L, while SummaRuNNer exceeds LEAD3 by +0.50 ROUGE-2 points and is worse by −0.20 points in terms of ROUGE-L. We thus conclude that EXTRACT is better when evaluated with ROUGE-2 and ROUGE-L. EXTRACT outperforms all abstractive models except for abstract-RL. ROUGE-2 is lower for abstract-RL which is more competitive when evaluated against ROUGE-1 and ROUGE-L.
section: Results As shown in
Our latent variable model (LATENT; Section 2.3) outperforms EXTRACT, despite being a strong baseline, which indicates that training with a loss directly based on gold summaries is useful. Differences among LEAD3, EXTRACT, and LATENT are all significant with a 0.95 confidence interval (estimated with the ROUGE script). Interestingly, when applying the compression model from Section 2.2 to the output of our latent model ( LATENT+COMPRESS ), performance drops considerably. This maybe because the compression model is a sentence level model and it removes phrases that are important for creating the document-level summaries.
section: Conclusions
We proposed a latent variable extractive summarization model which leverages human summaries directly with the help of a sentence compression model. Experimental results show that the proposed model can indeed improve over a strong extractive model while application of the compression model to the output of our extractive system leads to inferior output. In the future, we plan to explore ways to train compression models tailored to our summarization task.
