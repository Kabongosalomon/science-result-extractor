section: title
Factor-based Compositional Embedding Models
section: abstract
Introduction Word embeddings, which are distributed word representations learned by neural language models [1, 2, 3], have been shown to be powerful word representations. They have been successfully applied to a range of NLP tasks, including syntax [2, 4, 5] and semantics [6, 7, 8].
