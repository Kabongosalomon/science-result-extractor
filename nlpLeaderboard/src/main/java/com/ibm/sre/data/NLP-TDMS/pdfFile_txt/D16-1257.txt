section: title
Parsing as Language Modeling
section: abstract
We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve anew state of the art for constituency Penn Treebank parsing-93.8 F 1 on section 23, using 2-21 as training, 24 as development, plus tri-training. When trees are converted to Stan-ford dependencies, UAS and LAS are 95.9% and 94.1%.
section: Introduction
Recent work on deep learning syntactic parsing models has achieved notably good results, e.g., with 92.4 F 1 on Penn Treebank constituency parsing and with 92.8 F 1 . In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F 1 , with a comparatively simple architecture.
In the remainder of this section we outline the major difference between this and previous workviewing parsing as a language modeling problem. Section 2 looks more closely at three of the most relevant previous papers. We then describe our exact model (Section 3), followed by the experimental setup and results (Sections 4 and 5). There is a one-to-one mapping between a tree and its sequential form. (Part-of-speech tags are not used.)
section: Language Modeling
Formally, a language model (LM) is a probability distribution over strings of a language:
where x is a sentence and t indicates a word position. The efforts in language modeling go into computing P (x t |x 1 , · · · , x t−1 ), which as described next is useful for parsing as well.
section: Parsing as Language Modeling
A generative parsing model parses a sentence (x) into its phrasal structure (y) according to
where Y(x) lists all possible structures of x. If we think of a tree (x, y) as a sequence (z)) as illustrated in, we can define a probability distribution over (x, y) as follows:
which is equivalent to Equation (1). We have reduced parsing to language modeling and can use language modeling techniques of estimating P (z t |z 1 , · · · , z t−1 ) for parsing.
section: Previous Work
We look here at three neural net (NN) models closest to our research along various dimensions. The first ( gives the basic language modeling architecture that we have adopted, while the other two ( are parsing models that have the current best results in NN parsing.
section: LSTM-LM
The LSTM-LM of turns (x 1 , · · · , x t−1 ) into ht , a hidden state of an LSTM, and uses ht to guess x t :
where Wis a parameter matrix and [i] indexes ith element of a vector. The simplicity of the model makes it easily extendable and scalable, which has inspired a character-based LSTM-LM that works well for many languages and an ensemble of large LSTM-LMs for English with astonishing perplexity of 23.7 (. In this paper, we build a parsing model based on the LSTM-LM of.
section: MTP
Vinyals et al. observe that a phrasal structure (y) can be expressed as a sequence and build a machine translation parser (MTP), a sequence-tosequence model, which translates x into y using a conditional probability:
where the conditioning event (x, y 1 , · · · , y t−1 ) is modeled by an LSTM encoder and an LSTM decoder. The encoder maps x into he , a set of vectors that represents x, and the decoder obtains a summary vector (h t ) which is concatenation of the decoder's hidden state (h d t ) and weighted sum of word representations ( n i=1 α i he i ) with an alignment vector (α). Finally the decoder predicts y t given ht . Inspired by MTP, our model processes sequential trees.
section: RNNG
Recurrent Neural Network Grammars (RNNG), a generative parsing model, defines a joint distribution over a tree in terms of actions the model takes to generate the tree (:
where a is a sequence of actions whose output precisely matches the sequence of symbols in z, which implies Equation (3) is the same as Equation. RNNG and our model differ in how they compute the conditioning event (z 1 , · · · , z t−1 ): RNNG combines hidden states of three LSTMs that keep track of actions the model has taken, an incomplete tree the model has generated and words the model has generated whereas our model uses one LSTM's hidden state as shown in the next section.
section: Model
Our model, the model of applied to sequential trees and we call LSTM-LM from now on, is a joint distribution over trees:
where ht is a hidden state of an LSTM. Due to lack of an algorithm that searches through an exponentially large phrase-structure space, we use an n-best parser to reduce Y(x) to Y (x), whose size is polynomial, and use LSTM-LM to find y that satisfies
section: Hyper-parameters
The model has three LSTM layers with 1,500 units and gets trained with truncated backpropagation through time with mini-batch size 20 and step size 50. We initialize starting states with previous minibatch's last hidden states. The forget gate bias is initialized to be one
section: Experiments
We describe datasets we use for evaluation, detail training and development processes. 1
section: Data
We use the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993) for training (2-21), development (24) and testing    al., 2015) because in preliminary experiments Charniak parser) performed better when trained on all of 24 million trees than when trained on resampled two million trees. Given x, we produce Y (x), 50-best trees, with Charniak parser and find y with LSTM-LM as do with their discriminative and generative models.
section: Training and Development
section: Supervision
We unk words that appear fewer than 10 times in the WSJ training (6,922 types) and drop activations with probability 0.7. At the beginning of each epoch, we shuffle the order of trees in the training data. Both perplexity and F 1 of LSTM-LM (G) improve and then plateau. Perplexity, the Base Final 88.3 90.5 89.8 92.4 LSTM-LM (G) 89.7 92.6 We also evaluate our model with varying n-best trees including optimal 51-best trees that contain gold trees (51 o ). As shown in Table 1, the LSTM-LM (G) is robust given sufficiently large n, i.e. 50, but does not exhibit its full capacity because of search errors in Charniak parser. We address this problem in Section 5.3.
section: Semi-supervision
We unk words that appear at most once in the training (21,755 types). We drop activations with probability 0.45, smaller than 0.7, thanks to many silver trees, which help regularization. We train LSTM-LM (GS) on the WSJ and a different set of 400,000 NYT trees for each epoch except for the last one during which we use the WSJ only. Training takes 26 epochs and 68 hours on a Titan X. LSTM-LM (GS) achieves 92.5 F 1 on the development.
section: Results
section: Supervision
As shown in, with 92.6 F 1 LSTM-LM (G) outperforms an ensemble of five MTPs () and RNNG (, both of which are trained on the WSJ only.
section: Semi-supervision
We compare LSTM-LM (GS) to two very strong semi-supervised NN parsers: an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus 4 (HC) (; and an ensemble of six one-to-many sequence models trained on the HC and 4.5 millions of EnglishGerman translation sentence pairs (). We also compare LSTM-LM (GS) to best performing non-NN parsers in the literature. Parsers' parsing performance along with their training data is reported in. LSTM-LM (GS) outperforms all the other parsers with 93.1 F 1 .
section: Improved Semi-supervision
Due to search errors -good trees are missing in 50-best trees -in Charniak (G), our supervised and semi-supervised models do not exhibit their full potentials when Charniak (G) provides Y (x). To mitigate the search problem, we tri-train Charniak (GS) on all of 24 million NYT trees in addition to the WSJ, to yield Y (x). As shown in, both LSTM-LM (G) and LSTM-LM (GS) are affected by the quality of Y (x). A single LSTM-LM (GS) together with Charniak (GS) reaches 93.6 and an ensemble of eight LSTM-LMs (GS) with Charniak (GS) achieves anew state of the art, 93.8 F 1 . When trees are converted to Stanford dependencies, 5 UAS and LAS are 95.9% and 94.1%, 6 more than 1% higher than those of the state of the art dependency parser (. Why an indirect method (converting trees to dependencies) is more accurate than a direct one (dependency parsing) remains unanswered ().
section: Conclusion
The generative parsing model we presented in this paper is very powerful. In fact, we see that a generative parsing model, LSTM-LM, is more effective than discriminative parsing models. We suspect building large models with character embeddings would lead to further improvement as in language modeling (). We also wish to develop a complete parsing model using the LSTM-LM framework.
section: Base Oracle Final
section: Gold
Silver --92.8 WSJ (40K) BLLIP (1.8M) --92.4 WSJ (40K) -Choe et al. --92.6 WSJ (40K) NYT (2M) --92.8 HC (90K) HC (11M) --93.0 HC (90K) HC (11M) Charniak (G) + LSTM-LM (G) 89.7 96.7 92.6 WSJ (40K) -Charniak (G) + LSTM-LM (GS) 89.7 96.7 93.1 WSJ (40K) NYT (0/10M) Charniak (GS) + LSTM-LM (G) 91.2 97.1 92.9 WSJ (40K) NYT (24M/0) Charniak (GS) + LSTM-LM (GS) 91.2 97.1 93.6 WSJ (40K) NYT (24M/10M) Charniak (GS) + E(LSTM-LMs (GS)) 91.2 97.1 93.8 WSJ (40K) NYT (24M/11.2M): Evaluation of models trained on the WSJ and additional resources. Note that the numbers of and are not directly comparable as their models are evaluated on OntoNotesstyle trees instead of PTB-style trees. E(LSTM-LMs (GS)) is an ensemble of eight LSTM-LMs (GS). X/Y in Silver column indicates the number of silver trees used to train Charniak parser and LSTM-LM. For the ensemble model, we report the maximum number of trees used to train one of LSTM-LMs (GS).
at Brown University for setting up GPU machines and David McClosky for helping us train Charniak parser on millions trees.
