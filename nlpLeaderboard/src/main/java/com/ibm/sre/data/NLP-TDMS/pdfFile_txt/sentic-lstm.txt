section: title
Targeted Aspect-Based Sentiment Analysis via Embedding Commonsense Knowledge into an Attentive LSTM
section: abstract
Analyzing people's opinions and sentiments towards certain aspects is an important task of natural language understanding. In this paper, we propose a novel solution to targeted aspect-based sentiment analysis, which tackles the challenges of both aspect-based sentiment analysis and targeted sentiment analysis by exploiting commonsense knowledge. We augment the long short-term memory (LSTM) network with a hierarchical attention mechanism consisting of a target-level attention and a sentence-level attention. Commonsense knowledge of sentiment-related concepts is incorporated into the end-to-end training of a deep neural network for sentiment classification. In order to tightly integrate the common-sense knowledge into the recurrent encoder, we propose an extension of LSTM, termed Sentic LSTM. We conduct experiments on two publicly released datasets, which show that the combination of the proposed attention architecture and Sen-tic LSTM can outperform state-of-the-art methods in targeted aspect sentiment tasks.
section: Introduction
In recent years, sentiment analysis () has become increasingly popular for processing social media data on online communities, blogs, wikis, microblogging platforms, and other online collaborative media. Sentiment analysis is a branch of affective computing research ( ) that aims to classify text into either positive or negative, but sometimes also neutral (). Most of the literature is on English language but recently an increasing number of publications is tackling the multilinguality issue (.
While most works approach it as a simple categorization problem, sentiment analysis is actually a suitcase research problem () that requires tackling many natural language processing (NLP) tasks, including named entity recognition, word polarity disambiguation (), personality recognition (), sarcasm detection ( , and aspect extraction. The last one, in particular, is an extremely important subtask that, if ignored, can consistently reduce the accuracy of sentiment classification in the presence of multiple opinion targets.
Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
Hence, aspect-based sentiment analysis (ABSA) ( extends the typical setting of sentiment analysis with a more realistic assumption that polarity is associated with specific aspects (or product features) rather than the whole text unit. For example, in the sentence "The design of the space is good but the service is horrible", the sentiment expressed towards the two aspects ("space" and "service") is completely opposite. Through aggregating sentiment analysis with aspects, ABSA allows the model to produce a fine-grained understanding of people's opinion towards a particular product.
Targeted (or target-dependent) sentiment classification (), instead, resolves the sentiment polarity of a given target in its context, assuming that a sentence might express different opinions towards different targeted entities. For instance, in the sentence "I just logon my. is boring", the sentiment expressed towards is negative, while there is no clear sentiment for. Recently, targeted ABSA () has attempted to tackle the challenges of both ABSA and targeted sentiment analysis. The task is to jointly detect the aspect category and resolve the polarity of aspects with respect to a given target.
Deep learning methods) have achieved great accuracy when applied to ABSA and targeted sentiment analysis. Especially, neural sequential models, such as long short-term memory (LSTM) networks (Hochreiter and Schmidhuber 1997), are of growing interest for their capacity of representing sequential information. Moreover, most of these sequence-based methods incorporate the attention mechanism, which has its root in the alignment model of machine translation (Bahdanau, Cho, and Bengio 2014). Such mechanism takes an external memory and representations of a sequence as input and produces a probability distribution quantifying the concerns in each position of the sequence.
Despite these advances in sentiment analysis, we identify three problems remaining unsolved in current state-of-theart methods. Firstly, a given target might consist of multiple instances (mentions of the same target) or multiple words in a sentence, existing research assumes all instances are of equal importance and simply computes an average vector The Thirty-Second AAAI Conference on Artificial Intelligence over such instances. This oversimplification conflicts with the fact that one or more instances of the target are often more tightly tied with sentiment than others. Secondly, hierarchical attention exploited by existing methods only implicitly models the process of inferring the sentiment-bearing words related to the given target and aspect as black-box. Last but not least, existing research falls short in effectively incorporating into the deep neural network external knowledge, e.g., affective or commonsense knowledge, that could directly contribute to the identification of aspects and sentiment polarity. Without any constraints, moreover, the global attention model might tend to encode task-irrelevant information. To address these problems, our method simultaneously learns a target-specific instance attention as well as a global attention. In particular, our contribution is three-fold: 1. We propose a hierarchical attention model that explicitly attends to first the targets and then the whole sentence; 2. We extend the classic LSTM cell with components accounting for integration with external knowledge; 3. We incorporate affective commonsense knowledge into a deep neural network.
section: Related Work
In this section, we survey multiple research areas related to the proposed framework, namely: ABSA, targeted sentiment analysis, targeted ABSA, and finally works on incorporating external knowledge into deep neural models.
section: Aspect-Based Sentiment Analysis
ABSA is the task of classifying sentiment polarity with respect to a set of aspects. The biggest challenge faced by ABSA is how to effectively represent the aspect-specific sentiment information of the whole sentence. Early works on ABSA have mainly relied on feature-engineering to characterize sentences (). Motivated by the success of deep learning in representation learning, many recent works () utilize deep neural networks to generate sentence embeddings (dense vector representation of sentences) which are then fed to a classifier as a lowdimensional feature vector. Moreover, the representation can be enhanced by using the attention mechanism (, which is typically a multi-layer neural network taking as input the word sequence and aspects. For each word of the sentence, the attention vector quantifies its sentiment salience as well as the relevance to the given aspect. The resulting sentiment representation benefits from the attention mechanism for it overcomes the shortcoming of recurrent neural networks (RNNs), which suffer from information loss when only one single output (e.g., the output at the end of the sequence) is used by the classifier.
section: Targeted Sentiment Analysis
Targeted sentiment analysis aims to analyze sentiment with respect to targeted entities in the sentence. It is thus critical for targeted sentiment analysis methods, e.g., the targetdependent LSTM (TDLSTM) and target connection LSTM (TCLSTM) ( , to model the interaction between sentiment targets and the whole sentence. In order to obtain the target-dependent sentence representation, TDL-STM directly uses the hidden outputs of a bidirectional-LSTM sentence encoders panning the target mentions, while TCLSTM extends TDLSTM by concatenating each input word vector with a target vector. Similar to ABSA, attention models are also applicable to targeted sentiment analysis. Rather than using a single level of attention, deep memory networks (Tang, Qin, and Liu 2016) and recurrent attention models) have achieved superior performance by learning a deep attention over the singlelevel attention, as multiple passes (or hops) over the input sequence could refine the attended words again and again to find the most important words. All existing approaches have either ignored the problem of multiple target instances (or words) or simply used an averaging vector over target expressions (Tang, Qin, and Liu 2016;). Unlike such approaches, our method weights each target word with an attention weight so that a given target is represented by its most informative components.
section: Targeted Aspect-Based Sentiment Analysis
Two baseline systems () are proposed together with SentiHood: a feature-based logistic regression model and a LSTM-based model. The feature-based logistic regression model uses feature templates including n-grams tokens and POS tags extracted from the context of instances. The LSTM baseline can be seen as an adaptation of TDL-STM that simply uses the hidden outputs at the position of target instances assuming that all target instances are equally important.
section: Incorporating External Knowledge
External knowledge base has been typically used as a source of features
section: Methodology
In this section, we describe the proposed attention-based neural architecture in detail: we first proposed the task definition of targeted ABSA, followed by an overview of the whole neural architecture; afterwards, we describe instance attention and global attention model; lastly, we describe the proposed knowledge-embedded extension of LSTM cell.
section: Task Definition
A sentence s consists of a sequence of words. Similar to (), we consider all mentions of the same target as a single target. A target t composed of m words in sentence s, denoted as T = {t 1 , t 2 , ⋯, ti , ⋯, t m } with ti referring to the position of ith word in the target expression, the task of targeted ABSA can be divided into two subtasks. Firstly, it resolves the aspect categories oft belonging to a predefined set. Secondly, it classifies the sentiment polarity with respect to each aspect category associated with t. For example, the sentence "I live in [West London] for years. I like it and it is safe to live in much of. Except maybe. " contains two targets, and. Our objective is to detect the aspects and classify the sentiment polarity. The desired output for is ['general':positive; 'safety':positive], while output for should be ['general':negative; 'safety':negative].
section: Overview
In this section, we provide an overview of the proposed method. Our neural architecture consists of two components: the sequence encoder and a hierarchical attention component. illustrates how the neural architecture works. Given a sentence s = {w 1 , w 2 , ⋯, w L }, a look-up operation is first performed to convert input words into word embeddings {v , v , ⋯, v w L }. The sequence encoder, which is based on a bidirectional LSTM, transforms the word embeddings into a sequence of hidden outputs. The attention component is built on top of the hidden outputs. The target-level attention takes as input the hidden outputs at the positions of target expression (highlighted in brown) and computes a selfattention vector over these words.
The output of target-level attention component is a representation of the target. Afterwards, the target representation together with the aspect embeddings is used for computing a sentence-level attention transforming the whole sentence into a vector. The sentence-level attention component returns one sentence vector for each aspect and target pair. The aspect-based sentence vector is then fed into the corresponding multi-class (e.g., None, Neural, Negative, and Positive fora 4-class setting; or None, Negative, and Positive fora 3-class setting) classifier to resolve the sentiment polarity.
section: Long Short-Term Memory Network
The sentence is encoded using an extension of RNN (, termed LSTM (Hochreiter and Schmidhuber 1997), which was firstly introduced by to solve the vanishing and exploding gradient problem faced by the vanilla RNN. A typical LSTM cell contains three gates: forget gate, input gate and output gate. These gates determine the information to flow in and flow out at the current time step. The mathematical representations of the cell are as follows:
where f i , I i and oi are the forget gate, input gate and output gate, respectively. W f , WI , W o , bf , b I and b o are the weight matrix and bias scalar for each gate. Ci is the cell state and hi is the hidden output. A single LSTM typically encodes the sequence from only one direction. However, two LSTMs can also be stacked to be used as a bidirectional encoder, referred to as bidirectional LSTM. For a sentence s = {w 1 , w 2 , ⋯, w L }, bidirectional LSTM produces a sequence of hidden outputs,
where each element of H is a concatenation of the corresponding hidden outputs of both forward and backward LSTM cells.
section: Target-Level Attention
Based on the attention mechanism, we calculate an attention vector fora target expression. A target might consist of a consecutive or non-consecutive sequence of words, denoted as T = {t 1 , t 2 , ⋯, t m }, where ti is the location of an individual word in a target expression. The hidden outputs corresponding to T is denoted as
We compute the vector representation of a target t as
where the target attention vector α = {α 1 , α 2 , ⋯, α m } is distributed over target word sequence T . The attention vector α is a self-attention vector that takes nothing but the hidden output itself as input. The attention vector α of target expression is computed by feeding the hidden output into a bi-layer perceptron, as shown in Equation 3.
where
∈ R 1×dm are parameters of the attention component.
section: Sentence-Level Attention Model
Following the target-level attention, our model learns a target-and-aspect-specific sentence attention overall the words of a sentence. Given a sentence s of length L, the hidden outputs are denoted as
An attention model computes a linear combination of the hidden vectors into a single vector, i.e.,
where the vector β = [β 1 , β 2 , ⋯, β L ] is called the sentencelevel attention vector. Each element β i encodes the salience of the word w i in the sentence s with respect to the aspect a and target T . Existing research on targeted sentiment analysis or ABSA mostly uses targets or aspect terms as queries. At first, each hi is transformed to ad m dimensional vector by a multi-layer neural network with a tanh activation function, followed by a dense softmax layer to generate a probability distribution over the words in sentence s, i.e.,
section: Commonsense Knowledge
In order to improve the accuracy of sentiment classification, we use commonsense knowledge as our knowledge source to be embedded into the sequence encoder. In particular, we use SenticNet (), a commonsense knowledge base that contains 50,000 concepts associated with a rich set of affective properties. These affective properties provide not only concept-level representation but also semantic links to the aspects and their sentiment. For example, the concept 'rotten fish' has property "KindOffood" that directly relates with aspects such as 'restaurant' or 'food quality', but also emotions, e.g., 'joy', which can support polarity detection.
However, the high dimensionality of SenticNet hinders it from being used in deep neural models. AffectiveSpace () has been built to map the concepts of SenticNet to continuous low-dimensional embeddings without losing the semantic and affective relatedness of the original space. Based on this new space of concepts, we embed concept-level information into deep neural sequential models to better classify both aspects and sentiment in natural language text.
section: Sentic LSTM
In order to leverage SenticNet's affective commonsense knowledge efficiently, we propose an affective extension of LSTM, termed Sentic LSTM. It is reasonable to assume that SenticNet concepts contain information complementary to the textual word sequence as, by definition, commonsense knowledge is about concepts that are usually taken for granted and, hence, absent from text. Sentic LSTM aims to entitle the concepts with two important roles: 1) assisting with the filtering of information flowing from onetime step to the next and 2) providing complementary information to the memory cell. At each time step i, we assume that a set of knowledge concept candidates can be triggered and mapped to ad c dimensional space. We denote the set of K concepts as {μ i,1 , μ i,2 , ⋯, μ i,K }. First, we combine the candidate embeddings into a single vector as follows:
As we realized that there are only up to 4 extracted concepts for each time step, we simply use the average vector (although a more sophisticated attention model can also be easily employed to replace the averaging function).
Our affective extension of LSTM is illustrated in Equation 7. At first, we assume that affective concepts are meaningful cues to control the information of token-level information. For example, a multi-word concept 'rotten fish' might indicate that the word 'rotten' is a sentiment-related modifier of its next word 'fish' and, hence, less information should be filtered out at next time step. We thus add knowledge concepts to the forget, input, and output gate of standard LSTM to help filtering the information. The presence of affective concepts in the input gate is expected to prevent the memory cell from being affected by input tokens conflicting with pre-existing knowledge. Similarly, the output gate uses such knowledge to filter out irrelevant information stored in the memory.
Another important feature of Sentic LSTM is based on the assumption that the information from the concept-level output is complementary to the token level. Therefore, we extended the regular LSTM with an additional knowledge output gate o c i to output concept-level knowledge complementary to the token-level memory. Since AffectiveSpace is learned independently, we leverage a transformation matrix W c ∈ Rd h ×dμ to map it to the same space as the memory outputs. In other words, o c i models the relative contributions of token level and concept level.
Moreover, we notice that o c i * tanh(W c μ i ) actually resembles the functionality of the sentinel vector used by, which allows the model to choose whether to use affective knowledge or not.
section: Prediction and Parameter Learning
The objective to train our classier is defined as minimizing the sum of the cross-entropy losses of prediction on each target-aspect pair, i.e.,
where A is the set of predefined aspects, and pa c,t is the probability of the gold-standard polarity class c given target t with respect to a sentiment category a, which is defined by a softmax function,
where W p and b a s are the parameters to map the vector representation of target t to the polarity label of aspect a. To avoid overfitting, we add a dropout layer with dropout probability of 0.5 after the embedding layer. We stop the training process of our model after 10 epochs and select the model that achieves the best performance on the development set.
section: Experiments
section: Dataset and Resources
We evaluate our method on two datasets: SentiHood () and a subset of Semeval 2015 (). SentiHood was built by querying Yahoo! Answers with location names of London city. shows statistics of SentiHood. The whole dataset is split into train, test, and development set by the authors. Overall, the entire dataset contains 5,215 sentences, with 3,862 sentences containing a single target and 1,353 sentences containing multiple targets. It also shows that there are approximately two third of targets annotated with aspect-based sentiment polarity (train set: 2476 out of 2977; test set:1241 out of 1898; development set: 619 out of 955). On average, each sentimentbearing target has been annotated with 1.37 aspects. To show the generalizability of our methods, we build a subset of the dataset used by Semeval-2015. We remove sentences containing no targets as well as NULL targets. To be comparable with SentiHood, we combine targets with the same surface form within the same sentence as mentions of the same target. In total, we have 1,197 targets left in the training set and 542 targets left in the testing set. On average, each target has 1.06 aspects.  To inject the commonsense knowledge, we use a syntaxbased concept parser 1 to extract a set of concept candidates at each time step, and use AffectiveSpace 2 as the concept embeddings. In case no concepts are extracted, a zero vector is used as the concept input.
section: Experiment Setting
We evaluate our method on two sub-tasks of targeted ABSA: 1) aspect categorization and 2) aspect-based sentiment classification. Following), we treat the outputs of aspect-based classification as hierarchical classes. For aspect categorization, we output the label (e.g., in the 3-class setting, it outputs 'Positive', 'Negative', or 'None') with the highest probability for each aspect. For aspect-based sentiment classification, we ignore the scores of 'None'. For evaluating the aspect-based sentiment classification, we simply calculate the accuracy averaged over aspects. We evaluate aspect categorization as a multi-label classification problem so that results are averaged over targets instead of aspects.
We evaluate our methods and baseline systems using both loose and strict metrics. We report scores of three widely used evaluation metrics of multi-label classifier: Macro-F1, Micro-F1, and strict Accuracy. Given the dataset D, the ground-truth aspect categories of the target t ∈ Dis denoted as Y t , while the predicted aspect categories denoted as ̂ Y t . The three metrics can be computed as
, where σ(⋅) is an indicator function.
• Macro-F1 = 2 Ma-P×Ma-R Ma-P+Ma-R , which is based on MacroPrecision (Ma-P) and Micro-Recall (Ma-R) with Ma-P
, and Ma-R= 1
, which is based on MicroPrecision (Mi-P) and Micro-Recall (Mi-R), where Mi-P=
section: Performance Comparison
We compare our proposed method with the methods that have been proposed for targeted ABSA as well as methods proposed for ABSA or targeted sentiment analysis but applicable to targeted ABSA. Furthermore, we also compare the performances of several variants of our proposed method in order to highlight our technical contribution. We run the model for multiple times and report the results that perform best in the development set. For Semeval-2015 dataset, we report the results of the final epoch.
• TDLSTM: TDLSTM ( ) adopts Bi-LSTM to encode the sequential structure of a sentence and represents a given target using a vector averaged on the hidden outputs of target instances.
• LSTM + TA: Our method learns an instance attention on top of the outputs of LSTM to model the contribution of each instance.
• LSTM + TA + SA: In addition to target instance attention, we add a sentence-level attention to the model. • LSTM + TA + DMN SA: The sentence-level attention is replaced by a dynamic memory network with multiple hops (Tang, Qin, and Liu 2016). We run the memory network with different numbers of hops and report the result with 4 hops (best performance on development set of SentiHood). We exclude the case of zero hops as it corresponds to Bi-LSTM + TA + SA. show the performance on SentiHood and Semeval-15 dataset, respectively. In comparison with the non-attention baseline (Bi-LSTM+Avg.), we can find that our best attention-based model significantly improves aspect categorization (by more than 20%) and sentiment classification (approximately 10%) on SentiHood. However, it is notable that, on the Semeval-2015 dataset, the improvement is relatively smaller. We conjecture the reason is that SentiHood has masked the target as a special word "LOCA-TION", which resulted less informative than the full name of aspect targets that are used by Hence, using only the hidden outputs regarding the target does not suffice to represent the sentiment of the whole sentence in SentiHood. Compared with target averaging model, the target-level attention achieves some improvement (even though not significant), as the target attention is capable of identifying the part of target expressions with higher sentiment salience. On the other hand, it is notable that the two-step attention achieves significant improvement on both aspect categorization and sentiment classification, indicating that the target-and aspect-dependent sentence attention could retrieve information relevant to both tasks.
section: Results of Attention Model
To our surprise, using multiple hops in the sentence-level attention fails to bring in any improvement. The performance even falls down significantly on Semeval-2015 with a much smaller number of training instances but larger aspect set than SentiHood. We conjecture the reason is that using multi-hops increases the number of parameter to learn, which makes it less applicable to small and sparse datasets.
section: Visualization of Attention
We visualize the attention vectors of sentence-level attention in with respect to "Transition-location" and "Price" aspects. The two attention vectors have encoded quite different concerns in the word sequence. In the first example, the 'Transition-location' attention attends to the word "long", which is expressing a negative sentiment towards the target. In comparison, the 'Price' attention attends more to the word 'cheap', which is related to the aspect. That is to say, the two attention vectors are capable of distinguishing information related to different aspects. As visualized in, the target-level attention is capable of selecting the part of target expression of which the aspect or sentiment is easier to be resolved.
section: Results of Knowledge-Embedded LSTM
It can be seen from and 4 that injecting knowledge into the model improves the performance in general. Since AffectiveSpace encodes the information about affective properties that are semantically related to the aspects, it is reasonable to find out that it can improve performance on both tasks. The results show that our proposed Sentic LSTM outperforms baseline methods, even if not significantly.
An important outcome of the experiments is that Sentic LSTM significantly outperforms a baseline (LSTM + TA + SA + KB feat) feeding the knowledge features to the input layer, which confirms the efficacy of using a knowledge output gate to control the flow of background knowledge. Furthermore, the superior performance of Sentic LSTM over Recall LSTM and KBA indicates that the activated knowledge concepts can also help filtering the information that conflicts with the background knowledge.
section: Conclusion
In this paper, we proposed a neural architecture for the task of targeted ABSA. We explicitly modeled the attention as a two-step model which encodes targets and full sentence. The target-level attention learns to attend to the sentiment-salient part of a target expression and generates a more accurate representation of the target, while the sentence-level attention searches for the target-and aspect-dependent evidence over the full sentence. Moreover, we proposed an extension of the LSTM cell so that it could more effectively incorporate affective commonsense knowledge when encoding the sequence into a vector. In the future, we would like to collectively analyze the sentiment of multiple targets co-occurring in the same sentence and investigate the role of commonsense knowledge in modeling the relation between targets.
