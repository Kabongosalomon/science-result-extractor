section: title
Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM
section: abstract
This paper describes team Turing's submission to SemEval 2017 RumourEval: Determining rumour veracity and support for rumours (SemEval 2017 Task 8, Sub-task A). Subtask A addresses the challenge of rumour stance classification, which involves identifying the attitude of Twitter users towards the truthfulness of the rumour they are discussing. Stance classification is considered to bean important step towards rumour verification, therefore performing well in this task is expected to be useful in debunking false rumours. In this work we classify a set of Twit-ter posts discussing rumours into either supporting, denying, questioning or commenting on the underlying rumours. We propose a LSTM-based sequential model that, through modelling the conversational structure of tweets, which achieves an accuracy of 0.784 on the RumourEval test set outperforming all other systems in Sub-task A.
section: Introduction
In stance classification one is concerned with determining the attitude of the author of a text towards a target (. Targets can range from abstract ideas, to concrete entities and events. Stance classification is an active research area that has been studied in different domains (. Here we focus on stance classification of tweets towards the truthfulness of rumours circulating in Twitter conversations in the context of breaking news. Each conversation is defined by a tweet that initiates the conversation and a set of nested replies to it that form a conversation thread. The goal is to classify each of the tweets in the conversation thread as either supporting, denying, querying or commenting (SDQC) on the rumour initiated by the source tweet. Being able to detect stance automatically is very useful in the context of events provoking public resonance and associated rumours, as a first step towards verification of early reports (. For instance, it has been shown that rumours that are later proven to be false tend to spark significantly larger numbers of denying tweets than rumours that are later confirmed to be true ().
Here we focus on exploiting the conversational structure of social media threads for stance classification and introduce a novel LSTM-based approach to harness conversations.
section: Related Work
Single Tweet Stance Classification Stance classification for rumours was pioneered by as a binary classification task (support/denial). perform stance classification for rumours emerging during crises. Both works use tweets related to the same rumour during training and testing.
A model based on bidirectional LSTM encoding of tweets conditioned on targets has been shown to achieve state-of-the-art on the SemEval-2016 task 6 dataset (). However the RumourEval task is different as it addresses conversation threads. and consider the sequential nature of tweet threads in their works. employ Hawkes processes to classify temporal sequences of tweets. They show the importance of using both the textual content and temporal information about the tweets, disregarding the discourse structure. Zubiaga et Figure 1: Example of a conversation thread from the dataset with three branches, two of which are highlighted. The conversation has a tree structure, which can be split into individual branches by taking each leaf node with all its direct parents.
section: Sequential Stance Classification
al. (2016a) model the conversational structure of source tweets and subsequent replies: as a linear chain and as a tree. They use linear-and tree-versions of a CRF classifier, outperforming the approach by.
section: Dataset
The dataset provided for this task contains Twitter conversation threads associated with rumours around ten different events in breaking news, including the Paris shootings in Charlie Hebdo, the Ferguson unrest, the crash of a Germanwings plane. These events include 325 conversation threads consisting of 5568 underlying tweets annotated for stance at the tweet level (breakdown between training, testing and development sets is shown in: Per-class distribution of tweets in the training, development and testing sets.
Each thread includes a source tweet that initiates a conversation and nested tweets responding to either the source tweet or earlier replies. The thread can be split into linear branches of tweets, where a branch is defined as a chain of tweets that starts with a leaf tweet including its direct parent tweets, all the way up to the source tweet. shows an example of a conversation along with its annotations represented as a tree structure with highlighted branches. The depth of a tweet is the number of its parents starting from the root node. Branches 1 and 2 in have depth one whereas branch 3 has depth three. There is a clear class imbalance in favour of commenting tweets (66%) and supporting tweets (18%), whereas the denying (8%) and querying classes (8%) are under-represented (see). While this imbalance poses a challenge, it is also indicative of the realistic scenario where only a few users question the veracity of a statement.
section: System Description
section: Features
Prior to generating features for the tweets, we perform a pre-processing step where we remove nonalphabetic characters, convert all words to lowercase and tokenise texts. 1 Once tweet texts are preprocessed, we extract the following features:
• Word vectors: we use a word2vec () model pre-trained on the Google News dataset (300d) 2 using the gensim package ( ˇ Rehůřek and Sojka, 2010).
• Tweet lexicon: (1) count of negation words and (2) count of swear words. [As querying] @username Weren't you the one who abused her? • Punctuation: (1) presence of a period, presence of an exclamation mark, (3) presence of a question mark, (4) ratio of capital letters.
• Attachments: (1) presence of a URL and presence of images.
• Relation to other tweets (1) Word2Vec cosine similarity wrt source tweet, (2) Word2Vec cosine similarity wrt preceding tweet, and (3) Word2Vec cosine similarity wrt thread • Content length: (1) word count and (2) character count.
• Tweet role: whether the tweet is a source tweet of a conversation. Tweet representations are obtained by averaging word vectors in a tweet and then concatenating with the additional features into a single vector, at the preprocessing step. This set of features have shown to be the best comparing to using word2vec features on their own or any of the reduced combinations of these features.
section: Branch -LSTM Model
To tackle the task of rumour stance classificaiton, we propose branch-LSTM, a neural network architecture that uses layers of LSTM units (Hochreiter and Schmidhuber, 1997) to process the whole branch of tweets, thus incorporating structural information of the conversation (see the illustration of the branch-LSTM on the. The input at each time step i of the LSTM layer is the representation of the tweet as a vector. We record the output of each time step so as to attach a label to each tweet in a branch . This output is fed through several dense ReLU layers, a 50% dropout layer, and then through a softmax layer to obtain class probabilities. We use zero-padding and masks to account for the varying lengths of tweet branches. The model is trained using the categorical cross entropy loss function. Since there is overlap between branches originating from the same source tweet, we exclude the repeating tweets from the loss function using a mask at the training stage. The model uses tweet representation as the mean average of word vectors concatenated with extra features described above. Due to the short length of tweets, using more complex models for learning tweet representations, such as an LSTM that takes each word as input at each time step and returns the representation at the final time step, does not lead to a noticeable difference in the performance based on cross-validation experiments on the training and development sets, while taking significantly longer to train. We experimented with replacing the unidirectional LSTMs with bidirectional LSTMs but could observe no improvements inaccuracy (using cross-validation results on the training and development set).
section: Experimental Setup
The dataset is split into training, development and test sets by the task organisers. We determined the optimal set of hyperparameters via testing the performance of our model on the development set for different parameter combinations. We used the
0.561 0.621 0.000 0.762 0.860 Testing 0.784 0.434 0.403 0.000 0.462 0.873  Tree of Parzen Estimators (TPE) algorithm 6 to search the parameter space, which is defined as follows: the number of dense ReLU layers varies from one to four; the number of LSTM layers is one or two; the mini-batch size is either 32 or 64; the number of units in the ReLU layer is one of {100, 200, 300, 400, 500}, and in the LSTM layer one of {100, 200, 300}; the strength of the L2 regularisation is one of {0.0, 1e-4, 3e-4, 1e-3} and the number of epochs is selected from {30, 50, 70, 100}. We performed 100 trials of different parameter combinations optimising for accuracy on the development set in order to choose the best combination. We fixed hyperparameters to train the model on combined training and development sets and evaluated on the held out test set.
section: Results
The performance of our model on the testing and development set is shown in. Together with the accuracy we show macro-averaged Fscore and per-class macro-averaged F-scores as these metrics account for the class imbalance. The difference inaccuracy between testing and development set is minimal, however we see significant difference in Macro-F score due to different class balance in these sets. Macro-F score could be improved if we used it as a metric for optimising hyper-parameters. The branch-LSTM model predicts commenting, the majority class well, however it is unable to pick out any denying, the mostchallenging under-represented class. Most deny-  ing instances get misclassified as commenting (see), with only one tweet misclassified as querying and two as supporting). An increased amount of labelled data would be helpful to improve performance of this model. As we were considering conversation branches, it is interesting to analyse the performance distribution across different tweet depths (see). Maximum depth/branch length in the testing set is 13 with most tweets concentrated at depths from 0 to 3. Source tweets (depth zero) are usually supporting and the model predicts these very well, but performance of supporting tweets at other depths decreases. The model does not show a noticeable difference in performance on tweets of varying lengths.
section: Conclusions
This paper describes the Turing system entered in the SemEval-2017 Task 8 Subtask A. Our method decomposes the tree structure of conversations into linear sequences and achieves accuracy 0.784 on the testing set and sets the state-of-the-art for rumour stance classification. In future work we plan to explore different methods for modelling tree-structured conversations.
section: 478
