section: title
A Dependency-Based Neural Network for Relation Classification
section: abstract
Previous research on relation classification has verified the effectiveness of using dependency shortest paths or subtrees. In this paper, we further explore how to make full use of the combination of these dependency information. We first propose anew structure, termed augmented dependency path (ADP), which is composed of the shortest dependency path between two entities and the subtrees attached to the shortest path. To exploit the semantic representation behind the ADP structure, we develop dependency-based neural networks (DepNN): a recursive neural network designed to model the subtrees, and a convolutional neural network to capture the most important features on the shortest path. Experiments on the SemEval-2010 dataset show that our proposed method achieves state-of-art results.
section: Introduction
Relation classification aims to classify the semantic relations between two entities in a sentence. It plays a vital role in robust knowledge extraction from unstructured texts and serves as an intermediate step in a variety of natural language processing applications. Most existing approaches follow the machine learning based framework and focus on designing effective features to obtain better classification performance.
The effectiveness of using dependency relations between entities for relation classification has been reported in previous approaches. For example, carefully selected a set of features from tokenization and dependency parsing, and extended some of them to generate high order features * Contribution during internship at in different ways. designed a dependency tree kernel and attached more information including Part-of-Speech tag, chunking tag of each node in the tree. Interestingly, provided an important insight that the shortest path between two entities in a dependency graph concentrates most of the information for identifying the relation between them. developed these ideas by analyzing multiple subtrees with the guidance of pre-extracted keywords. Previous work showed that the most useful dependency information in relation classification includes the shortest dependency path and dependency subtrees. These two kinds of information serve different functions and their collaboration can boost the performance of relation classification (see Section 2 for detailed examples). However, how to uniformly and efficiently combine these two components is still an open problem. In this paper, we propose a novel structure named Augmented Dependency Path (ADP) which attaches dependency subtrees to words on a shortest dependency path and focus on exploring the semantic representation behind the ADP structure.
Recently, deep learning techniques have been widely used in exploring semantic representations behind complex structures. This provides us an opportunity to model the ADP structure in a neural network framework. Thus, we propose a dependency-based framework where two neural networks are used to model shortest dependency paths and dependency subtrees separately. One convolutional neural network (CNN) is applied over the shortest dependency path, because CNN is suitable for capturing the most useful features in a flat structure. A recursive neural network (RN-N) is used for extracting semantic representations from the dependency subtrees, since RNN is good at modeling hierarchical structures. To connect these two networks, each word on the shortest A thief who tried to steal the truck broke the ignition with screwdriver. On the Sabbath the priests broke the commandment with priestly work.  path is combined with a representation generated from its subtree, strengthening the semantic representation of the shortest path. In this way, the augmented dependency path is represented as a continuous semantic vector which can be further used for relation classification.
section: Problem Definition and Motivation
The task of relation classification can be defined as follows. Given a sentence S with a pair of entities e 1 and e 2 annotated, the task is to identify the semantic relation between e 1 and e 2 in accordance with a set of predefined relation classes (e.g., Content-Container, Cause-Effect). For example, in, the relation between two entities e 1 =thief and e 2 =screwdriver is InstrumentAgency. first used shortest dependency paths between two entities to capture the predicate-argument sequences (e.g., "thief←broke→screwdriver" in), which provide strong evidence for relation classification. As we observe, the shortest paths contain more information and the subtrees attached to each node on the shortest path are not exploited enough. For example, and 2b show two instances which have similar shortest dependency paths but belong to different relation classes. Methods only using the path will fail in this case. However, we can distinguish these two paths by virtue of the attached subtrees such as "dobj→commandment" and "dobj→ignition". Based on many observations like this, we propose the idea that combines the subtrees and the shortest path to form a more precise structure for classifying relations. This combined structure is called "augmented dependency path (ADP)", as illustrated in.
Next, our goal is to capture the semantic representation of the ADP structure between two entities. We first adopt a recursive neural network to model each word according to its attached dependency subtree. Based on the semantic information of each word, we design a convolutional neural network to obtain salient semantic features on the shortest dependency path.
section: Dependency-Based Neural Networks
In this section, we will introduce how we use neural network techniques and dependency information to explore the semantic connection between two entities. We dub our architecture of modeling ADP structures as dependency-based neural networks (DepNN). illustrates DepNN with a concrete example. First, we associate each word wand dependency relation r with a vector representation x w , x r ∈ R dim . For each word won the shortest dependency path, we develop an RNN from its leaf words up to the root to generate a subtree embedding cw and concatenate cw with x w to serve as the final representation of w. Next, a CNN is designed to model the shortest dependency path based on the representation of its words and relations. Finally our framework can efficiently represent the semantic connection between two entities with consideration of more comprehensive dependency information.
section: Modeling Dependency Subtree
The goal of modeling dependency subtrees is to find an appropriate representation for the words on the shortest path. We assume that each word w can be interpreted by itself and its children on the dependency subtree. Then, for each word won the subtree, its word embedding x w ∈ R dim and subtree representation cw ∈ R dimc are concatenated to form its final representation p w ∈ R dim+dimc . For a word that does not have a subtree, we set its subtree representation as c LEAF . The subtree representation of a word is derived through transforming the representations of its children words. During the bottom-up construction of the subtree, each word is associated with a dependency relation such as dobj as in. For each dependency relation r, we set a transformation matrix W r ∈ R dimc×(dim+dimc) which is learned during training. Then we can get,
where R (w,q) denotes the dependency relation between word wand its child word q. This process continues recursively up to the root word on the shortest path.
section: Modeling Shortest Dependency Path
To classify the semantic relation between two entities, we further explore the semantic representation behind their shortest dependency path, which can be seen as a sequence of words and dependency relations as the bold-font part in. As the convolutional neural network (CNN) is good at capturing the salient features from a sequence of objects, we design a CNN to tackle the shortest dependency path. A CNN contains a convolution operation over a window of object representations, followed by a pooling operation. As we know, a word won the shortest path is associated with the representation p w through modeling the subtree. For a dependency relation r on the shortest path, we set its representation as a vector x r ∈ R dim . As a sliding window is applied on the sequence, we set the window size ask. For example, when k = 3, the sliding windows of a shortest dependency path with n words are: {[r s w 1 r 1 ], [r 1 w 2 r 2 ], . . . , [r n−1 w n re ]} where r sand re are used to denote the beginning and end of a shortest dependency path between two entities.
We concatenate k neighboring words (or dependency relations) representations into anew vector. Assume X i ∈ R dim·k+dimc·nw as the concatenated representation of the i-th window, where n w is the number of words in one window. A convolution operation involves a filter W 1 ∈ R l×(dim·k+dimc·nw) , which operates on X i to produce anew feature vector Li with l dimensions,
where the bias term is ignored for simplicity. Then W 1 is applied to each possible window in the shortest dependency path to produce a feature map:
Next, we adopt the widely-used max-over-time pooling operation), which can retain the most important features, to obtain the final representation L from the feature map. That is,
section: Learning
Like other relation classification systems, we also incorporate some lexical level features such as named entity tags and WordNet hypernyms, which prove useful to this task. We concatenate them with the ADP representation L to produce a combined vector M . We then pass M to a fully connected sof tmax layer whose output is the probability distribution y over relation labels.
Then, the optimization objective is to minimize the cross-entropy error between the ground-truth label vector and the sof tmax output. Parameters are learned using the back-propagation method).
section: Experiments
We compare DepNN against multiple baselines on.
The training set includes 8000 sentences, and the test set includes 2717 sentences. There are 9 relation types, and each type has two directions. Instances which don't fall in any of these classes are labeled as Other. The official evaluation metric is the macro-averaged F1-score (excluding Other) and the direction is considered. We use dependency trees generated by the Stanford Parser () with the collapsed option.
section: Contributions of different components
We first show the contributions from different components of DepNN. Two different kinds of word embeddings for initialization are used in the experiments. One is the 50-d embeddings provided by SENNA). The second is the 200-d embeddings used in (), trained on Gigaword with word2vec 1 . All the hyperparameters are set with 5-fold crossvalidation.  We start with a baseline model using a CNN with only the words on the shortest path. We then add dependency relations and attached subtrees. The results indicate that both parts are effective for relation classification. The rich linguistic information embedded in the dependency relations and subtrees canon one hand, help distinguish different functions of the same word, and on the other hand infer an unseen word's role in the sentence. Finally, the lexical features are added and DepNN achieves state-of-the-art results.
section: Model
section: Comparison with Baselines
In this subsection, we compare DepNN with several baseline relation classification approaches. Here, DepNN and the baselines are all based on the 200-d embeddings trained on Gigaword due to the larger corpus and higher dimensions.
SVM (: This is the top performed system in   DT-RNN () : This is an RNN for modeling dependency trees. It combines node's word embedding with its children through a linear combination but not a subtree embedding. We adapt the augmented dependency path into a dependency subtree and apply DT-RNN.
As shown in, DepNN achieves the best result (83.6) using NER features. WordNet features can also improve the performance of DepN-N, but not as obvious as NER. had similar observations, since the larger number of WordNet tags may cause overfitting. SVM achieves a comparable result, though the quality of feature engineering highly relies on human experience and external NLP resources. MV-RNN models the constituent parse trees with a recursive procedure and its F1-score is about 1.8 percent lower than DepNN. Meanwhile, MVR-NN is very slow to train, since each word is associated with a matrix. Both CNN and FCM use features from the whole sentence and achieve similar performance. DT-RNN is the worst of all baselines, though it also considers the information from shortest dependency paths and attached subtrees. As we analyze, shortest dependency paths and subtrees play different roles in relation classification. However, we can see that DT-RNN does not distinguish the modeling processes of shortest paths and subtrees. This phenomenon is also seen in a kernel-based method, where the tree kernel performs worse than the shortest path kernel. We also look into the DepNN model and find it can identify different patterns of words and the dependency relations. For example, in the Instrument-Agency relation, the word "using" and the dependency relation "prep with" are found playing a major role.
section: Conclusion
In this paper, we propose to classify relations between entities by modeling the augmented dependency path in a neural network framework. We present a novel approach, DepNN, to taking advantages of both convolutional neural network and recursive neural network to model this structure. Experiment results demonstrate that DepNN achieves state-of-the-art performance.
of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2335-2344, Dublin, Ireland, August. Dublin City University and Association for Computational Linguistics.
section: 290
