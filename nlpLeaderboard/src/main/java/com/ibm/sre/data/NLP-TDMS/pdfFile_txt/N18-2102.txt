section: title
Multi-Reward Reinforced Summarization with Saliency and Entailment
section: abstract
ive text summarization is the task of compressing and rewriting along document into a short summary while maintaining saliency, directed logical entailment, and non-redundancy. In this work, we address these three important aspects of a good summary via a reinforcement learning approach with two novel reward functions: ROUGE-Sal and Entail, on top of a coverage-based baseline. The ROUGESal reward modifies the ROUGE metric by up-weighting the salient phrases/words detected via a keyphrase clas-sifier. The Entail reward gives high (length-normalized) scores to logically-entailed summaries using an entailment classifier. Further, we show superior performance improvement when these rewards are combined with traditional metric (ROUGE) based rewards, via our novel and effective multi-reward approach of optimizing multiple rewards simultaneously in alternate mini-batches. Our method achieves the new state-of-the-art results on CNN/Daily Mail dataset as well as strong improvements in a test-only transfer setup on DUC-2002.
section: Introduction
Abstractive summarization, the task of generating a natural short summary of along document, is more challenging than the extractive paradigm, which only involves selection of important sentences or grammatical sub-sentences). Advent of sequence-to-sequence deep neural networks and large human summarization datasets () made the abstractive summarization task more feasible and accurate, with recent ideas ranging from copypointer mechanism and redundancy coverage, to metric reward based reinforcement learning.
A good abstractive summary requires several important properties, e.g., it should choose the most salient information from the input document, be logically entailed by it, and avoid redundancy.
Coverage-based models address the latter redundancy issue), but there is still a lot of scope to teach current state-of-the-art models about saliency and logical entailment. Towards this goal, we improve the task of abstractive summarization via a reinforcement learning approach with the introduction of two novel rewards: 'ROUGESal' and 'Entail', and also demonstrate that these saliency and entailment skills allow for better generalizability and transfer.
Our ROUGESal reward gives higher weight to the important, salient words in the summary, in contrast to the traditional ROUGE metric which gives equal weight to all tokens. These weights are obtained from a novel saliency scorer, which is trained on a reading comprehension dataset's answer spans to give a saliency-based probability score to every token in the sentence. Our Entail reward gives higher weight to summaries whose sentences logically follow from the ground-truth summary. Further, we also add a length normalization constraint to our Entail reward, to importantly avoid misleadingly high entailment scores to very short sentences.
Empirically, we show that our new rewards with policy gradient approaches perform significantly better than a cross-entropy based state-of-the-art pointer-coverage baseline. We show further performance improvements by combining these rewards via our novel multi-reward optimization approach, where we optimize multiple rewards simultaneously in alternate mini-batches (hence avoiding complex scaling and weighting issues in reward combination), inspired from how humans take multiple concurrent types of rewards (feedback) to learn a task. Overall, our methods achieve the new state-of-the-art on the CNN/Daily Mail dataset as well as strong improvements in a testonly transfer setup on DUC-2002. Lastly, we present several analyses of our model's saliency, entailment, and abstractiveness skills.
section: Related Work
Earlier summarization work was based on extraction and compression-based approaches, with more focus on graph-based) and discourse tree-based) models. Recent focus has shifted towards abstractive, rewriting-based summarization based on parse trees, Abstract Meaning Representations (, and neural network models with pointercopy mechanism and coverage, as well as reinforcebased metric rewards (). We also use reinforce-based models, but with novel reward functions and better simultaneous multi-reward optimization methods.
Recognizing Textual Entailment (RTE), the task of classifying two sentences as entailment, contradiction, or neutral, has been used for Q&A and IE tasks (). Recent neural network models and large datasets) enabled stronger accuracies. Some previous work has explored the use of RTE by modeling graphbased relationships between sentences to select the most non-redundant sentences for summarization. Recently, Pasunuru and Bansal (2017) improved video captioning with entailment-corrected rewards. We instead directly use multi-sentence entailment knowledge (with additional length constraints) as a separate RL reward to improve abstractive summarization, while avoiding their penalty hyperparameter tuning.
For our saliency prediction model, we make use of the SQuAD reading comprehension dataset (, where the answer spans annotated by humans for important questions, serve as an interesting and effective proxy for keyphrase-style salient information in summarization. Some related previous work has incorporated document topic/subject classification) and webpage keyphrase extraction () to improve saliency in summarization. Some recent work has also used answer probabilities in a document to improve question generation.
section: Models
section: Baseline Sequence-to-Sequence Model
Our abstractive text summarization model is a simple sequence-to-sequence single-layer bidirectional encoder and unidirectional decoder LSTM-RNN, with attention (, pointer-copy, and coverage mechanism -please refer to for details.
section: Policy Gradient Reinforce
Traditional cross-entropy loss optimization for sequence generation has an exposure bias issue and the model is not optimized for the evaluated metrics (. Reinforce-based policy gradient approach addresses both of these issues by using its own distribution during training and by directly optimizing the non-differentiable evaluation metrics as rewards. We use the RE-INFORCE algorithm) to learn a policy p θ defined by the model parameters θ to predict the next action (word) and update its internal (LSTM) states. We minimize the loss function
where w sis the sequence of sampled words with w st sampled at time step t of the decoder. The derivative of this loss function with approximation using a single sample along with variance reduction with a bias estimator is:
There are several ways to calculate the baseline estimator; we employ the effective SCST approach (, as depicted in, where be = r(w a ), is based on the reward obtained by the current model using the test time inference algorithm, i.e., choosing the arg-max word w at of the final vocabulary distribution at each time step t of the decoder. We use the joint cross-entropy and reinforce loss so as to optimize the non-differentiable evaluation metric as reward while also maintaining the readability of the generated sentence (, which is defined as
where γ is a tunable hyperparameter.
section: Multi-reward Optimization
Optimizing multiple rewards at the same time is important and desired for many language generation tasks. One approach would be to use a weighted combination of these rewards, but this has the issue of finding the complex scaling and weight balance among these reward combinations.
To address this issue, we instead introduce a simple multi-reward optimization approach inspired from multi-task learning, where we have different tasks, and all of them share all the model parameters while having their own optimization function (different reward functions in this case). If r 1 and r 2 are two reward functions that we want to optimize simultaneously, then we train the two loss functions of Eqn. 2 in alternate mini-batches.
4 Rewards ROUGE Reward The first basic reward is based on the primary summarization metric of ROUGE package). Similar to, we found that ROUGE-L metric as a reward works better compared to ROUGE-1 and ROUGE-2 in terms of improving all the metric scores. 1 Since these metrics are based on simple phrase matching/n-gram overlap, they do not focus on important summarization factors such as salient phrase inclusion and directed logical entailment. Addressing these issues, we next introduce two new reward functions. Saliency Rewards ROUGE-based rewards have no knowledge about what information is salient in the summary, and hence we introduce a novel reward function called 'ROUGESal' which gives higher weight to the important, salient words/phrases when calculating the ROUGE score (which by default assumes all words are equally weighted). To learn these saliency weights, we train our saliency predictor on sentence and answer spans pairs from the popular SQuAD reading comprehension dataset () (Wikipedia domain), where we treat the humanannotated answer spans (avg. span length 3.2) for important questions as representative salient information in the document. As shown in, given a sentence as input, the predictor assigns a saliency probability to every token, using a simple bidirectional encoder with a softmax layer at every time step of the encoder hidden states to classify the token as salient or not. Finally, we use the probabilities given by this saliency prediction model as weights in the ROUGE matching formulation to achieve the final ROUGESal score (see appendix for details about our ROUGESal weighted precision, recall, and F-1 formulations).
Entailment Rewards A good summary should also be logically entailed by the given source document, i.e., contain no contradictory or unrelated information. used entailment-corrected phrase-matching metrics (CIDEnt) to improve the task of video captioning; we instead directly use the entailment knowledge from an entailment scorer and its multisentence, length-normalized extension as our 'Entail' reward, to improve the task of abstractive text summarization. We train the entailment classi-
section: Evaluation Metrics
We use the standard ROUGE package
section: Results
Baseline Cross-entropy Model Our abstractive summarization model has attention, pointer-copy, and coverage mechanism. First, we apply crossentropy optimization and achieve comparable re-2 Since the GT summary is correctly entailed by the source document, we directly (by transitivity) use this GT as premise for easier (shorter) encoding. We also tried using the full input document as premise but this didn't perform as well (most likely because the entailment classifiers are not trained on such long premises; and the problem with the sentence-tosentence avg. scoring approach is discussed below). We also tried summary-to-summary entailment scoring (similar to ROUGE-L) as well as pairwise sentence-to-sentence avg. scoring, but we found that avg. scoring of groundtruth summary (as premise) w.r.t. each generated summary's sentence (as hypothesis) works better (intuitive because each sentence in generated summary might be a compression of multiple sentences of GT summary or source document).) improves the performance on CNN/Daily Mail in all metrics with stat. significant scores (p < 0.001) as compared to the cross-entropy baseline (and also stat. signif. w.r.t.). Similar to, we use mixed loss function (XE+RL) for all our reinforcement experiments, to ensure good readability of generated summaries.
ROUGESal and Entail Rewards With our novel ROUGESal reward, we achieve stat. signif. improvements in all metrics w.r.t. the baseline as well as w.r.t. ROUGE-reward results (p < 0.001), showing that saliency knowledge is strongly improving the summarization model. For our Entail reward, we achieve stat. signif. improvements in ROUGE-L (p < 0.001) w.r.t. baseline and achieve the best METEOR score by a large margin. See Sec. 7 for analysis of the saliency/entailment skills learned by our models.
Multi-Reward Results Similar to ROUGESal, Entail is a better reward when combined with the complementary phrase-matching metric information in ROUGE; shows that the ROUGE+Entail multi-reward combination performs stat. signif. better than ROUGE-reward in ROUGE-1, ROUGE-L, and METEOR (p < 0.001), and better than Entail-reward in all Models R-1 R-2 R-L M Baseline 35.50 14.57 32.19 14.36 ROUGE 35  ROUGE metrics. Finally, we combined our two rewards ROUGESal+Entail to incorporate both saliency and entailment knowledge, and it gives the best results overall (p < 0.001 in all metrics w.r.t. both baseline and ROUGE-reward models), setting the new state-of-the-art. Test-Only Transfer (DUC-2002) Results Finally, we also tested our model's generalizability/transfer skills, where we take the models trained on CNN/Daily Mail and directly test them on DUC-2002 in a test-only setup. As shown in, our final ROUGESal+Entail multi-reward RL model is statistically significantly better than both the cross-entropy (pointer-generator + coverage) baseline as well as ROUGE reward RL model, in terms of all 4 metrics with a large margin (with p < 0.001). This demonstrates that our ROUGESal+Entail model learned better transferable and generalizable skills of saliency and logical entailment.
section: Output Analysis
Saliency Analysis We analyzed the output summaries generated by, and our baseline, ROUGE-reward and ROUGESal-reward models, using our saliency prediction model, and the scores are 27.95%, 28.00%, 28.80%, and 30.86%. We also used the original CNN/Daily Mail Cloze Q&A setup () with the fill-in-the-blank answers treated as salient information, and the results are 60.66%, 59.36%, 60.67%, and 64.66% for the four models. Both these experiments illustrate that our ROUGESal reward model is stat. signif. better in saliency than the, our baseline, and ROUGE-reward models (p < 0.001).
Entailment Analysis We also analyzed the entailment scores of the generated summaries from, and our baseline, ROUGEreward, and Entail-reward models, and the results are 27.33%, 27.21%, 28.23%, and 28.98%. 6 Our last three rows in are all stat. signif. better in all metrics with p < 0.001 compared to. Based on our ground-truth summary to output summary sentences' average entailment score (see Sec. 4); similar Models 2-gram 3-gram 4-gram See et al. 2.24 6.03 9.72 Baseline 2.23 5.58 8.81 ROUGE 2.69 6.57 10.23 ROUGESal 2.37 6.00 9.50 Entail 2.63 6.56 10.26: Abstractiveness: novel n-gram percentage.
We observe that our Entail-reward model achieves stat. significant entailment scores (p < 0.001) w.r.t. all the other three models.
Abstractiveness Analysis In order to measure the abstractiveness of our models, we followed the 'novel n-gram overlap' approach suggested in. First, we found that all our rewardbased RL models have significantly (p < 0.01) more novel n-grams than our cross-entropy baseline (see). Next, the Entail-reward model 'maintains' stat. equal abstractiveness as the ROUGE-reward model, likely because it encourages rewriting to create logical subsets of information, while the ROUGESal-reward model does a bit worse, probably because it focuses on copying more salient information (e.g., names). Compared to previous work (, our Entailreward and ROUGE-reward models achieve statistically significant improvement (p < 0.01) while ROUGESal is comparable.
section: Conclusion
We presented a summarization model trained with novel RL reward functions to improve the saliency and directed logical entailment aspects of a good summary. Further, we introduced the novel and effective multi-reward approach of optimizing multiple rewards simultaneously in alternate minibatches. We achieve the new state-of-the-art on CNN/Daily Mail and also strong test-only improvements on a DUC-2002 transfer setup.
Yongzheng Zhang, Nur Zincir-Heywood, and Evangelos Milios. 2004. World wide website summarization. Web Intelligence and Agent Systems: An International Journal 2(1):39-53.
section: A Supplementary Material
section: A.1 Saliency Rewards
Here, we describe the ROUGE-L formulation at summary-level and later describe how we incorporate saliency information into it. Given a reference summary of u sentences containing a total of m tokens ({w r,k } m k=1 ) and a generated summary of v sentences with a total of n tokens ({w c,k } n k=1 ), let r i be the reference summary sentence and c j be the generated summary sentence. Then, the precision (P lcs ), recall (R lcs ), and F-score (F lcs ) for ROUGE-L are defined as follows:
where LCS ∪ takes the union Longest Common Subsequence (LCS) between a reference summary sentence r i and every generated summary sentence c j (c j ∈ C), and β is defined in Lin (2004).
In the above ROUGE-L scores, we assume that every token has equal weight, i.e, 1. However, every summary has salient tokens which should be rewarded with more weight. Hence, we use the weights obtained from our novel saliency predictor to modify the ROUGE-L scores with salient information as follows:
where η(w) is the weight assigned by the saliency predictor for token w, and β is defined in Lin (2004). Let {w k } p k=1 be the union LCS set, then LCS * ∪ (r i , C) is defined as follows:
section: A.2 Experimental Setup
section: A.2.1 Datasets CNN/Daily Mail Dataset CNN/Daily
Mail dataset () is a collection of online articles and their summaries. The summaries are based on the human written highlights of these articles. The dataset has 287, 226 training pairs, 13, 368 validation pairs, and 11, 490 test pairs. We use the non-anonymous version of the dataset as described in. SQuAD Dataset We use Stanford Question Answering Dataset (SQuAD) for our saliency prediction model. We process the SQuAD dataset to collect the sentence and their corresponding salient phrases pairs. Here again, we use the standard split following previous work.
section: DUC Test Corpus
section: A.2.2 Training Details
During training, all our LSTM-RNNs are set with hidden state size of 256. We use a vocabulary size of 50k, where word embeddings are represented in 128 dimension, and both the encoder and decoder share the same embedding for each word. We encode the source document using a 400 timestep unrolled LSTM-RNN and 100 time-step unrolled LSTM-RNN for decoder. We clip the gradients to a maximum gradient norm value of 2.0 and use Adam optimizer () with a learning rate of 1 × 10 −3 for pointer baseline and 1 × 10 −4 while training along with coverage loss, and presents the performance of our saliency predictor (on the SQuAD-based dev set for answer span classification accuracy) and entailment classifier (on the Multi-NLI dev set accuracy). Our entailment classifier is comparable to the state-ofthe-art models. 9
section: A.3 Results
section: A.3.1 Saliency and Entailment Scorer
