section: title
Focused Hierarchical RNNs for Conditional Sequence Processing
section: abstract
Recurrent Neural Networks (RNNs) with attention mechanisms have obtained state-of-the-art results for many sequence processing tasks. Most of these models use a simple form of encoder with attention that looks over the entire sequence and assigns a weight to each token independently. We present a mechanism for focus-ing RNN encoders for sequence modelling tasks which allows them to attend to key parts of the input as needed. We formulate this using a multi-layer conditional sequence encoder that reads in one token at a time and makes a discrete decision on whether the token is relevant to the context or question being asked. The discrete gating mechanism takes in the context embedding and the current hidden state as inputs and controls information flow into the layer above. We train it using policy gradient methods. We evaluate this method on several types of tasks with different attributes. First, we evaluate the method on synthetic tasks which allow us to evaluate the model for its generalization ability and probe the behavior of the gates in more controlled settings. We then evaluate this approach on large scale Question Answering tasks including the challenging MS MARCO and SearchQA tasks. Our models shows consistent improvements for both tasks over prior work and our baselines. It has also shown to generalize significantly better on synthetic tasks as compared to the baselines.
section: Introduction
Recurrent Neural Networks (RNNs) with attention are wildly used for many sequence modeling tasks, such as: image captioning (, speech recognition (), text summarization () and Question and Answering (QA) (). The attention mechanism allows the model to look over the entire sequence and pickup the most relevant information. This not only allows the model to learn a dynamic summarization of the input sequence, it allows gradients to be passed directly to the earlier time-steps in the input sequence, which also helps with the vanishing and exploding gradient problem).
Most of these models use a simple form of encoder with attention that is identical to the first one proposed (, where the attention looks over the entire encoded sequence and assigns a soft weight to each token. However, for more complex tasks we conjecture that more structured encoding mechanisms may help the attention to more effectively identify and selectively process relevant information within the input.
Imagine reading a Wikipedia article and trying to identify information that is relevant to answering a question before one knows what the question is. Now, compare this to the situation where the context or question is given before reading the article. It would be much easier to read over the article, identify relevant information, group items and selectively process relevant information based on its relevance to a given context or question.
Keeping this intuition in mind, we have developed a focused RNN encoder that is modeled by a multi-layer RNN that groups input sub-sequences based on gates that are controlled or conditioned on a question or input context. We dub the core part of the minimal form of this model a focused hierarchical encoder module. Our approach represents a general framework that applies to many sequence modeling tasks where the context or question can be beneficial to focus (attend) over the input sequence. Our focused encoder module examined here is based on a two layer LSTM where the upper layer is updated when a group of relevant tokens has been read. The boundaries of the group are computed using a discrete gating mechanism that takes as input the lower and upper level units as well as the context or question, and it is trained using policy gradient methods.
We evaluate our model on several tasks of different levels of complexity. We began with toy tasks with limited vocabulary size, where we analyze the performance, the generalization ability as well as the gating and the attention mechanisms. We then move onto challenging large scale QA tasks such as MS MARCO) and SearchQA (. Our model outperforms the baseline for both tasks. For the SearchQA task, it significantly outperforms recently proposed methods).
The key contributions of our work are the following:
• We explore the use of a conditional discrete stochastic boundary gating mechanism that helps the encoder to focus on parts relevant to the context, we use a soft attention to look over the relevant states.
• We use a reinforcement learning approach to learn the boundary gating mechanism.
• The elements above form the building blocks of our proposed focused hierarchical encoder module, we examine its properties with synthetic data experiments and we show the benefits of using it for QA tasks.
Our model takes an input sequence, a question or context sequence and generates an answer. It can be applied to any sequence tasks where the context or question is beneficial to modulating the processing of an input sequence.
section: Focused Hierarchical RNN
section: Architecture
Our model consists of: the focused hierarchical encoder (FHE), the context encoder and the decoder. Compared to a regular RNN with attention, we replace the encoder with a context-aware focused RNN encoder. The focused hierarchical encoder is modeled by a twolayer LSTM. The lower layer operates at the input token level, while the upper layer focuses on tokens relevant to the context. We train a conditional boundary gate to decide, depending on the context or question, whether it is useful to update the upper-level LSTM with a summary of the current set of tokens or not.
Lower-level Layer As shown in, FHE has two layers. Let P = (x 1 , . . . , x n ) be the sequence of input tokens, ht be the LSTM hidden state and ct be the LSTM cell state at time t. To make our model as generic as possible, the lower-level layer maybe also augmented with other available information. The question for large QA tasks are non-trivial and hence we augment the lower-layer inputs with the question encoding at each step.
Conditional Boundary Gate For each token in the passage, the boundary gate decides if information at the current time step should be stored in the upper-level representation. We hypothesize that the question is essential in deciding how to represent the passage. To capture this dependency, the boundary gate computation is conditioned on the question embedding q. The question embedding can vary in complexity depending on the difficulty of the task. In the simplest setting, the question embedding is simply a retrieved vector.
The output of the boundary gate is a scalar b t ∈ (0, 1) that is taken to be the parameter of a Bernoulli distributioñ b t ∼ Bernoulli(b t ) that regulates the gate's opening at time step t. In the simplest case, the boundary gate forward pass is formulated as
where W b , b band w bare trainable weights, LReLU(·) is a leaky ReLU activation, and z t is the input that varies and depends on the task. In our experiments we used the following input
where is the element-wise product. Hence, we essentially use three groups of features: question representation multiplied with lower-layer hidden states, question, and lower-layer representations. These three groups are concatenated together and passed through an MLP to yield boundary gate decisions (i.e., open/close).
Ina more complex task that has stronger dependency on the upper-level hidden states (see the following paragraph), these can also be used to augment the boundary gate input z t .
Upper-level Layer The upper-layer LSTM states (denoted h u t ) update only when the corresponding lower-layer boundary gate is open.
Final Output The final output of FHE is a sequence of lower-level states H l = {h l 1 , . . . , h l n } and a sequence of upper-level states H u = {h u 1 , . . . , h u n }, where only k of them are unique and k = t ˜ b t , the number of times the boundary gates open over the length of the document. The upper-level states H u are typically the only ones being processed by the downstream modules. Hence, all downstream operations are performed faster than if they had to process H l (the effective size of H u is smaller than the size of H l ).
section: Training
We train our model to maximize the log-likelihood of the answer (A) given the context (Q) and the passage (P ), using the output probabilities given by our answer decoder:
Policy Gradient The discrete decisions involved in sampling the boundary variable˜bvariable˜variable˜b t make it impossible to use standard gradient back-propagation to learn the parameters of the boundary gate. We instead apply REINFORCE-style estimators. Denote by π b the model policy over the binary vector of decisions b = { ˜ b 1 , . . . , ˜ b n }. We need to take the derivative:
where the reward Rb can be formulated differently depending on the task. In our synthetic experiments (Section 4),
. For large scale natural language QA tasks (Section 5), we use Rb = log p(A | Q, P, b). The aforementioned gradient can be approximated by sampling from the policy π band computing the corresponding terms.
Rewards We use the final reward Rb for each decision in the sequence. Following previous work, we add an exploration term αH(π b ) that prevents the policy from collapsing too soon during training. The α is a hyperparameter to beset.
section: Sparsity Constraints
We add a constraint on the sparsity of the upper-level representations. We want the model to avoid grouping each token on its own and storing information at each step on the upper level (i.e., always opening the boundary gates). As a remedy we add a small penalty G(b) the model needs to pay for storing information at the upper level. In practice, we found the following formulation to work the best:
where β > 0 and γ ∈ [0, 1) are hyper-parameters and T is the input sequence length. Hence, β is the strength of penalty and γ is the proportion of the time the gates could open without being penalized. Intuitively, we let a certain number of gates until a open threshold γT without any penalty. Each open gate above the threshold is penalized. This is the same as constraining the policy to act within a certain region. One can skip γ (by setting γ = 0) and then the penalty is just βb t applied at each time step:
Note that hyper-parameters β and γ directly affect the sparsity of upper-level representations that can be formally defined as the average value of˜bof˜ of˜b t and will be called gate openness.
section: Related Work
As we have discussed above, our focused hierarchical encoder is modeled by a hierarchical RNN controlled by gates conditioned on an input context or question. The idea of using hierarchical RNNs to model data in which long term dependencies must be captured was first explored in. The HML-STM extends a 3-layered LSTM to have multiple gates at each time step, which decide which of the LSTM layers should be updated, and has been applied to unconditional character-level language modelling. In contrast we learn a context-conditional sequence segmentation that only encodes relevant information to the context; this information is fed to an attention mechanism to help with identifying the most relevant information.
The upper states of our network can be considered as a memory focusing on relevant information that can be attended to at each step of the answer generation process. In particular, we use a soft-attention mechanism (, which has become ubiquitous in conditional language generation systems. and use a layered, hierarchical attention in that they attend to both word and sentence level representations.
We use similar ideas but we learn how to attend to information within the sequence structure rather than relying on a fixed strategy. Another form of structured encoding encoding mechanism would be the, where the attention is separate into pairs of key and value. The key corresponds to the attention distribution and the value is used to encode the context.
section: Synthetic Experiments
We first study two synthetic tasks that allow us to analyze our proposed gating, attention mechanism and its generalization ability, and then in Section 5 we study the more complex tasks of natural language question and answering.
The synthetic tasks are the picking task and the Pixel-byPixel MNIST QA task. For the picking task, we analyze the gating mechanism and show how the model utilizes the question (context) to dynamically group the passage tokens and how the attention mechanism utilizes this information. We also test the generalization ability our model following the setup in (). For the Pixel-by-Pixel MNIST QA task, we show better accuracy with our FHE module over the baseline. The tasks are chosen due to the natural of the tasks. The gating mechanism for the picking task depends solely on the question, whereas the gating mechanism for the Pixel-by-Pixel MNIST QA task is independent of the question, but solely dependent on the data.
We compare the performance of our focused hierarchical encoder module to two baseline architectures: a 1-layer LSTM (LSTM1) and a 2-layer LSTM (LSTM2) 1 .
For the picking task, FHE utilizes less memory compare to LSTM2, as the baseline LSTM2 model needs to store and attend overall states, whereas FHE only needs to attend to unique elements of H u . For example, when gate openness is below 10%, the attention module for FHE only attends to than 10% of memory compared to a LSTM2 baseline model.. Sample points for picking task (sequence length n = 30).
The first k digits are underlined and the target mode is bolded.
section: INPUT TARGET SEQUENCE
Hyper-parameters All models (FHE, LSTM1 and LSTM2) fora certain task has the same number of hidden units (256 for picking task and 128 for Pixel-by-Pixel MNIST QA task). In FHE module we used α = 0 hence, we did not use exploration term mentioned in Section 2.2. Instead, we used simpler idea that is sufficient in the synthetic experiments conducted -we add a small value to b t (0.01 for picking task and 0.1 for Pixel-by-Pixel MNIST QA task) to encourage exploration. The values of β and γ depend on the task and are provided later. Learning rates used for all models are 0.0001 with the Adam optimizer).
section: Picking task
Given a sequence of randomly generated digits of length n, the goal of the picking task is to determine the most frequent digit within the first k digits 2 , where k ≤ n. Hence, the value of k is understood as the question. We study three tasks with input sequences of n ∈ {100, 200, 400} digits respectively. Sample points for the task are presented in.
The input digits xi are one-hot encoded vectors (size 10) and the question embedding q is a vector retrieved from the lookup table (that is learnt during training) with n entries. To obtain the final sequence representation, soft attention (as in) is applied on the upper-level states H u (for FHE and LSTM2) or the lower-level states H l (for LSTM1). Finally, the representation is concatenated with the question embedding and fed to one layer feed-forward neural network to produce the final prediction (i.e. probabilities for all 10 classes).
As introduced in Section 2.2, there are two hyperparameters (β and γ) that affect the sparsity of higher-level representations in FHE. We explore two approaches for determining their values.
One approach is to fix these hyper-parameters to a small. Accuracy (%) for picking task for LSTM1, LSTM2 and FHE-fixed. Our model and LSTM2 are on par with performing while LSTM1 is behind for longer input sequences.   value (for example β = 0.1 and γ = 0.25) in the beginning of training, such that the gates can almost freely open. Once the desired accuracy has been reached, we enforce constraints on our hyper-parameters. This provides a level of control over the accuracy-sparsity tradeoff -we used this approach with the requirement of achieving a desired accuracy a. We tested FHE models with a ∈ {80%, 90%, 95%, 98%} and call them FHE80, FHE90, etc. The relationship between accuracy and gate openness is visualized in.
section: LENGTH
Another approach is to set β and γ to a fixed value from the start, so the gate openness of the model is more restricted right from the start. We find that the model performs better with fixed the hyper-parameters (the results for β = 1 and γ = 10% are presented in as FHE-fixed).
The results achieved for each model and sequence length are presented in. For each setup at least two runs were performed and the difference in result between the pair were typically neglectable (< 0.5%).
The picking task is useful to validate our gating mechanism. Once trained we can inspect the positions of the opened gates. shows that our model learns to open gates around k'th step only and attend a single gate right after the k'th step. The lower-level LSTM is used to count the occurrences of the various digits. That information is then passed to the upper-level LSTM at a single gate. The atten-. A relationship between accuracy and gate openness for picking task and sequence length n = 100. The best performance is achieved for gate openness around 10%.. Gate openness (G) conditioned on the position asked (P). Focus (F) is the average of final attention weight set fora given step. Hence, focus sums to one and it is always lower than gate openness (because our model attends only over unique states). Result showed for sequence length n = 200. The first four plots illustrate FHE model having 99.4% accuracy and 10% gate openness, while the last four are for FHE model having 97% accuracy but 5% gate openness.. A visualization of the gating mechanism learned using the Pixel-by-Pixel MNIST dataset. Red pixels indicate agate opening and are overlayed on top of the digit which is white on a gray background. The digits are vectorized row-wise which explains why white pixels appear left of the red pixels.. Accuracy (%) for validation set of Pixel-by-Pixel MNIST QA task. Our model slightly outperform both LSTM1 and LSTM2.
section: LSTM1 LSTM2 FHE-FIXED
section: 98.4 99.1
tion mechanism then uses the information around the same time step to provide the mode (i.e. solve the task).
We tested the generalization ability of FHE. The models trained on short sequences (n = 200) were evaluated on longer sequences and k ≤ 200. The results are in. The models cannot be evaluated fork larger than maximum sequence length used during training because the question embeddings are parts of the models. FHE generalizes better to longer sequences by a wide margin. We believe this is due to the boundary gates being open for only the first k-steps, and the attention mechanism not attending over possibly misleading states.
section: Pixel-by-Pixel MNIST QA task
We adapt the Pixel-by-Pixel MNIST classification task ( to the question and answering setting. The passage encoder reads in MNIST digits one pixel at a time. The question asked is whether the image is a specific digit and the answer is either True or False. The data is balanced such that approximately half of the answers are True and the other half are False.
The LSTM2 reached an accuracy of 98.4% on the validation set, and FHE 3 outperformed the baseline by having an accuracy of 99.1%. shows a visualization of the gates for the passage encoder learned by the model. The model learns to open the boundary gate almost always around the digit. We also found that for this particular task, the gates do not depend on the question. We hypothesize that this is because it is much easier for the passage encoder to learn to open the gates when there is a white pixel. In any case, these experiments illustrate how our proposed mechanism modulates gates based on input questions and features in the data.
section: Large Scale Natural Language QA Tasks
Next, we explore the more complex task of natural language question answering. We study our approach using the MS MARCO and SearchQA datasets and tasks. These tasks are well-suited for our model since they both involve searching over along input passage for answers to a question. Our results are that for the MS MARCO task, we achieved scores higher than the baseline models. Our model on SearchQA significantly outperforms very recent work (). We also run ablation studies on the model for MS MARCO task to show the importance of each component in the model.
To obtain competitive results on these difficult questionanswering tasks we embed FHE with a modified version of both the question encoder and the answer decoder. All changes with respect to what was presented earlier are detailed in the following sections.
section: Question Encoder
Following recent work (, we use a bidirectional LSTM that first reads the question and then performs self-attention to construct a vector representation of it. At the model-level, the question-encoder module outputs the vector q, which is then used as conditioning information in a FHE.
section: Decoder
The answer decoder follows the standard decoding procedure in RNN with attention (). The only difference is that the decoder looks over the upper-level hidden states h u t learned using a FHE conditioned on the question. The upper-level states H u provide an abstracted, dynamic representation of the passage. Because they receive lower-layer input only when the boundary gate is open, the resulting hidden states can be viewed as a sectional summary of the tokens between these "open" time-steps. The upper layer thus summarizes a passage in a smaller number of states. This can be beneficial because it enables the encoder LSTM to maintain information over a longer time-horizon, reduces the number of hidden states, and makes learning the subsequent attention softmax layer easier.
Pointer Softmax In order to predict the next answer word and to avoid large-vocabulary issues, we use the pointer softmax). This method decomposes as two softmaxes: one places a distribution over a shortlist of words and the other places a distribution over words in the document. The softmax parameters are W o ∈ R |V |×D hand b o ∈ R|V |, where |V | is the size of the shortlist vocabulary 4 . A switching network enables the model to learn the mixture proportions over the two distributions. Switching variable z j determines how to interpolate between the indices in the document and the shortlist words. It is computed via an MLP. Let o j be the distribution of word in the shortlist, and α j be the distribution over words in the document index. Then the pointer softmax. SearchQA results measured in F1 and Exact Match (EM) for validation and test set. Our model and AMANDA) are on par with performing while the other models are behind.. MS MARCO results using BLEU-1 and Rouge-L evaluation. Our model clearly outperforms both standard memory networks and sequence-to-sequence models. In addition in both Bleu-1 and Rouge-L, we outperform strong baselines. Available results for the first three methods are taken from their respective papers (hence the not available ones). Ablation study results show that our model benefits the most from elementwise product between questions and context. The pointer softmax also gives a significant gain for performance. Hyper-parameters All components of the model (FHE, question encoder, decoder) in all natural language QA experiments uses 300 hidden units. FHE hyper-parameters were fixed (α = 0.001, β = 0.5, γ = 50%). We use the Adam optimizer) with a learning rate of 0.001.
section: MODELS
section: SearchQA Question and Answering Task
Search QA (.
section: MS MARCO Question and Answering Task
The Microsoft Machine Reading Comprehension Dataset (MS MARCO)) is one of the largest publicly available QA datasets. Each example in the dataset consists of a query, several context passages retrieved by the Bing search engine (ten per query on average), and several human generated answers (synthesized from the given contexts).
Span-based vs Generative Most of the recent question and answering models for MS MARCO are span-based 5 (. Span-based models are currently state of the art according to Bleu-1 and Rouge scores on the MS MARCO leaderboard, but are clearly limited as they cannot answer questions where the answer is not contained in the passage. In comparison, generative models, such as ours synthesize a novel answer for the given question. Generative models could learn a disentangled representation, and therefore generalize better. Our approach takes the first step towards closing the gap between generative models and span-based models.
We report the model performance using Bleu-1 and Rouge-L, which are the standard evaluation for MS MARCO task.
QA Results reports performance evaluated using the MS MARCO dataset. Specifically, we evaluate the quality of the generated answers for different models. Our model outperforms all competing methods in terms of test set Bleu-1 and Rouge-L.
Ablation Studies shows also the results of learning our model without some of its key components. The ablation studies are evaluated on the validation set only.
The largest gain came from the elementwise-product between question and context. This result is to be expected, since it is difficult for the model to encode the appropriate information without direct knowledge of the question.
The pointer softmax is another important module of the model. The MS MARCO dataset contains many rare words, with around 90% of words appealing less than 20 times in the dataset. It is difficult for the model to generate words it has only seen a few times, and therefore the pointer-softmax provides a significant gain.
Our experiments also show the importance of learned boundaries. This results is supportive of our hypothesis that learned boundaries help with better document encoding, and therefore generates better answers.
Overall, the different components in our model are all needed to achieve the final score.
Model Exploration reports the results of our attention mechanism on an example from the MS MARCO dataset. Our attention focuses on the relevant passage (the one that contains the answer) as well as other salient phrases of the passage given the question.
Human Evaluation We performed a human evaluation study to compare answers generated by our model to answers generated by the LSTM1 baseline model in.
We randomly selected 23 test-set questions and their corresponding answers. The order of the questions are randomized for each questionnaire. We collected a total of 690 responses (30 volunteers each given 23 examples) where volunteers were shown both answers side-by-side and were asked to pick their preferred answers. 63% of the time, volunteers preferred the answers generated from our model. Volunteers are students from our lab, and were not aware of which samples came from which model.
section: Conclusion
We introduced a focusing mechanism for encoder recurrent neural networks and evaluated our approach on the popular task of natural-language question answering. Our proposed model uses a discrete stochastic gating function that conditions on a vector representation of the question to control information flow from a word-level representation to a concept-level representation of the document. We trained the gates with policy gradient techniques. Using synthetic tasks we showed that the mechanism correctly learns when to open the gates given the context (question) and the input (passage). Further, experiments on MS MARCO and SearchQA -recent large-scale QA datasets -showed that our proposed model outperforms strong baselines and in the case of SearchQA outperforms prior work.
