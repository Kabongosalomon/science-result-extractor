section: title
Relation Classification via Convolutional Deep Neural Network
section: abstract
The state-of-the-art methods used for relation classification are primarily based on statistical machine learning, and their performance strongly depends on the quality of the extracted features. The extracted features are often derived from the output of pre-existing natural language processing (NLP) systems, which leads to the propagation of the errors in the existing tools and hinders the performance of these systems. In this paper, we exploit a convolutional deep neural network (DNN) to extract lexical and sentence level features. Our method takes all of the word tokens as input without complicated pre-processing. First, the word tokens are transformed to vectors by looking up word embeddings 1. Then, lexical level features are extracted according to the given nouns. Meanwhile, sentence level features are learned using a convolutional approach. These two level features are concatenated to form the final extracted feature vector. Finally, the features are fed into a softmax classifier to predict the relationship between two marked nouns. The experimental results demonstrate that our approach significantly outperforms the state-of-the-art methods.
section: Introduction
The task of relation classification is to predict semantic relations between pairs of nominals and can be defined as follows: given a sentence S with the annotated pairs of nominals e 1 and e 2 , we aim to identify the relations between e 1 and e 2 (. There is considerable interest in automatic relation classification, both as an end in itself and as an intermediate step in a variety of NLP applications.
The most representative methods for relation classification use supervised paradigm; such methods have been shown to be effective and yield relatively high performance (. Supervised approaches are further divided into feature-based methods and kernel-based methods. Feature-based methods use a set of features that are selected after performing textual analysis. They convert these features into symbolic IDs, which are then transformed into a vector using a paradigm that is similar to the bag-of-words model 2 . Conversely, kernel-based methods require pre-processed input data in the form of parse trees (such as dependency parse trees). These approaches are effective because they leverage a large body of linguistic knowledge. However, the extracted features or elaborately designed kernels are often derived from the output of preexisting NLP systems, which leads to the propagation of the errors in the existing tools and hinders the performance of such systems (. It is attractive to consider extracting features that are as independent from existing NLP tools as possible.
To identify the relations between pairs of nominals, it is necessary to a skillfully combine lexical and sentence level clues from diverse syntactic and semantic structures in a sentence. For example, in the sentence "The [fire] e 1 inside WTC was caused by exploding e 2 ", to identify that fire and fuel are in a
Cause-Effect relationship, we usually leverage the marked nouns and the meanings of the entire sentence. In this paper, we exploit a convolutional DNN to extract lexical and sentence level features for relation classification. Our method takes all of the word tokens as input without complicated pre-processing, such as Part-of-Speech (POS) tagging and syntactic parsing. First, all the word tokens are transformed into vectors by looking up word embeddings. Then, lexical level features are extracted according to the given nouns. Meanwhile, sentence level features are learned using a convolutional approach. These two level features are concatenated to form the final extracted feature vector. Finally, the features are feed into a softmax classifier to predict the relationship between two marked nouns.
The idea of extracting features for NLP using convolutional DNN was previously explored by, in the context of POS tagging, chunking (CHUNK), Named Entity Recognition (NER) and Semantic Role Labeling (SRL). Our work shares similar intuition with that of), all of the tasks are considered as the sequential labeling problems in which each word in the input sentence is given a tag. However, our task, "relation classification", can be considered a multi-class classification problem, which results in a different objective function. Moreover, relation classification is defined as assigning relation labels to pairs of words. It is thus necessary to specify which pairs of words to which we expect to assign relation labels. For that purpose, the position features (PF) are exploited to encode the relative distances to the target noun pairs. To the best of our knowledge, this work is the first example of using a convolutional DNN for relation classification.
The contributions of this paper can be summarized as follows.
• We explore the feasibility of performing relation classification without complicated NLP preprocessing. A convolutional DNN is employed to extract lexical and sentence level features.
• To specify pairs of words to which relation labels should be assigned, position features are proposed to encode the relative distances to the target noun pairs in the convolutional DNN.
• We conduct experiments using the SemEval-2010 Task 8 dataset. The experimental results demonstrate that the proposed position features are critical for relation classification. The extracted lexical and sentence level features are effective for relation classification. Our approach outperforms the state-of-the-art methods.
section: Related Work
Relation classification is one of the most important topics in NLP. Many approaches have been explored for relation classification, including unsupervised relation discovery and supervised classification. Researchers have proposed various features to identify the relations between nominals using different methods.
In the unsupervised paradigms, contextual features are used. Distributional hypothesis theory indicates that words that occur in the same context tend to have similar meanings. Accordingly, it is assumed that the pairs of nominals that occur in similar contexts tend to have similar relations. Hasegawa et al. (2004) adopted a hierarchical clustering method to cluster the contexts of nominals and simply selected the most frequent words in the contexts to represent the relation between the nominals. proposed a novel unsupervised method based on model order selection and discriminative label identification to address this problem.
In the supervised paradigm, relation classification is considered a multi-classification problem, and researchers concentrate on extracting more complex features. Generally, these methods can be categorized into two types: feature-based and kernel-based. In feature-based methods, a diverse set of strategies have been exploited to convert the classification clues (such as sequences and parse trees) into feature vectors). Feature-based methods suffer from the problem of selecting a suitable feature set when converting the structured representation into feature vectors. Kernel-based methods provide a natural alternative to exploit rich representations of the input classification clues, such as syntactic parse trees. Kernel-based methods allow the use of a large set of features without explicitly extracting the features. Various kernels, such as the convolution tree kernel (Qian et The supervised method has been demonstrated to be effective for relation detection and yields relatively high performance. However, the performance of this method strongly depends on the quality of the designed features. With the recent revival of interest in DNN, many researchers have concentrated on using Deep Learning to learn features. In NLP, such methods are primarily based on learning a distributed representation for each word, which is also called a word embeddings (. present a novel recursive neural network (RNN) for relation classification that learns vectors in the syntactic tree path that connects two nominals to determine their semantic relationship. also use an RNN for relation classification; their method allows for the explicit weighting of important phrases for the target task. As mentioned in Section 1, it is difficult to design high quality features using the existing NLP tools. In this paper, we propose a convolutional DNN to extract lexical and sentence level features for relation classification; our method effectively alleviates the shortcomings of traditional features. describes the architecture of the neural network that we use for relation classification. The network takes an input sentence and discovers multiple levels of feature extraction, where higher levels represent more abstract aspects of the inputs. It primarily includes the following three components: Word Representation, Feature Extraction and Output. The system does not need any complicated syntactic or semantic preprocessing, and the input of the system is a sentence with two marked nouns. Then, the word tokens are transformed into vectors by looking up word embeddings. In succession, the lexical and sentence level features are respectively extracted and then directly concatenated to form the final feature vector. Finally, to compute the confidence of each relation, the feature vector is fed into a softmax classifier. The output of the classifier is a vector, the dimension of which is equal to the number of predefined relation types. The value of each dimension is the confidence score of the corresponding relation.
section: Methodology
section: The Neural Network Architecture
section: Features Remark L1 Noun 1 L2 Noun 2 L3
Left and right tokens of noun 1 L4
Left and right tokens of noun 2 L5
WordNet hypernyms of nouns: Lexical level features.
section: Word Representation
In the word representation component, each input word token is transformed into a vector by looking up word embeddings. reported that word embeddings learned from significant amounts of unlabeled data are far more satisfactory than the randomly initialized embeddings. In relation classification, we should first concentrate on learning discriminative word embeddings, which carry more syntactic and semantic information, using significant amounts of unlabeled data. Unfortunately, it usually takes along time to train the word embeddings 3 . However, there are many trained word embeddings that are freely available (. A comparison of the available word embeddings is beyond the scope of this paper. Our experiments directly utilize the trained embeddings provided by.
section: Lexical Level Features
Lexical level features serve as important cues for deciding relations. The traditional lexical level features primarily include the nouns themselves, the types of the pairs of nominals and word sequences between the entities, the quality of which strongly depends on the results of existing NLP tools. Alternatively, this paper uses generic word embeddings as the source of base features. We select the word embeddings of marked nouns and the context tokens. Moreover, the WordNet hypernyms 4 are adopted as MVRNN (). All of these features are concatenated into our lexical level features vector l. presents the selected word embeddings that are related to the marked nouns in the sentence.
section: Sentence Level Features
As mentioned in section 3.2, all of the tokens are represented as word vectors, which have been demonstrated to correlate well with human judgments of word similarity. Despite their success, single word vector models are severely limited because they do not capture long distance features and semantic compositionality, the important quality of natural language that allows humans to understand the meanings of a longer expression. In this section, we propose a max-pooled convolutional neural network to offer sentence level representation and automatically extract sentence level features. shows the framework for sentence level feature extraction. In the Window Processing component, each token is further represented as Word Features (WF) and Position Features (PF) (see section 3.4.1 and 3.4.2). Then, the vector goes through a convolutional component. Finally, we obtain the sentence level features through a non-linear transformation.
section: Word Features
Distributional hypothesis theory  The marked nouns are associated with a label y that defines the relation type that the marked pair contains. Each word is also associated with an index into the word embeddings. All of the word tokens of the sentence S are then represented as a list of vectors (x 0 , x 1 , · · · , x 6 ), where xi corresponds to the word embedding of the i-th word in the sentence. To use a context size of w, we combine the size w windows of vectors into a richer feature. For example, when we take w = 3, the WF of the third word "moving" in the sentence S is expressed as [x 2 , x 3 , x 4 ]. Similarly, considering the whole sentence, the WF can be represented as follows:
section: Position Features
Relation classification is a very complex task. Traditionally, structure features (e.g., the shortest dependency path between nominals) are used to solve this problem ( ). Apparently, it is not possible to capture such structure information only through WF. It is necessary to specify which input tokens are the target nouns in the sentence. For this purpose, PF are proposed for relation classification. In this paper, the PF is the combination of the relative distances of the current word tow 1 and w 2 . For example, the relative distances of "moving" in sentence S to "people" and "downtown" are 3 and -3, respectively. In our method, the relative distances also are mapped to a vector of dimension d e (a hyperparameter); this vector is randomly initialized. Then, we obtain the distance vectors d 1 and d 2 with respect to the relative distances of the current word tow 1 and w 2 , and PF = [d. Combining the WF and PF, the word is represented as T , which is subsequently fed into the convolution component of the algorithm.
section: Convolution
We will see that the word representation approach can capture contextual information through combinations of vectors in a window. However, it only produces local features around each word of the sentence. In relation classification, an input sentence that is marked with target nouns only corresponds to a relation type rather than predicting label for each word. Thus, it might be necessary to utilize all of the local features and predict a relation globally. When using neural network, the convolution approach is a natural method to merge all of the features. Similar to Collobert et al., we first process the output of Window Processing using a linear transformation.
X ∈ Rn 0 ×t is the output of the Window Processing task, where n 0 = w × n, n (a hyperparameter) is the dimension of feature vector, and t is the token number of the input sentence. W 1 ∈ Rn 1 ×n 0 , where n 1 (a hyperparameter) is the size of hidden layer 1, is the linear transformation matrix. We can see that the features share the same weights across all times, which greatly reduces the number of free parameters to learn. After the linear transformation is applied, the output Z ∈ Rn 1 ×t is dependent on t. To determine the most useful feature in the each dimension of the feature vectors, we perform a max operation overtime on Z.
where Z(i, ·) denote the i-th row of matrix Z. Finally, we obtain the feature vector m = {m 1 , m 2 , · · · , m n 1 }, the dimension of which is no longer related to the sentence length.
section: Sentence Level Feature Vector
To learn more complex features, we designed a non-linear layer and selected hyperbolic tanh as the activation function. One useful property of tanh is that its derivative can be expressed in terms of the function value itself:
It has the advantage of making it easy to compute the gradient in the backpropagation training procedure. Formally, the non-linear transformation can be written as
W 2 ∈ Rn 2 ×n 1 is the linear transformation matrix, where n 2 (a hyperparameter) is the size of hidden layer 2. Compared with m ∈ Rn 1 ×1 , g ∈ Rn 2 ×1 can be considered higher level features (sentence level features).
section: Output
The automatically learned lexical and sentence level features mentioned above are concatenated into a single vector f =. To compute the confidence of each relation, the feature vector f ∈ Rn 3 ×1 (n 3 equals n 2 plus the dimension of the lexical level features) is fed into a softmax classifier.
W 3 ∈ Rn 4 ×n 3 is the transformation matrix and o ∈ Rn 4 ×1 is the final output of the network, where n 4 is equal to the number of possible relation types for the relation classification system. Each output can be then interpreted as the confidence score of the corresponding relation. This score can be interpreted as a conditional probability by applying a softmax operation (see Section 3.6).
section: Backpropagation Training
The DNN based relation classification method proposed here could be stated as a quintuple θ = (X, N, W 1 , W 2 , W 3 ) 6 . In this paper, each input sentence is considered independently. Given an input example s, the network with parameter θ outputs the vector o, where the i-th component oi contains the score for relation i. To obtain the conditional probability p(i|x, θ), we apply a softmax operation overall relation types:
Given all our (suppose T ) training examples (x (i) ; y (i) ), we can then write down the log likelihood of the parameters as follows:
To compute the network parameter θ, we maximize the log likelihood J(θ) using a simple optimization technique called stochastic gradient descent (SGD). N, W 1 , W 2 and W 3 are randomly initialized and X is initialized using the word embeddings. Because the parameters are in different layers of the neural network, we implement the backpropagation algorithm: the differentiation chain rule is applied through the network until the word embedding layer is reached by iteratively selecting an example (x, y) and applying the following update rule.
section: Dataset and Evaluation Metrics
To evaluate the performance of our proposed method, we use the SemEval-2010 Task 8 dataset). The dataset is freely available 7 and contains 10,717 annotated examples, including 8,000 training instances and 2,717 test instances. There are 9 relationships (with two directions) and an undirected Other class. The following are examples of the included relationships: Cause-Effect, Component-Whole and Entity-Origin. In the official evaluation framework, directionality is taken into account. A pair is counted as correct if the order of the words in the relationship is correct. For example, both of the following instances S 1 and S 2 have the relationship Component-Whole.  However, these two instances cannot be classified into the same category because ComponentWhole(e 1 ,e 2 ) and Component-Whole(e 2 ,e 1 ) are different relationships. Furthermore, the official ranking of the participating systems is based on the macro-averaged F1-scores for the nine proper relations (excluding Other). To compare our results with those obtained in previous studies, we adopt the macroaveraged F1-score and also account for directionality into account in our following experiments 8 .
section: Experiments
In this section, we conduct three sets of experiments. The first is to test several variants via crossvalidation to gain some understanding of how the choice of hyperparameters impacts upon the performance. In the second set of experiments, we make comparison of the performance among the convolutional DNN learned features and various traditional features. The goal of the third set of experiments is to evaluate the effectiveness of each extracted feature.
section: Parameter Settings
In this section, we experimentally study the effects of the three parameters in our proposed method: the window size in the convolutional component w, the number of hidden layer 1, and the number of hidden layer 2. Because there is no official development dataset, we tuned the hyperparameters by trying different architectures via 5-fold cross-validation. In, we respectively vary the number of hyper parameters w, n 1 and n 2 and compute the F1. We can see that it does not improve the performance when the window size is greater than 3. Moreover, because the size of our training dataset is limited, the network is prone to overfitting, especially when using large hidden layers. From, we can see that the parameters have a limited impact on the results when increasing the numbers of both hidden layers 1 and 2. Because the distance dimension has little effect on the result (this is not illustrated in), we heuristically choose d e = 5. Finally, the word dimension and learning rate are the same as in. reports all the hyperparameters used in the following experiments.
Hyperparameter Window size Word dim. Distance dim. Hidden layer 1 Hidden layer 2 Learning rate Value w = 3 n = 50 de = 5 n1 = 200 n2 = 100 λ = 0.01: Hyperparameters used in our experiments.
section: Results of Comparison Experiments
To obtain the final performance of our automatically learned features, we select seven approaches as competitors to be compared with our method in: Classifier, their feature sets and the F1-score for relation classification.
proposed by. This method learns vectors in the syntactic tree path that connect two nominals to determine their semantic relationship. The MVRNN model builds a single compositional semantics for the minimal constituent, including both nominals as. It is almost certainly too much to expect a single fixed transformation to be able to capture the meaning combination effects of all natural language operators. Thus, MVRNN assigns a matrix to every word and modifies the meanings of other words instead of only considering word embeddings in the recursive procedure. illustrates the macro-averaged F1 measure results for these competing methods along with the resources, features and classifier used by each method. Based on these results, we make the following observations:
(1) Richer feature sets lead to better performance when using traditional features. This improvement can be explained by the need for semantic generalization from training to test data. The quality of traditional features relies on human ingenuity and prior NLP knowledge. It is almost impossible to manually choose the best feature sets.
(2) RNN and MVRNN contain feature learning procedures; thus, they depend on the syntactic tree used in the recursive procedures. Errors in syntactic parsing inhibit the ability of these methods to learn high quality features. RNN cannot achieve a higher performance than the best method that uses traditional features, even when POS, NER and WordNet are added to the training dataset. Compared with RNN, the MVRNN model can capture the meaning combination effectively and achieve a higher performance.
(3) Our method achieves the best performance among all of the compared methods. We also perform a t-test (p 0.05), which indicates that our method significantly outperforms all of the compared methods.  In our method, the network extract lexical and sentence level features. The lexical level features primarily contain five sets of features (L1 to L5). We performed ablation tests on the five sets of features from the lexical part of to determine which type of features contributed the most. The results are presented in, from which we can observe that our learned lexical level features are effective for relation classification. The F1-score is improved remarkably when new features are added. Similarly, we perform experiment on the sentence level features. The system achieves approximately 9.2% improvements when adding PF. When all of the lexical and sentence level features are combined, we achieve the best result.
section: The Effect of Learned Features
section: Conclusion
In this paper, we exploit a convolutional deep neural network (DNN) to extract lexical and sentence level features for relation classification. In the network, position features (PF) are successfully proposed to specify the pairs of nominals to which we expect to assign relation labels. The system obtains a significant improvement when PF are added. The automatically learned features yield excellent results and can replace the elaborately designed features that are based on the outputs of existing NLP tools.
