true	P14-1025.pdf#0.437(0.565)	Senseval 3, F1	MKWC ERR  Table 2: WSD accuracy for MKWC and HDP-WSI  on the WordNet-annotated datasets, as compared  to the upper-bound based on actual first sense in  the corpus (higher values indicate better perfor- mance; the best system in each row [other than the  FS CORPUS upper bound] is indicated in boldface).
true	P14-1025.pdf#0.555(0.693)	Senseval 3, F1	HDP - WSI ERR  Table 2: WSD accuracy for MKWC and HDP-WSI  on the WordNet-annotated datasets, as compared  to the upper-bound based on actual first sense in  the corpus (higher values indicate better perfor- mance; the best system in each row [other than the  FS CORPUS upper bound] is indicated in boldface).
true	P14-1025.pdf#0.407(0.777)	Senseval 3, F1	MKWC  Table 2: WSD accuracy for MKWC and HDP-WSI  on the WordNet-annotated datasets, as compared  to the upper-bound based on actual first sense in  the corpus (higher values indicate better perfor- mance; the best system in each row [other than the  FS CORPUS upper bound] is indicated in boldface).
true	P14-1025.pdf#0.214	Senseval 3, F1	Table 3: Sense distribution evaluation of MKWC  and HDP-WSI on the WordNet-annotated datasets,  evaluated using JS divergence (lower values indi- cate better performance; the best system in each  row is indicated in boldface).
true	P14-1025.pdf#0.363	Senseval 3, F1	HDP - WSI  Table 3: Sense distribution evaluation of MKWC  and HDP-WSI on the WordNet-annotated datasets,  evaluated using JS divergence (lower values indi- cate better performance; the best system in each  row is indicated in boldface).
true	P14-1025.pdf#0.375	Senseval 3, F1	HDP - WSI  Table 3: Sense distribution evaluation of MKWC  and HDP-WSI on the WordNet-annotated datasets,  evaluated using JS divergence (lower values indi- cate better performance; the best system in each  row is indicated in boldface).
true	P14-1025.pdf#0.514(0.895)	Senseval 3, F1	HDP - WSI  Table 4: WSD accuracy for HDP-WSI on the  Macmillan-annotated datasets, as compared to the  upper-bound based on actual first sense in the cor- pus (higher values indicate better performance; the  best system in each row [other than the FS CORPUS  upper bound] is indicated in boldface).
true	P14-1025.pdf#0.335(0.716)	Senseval 3, F1	HDP - WSI ERR  Table 4: WSD accuracy for HDP-WSI on the  Macmillan-annotated datasets, as compared to the  upper-bound based on actual first sense in the cor- pus (higher values indicate better performance; the  best system in each row [other than the FS CORPUS  upper bound] is indicated in boldface).
true	P14-1025.pdf#0.156	Senseval 3, F1	Table 5: Sense distribution evaluation of HDP- WSI on the Macmillan-annotated datasets as com- pared to corpus-and dictionary-based first sense  methods, evaluated using JS divergence (lower  values indicate better performance; the best sys- tem in each row is indicated in boldface).
true	P14-1025.pdf#0.171	Senseval 3, F1	HDP - WSI  Table 5: Sense distribution evaluation of HDP- WSI on the Macmillan-annotated datasets as com- pared to corpus-and dictionary-based first sense  methods, evaluated using JS divergence (lower  values indicate better performance; the best sys- tem in each row is indicated in boldface).
true	P14-1025.pdf#0.437(0.565)	Senseval 2, F1	MKWC ERR  Table 2: WSD accuracy for MKWC and HDP-WSI  on the WordNet-annotated datasets, as compared  to the upper-bound based on actual first sense in  the corpus (higher values indicate better perfor- mance; the best system in each row [other than the  FS CORPUS upper bound] is indicated in boldface).
true	P14-1025.pdf#0.555(0.693)	Senseval 2, F1	HDP - WSI ERR  Table 2: WSD accuracy for MKWC and HDP-WSI  on the WordNet-annotated datasets, as compared  to the upper-bound based on actual first sense in  the corpus (higher values indicate better perfor- mance; the best system in each row [other than the  FS CORPUS upper bound] is indicated in boldface).
true	P14-1025.pdf#0.407(0.777)	Senseval 2, F1	MKWC  Table 2: WSD accuracy for MKWC and HDP-WSI  on the WordNet-annotated datasets, as compared  to the upper-bound based on actual first sense in  the corpus (higher values indicate better perfor- mance; the best system in each row [other than the  FS CORPUS upper bound] is indicated in boldface).
true	P14-1025.pdf#0.214	Senseval 2, F1	Table 3: Sense distribution evaluation of MKWC  and HDP-WSI on the WordNet-annotated datasets,  evaluated using JS divergence (lower values indi- cate better performance; the best system in each  row is indicated in boldface).
true	P14-1025.pdf#0.363	Senseval 2, F1	HDP - WSI  Table 3: Sense distribution evaluation of MKWC  and HDP-WSI on the WordNet-annotated datasets,  evaluated using JS divergence (lower values indi- cate better performance; the best system in each  row is indicated in boldface).
true	P14-1025.pdf#0.375	Senseval 2, F1	HDP - WSI  Table 3: Sense distribution evaluation of MKWC  and HDP-WSI on the WordNet-annotated datasets,  evaluated using JS divergence (lower values indi- cate better performance; the best system in each  row is indicated in boldface).
true	P14-1025.pdf#0.514(0.895)	Senseval 2, F1	HDP - WSI  Table 4: WSD accuracy for HDP-WSI on the  Macmillan-annotated datasets, as compared to the  upper-bound based on actual first sense in the cor- pus (higher values indicate better performance; the  best system in each row [other than the FS CORPUS  upper bound] is indicated in boldface).
true	P14-1025.pdf#0.335(0.716)	Senseval 2, F1	HDP - WSI ERR  Table 4: WSD accuracy for HDP-WSI on the  Macmillan-annotated datasets, as compared to the  upper-bound based on actual first sense in the cor- pus (higher values indicate better performance; the  best system in each row [other than the FS CORPUS  upper bound] is indicated in boldface).
true	P14-1025.pdf#0.156	Senseval 2, F1	Table 5: Sense distribution evaluation of HDP- WSI on the Macmillan-annotated datasets as com- pared to corpus-and dictionary-based first sense  methods, evaluated using JS divergence (lower  values indicate better performance; the best sys- tem in each row is indicated in boldface).
true	P14-1025.pdf#0.171	Senseval 2, F1	HDP - WSI  Table 5: Sense distribution evaluation of HDP- WSI on the Macmillan-annotated datasets as com- pared to corpus-and dictionary-based first sense  methods, evaluated using JS divergence (lower  values indicate better performance; the best sys- tem in each row is indicated in boldface).
true	P14-1025.pdf#0.437(0.565)	SemEval 2007, F1	MKWC ERR  Table 2: WSD accuracy for MKWC and HDP-WSI  on the WordNet-annotated datasets, as compared  to the upper-bound based on actual first sense in  the corpus (higher values indicate better perfor- mance; the best system in each row [other than the  FS CORPUS upper bound] is indicated in boldface).
true	P14-1025.pdf#0.555(0.693)	SemEval 2007, F1	HDP - WSI ERR  Table 2: WSD accuracy for MKWC and HDP-WSI  on the WordNet-annotated datasets, as compared  to the upper-bound based on actual first sense in  the corpus (higher values indicate better perfor- mance; the best system in each row [other than the  FS CORPUS upper bound] is indicated in boldface).
true	P14-1025.pdf#0.407(0.777)	SemEval 2007, F1	MKWC  Table 2: WSD accuracy for MKWC and HDP-WSI  on the WordNet-annotated datasets, as compared  to the upper-bound based on actual first sense in  the corpus (higher values indicate better perfor- mance; the best system in each row [other than the  FS CORPUS upper bound] is indicated in boldface).
true	P14-1025.pdf#0.214	SemEval 2007, F1	Table 3: Sense distribution evaluation of MKWC  and HDP-WSI on the WordNet-annotated datasets,  evaluated using JS divergence (lower values indi- cate better performance; the best system in each  row is indicated in boldface).
true	P14-1025.pdf#0.363	SemEval 2007, F1	HDP - WSI  Table 3: Sense distribution evaluation of MKWC  and HDP-WSI on the WordNet-annotated datasets,  evaluated using JS divergence (lower values indi- cate better performance; the best system in each  row is indicated in boldface).
true	P14-1025.pdf#0.375	SemEval 2007, F1	HDP - WSI  Table 3: Sense distribution evaluation of MKWC  and HDP-WSI on the WordNet-annotated datasets,  evaluated using JS divergence (lower values indi- cate better performance; the best system in each  row is indicated in boldface).
true	P14-1025.pdf#0.514(0.895)	SemEval 2007, F1	HDP - WSI  Table 4: WSD accuracy for HDP-WSI on the  Macmillan-annotated datasets, as compared to the  upper-bound based on actual first sense in the cor- pus (higher values indicate better performance; the  best system in each row [other than the FS CORPUS  upper bound] is indicated in boldface).
true	P14-1025.pdf#0.335(0.716)	SemEval 2007, F1	HDP - WSI ERR  Table 4: WSD accuracy for HDP-WSI on the  Macmillan-annotated datasets, as compared to the  upper-bound based on actual first sense in the cor- pus (higher values indicate better performance; the  best system in each row [other than the FS CORPUS  upper bound] is indicated in boldface).
true	P14-1025.pdf#0.156	SemEval 2007, F1	Table 5: Sense distribution evaluation of HDP- WSI on the Macmillan-annotated datasets as com- pared to corpus-and dictionary-based first sense  methods, evaluated using JS divergence (lower  values indicate better performance; the best sys- tem in each row is indicated in boldface).
true	P14-1025.pdf#0.171	SemEval 2007, F1	HDP - WSI  Table 5: Sense distribution evaluation of HDP- WSI on the Macmillan-annotated datasets as com- pared to corpus-and dictionary-based first sense  methods, evaluated using JS divergence (lower  values indicate better performance; the best sys- tem in each row is indicated in boldface).
true	P14-1025.pdf#0.437(0.565)	SemEval 2015, F1	MKWC ERR  Table 2: WSD accuracy for MKWC and HDP-WSI  on the WordNet-annotated datasets, as compared  to the upper-bound based on actual first sense in  the corpus (higher values indicate better perfor- mance; the best system in each row [other than the  FS CORPUS upper bound] is indicated in boldface).
true	P14-1025.pdf#0.555(0.693)	SemEval 2015, F1	HDP - WSI ERR  Table 2: WSD accuracy for MKWC and HDP-WSI  on the WordNet-annotated datasets, as compared  to the upper-bound based on actual first sense in  the corpus (higher values indicate better perfor- mance; the best system in each row [other than the  FS CORPUS upper bound] is indicated in boldface).
true	P14-1025.pdf#0.407(0.777)	SemEval 2015, F1	MKWC  Table 2: WSD accuracy for MKWC and HDP-WSI  on the WordNet-annotated datasets, as compared  to the upper-bound based on actual first sense in  the corpus (higher values indicate better perfor- mance; the best system in each row [other than the  FS CORPUS upper bound] is indicated in boldface).
true	P14-1025.pdf#0.214	SemEval 2015, F1	Table 3: Sense distribution evaluation of MKWC  and HDP-WSI on the WordNet-annotated datasets,  evaluated using JS divergence (lower values indi- cate better performance; the best system in each  row is indicated in boldface).
true	P14-1025.pdf#0.363	SemEval 2015, F1	HDP - WSI  Table 3: Sense distribution evaluation of MKWC  and HDP-WSI on the WordNet-annotated datasets,  evaluated using JS divergence (lower values indi- cate better performance; the best system in each  row is indicated in boldface).
true	P14-1025.pdf#0.375	SemEval 2015, F1	HDP - WSI  Table 3: Sense distribution evaluation of MKWC  and HDP-WSI on the WordNet-annotated datasets,  evaluated using JS divergence (lower values indi- cate better performance; the best system in each  row is indicated in boldface).
true	P14-1025.pdf#0.514(0.895)	SemEval 2015, F1	HDP - WSI  Table 4: WSD accuracy for HDP-WSI on the  Macmillan-annotated datasets, as compared to the  upper-bound based on actual first sense in the cor- pus (higher values indicate better performance; the  best system in each row [other than the FS CORPUS  upper bound] is indicated in boldface).
true	P14-1025.pdf#0.335(0.716)	SemEval 2015, F1	HDP - WSI ERR  Table 4: WSD accuracy for HDP-WSI on the  Macmillan-annotated datasets, as compared to the  upper-bound based on actual first sense in the cor- pus (higher values indicate better performance; the  best system in each row [other than the FS CORPUS  upper bound] is indicated in boldface).
true	P14-1025.pdf#0.156	SemEval 2015, F1	Table 5: Sense distribution evaluation of HDP- WSI on the Macmillan-annotated datasets as com- pared to corpus-and dictionary-based first sense  methods, evaluated using JS divergence (lower  values indicate better performance; the best sys- tem in each row is indicated in boldface).
true	P14-1025.pdf#0.171	SemEval 2015, F1	HDP - WSI  Table 5: Sense distribution evaluation of HDP- WSI on the Macmillan-annotated datasets as com- pared to corpus-and dictionary-based first sense  methods, evaluated using JS divergence (lower  values indicate better performance; the best sys- tem in each row is indicated in boldface).
true	P14-1025.pdf#0.437(0.565)	SemEval 2013, F1	MKWC ERR  Table 2: WSD accuracy for MKWC and HDP-WSI  on the WordNet-annotated datasets, as compared  to the upper-bound based on actual first sense in  the corpus (higher values indicate better perfor- mance; the best system in each row [other than the  FS CORPUS upper bound] is indicated in boldface).
true	P14-1025.pdf#0.555(0.693)	SemEval 2013, F1	HDP - WSI ERR  Table 2: WSD accuracy for MKWC and HDP-WSI  on the WordNet-annotated datasets, as compared  to the upper-bound based on actual first sense in  the corpus (higher values indicate better perfor- mance; the best system in each row [other than the  FS CORPUS upper bound] is indicated in boldface).
true	P14-1025.pdf#0.407(0.777)	SemEval 2013, F1	MKWC  Table 2: WSD accuracy for MKWC and HDP-WSI  on the WordNet-annotated datasets, as compared  to the upper-bound based on actual first sense in  the corpus (higher values indicate better perfor- mance; the best system in each row [other than the  FS CORPUS upper bound] is indicated in boldface).
true	P14-1025.pdf#0.214	SemEval 2013, F1	Table 3: Sense distribution evaluation of MKWC  and HDP-WSI on the WordNet-annotated datasets,  evaluated using JS divergence (lower values indi- cate better performance; the best system in each  row is indicated in boldface).
true	P14-1025.pdf#0.363	SemEval 2013, F1	HDP - WSI  Table 3: Sense distribution evaluation of MKWC  and HDP-WSI on the WordNet-annotated datasets,  evaluated using JS divergence (lower values indi- cate better performance; the best system in each  row is indicated in boldface).
true	P14-1025.pdf#0.375	SemEval 2013, F1	HDP - WSI  Table 3: Sense distribution evaluation of MKWC  and HDP-WSI on the WordNet-annotated datasets,  evaluated using JS divergence (lower values indi- cate better performance; the best system in each  row is indicated in boldface).
true	P14-1025.pdf#0.514(0.895)	SemEval 2013, F1	HDP - WSI  Table 4: WSD accuracy for HDP-WSI on the  Macmillan-annotated datasets, as compared to the  upper-bound based on actual first sense in the cor- pus (higher values indicate better performance; the  best system in each row [other than the FS CORPUS  upper bound] is indicated in boldface).
true	P14-1025.pdf#0.335(0.716)	SemEval 2013, F1	HDP - WSI ERR  Table 4: WSD accuracy for HDP-WSI on the  Macmillan-annotated datasets, as compared to the  upper-bound based on actual first sense in the cor- pus (higher values indicate better performance; the  best system in each row [other than the FS CORPUS  upper bound] is indicated in boldface).
true	P14-1025.pdf#0.156	SemEval 2013, F1	Table 5: Sense distribution evaluation of HDP- WSI on the Macmillan-annotated datasets as com- pared to corpus-and dictionary-based first sense  methods, evaluated using JS divergence (lower  values indicate better performance; the best sys- tem in each row is indicated in boldface).
true	P14-1025.pdf#0.171	SemEval 2013, F1	HDP - WSI  Table 5: Sense distribution evaluation of HDP- WSI on the Macmillan-annotated datasets as com- pared to corpus-and dictionary-based first sense  methods, evaluated using JS divergence (lower  values indicate better performance; the best sys- tem in each row is indicated in boldface).
true	D15-1089.pdf#0.927*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	* word PL04 kernel  Table 1: Accuracy results on the test set for PL05 (20%), standard test split for Twitter (25%) and News  (50%) and from 5-fold CV for the other datasets (Amazon, PL04 and MPQA). Bold font marks the best  performance in the column. * indicates statistical significance at p < 0.01 using micro sign test against  the bigram baseline (delta word kernel) of the same column and with the same phrase definition.
true	D15-1089.pdf#0.797	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	word MPQA kernel  Table 1: Accuracy results on the test set for PL05 (20%), standard test split for Twitter (25%) and News  (50%) and from 5-fold CV for the other datasets (Amazon, PL04 and MPQA). Bold font marks the best  performance in the column. * indicates statistical significance at p < 0.01 using micro sign test against  the bigram baseline (delta word kernel) of the same column and with the same phrase definition.
true	D15-1089.pdf#0.813	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	* word Amazon kernel  Table 1: Accuracy results on the test set for PL05 (20%), standard test split for Twitter (25%) and News  (50%) and from 5-fold CV for the other datasets (Amazon, PL04 and MPQA). Bold font marks the best  performance in the column. * indicates statistical significance at p < 0.01 using micro sign test against  the bigram baseline (delta word kernel) of the same column and with the same phrase definition.
true	D15-1089.pdf#0.806*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	word News kernel  Table 1: Accuracy results on the test set for PL05 (20%), standard test split for Twitter (25%) and News  (50%) and from 5-fold CV for the other datasets (Amazon, PL04 and MPQA). Bold font marks the best  performance in the column. * indicates statistical significance at p < 0.01 using micro sign test against  the bigram baseline (delta word kernel) of the same column and with the same phrase definition.
true	D15-1089.pdf#0.801	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	* word PL05 kernel  Table 1: Accuracy results on the test set for PL05 (20%), standard test split for Twitter (25%) and News  (50%) and from 5-fold CV for the other datasets (Amazon, PL04 and MPQA). Bold font marks the best  performance in the column. * indicates statistical significance at p < 0.01 using micro sign test against  the bigram baseline (delta word kernel) of the same column and with the same phrase definition.
true	D15-1089.pdf#0.808	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	* word Twitter kernel  Table 1: Accuracy results on the test set for PL05 (20%), standard test split for Twitter (25%) and News  (50%) and from 5-fold CV for the other datasets (Amazon, PL04 and MPQA). Bold font marks the best  performance in the column. * indicates statistical significance at p < 0.01 using micro sign test against  the bigram baseline (delta word kernel) of the same column and with the same phrase definition.
true	D15-1089.pdf#0.927	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	word PL04 kernel  Table 1: Accuracy results on the test set for PL05 (20%), standard test split for Twitter (25%) and News  (50%) and from 5-fold CV for the other datasets (Amazon, PL04 and MPQA). Bold font marks the best  performance in the column. * indicates statistical significance at p < 0.01 using micro sign test against  the bigram baseline (delta word kernel) of the same column and with the same phrase definition.
true	D15-1089.pdf#0.927*	SearchQA, N-gram F1	* word PL04 kernel  Table 1: Accuracy results on the test set for PL05 (20%), standard test split for Twitter (25%) and News  (50%) and from 5-fold CV for the other datasets (Amazon, PL04 and MPQA). Bold font marks the best  performance in the column. * indicates statistical significance at p < 0.01 using micro sign test against  the bigram baseline (delta word kernel) of the same column and with the same phrase definition.
true	D15-1089.pdf#0.797	SearchQA, N-gram F1	word MPQA kernel  Table 1: Accuracy results on the test set for PL05 (20%), standard test split for Twitter (25%) and News  (50%) and from 5-fold CV for the other datasets (Amazon, PL04 and MPQA). Bold font marks the best  performance in the column. * indicates statistical significance at p < 0.01 using micro sign test against  the bigram baseline (delta word kernel) of the same column and with the same phrase definition.
true	D15-1089.pdf#0.813	SearchQA, N-gram F1	* word Amazon kernel  Table 1: Accuracy results on the test set for PL05 (20%), standard test split for Twitter (25%) and News  (50%) and from 5-fold CV for the other datasets (Amazon, PL04 and MPQA). Bold font marks the best  performance in the column. * indicates statistical significance at p < 0.01 using micro sign test against  the bigram baseline (delta word kernel) of the same column and with the same phrase definition.
true	D15-1089.pdf#0.806*	SearchQA, N-gram F1	word News kernel  Table 1: Accuracy results on the test set for PL05 (20%), standard test split for Twitter (25%) and News  (50%) and from 5-fold CV for the other datasets (Amazon, PL04 and MPQA). Bold font marks the best  performance in the column. * indicates statistical significance at p < 0.01 using micro sign test against  the bigram baseline (delta word kernel) of the same column and with the same phrase definition.
true	D15-1089.pdf#0.801	SearchQA, N-gram F1	* word PL05 kernel  Table 1: Accuracy results on the test set for PL05 (20%), standard test split for Twitter (25%) and News  (50%) and from 5-fold CV for the other datasets (Amazon, PL04 and MPQA). Bold font marks the best  performance in the column. * indicates statistical significance at p < 0.01 using micro sign test against  the bigram baseline (delta word kernel) of the same column and with the same phrase definition.
true	D15-1089.pdf#0.808	SearchQA, N-gram F1	* word Twitter kernel  Table 1: Accuracy results on the test set for PL05 (20%), standard test split for Twitter (25%) and News  (50%) and from 5-fold CV for the other datasets (Amazon, PL04 and MPQA). Bold font marks the best  performance in the column. * indicates statistical significance at p < 0.01 using micro sign test against  the bigram baseline (delta word kernel) of the same column and with the same phrase definition.
true	D15-1089.pdf#0.927	SearchQA, N-gram F1	word PL04 kernel  Table 1: Accuracy results on the test set for PL05 (20%), standard test split for Twitter (25%) and News  (50%) and from 5-fold CV for the other datasets (Amazon, PL04 and MPQA). Bold font marks the best  performance in the column. * indicates statistical significance at p < 0.01 using micro sign test against  the bigram baseline (delta word kernel) of the same column and with the same phrase definition.
true	D13-1183.pdf#0.92	New York Times Corpus, P@10%	System P / R prec  Table 1: Comparison with methods using parallel news  corpora
true	D13-1183.pdf#0.87	New York Times Corpus, P@10%	System P / R diverse prec  Table 1: Comparison with methods using parallel news  corpora
true	D13-1183.pdf#0.93	New York Times Corpus, P@10%	59 all prec  Table 2: Comparison with methods using the distribu- tional hypothesis
true	D13-1183.pdf#0.87	New York Times Corpus, P@10%	59 diverse prec  Table 2: Comparison with methods using the distribu- tional hypothesis
true	D11-1004.pdf#2	WMT 2014 EN-FR, BLEU	length  Table 1: Number of tested combinations for the experi- ments of
true	D11-1004.pdf#4	WMT 2014 EN-FR, BLEU	length  Table 1: Number of tested combinations for the experi- ments of
true	D11-1004.pdf#4)	WMT 2014 EN-FR, BLEU	order  Table 1: Number of tested combinations for the experi- ments of
true	D11-1004.pdf#8	WMT 2014 EN-FR, BLEU	length  Table 1: Number of tested combinations for the experi- ments of
true	D11-1004.pdf#1.33×1020	WMT 2014 EN-FR, BLEU	total comb .  Table 1: Number of tested combinations for the experi- ments of
true	D11-1004.pdf#8)	WMT 2014 EN-FR, BLEU	order  Table 1: Number of tested combinations for the experi- ments of
true	D11-1004.pdf#1	WMT 2014 EN-FR, BLEU	length  Table 1: Number of tested combinations for the experi- ments of
true	D11-1004.pdf#2)	WMT 2014 EN-FR, BLEU	order  Table 1: Number of tested combinations for the experi- ments of
true	D11-1004.pdf#2.31×1010	WMT 2014 EN-FR, BLEU	total comb .  Table 1: Number of tested combinations for the experi- ments of
true	D11-1004.pdf#1CPUofamoderncomputer).Eachcurverepresentsa	WMT 2014 EN-FR, BLEU	dimension ( D ) order 2 , 624 O ( 8N ) 10 , 000 1024 1 , 000 128 seconds 100 32 16  Table 1: Number of tested combinations for the experi- ments of
true	D11-1004.pdf#2	WMT 2014 EN-DE, BLEU	length  Table 1: Number of tested combinations for the experi- ments of
true	D11-1004.pdf#4	WMT 2014 EN-DE, BLEU	length  Table 1: Number of tested combinations for the experi- ments of
true	D11-1004.pdf#4)	WMT 2014 EN-DE, BLEU	order  Table 1: Number of tested combinations for the experi- ments of
true	D11-1004.pdf#8	WMT 2014 EN-DE, BLEU	length  Table 1: Number of tested combinations for the experi- ments of
true	D11-1004.pdf#1.33×1020	WMT 2014 EN-DE, BLEU	total comb .  Table 1: Number of tested combinations for the experi- ments of
true	D11-1004.pdf#8)	WMT 2014 EN-DE, BLEU	order  Table 1: Number of tested combinations for the experi- ments of
true	D11-1004.pdf#1	WMT 2014 EN-DE, BLEU	length  Table 1: Number of tested combinations for the experi- ments of
true	D11-1004.pdf#2)	WMT 2014 EN-DE, BLEU	order  Table 1: Number of tested combinations for the experi- ments of
true	D11-1004.pdf#2.31×1010	WMT 2014 EN-DE, BLEU	total comb .  Table 1: Number of tested combinations for the experi- ments of
true	D11-1004.pdf#1CPUofamoderncomputer).Eachcurverepresentsa	WMT 2014 EN-DE, BLEU	dimension ( D ) order 2 , 624 O ( 8N ) 10 , 000 1024 1 , 000 128 seconds 100 32 16  Table 1: Number of tested combinations for the experi- ments of
true	P15-2117.pdf#56.16	SearchQA, Unigram Acc	R  Table 2: Macro-averaged results(%)
true	P15-2117.pdf#56.14	SearchQA, Unigram Acc	F1  Table 2: Macro-averaged results(%)
true	P15-2117.pdf#56.41	SearchQA, Unigram Acc	P  Table 2: Macro-averaged results(%)
true	P13-2017.pdf#72.30	Penn Treebank, UAS	Target Test Language Unlabeled Attachment Score ( UAS ) Romance ES  Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
true	P13-2017.pdf#27.73	Penn Treebank, UAS	Target Test Language Labeled Attachment Score ( LAS ) Romance KO  Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
true	P13-2017.pdf#61.20	Penn Treebank, UAS	Target Test Language Unlabeled Attachment Score ( UAS ) Germanic EN  Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
true	P13-2017.pdf#49.71	Penn Treebank, UAS	Target Test Language Labeled Attachment Score ( LAS ) Germanic EN  Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
true	P13-2017.pdf#63.65	Penn Treebank, UAS	Target Test Language Labeled Attachment Score ( LAS ) Romance FR  Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
true	P13-2017.pdf#70.56	Penn Treebank, UAS	Target Test Language Unlabeled Attachment Score ( UAS ) Germanic SV  Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
true	P13-2017.pdf#57.04	Penn Treebank, UAS	Target Test Language Labeled Attachment Score ( LAS ) Germanic SV  Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
true	P13-2017.pdf#62.56	Penn Treebank, UAS	Target Test Language Labeled Attachment Score ( LAS ) Romance ES  Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
true	P13-2017.pdf#61.25	Penn Treebank, UAS	Target Test Language Unlabeled Attachment Score ( UAS ) Training Germanic DE  Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
true	P13-2017.pdf#42.37	Penn Treebank, UAS	Target Test Language Unlabeled Attachment Score ( UAS ) Romance KO  Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
true	P13-2017.pdf#52.19	Penn Treebank, UAS	Target Test Language Labeled Attachment Score ( LAS ) Germanic DE  Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
true	P13-2017.pdf#75.12	Penn Treebank, UAS	Target Test Language Unlabeled Attachment Score ( UAS ) Romance FR  Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
true	D12-1126.pdf#89.60	VLSP 2013 POS tagging shared task, Accuracy	Texts with Static and Dynamic Features Table 7 : Performances of POS Tagging on Chinese - only R  Table 7: Performances of POS Tagging on Chinese-only  Texts with Static and Dynamic Features
true	D12-1126.pdf#89.68	VLSP 2013 POS tagging shared task, Accuracy	Texts with Static and Dynamic Features Table 7 : Performances of POS Tagging on Chinese - only P  Table 7: Performances of POS Tagging on Chinese-only  Texts with Static and Dynamic Features
true	D12-1126.pdf#89.64	VLSP 2013 POS tagging shared task, Accuracy	Texts with Static and Dynamic Features Table 7 : Performances of POS Tagging on Chinese - only F1  Table 7: Performances of POS Tagging on Chinese-only  Texts with Static and Dynamic Features
true	D12-1126.pdf#79.82	VLSP 2013 POS tagging shared task, Accuracy	2 Performances of POS Tagging on Dataset H EN G Dataset F1  Table 9.
true	D12-1126.pdf#77.61	VLSP 2013 POS tagging shared task, Accuracy	2 Performances of POS Tagging on Dataset H OOV Dataset oov  Table 9.
true	D12-1126.pdf#89.81	VLSP 2013 POS tagging shared task, Accuracy	2 Performances of POS Tagging on Dataset H Total Dataset F1  Table 9.
true	D12-1126.pdf#89.81	VLSP 2013 POS tagging shared task, Accuracy	2 Performances of POS Tagging on Dataset H Total Dataset F1  Table 9: Performances of POS Tagging on Dataset H 1  and H 2
true	D12-1126.pdf#79.82	VLSP 2013 POS tagging shared task, Accuracy	2 Performances of POS Tagging on Dataset H EN G Dataset F1  Table 9: Performances of POS Tagging on Dataset H 1  and H 2
true	D12-1126.pdf#77.61	VLSP 2013 POS tagging shared task, Accuracy	2 Performances of POS Tagging on Dataset H OOV Dataset oov  Table 9: Performances of POS Tagging on Dataset H 1  and H 2
true	D12-1126.pdf#2	VLSP 2013 POS tagging shared task, Accuracy	D Table 10 : The Synthetic Chinese - English Mixed Dataset NN 8957 845 13154  Table 10.
true	D12-1126.pdf#1	VLSP 2013 POS tagging shared task, Accuracy	C Table 10 : The Synthetic Chinese - English Mixed Dataset NN 8957  Table 10.
true	D12-1126.pdf#1	VLSP 2013 POS tagging shared task, Accuracy	C Table 10 : The Synthetic Chinese - English Mixed Dataset NN 8957  Table 10: The Synthetic Chinese-English Mixed Dataset
true	D12-1126.pdf#2	VLSP 2013 POS tagging shared task, Accuracy	D Table 10 : The Synthetic Chinese - English Mixed Dataset NN 8957 845 13154  Table 10: The Synthetic Chinese-English Mixed Dataset
true	D12-1126.pdf#2	VLSP 2013 POS tagging shared task, Accuracy	D Table 10 : The Synthetic Chinese - English Mixed Dataset NN 8957 845 13154  Table 11. On dataset E,  our method achieves 6.78% higher performance on  tagging EN G labels than traditional static features.  This result is reasonable because our model can use  more flexible feature templates to extract features  and reduce the problem of being dependent on spe- cific English words.  Tables 12/13/14/15/16/17 show the detailed re- sults on datasets A/B/C/D 1 /D 2 /E.
true	D12-1126.pdf#1	VLSP 2013 POS tagging shared task, Accuracy	C Table 10 : The Synthetic Chinese - English Mixed Dataset NN 8957  Table 11. On dataset E,  our method achieves 6.78% higher performance on  tagging EN G labels than traditional static features.  This result is reasonable because our model can use  more flexible feature templates to extract features  and reduce the problem of being dependent on spe- cific English words.  Tables 12/13/14/15/16/17 show the detailed re- sults on datasets A/B/C/D 1 /D 2 /E.
true	D12-1126.pdf#73.20	VLSP 2013 POS tagging shared task, Accuracy	Table 12 : Performances on Dataset A F1  Table 12: Performances on Dataset A
true	D12-1126.pdf#87.58	VLSP 2013 POS tagging shared task, Accuracy	Table 12 : Performances on Dataset A F1  Table 12: Performances on Dataset A
true	D12-1126.pdf#71.30	VLSP 2013 POS tagging shared task, Accuracy	Table 13 : Performances on Dataset B Table 12 : Performances on Dataset A F1 F1  Table 12: Performances on Dataset A
true	D12-1126.pdf#86.11	VLSP 2013 POS tagging shared task, Accuracy	Table 13 : Performances on Dataset B Table 12 : Performances on Dataset A F1 F1  Table 12: Performances on Dataset A
true	D12-1126.pdf#46.73	VLSP 2013 POS tagging shared task, Accuracy	Table 13 : Performances on Dataset B Table 12 : Performances on Dataset A F1 F1  Table 12: Performances on Dataset A
true	D12-1126.pdf#46.73	VLSP 2013 POS tagging shared task, Accuracy	Table 13 : Performances on Dataset B Table 12 : Performances on Dataset A F1 F1  Table 13: Performances on Dataset B
true	D12-1126.pdf#86.11	VLSP 2013 POS tagging shared task, Accuracy	Table 13 : Performances on Dataset B Table 12 : Performances on Dataset A F1 F1  Table 13: Performances on Dataset B
true	D12-1126.pdf#73.20	VLSP 2013 POS tagging shared task, Accuracy	Table 12 : Performances on Dataset A F1  Table 13: Performances on Dataset B
true	D12-1126.pdf#71.30	VLSP 2013 POS tagging shared task, Accuracy	Table 13 : Performances on Dataset B Table 12 : Performances on Dataset A F1 F1  Table 13: Performances on Dataset B
true	D12-1126.pdf#87.58	VLSP 2013 POS tagging shared task, Accuracy	Table 12 : Performances on Dataset A F1  Table 13: Performances on Dataset B
true	D12-1126.pdf#20.93	VLSP 2013 POS tagging shared task, Accuracy	Table 15 : Performances on Dataset D 1 Table 14 : Performances on Dataset C F1 F1  Table 14: Performances on Dataset C
true	D12-1126.pdf#66.34	VLSP 2013 POS tagging shared task, Accuracy	Table 14 : Performances on Dataset C F1  Table 14: Performances on Dataset C
true	D12-1126.pdf#52.95	VLSP 2013 POS tagging shared task, Accuracy	Table 14 : Performances on Dataset C F1  Table 14: Performances on Dataset C
true	D12-1126.pdf#33.33	VLSP 2013 POS tagging shared task, Accuracy	Table 15 : Performances on Dataset D 1 Table 14 : Performances on Dataset C F1 F1  Table 14: Performances on Dataset C
true	D12-1126.pdf#68.29	VLSP 2013 POS tagging shared task, Accuracy	Table 15 : Performances on Dataset D 1 Table 14 : Performances on Dataset C F1 F1  Table 14: Performances on Dataset C
true	D12-1126.pdf#82.20	VLSP 2013 POS tagging shared task, Accuracy	Table 15 : Performances on Dataset D 1 Table 14 : Performances on Dataset C F1 F1  Table 14: Performances on Dataset C
true	D12-1126.pdf#47.52	VLSP 2013 POS tagging shared task, Accuracy	Table 14 : Performances on Dataset C F1  Table 14: Performances on Dataset C
true	D12-1126.pdf#77.41	VLSP 2013 POS tagging shared task, Accuracy	Table 14 : Performances on Dataset C F1  Table 14: Performances on Dataset C
true	D12-1126.pdf#20.93	VLSP 2013 POS tagging shared task, Accuracy	Table 15 : Performances on Dataset D 1 F1  Table 16: Performances on Dataset D 2
true	D12-1126.pdf#31.68	VLSP 2013 POS tagging shared task, Accuracy	Table 16 : Performances on Dataset D 2 Table 15 : Performances on Dataset D 1 F1 F1  Table 16: Performances on Dataset D 2
true	D12-1126.pdf#26.83	VLSP 2013 POS tagging shared task, Accuracy	Table 16 : Performances on Dataset D 2 Table 15 : Performances on Dataset D 1 F1 F1  Table 16: Performances on Dataset D 2
true	D12-1126.pdf#82.20	VLSP 2013 POS tagging shared task, Accuracy	Table 15 : Performances on Dataset D 1 F1  Table 16: Performances on Dataset D 2
true	D12-1126.pdf#68.29	VLSP 2013 POS tagging shared task, Accuracy	Table 15 : Performances on Dataset D 1 F1  Table 16: Performances on Dataset D 2
true	D12-1126.pdf#33.33	VLSP 2013 POS tagging shared task, Accuracy	Table 15 : Performances on Dataset D 1 F1  Table 16: Performances on Dataset D 2
true	D12-1126.pdf#70.02	VLSP 2013 POS tagging shared task, Accuracy	Table 16 : Performances on Dataset D 2 Table 15 : Performances on Dataset D 1 F1 F1  Table 16: Performances on Dataset D 2
true	D12-1126.pdf#80.95	VLSP 2013 POS tagging shared task, Accuracy	Table 16 : Performances on Dataset D 2 Table 15 : Performances on Dataset D 1 F1 F1  Table 16: Performances on Dataset D 2
true	D12-1126.pdf#64.14	VLSP 2013 POS tagging shared task, Accuracy	Table 17 : Performances on Dataset E F1  Table 17: Performances on Dataset E
true	D12-1126.pdf#26.97	VLSP 2013 POS tagging shared task, Accuracy	Table 17 : Performances on Dataset E F1  Table 17: Performances on Dataset E
true	D12-1126.pdf#49.24	VLSP 2013 POS tagging shared task, Accuracy	Table 17 : Performances on Dataset E F1  Table 17: Performances on Dataset E
true	D12-1126.pdf#55.24	VLSP 2013 POS tagging shared task, Accuracy	Table 17 : Performances on Dataset E F1  Table 17: Performances on Dataset E
true	D12-1126.pdf#77.43	VLSP 2013 POS tagging shared task, Accuracy	Table 17 : Performances on Dataset E F1  Table 17: Performances on Dataset E
true	D12-1126.pdf#26.97	VLSP 2013 POS tagging shared task, Accuracy	Table 17 : Performances on Dataset E F1  Table 18: Examples in Real Dataset of Mixed Texts
true	D12-1126.pdf#64.14	VLSP 2013 POS tagging shared task, Accuracy	Table 17 : Performances on Dataset E F1  Table 18: Examples in Real Dataset of Mixed Texts
true	D12-1126.pdf#49.24	VLSP 2013 POS tagging shared task, Accuracy	Table 17 : Performances on Dataset E F1  Table 18: Examples in Real Dataset of Mixed Texts
true	D12-1126.pdf#55.24	VLSP 2013 POS tagging shared task, Accuracy	Table 17 : Performances on Dataset E F1  Table 18: Examples in Real Dataset of Mixed Texts
true	D12-1126.pdf#77.43	VLSP 2013 POS tagging shared task, Accuracy	Table 17 : Performances on Dataset E F1  Table 18: Examples in Real Dataset of Mixed Texts
true	D12-1126.pdf#68.09	VLSP 2013 POS tagging shared task, Accuracy	Table 21 : Performances of Model B on Dataset R F1  Table 20: Performances of POS Tagging on R
true	D12-1126.pdf#89.43	VLSP 2013 POS tagging shared task, Accuracy	Table 21 : Performances of Model B on Dataset R F1  Table 20: Performances of POS Tagging on R
true	D12-1126.pdf#68.66	VLSP 2013 POS tagging shared task, Accuracy	Table 21 : Performances of Model B on Dataset R F1  Table 20: Performances of POS Tagging on R
true	D12-1126.pdf#66.67	VLSP 2013 POS tagging shared task, Accuracy	Table 22 : Performances of Model C on Dataset R F1  Table 21: Performances of Model B on Dataset R
true	D12-1126.pdf#83.17	VLSP 2013 POS tagging shared task, Accuracy	Table 22 : Performances of Model C on Dataset R F1  Table 21: Performances of Model B on Dataset R
true	D12-1126.pdf#71.01	VLSP 2013 POS tagging shared task, Accuracy	Table 22 : Performances of Model C on Dataset R F1  Table 21: Performances of Model B on Dataset R
true	D12-1126.pdf#50.00	VLSP 2013 POS tagging shared task, Accuracy	Table 22 : Performances of Model C on Dataset R F1  Table 21: Performances of Model B on Dataset R
true	D12-1126.pdf#50.00	VLSP 2013 POS tagging shared task, Accuracy	Table 22 : Performances of Model C on Dataset R F1  Table 22: Performances of Model C on Dataset R
true	D12-1126.pdf#83.17	VLSP 2013 POS tagging shared task, Accuracy	Table 22 : Performances of Model C on Dataset R F1  Table 22: Performances of Model C on Dataset R
true	D12-1126.pdf#71.01	VLSP 2013 POS tagging shared task, Accuracy	Table 22 : Performances of Model C on Dataset R F1  Table 22: Performances of Model C on Dataset R
true	D12-1126.pdf#66.67	VLSP 2013 POS tagging shared task, Accuracy	Table 22 : Performances of Model C on Dataset R F1  Table 22: Performances of Model C on Dataset R
true	D12-1029.pdf#78.26	Penn Treebank, UAS	NE+LBA UAS  Table 1: Parsing accuracy (LAS and UAS, excluding punctuation) of Nivre's arc-eager parser without modification  (NE), with the LEFT-BUFFER-ARC transition added (NE+LBA) and with the RIGHT-BUFFER-ARC transition added  (NE+RBA). Best results for each language are shown in boldface.
true	D12-1029.pdf#85.21	Penn Treebank, UAS	NE+LBA LAS  Table 1: Parsing accuracy (LAS and UAS, excluding punctuation) of Nivre's arc-eager parser without modification  (NE), with the LEFT-BUFFER-ARC transition added (NE+LBA) and with the RIGHT-BUFFER-ARC transition added  (NE+RBA). Best results for each language are shown in boldface.
true	D12-1029.pdf#90.20	Penn Treebank, UAS	NE+LBA UAS  Table 1: Parsing accuracy (LAS and UAS, excluding punctuation) of Nivre's arc-eager parser without modification  (NE), with the LEFT-BUFFER-ARC transition added (NE+LBA) and with the RIGHT-BUFFER-ARC transition added  (NE+RBA). Best results for each language are shown in boldface.
true	D12-1029.pdf#64.33	Penn Treebank, UAS	NE+RBA LAS  Table 1: Parsing accuracy (LAS and UAS, excluding punctuation) of Nivre's arc-eager parser without modification  (NE), with the LEFT-BUFFER-ARC transition added (NE+LBA) and with the RIGHT-BUFFER-ARC transition added  (NE+RBA). Best results for each language are shown in boldface.
true	D12-1029.pdf#90.58	Penn Treebank, UAS	NE+LBA UAS  Table 1: Parsing accuracy (LAS and UAS, excluding punctuation) of Nivre's arc-eager parser without modification  (NE), with the LEFT-BUFFER-ARC transition added (NE+LBA) and with the RIGHT-BUFFER-ARC transition added  (NE+RBA). Best results for each language are shown in boldface.
true	D12-1029.pdf#74.73	Penn Treebank, UAS	NE+RBA UAS  Table 1: Parsing accuracy (LAS and UAS, excluding punctuation) of Nivre's arc-eager parser without modification  (NE), with the LEFT-BUFFER-ARC transition added (NE+LBA) and with the RIGHT-BUFFER-ARC transition added  (NE+RBA). Best results for each language are shown in boldface.
true	D12-1029.pdf#86.48	Penn Treebank, UAS	NE+RBA LAS  Table 1: Parsing accuracy (LAS and UAS, excluding punctuation) of Nivre's arc-eager parser without modification  (NE), with the LEFT-BUFFER-ARC transition added (NE+LBA) and with the RIGHT-BUFFER-ARC transition added  (NE+RBA). Best results for each language are shown in boldface.
true	D12-1029.pdf#86.92	Penn Treebank, UAS	NE+LBA LAS  Table 1: Parsing accuracy (LAS and UAS, excluding punctuation) of Nivre's arc-eager parser without modification  (NE), with the LEFT-BUFFER-ARC transition added (NE+LBA) and with the RIGHT-BUFFER-ARC transition added  (NE+RBA). Best results for each language are shown in boldface.
true	D12-1029.pdf#88.90	Penn Treebank, UAS	NE+RBA UAS  Table 1: Parsing accuracy (LAS and UAS, excluding punctuation) of Nivre's arc-eager parser without modification  (NE), with the LEFT-BUFFER-ARC transition added (NE+LBA) and with the RIGHT-BUFFER-ARC transition added  (NE+RBA). Best results for each language are shown in boldface.
true	D12-1029.pdf#67.78	Penn Treebank, UAS	NE+LBA LAS  Table 1: Parsing accuracy (LAS and UAS, excluding punctuation) of Nivre's arc-eager parser without modification  (NE), with the LEFT-BUFFER-ARC transition added (NE+LBA) and with the RIGHT-BUFFER-ARC transition added  (NE+RBA). Best results for each language are shown in boldface.
true	D12-1029.pdf#88.83	Penn Treebank, UAS	NE+LBA UAS  Table 1: Parsing accuracy (LAS and UAS, excluding punctuation) of Nivre's arc-eager parser without modification  (NE), with the LEFT-BUFFER-ARC transition added (NE+LBA) and with the RIGHT-BUFFER-ARC transition added  (NE+RBA). Best results for each language are shown in boldface.
true	D12-1029.pdf#86.62	Penn Treebank, UAS	NE+RBA LAS  Table 1: Parsing accuracy (LAS and UAS, excluding punctuation) of Nivre's arc-eager parser without modification  (NE), with the LEFT-BUFFER-ARC transition added (NE+LBA) and with the RIGHT-BUFFER-ARC transition added  (NE+RBA). Best results for each language are shown in boldface.
true	D12-1029.pdf#84.24	Penn Treebank, UAS	NE+LBA UAS  Table 1: Parsing accuracy (LAS and UAS, excluding punctuation) of Nivre's arc-eager parser without modification  (NE), with the LEFT-BUFFER-ARC transition added (NE+LBA) and with the RIGHT-BUFFER-ARC transition added  (NE+RBA). Best results for each language are shown in boldface.
true	D12-1029.pdf#83.33	Penn Treebank, UAS	NE+LBA LAS  Table 1: Parsing accuracy (LAS and UAS, excluding punctuation) of Nivre's arc-eager parser without modification  (NE), with the LEFT-BUFFER-ARC transition added (NE+LBA) and with the RIGHT-BUFFER-ARC transition added  (NE+RBA). Best results for each language are shown in boldface.
true	D12-1029.pdf#90.64	Penn Treebank, UAS	NE+RBA UAS  Table 1: Parsing accuracy (LAS and UAS, excluding punctuation) of Nivre's arc-eager parser without modification  (NE), with the LEFT-BUFFER-ARC transition added (NE+LBA) and with the RIGHT-BUFFER-ARC transition added  (NE+RBA). Best results for each language are shown in boldface.
true	D12-1029.pdf#78.70	Penn Treebank, UAS	NE+LBA LAS  Table 1: Parsing accuracy (LAS and UAS, excluding punctuation) of Nivre's arc-eager parser without modification  (NE), with the LEFT-BUFFER-ARC transition added (NE+LBA) and with the RIGHT-BUFFER-ARC transition added  (NE+RBA). Best results for each language are shown in boldface.
true	D12-1029.pdf#91.07	Penn Treebank, UAS	NE+LBA LBA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#81.30	Penn Treebank, UAS	NE+LBA RA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#58.28	Penn Treebank, UAS	NE LA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#94.77	Penn Treebank, UAS	NE LA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#97.07	Penn Treebank, UAS	NE+LBA LBA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#84.16	Penn Treebank, UAS	NE+LBA RA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#85.11	Penn Treebank, UAS	NE+LBA RA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#94.30	Penn Treebank, UAS	NE+LBA LBA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#77.46	Penn Treebank, UAS	NE+LBA LBA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#90.73	Penn Treebank, UAS	NE+RBA RBA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#87.75	Penn Treebank, UAS	NE LA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#88.01	Penn Treebank, UAS	NE+RBA RBA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#95.98	Penn Treebank, UAS	NE+RBA RBA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#86.25	Penn Treebank, UAS	NE+RBA LA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#74.21	Penn Treebank, UAS	NE RA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#92.30	Penn Treebank, UAS	NE+RBA RBA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#60.23	Penn Treebank, UAS	NE+RBA LA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#89.00	Penn Treebank, UAS	NE+LBA LBA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#78.34	Penn Treebank, UAS	NE+LBA RA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#91.72	Penn Treebank, UAS	NE+LBA LBA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#87.23	Penn Treebank, UAS	NE+LBA RA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#90.23	Penn Treebank, UAS	NE+LBA+RBA LA *  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#82.58	Penn Treebank, UAS	NE+RBA RBA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#68.65	Penn Treebank, UAS	NE+LBA RA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#85.96	Penn Treebank, UAS	NE+RBA LA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#72.78	Penn Treebank, UAS	NE+LBA LBA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#89.55	Penn Treebank, UAS	NE+RBA LA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#92.29	Penn Treebank, UAS	NE+RBA RBA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#79.70	Penn Treebank, UAS	NE+RBA RBA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#85.79	Penn Treebank, UAS	NE RA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#92.83	Penn Treebank, UAS	NE+LBA LBA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#73.91	Penn Treebank, UAS	NE+RBA RBA  Table 3: Labelled precision of the arcs built by each transition of Nivre's arc-eager parser without modification (NE),  with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added  (NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)  when it is acting only on a "hard" subset of leftward (rightward) arcs, and thus its precision is not directly comparable  to that of (LA, RA). Best results for each language and transition are shown in boldface.
true	D12-1029.pdf#90.64	Penn Treebank, UAS	NE+LBA / RBA UAS  Table 3. As we can see, projec- tive buffer transitions achieve better precision than  standard transitions, but this is not surprising since  they act only on "easy" arcs of length 1. There- fore, this high precision does not mean that they ac- tually build arcs more accurately than the standard  transitions, since it is not measured on the same set  of arcs. Similarly, adding a projective buffer tran- sition decreases the precision of its corresponding  standard transition, but this is because the standard  transition is then dealing only with "harder" arcs of  length greather than 1, not because it is making more  errors. A more interesting insight comes from com- paring transitions that are acting on the same tar- get set of arcs: we see that, in the languages where  LEFT-BUFFER-ARC is beneficial, the addition of  this transition always improves the precision of the  standard RIGHT-ARC transition; and the converse  happens with RIGHT-BUFFER-ARC with respect to  LEFT-ARC. This further backs the hypothesis that  the filtering of "easy" links achieved by projective  buffer transitions makes it easier for the classifier to  decide among standard transitions.
true	D12-1029.pdf#84.58	Penn Treebank, UAS	NE+PP ( CoNLL X ) LAS  Table 3. As we can see, projec- tive buffer transitions achieve better precision than  standard transitions, but this is not surprising since  they act only on "easy" arcs of length 1. There- fore, this high precision does not mean that they ac- tually build arcs more accurately than the standard  transitions, since it is not measured on the same set  of arcs. Similarly, adding a projective buffer tran- sition decreases the precision of its corresponding  standard transition, but this is because the standard  transition is then dealing only with "harder" arcs of  length greather than 1, not because it is making more  errors. A more interesting insight comes from com- paring transitions that are acting on the same tar- get set of arcs: we see that, in the languages where  LEFT-BUFFER-ARC is beneficial, the addition of  this transition always improves the precision of the  standard RIGHT-ARC transition; and the converse  happens with RIGHT-BUFFER-ARC with respect to  LEFT-ARC. This further backs the hypothesis that  the filtering of "easy" links achieved by projective  buffer transitions makes it easier for the classifier to  decide among standard transitions.
true	D12-1029.pdf#86.48	Penn Treebank, UAS	NE+LBA / RBA LAS  Table 3. As we can see, projec- tive buffer transitions achieve better precision than  standard transitions, but this is not surprising since  they act only on "easy" arcs of length 1. There- fore, this high precision does not mean that they ac- tually build arcs more accurately than the standard  transitions, since it is not measured on the same set  of arcs. Similarly, adding a projective buffer tran- sition decreases the precision of its corresponding  standard transition, but this is because the standard  transition is then dealing only with "harder" arcs of  length greather than 1, not because it is making more  errors. A more interesting insight comes from com- paring transitions that are acting on the same tar- get set of arcs: we see that, in the languages where  LEFT-BUFFER-ARC is beneficial, the addition of  this transition always improves the precision of the  standard RIGHT-ARC transition; and the converse  happens with RIGHT-BUFFER-ARC with respect to  LEFT-ARC. This further backs the hypothesis that  the filtering of "easy" links achieved by projective  buffer transitions makes it easier for the classifier to  decide among standard transitions.
true	D12-1029.pdf#85.21	Penn Treebank, UAS	NE+LBA / RBA LAS  Table 3. As we can see, projec- tive buffer transitions achieve better precision than  standard transitions, but this is not surprising since  they act only on "easy" arcs of length 1. There- fore, this high precision does not mean that they ac- tually build arcs more accurately than the standard  transitions, since it is not measured on the same set  of arcs. Similarly, adding a projective buffer tran- sition decreases the precision of its corresponding  standard transition, but this is because the standard  transition is then dealing only with "harder" arcs of  length greather than 1, not because it is making more  errors. A more interesting insight comes from com- paring transitions that are acting on the same tar- get set of arcs: we see that, in the languages where  LEFT-BUFFER-ARC is beneficial, the addition of  this transition always improves the precision of the  standard RIGHT-ARC transition; and the converse  happens with RIGHT-BUFFER-ARC with respect to  LEFT-ARC. This further backs the hypothesis that  the filtering of "easy" links achieved by projective  buffer transitions makes it easier for the classifier to  decide among standard transitions.
true	D12-1029.pdf#65.68	Penn Treebank, UAS	NE+PP ( CoNLL X ) LAS  Table 3. As we can see, projec- tive buffer transitions achieve better precision than  standard transitions, but this is not surprising since  they act only on "easy" arcs of length 1. There- fore, this high precision does not mean that they ac- tually build arcs more accurately than the standard  transitions, since it is not measured on the same set  of arcs. Similarly, adding a projective buffer tran- sition decreases the precision of its corresponding  standard transition, but this is because the standard  transition is then dealing only with "harder" arcs of  length greather than 1, not because it is making more  errors. A more interesting insight comes from com- paring transitions that are acting on the same tar- get set of arcs: we see that, in the languages where  LEFT-BUFFER-ARC is beneficial, the addition of  this transition always improves the precision of the  standard RIGHT-ARC transition; and the converse  happens with RIGHT-BUFFER-ARC with respect to  LEFT-ARC. This further backs the hypothesis that  the filtering of "easy" links achieved by projective  buffer transitions makes it easier for the classifier to  decide among standard transitions.
true	D12-1029.pdf#88.90	Penn Treebank, UAS	NE+LBA / RBA UAS  Table 3. As we can see, projec- tive buffer transitions achieve better precision than  standard transitions, but this is not surprising since  they act only on "easy" arcs of length 1. There- fore, this high precision does not mean that they ac- tually build arcs more accurately than the standard  transitions, since it is not measured on the same set  of arcs. Similarly, adding a projective buffer tran- sition decreases the precision of its corresponding  standard transition, but this is because the standard  transition is then dealing only with "harder" arcs of  length greather than 1, not because it is making more  errors. A more interesting insight comes from com- paring transitions that are acting on the same tar- get set of arcs: we see that, in the languages where  LEFT-BUFFER-ARC is beneficial, the addition of  this transition always improves the precision of the  standard RIGHT-ARC transition; and the converse  happens with RIGHT-BUFFER-ARC with respect to  LEFT-ARC. This further backs the hypothesis that  the filtering of "easy" links achieved by projective  buffer transitions makes it easier for the classifier to  decide among standard transitions.
true	D12-1029.pdf#84.80	Penn Treebank, UAS	NE+PP ( CoNLL X ) UAS  Table 3. As we can see, projec- tive buffer transitions achieve better precision than  standard transitions, but this is not surprising since  they act only on "easy" arcs of length 1. There- fore, this high precision does not mean that they ac- tually build arcs more accurately than the standard  transitions, since it is not measured on the same set  of arcs. Similarly, adding a projective buffer tran- sition decreases the precision of its corresponding  standard transition, but this is because the standard  transition is then dealing only with "harder" arcs of  length greather than 1, not because it is making more  errors. A more interesting insight comes from com- paring transitions that are acting on the same tar- get set of arcs: we see that, in the languages where  LEFT-BUFFER-ARC is beneficial, the addition of  this transition always improves the precision of the  standard RIGHT-ARC transition; and the converse  happens with RIGHT-BUFFER-ARC with respect to  LEFT-ARC. This further backs the hypothesis that  the filtering of "easy" links achieved by projective  buffer transitions makes it easier for the classifier to  decide among standard transitions.
true	D12-1029.pdf#78.70	Penn Treebank, UAS	NE+LBA / RBA LAS  Table 3. As we can see, projec- tive buffer transitions achieve better precision than  standard transitions, but this is not surprising since  they act only on "easy" arcs of length 1. There- fore, this high precision does not mean that they ac- tually build arcs more accurately than the standard  transitions, since it is not measured on the same set  of arcs. Similarly, adding a projective buffer tran- sition decreases the precision of its corresponding  standard transition, but this is because the standard  transition is then dealing only with "harder" arcs of  length greather than 1, not because it is making more  errors. A more interesting insight comes from com- paring transitions that are acting on the same tar- get set of arcs: we see that, in the languages where  LEFT-BUFFER-ARC is beneficial, the addition of  this transition always improves the precision of the  standard RIGHT-ARC transition; and the converse  happens with RIGHT-BUFFER-ARC with respect to  LEFT-ARC. This further backs the hypothesis that  the filtering of "easy" links achieved by projective  buffer transitions makes it easier for the classifier to  decide among standard transitions.
true	D12-1029.pdf#89.50	Penn Treebank, UAS	NE+PP ( CoNLL X ) UAS  Table 3. As we can see, projec- tive buffer transitions achieve better precision than  standard transitions, but this is not surprising since  they act only on "easy" arcs of length 1. There- fore, this high precision does not mean that they ac- tually build arcs more accurately than the standard  transitions, since it is not measured on the same set  of arcs. Similarly, adding a projective buffer tran- sition decreases the precision of its corresponding  standard transition, but this is because the standard  transition is then dealing only with "harder" arcs of  length greather than 1, not because it is making more  errors. A more interesting insight comes from com- paring transitions that are acting on the same tar- get set of arcs: we see that, in the languages where  LEFT-BUFFER-ARC is beneficial, the addition of  this transition always improves the precision of the  standard RIGHT-ARC transition; and the converse  happens with RIGHT-BUFFER-ARC with respect to  LEFT-ARC. This further backs the hypothesis that  the filtering of "easy" links achieved by projective  buffer transitions makes it easier for the classifier to  decide among standard transitions.
true	D12-1029.pdf#87.60	Penn Treebank, UAS	NE+PP ( CoNLL X ) LAS  Table 3. As we can see, projec- tive buffer transitions achieve better precision than  standard transitions, but this is not surprising since  they act only on "easy" arcs of length 1. There- fore, this high precision does not mean that they ac- tually build arcs more accurately than the standard  transitions, since it is not measured on the same set  of arcs. Similarly, adding a projective buffer tran- sition decreases the precision of its corresponding  standard transition, but this is because the standard  transition is then dealing only with "harder" arcs of  length greather than 1, not because it is making more  errors. A more interesting insight comes from com- paring transitions that are acting on the same tar- get set of arcs: we see that, in the languages where  LEFT-BUFFER-ARC is beneficial, the addition of  this transition always improves the precision of the  standard RIGHT-ARC transition; and the converse  happens with RIGHT-BUFFER-ARC with respect to  LEFT-ARC. This further backs the hypothesis that  the filtering of "easy" links achieved by projective  buffer transitions makes it easier for the classifier to  decide among standard transitions.
true	D12-1029.pdf#86.92	Penn Treebank, UAS	NE+PP ( CoNLL X ) LAS  Table 3. As we can see, projec- tive buffer transitions achieve better precision than  standard transitions, but this is not surprising since  they act only on "easy" arcs of length 1. There- fore, this high precision does not mean that they ac- tually build arcs more accurately than the standard  transitions, since it is not measured on the same set  of arcs. Similarly, adding a projective buffer tran- sition decreases the precision of its corresponding  standard transition, but this is because the standard  transition is then dealing only with "harder" arcs of  length greather than 1, not because it is making more  errors. A more interesting insight comes from com- paring transitions that are acting on the same tar- get set of arcs: we see that, in the languages where  LEFT-BUFFER-ARC is beneficial, the addition of  this transition always improves the precision of the  standard RIGHT-ARC transition; and the converse  happens with RIGHT-BUFFER-ARC with respect to  LEFT-ARC. This further backs the hypothesis that  the filtering of "easy" links achieved by projective  buffer transitions makes it easier for the classifier to  decide among standard transitions.
true	D12-1029.pdf#90.20	Penn Treebank, UAS	NE+LBA / RBA UAS  Table 3. As we can see, projec- tive buffer transitions achieve better precision than  standard transitions, but this is not surprising since  they act only on "easy" arcs of length 1. There- fore, this high precision does not mean that they ac- tually build arcs more accurately than the standard  transitions, since it is not measured on the same set  of arcs. Similarly, adding a projective buffer tran- sition decreases the precision of its corresponding  standard transition, but this is because the standard  transition is then dealing only with "harder" arcs of  length greather than 1, not because it is making more  errors. A more interesting insight comes from com- paring transitions that are acting on the same tar- get set of arcs: we see that, in the languages where  LEFT-BUFFER-ARC is beneficial, the addition of  this transition always improves the precision of the  standard RIGHT-ARC transition; and the converse  happens with RIGHT-BUFFER-ARC with respect to  LEFT-ARC. This further backs the hypothesis that  the filtering of "easy" links achieved by projective  buffer transitions makes it easier for the classifier to  decide among standard transitions.
true	D12-1029.pdf#67.78	Penn Treebank, UAS	NE+LBA / RBA LAS  Table 3. As we can see, projec- tive buffer transitions achieve better precision than  standard transitions, but this is not surprising since  they act only on "easy" arcs of length 1. There- fore, this high precision does not mean that they ac- tually build arcs more accurately than the standard  transitions, since it is not measured on the same set  of arcs. Similarly, adding a projective buffer tran- sition decreases the precision of its corresponding  standard transition, but this is because the standard  transition is then dealing only with "harder" arcs of  length greather than 1, not because it is making more  errors. A more interesting insight comes from com- paring transitions that are acting on the same tar- get set of arcs: we see that, in the languages where  LEFT-BUFFER-ARC is beneficial, the addition of  this transition always improves the precision of the  standard RIGHT-ARC transition; and the converse  happens with RIGHT-BUFFER-ARC with respect to  LEFT-ARC. This further backs the hypothesis that  the filtering of "easy" links achieved by projective  buffer transitions makes it easier for the classifier to  decide among standard transitions.
true	D12-1029.pdf#91.22	Penn Treebank, UAS	NE+PP ( CoNLL X ) UAS  Table 3. As we can see, projec- tive buffer transitions achieve better precision than  standard transitions, but this is not surprising since  they act only on "easy" arcs of length 1. There- fore, this high precision does not mean that they ac- tually build arcs more accurately than the standard  transitions, since it is not measured on the same set  of arcs. Similarly, adding a projective buffer tran- sition decreases the precision of its corresponding  standard transition, but this is because the standard  transition is then dealing only with "harder" arcs of  length greather than 1, not because it is making more  errors. A more interesting insight comes from com- paring transitions that are acting on the same tar- get set of arcs: we see that, in the languages where  LEFT-BUFFER-ARC is beneficial, the addition of  this transition always improves the precision of the  standard RIGHT-ARC transition; and the converse  happens with RIGHT-BUFFER-ARC with respect to  LEFT-ARC. This further backs the hypothesis that  the filtering of "easy" links achieved by projective  buffer transitions makes it easier for the classifier to  decide among standard transitions.
true	D12-1029.pdf#75.82	Penn Treebank, UAS	NE+PP ( CoNLL X ) UAS  Table 3. As we can see, projec- tive buffer transitions achieve better precision than  standard transitions, but this is not surprising since  they act only on "easy" arcs of length 1. There- fore, this high precision does not mean that they ac- tually build arcs more accurately than the standard  transitions, since it is not measured on the same set  of arcs. Similarly, adding a projective buffer tran- sition decreases the precision of its corresponding  standard transition, but this is because the standard  transition is then dealing only with "harder" arcs of  length greather than 1, not because it is making more  errors. A more interesting insight comes from com- paring transitions that are acting on the same tar- get set of arcs: we see that, in the languages where  LEFT-BUFFER-ARC is beneficial, the addition of  this transition always improves the precision of the  standard RIGHT-ARC transition; and the converse  happens with RIGHT-BUFFER-ARC with respect to  LEFT-ARC. This further backs the hypothesis that  the filtering of "easy" links achieved by projective  buffer transitions makes it easier for the classifier to  decide among standard transitions.
true	D12-1029.pdf#78.26	Penn Treebank, UAS	NE+LBA / RBA UAS  Table 3. As we can see, projec- tive buffer transitions achieve better precision than  standard transitions, but this is not surprising since  they act only on "easy" arcs of length 1. There- fore, this high precision does not mean that they ac- tually build arcs more accurately than the standard  transitions, since it is not measured on the same set  of arcs. Similarly, adding a projective buffer tran- sition decreases the precision of its corresponding  standard transition, but this is because the standard  transition is then dealing only with "harder" arcs of  length greather than 1, not because it is making more  errors. A more interesting insight comes from com- paring transitions that are acting on the same tar- get set of arcs: we see that, in the languages where  LEFT-BUFFER-ARC is beneficial, the addition of  this transition always improves the precision of the  standard RIGHT-ARC transition; and the converse  happens with RIGHT-BUFFER-ARC with respect to  LEFT-ARC. This further backs the hypothesis that  the filtering of "easy" links achieved by projective  buffer transitions makes it easier for the classifier to  decide among standard transitions.
true	D12-1029.pdf#67.78	Penn Treebank, UAS	Arabic  Table 5: Comparison of the Arabic and Danish LAS ob- tained by the arc-eager parser with projective buffer tran- sitions in comparison to other parsers in the literature that  report results on these datasets.
true	D12-1029.pdf#85.21	Penn Treebank, UAS	Danish  Table 5: Comparison of the Arabic and Danish LAS ob- tained by the arc-eager parser with projective buffer tran- sitions in comparison to other parsers in the literature that  report results on these datasets.
true	D12-1029.pdf#67.21	Penn Treebank, UAS	NE+LNBA / RNBA LAS  Table 7: Comparison of the parsing accuracy (LAS  and UAS, excluding punctuation) of Nivre's arc- eager parser with non-projective buffer transitions  (NE+LNBA/RNBA) and the parser with the pseudo- projective transformation (Nivre et al., 2006).
true	D12-1029.pdf#84.80	Penn Treebank, UAS	NE+PP ( CoNLL X ) UAS  Table 7: Comparison of the parsing accuracy (LAS  and UAS, excluding punctuation) of Nivre's arc- eager parser with non-projective buffer transitions  (NE+LNBA/RNBA) and the parser with the pseudo- projective transformation (Nivre et al., 2006).
true	D12-1029.pdf#75.82	Penn Treebank, UAS	NE+PP ( CoNLL X ) UAS  Table 7: Comparison of the parsing accuracy (LAS  and UAS, excluding punctuation) of Nivre's arc- eager parser with non-projective buffer transitions  (NE+LNBA/RNBA) and the parser with the pseudo- projective transformation (Nivre et al., 2006).
true	D12-1029.pdf#87.71	Penn Treebank, UAS	NE+LNBA / RNBA LAS  Table 7: Comparison of the parsing accuracy (LAS  and UAS, excluding punctuation) of Nivre's arc- eager parser with non-projective buffer transitions  (NE+LNBA/RNBA) and the parser with the pseudo- projective transformation (Nivre et al., 2006).
true	D12-1029.pdf#91.39	Penn Treebank, UAS	NE+LNBA / RNBA UAS  Table 7: Comparison of the parsing accuracy (LAS  and UAS, excluding punctuation) of Nivre's arc- eager parser with non-projective buffer transitions  (NE+LNBA/RNBA) and the parser with the pseudo- projective transformation (Nivre et al., 2006).
true	D12-1029.pdf#87.60	Penn Treebank, UAS	NE+PP ( CoNLL X ) LAS  Table 7: Comparison of the parsing accuracy (LAS  and UAS, excluding punctuation) of Nivre's arc- eager parser with non-projective buffer transitions  (NE+LNBA/RNBA) and the parser with the pseudo- projective transformation (Nivre et al., 2006).
true	D12-1029.pdf#77.92	Penn Treebank, UAS	NE+LNBA / RNBA UAS  Table 7: Comparison of the parsing accuracy (LAS  and UAS, excluding punctuation) of Nivre's arc- eager parser with non-projective buffer transitions  (NE+LNBA/RNBA) and the parser with the pseudo- projective transformation (Nivre et al., 2006).
true	D12-1029.pdf#89.98	Penn Treebank, UAS	NE+LNBA / RNBA UAS  Table 7: Comparison of the parsing accuracy (LAS  and UAS, excluding punctuation) of Nivre's arc- eager parser with non-projective buffer transitions  (NE+LNBA/RNBA) and the parser with the pseudo- projective transformation (Nivre et al., 2006).
true	D12-1029.pdf#86.96	Penn Treebank, UAS	NE+LNBA / RNBA LAS  Table 7: Comparison of the parsing accuracy (LAS  and UAS, excluding punctuation) of Nivre's arc- eager parser with non-projective buffer transitions  (NE+LNBA/RNBA) and the parser with the pseudo- projective transformation (Nivre et al., 2006).
true	D12-1029.pdf#65.68	Penn Treebank, UAS	NE+PP ( CoNLL X ) LAS  Table 7: Comparison of the parsing accuracy (LAS  and UAS, excluding punctuation) of Nivre's arc- eager parser with non-projective buffer transitions  (NE+LNBA/RNBA) and the parser with the pseudo- projective transformation (Nivre et al., 2006).
true	D12-1029.pdf#85.09	Penn Treebank, UAS	NE+LNBA / RNBA LAS  Table 7: Comparison of the parsing accuracy (LAS  and UAS, excluding punctuation) of Nivre's arc- eager parser with non-projective buffer transitions  (NE+LNBA/RNBA) and the parser with the pseudo- projective transformation (Nivre et al., 2006).
true	D12-1029.pdf#78.88	Penn Treebank, UAS	NE+LNBA / RNBA LAS  Table 7: Comparison of the parsing accuracy (LAS  and UAS, excluding punctuation) of Nivre's arc- eager parser with non-projective buffer transitions  (NE+LNBA/RNBA) and the parser with the pseudo- projective transformation (Nivre et al., 2006).
true	D12-1029.pdf#91.22	Penn Treebank, UAS	NE+PP ( CoNLL X ) UAS  Table 7: Comparison of the parsing accuracy (LAS  and UAS, excluding punctuation) of Nivre's arc- eager parser with non-projective buffer transitions  (NE+LNBA/RNBA) and the parser with the pseudo- projective transformation (Nivre et al., 2006).
true	D12-1029.pdf#89.50	Penn Treebank, UAS	NE+PP ( CoNLL X ) UAS  Table 7: Comparison of the parsing accuracy (LAS  and UAS, excluding punctuation) of Nivre's arc- eager parser with non-projective buffer transitions  (NE+LNBA/RNBA) and the parser with the pseudo- projective transformation (Nivre et al., 2006).
true	D12-1029.pdf#84.58	Penn Treebank, UAS	NE+PP ( CoNLL X ) LAS  Table 7: Comparison of the parsing accuracy (LAS  and UAS, excluding punctuation) of Nivre's arc- eager parser with non-projective buffer transitions  (NE+LNBA/RNBA) and the parser with the pseudo- projective transformation (Nivre et al., 2006).
true	D12-1029.pdf#88.98	Penn Treebank, UAS	NE+LNBA / RNBA UAS  Table 7: Comparison of the parsing accuracy (LAS  and UAS, excluding punctuation) of Nivre's arc- eager parser with non-projective buffer transitions  (NE+LNBA/RNBA) and the parser with the pseudo- projective transformation (Nivre et al., 2006).
true	D12-1029.pdf#80.96	Penn Treebank, UAS	PP -  Table 8: Comparison of the precision and recall for pro- jective (PP, PR) and non-projective (NP, NR) arcs, av- eraged over all datasets, obtained by Nivre's arc-eager  parser with and without non-projective buffer transitions  (NE+LNBA/RNBA, NE) and the parser with the pseudo- projective transformation (Nivre et al., 2006).
true	D12-1029.pdf#58.87	Penn Treebank, UAS	NP -  Table 8: Comparison of the precision and recall for pro- jective (PP, PR) and non-projective (NP, NR) arcs, av- eraged over all datasets, obtained by Nivre's arc-eager  parser with and without non-projective buffer transitions  (NE+LNBA/RNBA, NE) and the parser with the pseudo- projective transformation (Nivre et al., 2006).
true	D12-1029.pdf#81.33	Penn Treebank, UAS	PR -  Table 8: Comparison of the precision and recall for pro- jective (PP, PR) and non-projective (NP, NR) arcs, av- eraged over all datasets, obtained by Nivre's arc-eager  parser with and without non-projective buffer transitions  (NE+LNBA/RNBA, NE) and the parser with the pseudo- projective transformation (Nivre et al., 2006).
true	D12-1029.pdf#29.57	Penn Treebank, UAS	NR -  Table 8: Comparison of the precision and recall for pro- jective (PP, PR) and non-projective (NP, NR) arcs, av- eraged over all datasets, obtained by Nivre's arc-eager  parser with and without non-projective buffer transitions  (NE+LNBA/RNBA, NE) and the parser with the pseudo- projective transformation (Nivre et al., 2006).
true	P13-2096.pdf#25.29	Senseval 2, F1	HIN - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#17.93	Senseval 2, F1	HIN - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#38.64	Senseval 2, F1	HIN - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#55.24	Senseval 2, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#46.56	Senseval 2, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#15.99	Senseval 2, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#31.85	Senseval 2, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#38.33	Senseval 2, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#35.60	Senseval 2, F1	MAR - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#36.29	Senseval 2, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#18.49	Senseval 2, F1	MAR - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#18.29	Senseval 2, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#55.23	Senseval 2, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#52.49	Senseval 2, F1	MAR - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#60.38	Senseval 2, F1	HIN - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#34.91	Senseval 2, F1	MAR - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#35.60	Senseval 2, F1	MAR - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#36.29	Senseval 2, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#60.38	Senseval 2, F1	HIN - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#25.29	Senseval 2, F1	HIN - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#15.99	Senseval 2, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#18.49	Senseval 2, F1	MAR - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#46.56	Senseval 2, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#55.24	Senseval 2, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#52.49	Senseval 2, F1	MAR - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#34.91	Senseval 2, F1	MAR - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#38.64	Senseval 2, F1	HIN - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#18.29	Senseval 2, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#17.93	Senseval 2, F1	HIN - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#31.85	Senseval 2, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#38.33	Senseval 2, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#55.23	Senseval 2, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#25.29	SemEval 2015, F1	HIN - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#17.93	SemEval 2015, F1	HIN - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#38.64	SemEval 2015, F1	HIN - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#55.24	SemEval 2015, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#46.56	SemEval 2015, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#15.99	SemEval 2015, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#31.85	SemEval 2015, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#38.33	SemEval 2015, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#35.60	SemEval 2015, F1	MAR - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#36.29	SemEval 2015, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#18.49	SemEval 2015, F1	MAR - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#18.29	SemEval 2015, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#55.23	SemEval 2015, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#52.49	SemEval 2015, F1	MAR - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#60.38	SemEval 2015, F1	HIN - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#34.91	SemEval 2015, F1	MAR - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#35.60	SemEval 2015, F1	MAR - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#36.29	SemEval 2015, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#60.38	SemEval 2015, F1	HIN - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#25.29	SemEval 2015, F1	HIN - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#15.99	SemEval 2015, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#18.49	SemEval 2015, F1	MAR - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#46.56	SemEval 2015, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#55.24	SemEval 2015, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#52.49	SemEval 2015, F1	MAR - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#34.91	SemEval 2015, F1	MAR - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#38.64	SemEval 2015, F1	HIN - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#18.29	SemEval 2015, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#17.93	SemEval 2015, F1	HIN - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#31.85	SemEval 2015, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#38.33	SemEval 2015, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#55.23	SemEval 2015, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#25.29	SemEval 2013, F1	HIN - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#17.93	SemEval 2013, F1	HIN - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#38.64	SemEval 2013, F1	HIN - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#55.24	SemEval 2013, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#46.56	SemEval 2013, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#15.99	SemEval 2013, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#31.85	SemEval 2013, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#38.33	SemEval 2013, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#35.60	SemEval 2013, F1	MAR - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#36.29	SemEval 2013, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#18.49	SemEval 2013, F1	MAR - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#18.29	SemEval 2013, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#55.23	SemEval 2013, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#52.49	SemEval 2013, F1	MAR - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#60.38	SemEval 2013, F1	HIN - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#34.91	SemEval 2013, F1	MAR - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#35.60	SemEval 2013, F1	MAR - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#36.29	SemEval 2013, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#60.38	SemEval 2013, F1	HIN - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#25.29	SemEval 2013, F1	HIN - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#15.99	SemEval 2013, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#18.49	SemEval 2013, F1	MAR - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#46.56	SemEval 2013, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#55.24	SemEval 2013, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#52.49	SemEval 2013, F1	MAR - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#34.91	SemEval 2013, F1	MAR - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#38.64	SemEval 2013, F1	HIN - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#18.29	SemEval 2013, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#17.93	SemEval 2013, F1	HIN - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#31.85	SemEval 2013, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#38.33	SemEval 2013, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#55.23	SemEval 2013, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#25.29	SemEval 2007, F1	HIN - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#17.93	SemEval 2007, F1	HIN - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#38.64	SemEval 2007, F1	HIN - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#55.24	SemEval 2007, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#46.56	SemEval 2007, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#15.99	SemEval 2007, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#31.85	SemEval 2007, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#38.33	SemEval 2007, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#35.60	SemEval 2007, F1	MAR - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#36.29	SemEval 2007, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#18.49	SemEval 2007, F1	MAR - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#18.29	SemEval 2007, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#55.23	SemEval 2007, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#52.49	SemEval 2007, F1	MAR - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#60.38	SemEval 2007, F1	HIN - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#34.91	SemEval 2007, F1	MAR - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#35.60	SemEval 2007, F1	MAR - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#36.29	SemEval 2007, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#60.38	SemEval 2007, F1	HIN - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#25.29	SemEval 2007, F1	HIN - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#15.99	SemEval 2007, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#18.49	SemEval 2007, F1	MAR - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#46.56	SemEval 2007, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#55.24	SemEval 2007, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#52.49	SemEval 2007, F1	MAR - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#34.91	SemEval 2007, F1	MAR - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#38.64	SemEval 2007, F1	HIN - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#18.29	SemEval 2007, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#17.93	SemEval 2007, F1	HIN - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#31.85	SemEval 2007, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#38.33	SemEval 2007, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#55.23	SemEval 2007, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#25.29	Senseval 3, F1	HIN - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#17.93	Senseval 3, F1	HIN - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#38.64	Senseval 3, F1	HIN - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#55.24	Senseval 3, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#46.56	Senseval 3, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#15.99	Senseval 3, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#31.85	Senseval 3, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#38.33	Senseval 3, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#35.60	Senseval 3, F1	MAR - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#36.29	Senseval 3, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#18.49	Senseval 3, F1	MAR - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#18.29	Senseval 3, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#55.23	Senseval 3, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#52.49	Senseval 3, F1	MAR - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#60.38	Senseval 3, F1	HIN - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#34.91	Senseval 3, F1	MAR - HEALTH VERB  Table 1: Comparison(F-Score) of EM-C and EM for Health domain
true	P13-2096.pdf#35.60	Senseval 3, F1	MAR - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#36.29	Senseval 3, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#60.38	Senseval 3, F1	HIN - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#25.29	Senseval 3, F1	HIN - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#15.99	Senseval 3, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#18.49	Senseval 3, F1	MAR - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#46.56	Senseval 3, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#55.24	Senseval 3, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#52.49	Senseval 3, F1	MAR - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#34.91	Senseval 3, F1	MAR - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#38.64	Senseval 3, F1	HIN - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#18.29	Senseval 3, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#17.93	Senseval 3, F1	HIN - HEALTH VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#31.85	Senseval 3, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain HIN - HEALTH VERB HIN - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#38.33	Senseval 3, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P13-2096.pdf#55.23	Senseval 3, F1	Table 1 : Comparison ( F - Score ) of EM - C and EM for Health domain MAR - HEALTH VERB MAR - TOURISM VERB  Table 2: Comparison(F-Score) of EM-C and EM for Tourism domain
true	P10-1154.pdf#84.4	SemEval 2015, F1	A  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#79.6	SemEval 2015, F1	F 1  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#77.5	SemEval 2015, F1	R  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#82.2	SemEval 2015, F1	P  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#96.2	SemEval 2015, F1	Nouns only Algorithm P  Table 2: Performance on Semeval-2007 coarse- grained all-words WSD (nouns only subset).
true	P10-1154.pdf#77.4	SemEval 2015, F1	Nouns only Algorithm R  Table 2: Performance on Semeval-2007 coarse- grained all-words WSD (nouns only subset).
true	P10-1154.pdf#79.4	SemEval 2015, F1	Nouns only Algorithm F 1  Table 2: Performance on Semeval-2007 coarse- grained all-words WSD (nouns only subset).
true	P10-1154.pdf#82.5	SemEval 2015, F1	Algorithm All words P / R / F  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#84.1	SemEval 2015, F1	Algorithm Nouns only P / R / F 1  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#85.5	SemEval 2015, F1	Algorithm Nouns only P / R / F 1  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#81.7	SemEval 2015, F1	Algorithm All words P / R / F  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#83.2	SemEval 2015, F1	Algorithm All words P / R / F  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#42.0	SemEval 2015, F1	Finance P / R / F 1  Table 4: Performance on the Sports and Finance  sections of the dataset from Koeling et al. (2005):   † indicates results from Agirre et al. (2009).
true	P10-1154.pdf#47.8	SemEval 2015, F1	Finance P / R / F  Table 4: Performance on the Sports and Finance  sections of the dataset from Koeling et al. (2005):   † indicates results from Agirre et al. (2009).
true	P10-1154.pdf#84.4	SemEval 2013, F1	A  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#79.6	SemEval 2013, F1	F 1  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#77.5	SemEval 2013, F1	R  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#82.2	SemEval 2013, F1	P  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#96.2	SemEval 2013, F1	Nouns only Algorithm P  Table 2: Performance on Semeval-2007 coarse- grained all-words WSD (nouns only subset).
true	P10-1154.pdf#77.4	SemEval 2013, F1	Nouns only Algorithm R  Table 2: Performance on Semeval-2007 coarse- grained all-words WSD (nouns only subset).
true	P10-1154.pdf#79.4	SemEval 2013, F1	Nouns only Algorithm F 1  Table 2: Performance on Semeval-2007 coarse- grained all-words WSD (nouns only subset).
true	P10-1154.pdf#82.5	SemEval 2013, F1	Algorithm All words P / R / F  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#84.1	SemEval 2013, F1	Algorithm Nouns only P / R / F 1  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#85.5	SemEval 2013, F1	Algorithm Nouns only P / R / F 1  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#81.7	SemEval 2013, F1	Algorithm All words P / R / F  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#83.2	SemEval 2013, F1	Algorithm All words P / R / F  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#42.0	SemEval 2013, F1	Finance P / R / F 1  Table 4: Performance on the Sports and Finance  sections of the dataset from Koeling et al. (2005):   † indicates results from Agirre et al. (2009).
true	P10-1154.pdf#47.8	SemEval 2013, F1	Finance P / R / F  Table 4: Performance on the Sports and Finance  sections of the dataset from Koeling et al. (2005):   † indicates results from Agirre et al. (2009).
true	P10-1154.pdf#84.4	SemEval 2007, F1	A  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#79.6	SemEval 2007, F1	F 1  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#77.5	SemEval 2007, F1	R  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#82.2	SemEval 2007, F1	P  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#96.2	SemEval 2007, F1	Nouns only Algorithm P  Table 2: Performance on Semeval-2007 coarse- grained all-words WSD (nouns only subset).
true	P10-1154.pdf#77.4	SemEval 2007, F1	Nouns only Algorithm R  Table 2: Performance on Semeval-2007 coarse- grained all-words WSD (nouns only subset).
true	P10-1154.pdf#79.4	SemEval 2007, F1	Nouns only Algorithm F 1  Table 2: Performance on Semeval-2007 coarse- grained all-words WSD (nouns only subset).
true	P10-1154.pdf#82.5	SemEval 2007, F1	Algorithm All words P / R / F  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#84.1	SemEval 2007, F1	Algorithm Nouns only P / R / F 1  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#85.5	SemEval 2007, F1	Algorithm Nouns only P / R / F 1  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#81.7	SemEval 2007, F1	Algorithm All words P / R / F  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#83.2	SemEval 2007, F1	Algorithm All words P / R / F  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#42.0	SemEval 2007, F1	Finance P / R / F 1  Table 4: Performance on the Sports and Finance  sections of the dataset from Koeling et al. (2005):   † indicates results from Agirre et al. (2009).
true	P10-1154.pdf#47.8	SemEval 2007, F1	Finance P / R / F  Table 4: Performance on the Sports and Finance  sections of the dataset from Koeling et al. (2005):   † indicates results from Agirre et al. (2009).
true	P10-1154.pdf#84.4	AG News, Error	A  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#79.6	AG News, Error	F 1  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#77.5	AG News, Error	R  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#82.2	AG News, Error	P  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#96.2	AG News, Error	Nouns only Algorithm P  Table 2: Performance on Semeval-2007 coarse- grained all-words WSD (nouns only subset).
true	P10-1154.pdf#77.4	AG News, Error	Nouns only Algorithm R  Table 2: Performance on Semeval-2007 coarse- grained all-words WSD (nouns only subset).
true	P10-1154.pdf#79.4	AG News, Error	Nouns only Algorithm F 1  Table 2: Performance on Semeval-2007 coarse- grained all-words WSD (nouns only subset).
true	P10-1154.pdf#82.5	AG News, Error	Algorithm All words P / R / F  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#84.1	AG News, Error	Algorithm Nouns only P / R / F 1  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#85.5	AG News, Error	Algorithm Nouns only P / R / F 1  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#81.7	AG News, Error	Algorithm All words P / R / F  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#83.2	AG News, Error	Algorithm All words P / R / F  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#42.0	AG News, Error	Finance P / R / F 1  Table 4: Performance on the Sports and Finance  sections of the dataset from Koeling et al. (2005):   † indicates results from Agirre et al. (2009).
true	P10-1154.pdf#47.8	AG News, Error	Finance P / R / F  Table 4: Performance on the Sports and Finance  sections of the dataset from Koeling et al. (2005):   † indicates results from Agirre et al. (2009).
true	P10-1154.pdf#84.4	Senseval 2, F1	A  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#79.6	Senseval 2, F1	F 1  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#77.5	Senseval 2, F1	R  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#82.2	Senseval 2, F1	P  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#96.2	Senseval 2, F1	Nouns only Algorithm P  Table 2: Performance on Semeval-2007 coarse- grained all-words WSD (nouns only subset).
true	P10-1154.pdf#77.4	Senseval 2, F1	Nouns only Algorithm R  Table 2: Performance on Semeval-2007 coarse- grained all-words WSD (nouns only subset).
true	P10-1154.pdf#79.4	Senseval 2, F1	Nouns only Algorithm F 1  Table 2: Performance on Semeval-2007 coarse- grained all-words WSD (nouns only subset).
true	P10-1154.pdf#82.5	Senseval 2, F1	Algorithm All words P / R / F  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#84.1	Senseval 2, F1	Algorithm Nouns only P / R / F 1  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#85.5	Senseval 2, F1	Algorithm Nouns only P / R / F 1  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#81.7	Senseval 2, F1	Algorithm All words P / R / F  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#83.2	Senseval 2, F1	Algorithm All words P / R / F  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#42.0	Senseval 2, F1	Finance P / R / F 1  Table 4: Performance on the Sports and Finance  sections of the dataset from Koeling et al. (2005):   † indicates results from Agirre et al. (2009).
true	P10-1154.pdf#47.8	Senseval 2, F1	Finance P / R / F  Table 4: Performance on the Sports and Finance  sections of the dataset from Koeling et al. (2005):   † indicates results from Agirre et al. (2009).
true	P10-1154.pdf#84.4	Senseval 3, F1	A  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#79.6	Senseval 3, F1	F 1  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#77.5	Senseval 3, F1	R  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#82.2	Senseval 3, F1	P  Table 1: Performance of the mapping algorithm.
true	P10-1154.pdf#96.2	Senseval 3, F1	Nouns only Algorithm P  Table 2: Performance on Semeval-2007 coarse- grained all-words WSD (nouns only subset).
true	P10-1154.pdf#77.4	Senseval 3, F1	Nouns only Algorithm R  Table 2: Performance on Semeval-2007 coarse- grained all-words WSD (nouns only subset).
true	P10-1154.pdf#79.4	Senseval 3, F1	Nouns only Algorithm F 1  Table 2: Performance on Semeval-2007 coarse- grained all-words WSD (nouns only subset).
true	P10-1154.pdf#82.5	Senseval 3, F1	Algorithm All words P / R / F  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#84.1	Senseval 3, F1	Algorithm Nouns only P / R / F 1  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#85.5	Senseval 3, F1	Algorithm Nouns only P / R / F 1  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#81.7	Senseval 3, F1	Algorithm All words P / R / F  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#83.2	Senseval 3, F1	Algorithm All words P / R / F  Table 3: Performance on Semeval-2007 coarse- grained all-words WSD with MFS as a back-off  strategy when no sense assignment is attempted.
true	P10-1154.pdf#42.0	Senseval 3, F1	Finance P / R / F 1  Table 4: Performance on the Sports and Finance  sections of the dataset from Koeling et al. (2005):   † indicates results from Agirre et al. (2009).
true	P10-1154.pdf#47.8	Senseval 3, F1	Finance P / R / F  Table 4: Performance on the Sports and Finance  sections of the dataset from Koeling et al. (2005):   † indicates results from Agirre et al. (2009).
true	N15-3001.pdf#49.2	New York Times Corpus, P@10%	Predicted EDUs P  Table 1: Performance of the two discourse parsers: one  relying on constituent-based syntactic parsing (C), and  another using a dependency parser (D). We report end- to-end results on the 18 relations with nuclearity infor- mation used by (Hernault et al., 2010; Feng and Hirst,  2012), using both manual segmentation of text into EDUs  (left table block), and EDUs predicted by the parser  (right block). We used the Precision/Recall/F 1 metrics  introduced by Marcu (2000). The ablation test removes  various feature groups: features extracted from the de- pendency representation (dep), features from constituent  syntax (const), and coreference features (coref). We com- pare against previous work that reported end-to-end per- formance of their corresponding approaches (Hernault et  al., 2010; Joty et al., 2013; Joty and Moschitti, 2014).
true	N15-3001.pdf#57.3	New York Times Corpus, P@10%	Manual EDUs F 1  Table 1: Performance of the two discourse parsers: one  relying on constituent-based syntactic parsing (C), and  another using a dependency parser (D). We report end- to-end results on the 18 relations with nuclearity infor- mation used by (Hernault et al., 2010; Feng and Hirst,  2012), using both manual segmentation of text into EDUs  (left table block), and EDUs predicted by the parser  (right block). We used the Precision/Recall/F 1 metrics  introduced by Marcu (2000). The ablation test removes  various feature groups: features extracted from the de- pendency representation (dep), features from constituent  syntax (const), and coreference features (coref). We com- pare against previous work that reported end-to-end per- formance of their corresponding approaches (Hernault et  al., 2010; Joty et al., 2013; Joty and Moschitti, 2014).
true	N15-3001.pdf#48.5	New York Times Corpus, P@10%	Predicted EDUs R  Table 1: Performance of the two discourse parsers: one  relying on constituent-based syntactic parsing (C), and  another using a dependency parser (D). We report end- to-end results on the 18 relations with nuclearity infor- mation used by (Hernault et al., 2010; Feng and Hirst,  2012), using both manual segmentation of text into EDUs  (left table block), and EDUs predicted by the parser  (right block). We used the Precision/Recall/F 1 metrics  introduced by Marcu (2000). The ablation test removes  various feature groups: features extracted from the de- pendency representation (dep), features from constituent  syntax (const), and coreference features (coref). We com- pare against previous work that reported end-to-end per- formance of their corresponding approaches (Hernault et  al., 2010; Joty et al., 2013; Joty and Moschitti, 2014).
true	N15-3001.pdf#48.9	New York Times Corpus, P@10%	Predicted EDUs F 1  Table 1: Performance of the two discourse parsers: one  relying on constituent-based syntactic parsing (C), and  another using a dependency parser (D). We report end- to-end results on the 18 relations with nuclearity infor- mation used by (Hernault et al., 2010; Feng and Hirst,  2012), using both manual segmentation of text into EDUs  (left table block), and EDUs predicted by the parser  (right block). We used the Precision/Recall/F 1 metrics  introduced by Marcu (2000). The ablation test removes  various feature groups: features extracted from the de- pendency representation (dep), features from constituent  syntax (const), and coreference features (coref). We com- pare against previous work that reported end-to-end per- formance of their corresponding approaches (Hernault et  al., 2010; Joty et al., 2013; Joty and Moschitti, 2014).
true	N15-3001.pdf#49.2	Penn Treebank, LAS	Predicted EDUs P  Table 1: Performance of the two discourse parsers: one  relying on constituent-based syntactic parsing (C), and  another using a dependency parser (D). We report end- to-end results on the 18 relations with nuclearity infor- mation used by (Hernault et al., 2010; Feng and Hirst,  2012), using both manual segmentation of text into EDUs  (left table block), and EDUs predicted by the parser  (right block). We used the Precision/Recall/F 1 metrics  introduced by Marcu (2000). The ablation test removes  various feature groups: features extracted from the de- pendency representation (dep), features from constituent  syntax (const), and coreference features (coref). We com- pare against previous work that reported end-to-end per- formance of their corresponding approaches (Hernault et  al., 2010; Joty et al., 2013; Joty and Moschitti, 2014).
true	N15-3001.pdf#57.3	Penn Treebank, LAS	Manual EDUs F 1  Table 1: Performance of the two discourse parsers: one  relying on constituent-based syntactic parsing (C), and  another using a dependency parser (D). We report end- to-end results on the 18 relations with nuclearity infor- mation used by (Hernault et al., 2010; Feng and Hirst,  2012), using both manual segmentation of text into EDUs  (left table block), and EDUs predicted by the parser  (right block). We used the Precision/Recall/F 1 metrics  introduced by Marcu (2000). The ablation test removes  various feature groups: features extracted from the de- pendency representation (dep), features from constituent  syntax (const), and coreference features (coref). We com- pare against previous work that reported end-to-end per- formance of their corresponding approaches (Hernault et  al., 2010; Joty et al., 2013; Joty and Moschitti, 2014).
true	N15-3001.pdf#48.5	Penn Treebank, LAS	Predicted EDUs R  Table 1: Performance of the two discourse parsers: one  relying on constituent-based syntactic parsing (C), and  another using a dependency parser (D). We report end- to-end results on the 18 relations with nuclearity infor- mation used by (Hernault et al., 2010; Feng and Hirst,  2012), using both manual segmentation of text into EDUs  (left table block), and EDUs predicted by the parser  (right block). We used the Precision/Recall/F 1 metrics  introduced by Marcu (2000). The ablation test removes  various feature groups: features extracted from the de- pendency representation (dep), features from constituent  syntax (const), and coreference features (coref). We com- pare against previous work that reported end-to-end per- formance of their corresponding approaches (Hernault et  al., 2010; Joty et al., 2013; Joty and Moschitti, 2014).
true	N15-3001.pdf#48.9	Penn Treebank, LAS	Predicted EDUs F 1  Table 1: Performance of the two discourse parsers: one  relying on constituent-based syntactic parsing (C), and  another using a dependency parser (D). We report end- to-end results on the 18 relations with nuclearity infor- mation used by (Hernault et al., 2010; Feng and Hirst,  2012), using both manual segmentation of text into EDUs  (left table block), and EDUs predicted by the parser  (right block). We used the Precision/Recall/F 1 metrics  introduced by Marcu (2000). The ablation test removes  various feature groups: features extracted from the de- pendency representation (dep), features from constituent  syntax (const), and coreference features (coref). We com- pare against previous work that reported end-to-end per- formance of their corresponding approaches (Hernault et  al., 2010; Joty et al., 2013; Joty and Moschitti, 2014).
true	P12-1069.pdf#96.6	Penn Treebank, F1	- root - - -  Table 1: Results for test data: Time measures the parsing time per sentence in seconds. Accuracy is an unlabeled  attachment score, complete is a sentence complete rate, and root is a correct root rate.  *  indicates our experiments.
true	P12-1069.pdf#45.9	Penn Treebank, F1	- complete - - -  Table 1: Results for test data: Time measures the parsing time per sentence in seconds. Accuracy is an unlabeled  attachment score, complete is a sentence complete rate, and root is a correct root rate.  *  indicates our experiments.
true	P12-1069.pdf#92.6	Penn Treebank, F1	- accuracy - - -  Table 1: Results for test data: Time measures the parsing time per sentence in seconds. Accuracy is an unlabeled  attachment score, complete is a sentence complete rate, and root is a correct root rate.  *  indicates our experiments.
true	P12-1069.pdf#96.6	Penn Treebank, UAS	- root - - -  Table 1: Results for test data: Time measures the parsing time per sentence in seconds. Accuracy is an unlabeled  attachment score, complete is a sentence complete rate, and root is a correct root rate.  *  indicates our experiments.
true	P12-1069.pdf#45.9	Penn Treebank, UAS	- complete - - -  Table 1: Results for test data: Time measures the parsing time per sentence in seconds. Accuracy is an unlabeled  attachment score, complete is a sentence complete rate, and root is a correct root rate.  *  indicates our experiments.
true	P12-1069.pdf#92.6	Penn Treebank, UAS	- accuracy - - -  Table 1: Results for test data: Time measures the parsing time per sentence in seconds. Accuracy is an unlabeled  attachment score, complete is a sentence complete rate, and root is a correct root rate.  *  indicates our experiments.
true	P12-1069.pdf#96.6	Penn Treebank, POS	- root - - -  Table 1: Results for test data: Time measures the parsing time per sentence in seconds. Accuracy is an unlabeled  attachment score, complete is a sentence complete rate, and root is a correct root rate.  *  indicates our experiments.
true	P12-1069.pdf#45.9	Penn Treebank, POS	- complete - - -  Table 1: Results for test data: Time measures the parsing time per sentence in seconds. Accuracy is an unlabeled  attachment score, complete is a sentence complete rate, and root is a correct root rate.  *  indicates our experiments.
true	P12-1069.pdf#92.6	Penn Treebank, POS	- accuracy - - -  Table 1: Results for test data: Time measures the parsing time per sentence in seconds. Accuracy is an unlabeled  attachment score, complete is a sentence complete rate, and root is a correct root rate.  *  indicates our experiments.
true	P12-1069.pdf#96.6	Penn Treebank, LAS	- root - - -  Table 1: Results for test data: Time measures the parsing time per sentence in seconds. Accuracy is an unlabeled  attachment score, complete is a sentence complete rate, and root is a correct root rate.  *  indicates our experiments.
true	P12-1069.pdf#45.9	Penn Treebank, LAS	- complete - - -  Table 1: Results for test data: Time measures the parsing time per sentence in seconds. Accuracy is an unlabeled  attachment score, complete is a sentence complete rate, and root is a correct root rate.  *  indicates our experiments.
true	P12-1069.pdf#92.6	Penn Treebank, LAS	- accuracy - - -  Table 1: Results for test data: Time measures the parsing time per sentence in seconds. Accuracy is an unlabeled  attachment score, complete is a sentence complete rate, and root is a correct root rate.  *  indicates our experiments.
true	N10-1124.pdf#+35.0%‡	SearchQA, Unigram Acc	MAP % Avg .  Table 3: Cross-validation (train→test) scores for the baseline (Kullback-Leibler divergence), model-1 with 1 and 2- pass tuning, model-2 and their combination (model-1+2). Relative increase over the baseline is also given (averaged  between Qa and Qb). (  ‡ : t.test < 0.01)
true	N10-1124.pdf#+60.5%‡	SearchQA, Unigram Acc	P@10 % Avg .  Table 3: Cross-validation (train→test) scores for the baseline (Kullback-Leibler divergence), model-1 with 1 and 2- pass tuning, model-2 and their combination (model-1+2). Relative increase over the baseline is also given (averaged  between Qa and Qb). (  ‡ : t.test < 0.01)
true	N10-1124.pdf#+37.5%‡	SearchQA, Unigram Acc	MAP % Avg .  Table 3: Cross-validation (train→test) scores for the baseline (Kullback-Leibler divergence), model-1 with 1 and 2- pass tuning, model-2 and their combination (model-1+2). Relative increase over the baseline is also given (averaged  between Qa and Qb). (  ‡ : t.test < 0.01)
true	N10-1124.pdf#+70.0%‡	SearchQA, Unigram Acc	P@10 % Avg .  Table 3: Cross-validation (train→test) scores for the baseline (Kullback-Leibler divergence), model-1 with 1 and 2- pass tuning, model-2 and their combination (model-1+2). Relative increase over the baseline is also given (averaged  between Qa and Qb). (  ‡ : t.test < 0.01)
true	N10-1124.pdf#+64.1%‡	SearchQA, Unigram Acc	P@10 % Avg .  Table 3: Cross-validation (train→test) scores for the baseline (Kullback-Leibler divergence), model-1 with 1 and 2- pass tuning, model-2 and their combination (model-1+2). Relative increase over the baseline is also given (averaged  between Qa and Qb). (  ‡ : t.test < 0.01)
true	N10-1124.pdf#+11.0%	SearchQA, Unigram Acc	P@10 % Avg .  Table 3: Cross-validation (train→test) scores for the baseline (Kullback-Leibler divergence), model-1 with 1 and 2- pass tuning, model-2 and their combination (model-1+2). Relative increase over the baseline is also given (averaged  between Qa and Qb). (  ‡ : t.test < 0.01)
true	N10-1124.pdf#+61.5%‡	SearchQA, Unigram Acc	MAP % Avg .  Table 3: Cross-validation (train→test) scores for the baseline (Kullback-Leibler divergence), model-1 with 1 and 2- pass tuning, model-2 and their combination (model-1+2). Relative increase over the baseline is also given (averaged  between Qa and Qb). (  ‡ : t.test < 0.01)
true	N10-1124.pdf#+22.5%‡	SearchQA, Unigram Acc	MAP % Avg .  Table 3: Cross-validation (train→test) scores for the baseline (Kullback-Leibler divergence), model-1 with 1 and 2- pass tuning, model-2 and their combination (model-1+2). Relative increase over the baseline is also given (averaged  between Qa and Qb). (  ‡ : t.test < 0.01)
true	D15-1213.pdf#54.1	benchmark Vietnamese dependency treebank VnDT, UAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#68.9	benchmark Vietnamese dependency treebank VnDT, UAS	NT - Select LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#78.6	benchmark Vietnamese dependency treebank VnDT, UAS	Ours UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#64.5	benchmark Vietnamese dependency treebank VnDT, UAS	Direct LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#59.8	benchmark Vietnamese dependency treebank VnDT, UAS	Ours  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#72.6	benchmark Vietnamese dependency treebank VnDT, UAS	Ours UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#71.7	benchmark Vietnamese dependency treebank VnDT, UAS	Ours UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#61.0	benchmark Vietnamese dependency treebank VnDT, UAS	Ours UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#78.0	benchmark Vietnamese dependency treebank VnDT, UAS	Ours UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#69.4	benchmark Vietnamese dependency treebank VnDT, UAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#79.4	benchmark Vietnamese dependency treebank VnDT, UAS	NT - Select UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#75.9	benchmark Vietnamese dependency treebank VnDT, UAS	Direct UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#62.5	benchmark Vietnamese dependency treebank VnDT, UAS	Ours UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#70.5	benchmark Vietnamese dependency treebank VnDT, UAS	Ours  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#21.3	benchmark Vietnamese dependency treebank VnDT, UAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#72.5	benchmark Vietnamese dependency treebank VnDT, UAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#55.1	benchmark Vietnamese dependency treebank VnDT, UAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#30.5	benchmark Vietnamese dependency treebank VnDT, UAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#69.4	benchmark Vietnamese dependency treebank VnDT, UAS	NT - Select LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#79.1	benchmark Vietnamese dependency treebank VnDT, UAS	NT - Select UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#68.3	benchmark Vietnamese dependency treebank VnDT, UAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#43.5	benchmark Vietnamese dependency treebank VnDT, UAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#70.7	benchmark Vietnamese dependency treebank VnDT, UAS	Ours UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#79.6	benchmark Vietnamese dependency treebank VnDT, UAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#73.9	benchmark Vietnamese dependency treebank VnDT, UAS	Semi - supervised Transfer Multiway LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#68.5	benchmark Vietnamese dependency treebank VnDT, UAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#76.6	benchmark Vietnamese dependency treebank VnDT, UAS	Semi - supervised Transfer Sup50 UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#82.5	benchmark Vietnamese dependency treebank VnDT, UAS	Supervised Parsing ( RBGParser ) Ours  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#77.9	benchmark Vietnamese dependency treebank VnDT, UAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#70.2	benchmark Vietnamese dependency treebank VnDT, UAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#77.2	benchmark Vietnamese dependency treebank VnDT, UAS	Supervised Parsing ( RBGParser ) Ours  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#70.4	benchmark Vietnamese dependency treebank VnDT, UAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#54.7	benchmark Vietnamese dependency treebank VnDT, UAS	Semi - supervised Transfer Sup50 LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#61.3	benchmark Vietnamese dependency treebank VnDT, UAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#65.6	benchmark Vietnamese dependency treebank VnDT, UAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#72.6	benchmark Vietnamese dependency treebank VnDT, UAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#78.4	benchmark Vietnamese dependency treebank VnDT, UAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#74.2	benchmark Vietnamese dependency treebank VnDT, UAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#80.9	benchmark Vietnamese dependency treebank VnDT, UAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#70.5	benchmark Vietnamese dependency treebank VnDT, UAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#79.3	benchmark Vietnamese dependency treebank VnDT, UAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#79.1	benchmark Vietnamese dependency treebank VnDT, UAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#67.9	benchmark Vietnamese dependency treebank VnDT, UAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#71.8	benchmark Vietnamese dependency treebank VnDT, UAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#79.3	benchmark Vietnamese dependency treebank VnDT, UAS	Semi - supervised Transfer Multiway UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#78.3	benchmark Vietnamese dependency treebank VnDT, UAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#54.1	benchmark Vietnamese dependency treebank VnDT, LAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#68.9	benchmark Vietnamese dependency treebank VnDT, LAS	NT - Select LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#78.6	benchmark Vietnamese dependency treebank VnDT, LAS	Ours UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#64.5	benchmark Vietnamese dependency treebank VnDT, LAS	Direct LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#59.8	benchmark Vietnamese dependency treebank VnDT, LAS	Ours  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#72.6	benchmark Vietnamese dependency treebank VnDT, LAS	Ours UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#71.7	benchmark Vietnamese dependency treebank VnDT, LAS	Ours UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#61.0	benchmark Vietnamese dependency treebank VnDT, LAS	Ours UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#78.0	benchmark Vietnamese dependency treebank VnDT, LAS	Ours UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#69.4	benchmark Vietnamese dependency treebank VnDT, LAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#79.4	benchmark Vietnamese dependency treebank VnDT, LAS	NT - Select UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#75.9	benchmark Vietnamese dependency treebank VnDT, LAS	Direct UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#62.5	benchmark Vietnamese dependency treebank VnDT, LAS	Ours UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#70.5	benchmark Vietnamese dependency treebank VnDT, LAS	Ours  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#21.3	benchmark Vietnamese dependency treebank VnDT, LAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#72.5	benchmark Vietnamese dependency treebank VnDT, LAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#55.1	benchmark Vietnamese dependency treebank VnDT, LAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#30.5	benchmark Vietnamese dependency treebank VnDT, LAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#69.4	benchmark Vietnamese dependency treebank VnDT, LAS	NT - Select LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#79.1	benchmark Vietnamese dependency treebank VnDT, LAS	NT - Select UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#68.3	benchmark Vietnamese dependency treebank VnDT, LAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#43.5	benchmark Vietnamese dependency treebank VnDT, LAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#70.7	benchmark Vietnamese dependency treebank VnDT, LAS	Ours UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#79.6	benchmark Vietnamese dependency treebank VnDT, LAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#73.9	benchmark Vietnamese dependency treebank VnDT, LAS	Semi - supervised Transfer Multiway LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#68.5	benchmark Vietnamese dependency treebank VnDT, LAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#76.6	benchmark Vietnamese dependency treebank VnDT, LAS	Semi - supervised Transfer Sup50 UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#82.5	benchmark Vietnamese dependency treebank VnDT, LAS	Supervised Parsing ( RBGParser ) Ours  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#77.9	benchmark Vietnamese dependency treebank VnDT, LAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#70.2	benchmark Vietnamese dependency treebank VnDT, LAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#77.2	benchmark Vietnamese dependency treebank VnDT, LAS	Supervised Parsing ( RBGParser ) Ours  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#70.4	benchmark Vietnamese dependency treebank VnDT, LAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#54.7	benchmark Vietnamese dependency treebank VnDT, LAS	Semi - supervised Transfer Sup50 LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#61.3	benchmark Vietnamese dependency treebank VnDT, LAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#65.6	benchmark Vietnamese dependency treebank VnDT, LAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#72.6	benchmark Vietnamese dependency treebank VnDT, LAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#78.4	benchmark Vietnamese dependency treebank VnDT, LAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#74.2	benchmark Vietnamese dependency treebank VnDT, LAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#80.9	benchmark Vietnamese dependency treebank VnDT, LAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#70.5	benchmark Vietnamese dependency treebank VnDT, LAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#79.3	benchmark Vietnamese dependency treebank VnDT, LAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#79.1	benchmark Vietnamese dependency treebank VnDT, LAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#67.9	benchmark Vietnamese dependency treebank VnDT, LAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#71.8	benchmark Vietnamese dependency treebank VnDT, LAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#79.3	benchmark Vietnamese dependency treebank VnDT, LAS	Semi - supervised Transfer Multiway UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#78.3	benchmark Vietnamese dependency treebank VnDT, LAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#54.1	Penn Treebank, UAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#68.9	Penn Treebank, UAS	NT - Select LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#78.6	Penn Treebank, UAS	Ours UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#64.5	Penn Treebank, UAS	Direct LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#59.8	Penn Treebank, UAS	Ours  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#72.6	Penn Treebank, UAS	Ours UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#71.7	Penn Treebank, UAS	Ours UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#61.0	Penn Treebank, UAS	Ours UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#78.0	Penn Treebank, UAS	Ours UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#69.4	Penn Treebank, UAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#79.4	Penn Treebank, UAS	NT - Select UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#75.9	Penn Treebank, UAS	Direct UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#62.5	Penn Treebank, UAS	Ours UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#70.5	Penn Treebank, UAS	Ours  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#21.3	Penn Treebank, UAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#72.5	Penn Treebank, UAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#55.1	Penn Treebank, UAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#30.5	Penn Treebank, UAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#69.4	Penn Treebank, UAS	NT - Select LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#79.1	Penn Treebank, UAS	NT - Select UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#68.3	Penn Treebank, UAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#43.5	Penn Treebank, UAS	Ours LAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#70.7	Penn Treebank, UAS	Ours UAS  Table 5: Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of  different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi- way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model  without tensor component, corresponding to a re-implementation of previous transfer model (Täckström  et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the  results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS.
true	D15-1213.pdf#79.6	Penn Treebank, UAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#73.9	Penn Treebank, UAS	Semi - supervised Transfer Multiway LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#68.5	Penn Treebank, UAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#76.6	Penn Treebank, UAS	Semi - supervised Transfer Sup50 UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#82.5	Penn Treebank, UAS	Supervised Parsing ( RBGParser ) Ours  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#77.9	Penn Treebank, UAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#70.2	Penn Treebank, UAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#77.2	Penn Treebank, UAS	Supervised Parsing ( RBGParser ) Ours  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#70.4	Penn Treebank, UAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#54.7	Penn Treebank, UAS	Semi - supervised Transfer Sup50 LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#61.3	Penn Treebank, UAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#65.6	Penn Treebank, UAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#72.6	Penn Treebank, UAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#78.4	Penn Treebank, UAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#74.2	Penn Treebank, UAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#80.9	Penn Treebank, UAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#70.5	Penn Treebank, UAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#79.3	Penn Treebank, UAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#79.1	Penn Treebank, UAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#67.9	Penn Treebank, UAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#71.8	Penn Treebank, UAS	Supervised Parsing ( RBGParser ) Ours LAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#79.3	Penn Treebank, UAS	Semi - supervised Transfer Multiway UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1213.pdf#78.3	Penn Treebank, UAS	Supervised Parsing ( RBGParser ) Ours UAS  Table 7: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50  annotated sentences in the target language are available. "Sup50" columns show the results of our model  when only supervised data in the target language is available. We also include in the last two columns  the supervised training results with partial or full lexicalization as the performance upper bound. Other  columns have the same meaning as in
true	D15-1177.pdf#58.5	SemEval 2007, F1	- - -  Table 2: Results for the word similarity task  (Spearman's ρ × 100).
true	D15-1177.pdf#62.5	SemEval 2007, F1	- - -  Table 2: Results for the word similarity task  (Spearman's ρ × 100).
true	D15-1177.pdf#61.8	SemEval 2007, F1	Table 2: Results for the word similarity task  (Spearman's ρ × 100).
true	D15-1177.pdf#74.9	SemEval 2007, F1	Table 3: Results with different error functions for  the paraphrase detection task (accuracy × 100).
true	D15-1177.pdf#74.3	SemEval 2007, F1	Cosine  Table 3: Results with different error functions for  the paraphrase detection task (accuracy × 100).
true	D15-1177.pdf#76.0	SemEval 2007, F1	Soft DD  Table 4.
true	D15-1177.pdf#75.2	SemEval 2007, F1	Soft DD -  Table 4.
true	D15-1177.pdf#76.0	SemEval 2007, F1	Soft DD  Table 4: Different disambiguation choices for the  paraphrase detection task (accuracy × 100).
true	D15-1177.pdf#75.2	SemEval 2007, F1	Soft DD -  Table 4: Different disambiguation choices for the  paraphrase detection task (accuracy × 100).
true	D15-1177.pdf#76.6	SemEval 2007, F1	Dynamic  Table 5: Results with average pooling for the para- phrase detection task (accuracy × 100).
true	D15-1177.pdf#77.6	SemEval 2007, F1	Table 5: Results with average pooling for the para- phrase detection task (accuracy × 100).
true	D15-1177.pdf#85.3	SemEval 2007, F1	F1  Table 6: Cross-model comparison in the para- phrase detection task.
true	D15-1177.pdf#78.6	SemEval 2007, F1	Acc .  Table 6: Cross-model comparison in the para- phrase detection task.
true	D15-1177.pdf#58.5	Senseval 3, F1	- - -  Table 2: Results for the word similarity task  (Spearman's ρ × 100).
true	D15-1177.pdf#62.5	Senseval 3, F1	- - -  Table 2: Results for the word similarity task  (Spearman's ρ × 100).
true	D15-1177.pdf#61.8	Senseval 3, F1	Table 2: Results for the word similarity task  (Spearman's ρ × 100).
true	D15-1177.pdf#74.9	Senseval 3, F1	Table 3: Results with different error functions for  the paraphrase detection task (accuracy × 100).
true	D15-1177.pdf#74.3	Senseval 3, F1	Cosine  Table 3: Results with different error functions for  the paraphrase detection task (accuracy × 100).
true	D15-1177.pdf#76.0	Senseval 3, F1	Soft DD  Table 4.
true	D15-1177.pdf#75.2	Senseval 3, F1	Soft DD -  Table 4.
true	D15-1177.pdf#76.0	Senseval 3, F1	Soft DD  Table 4: Different disambiguation choices for the  paraphrase detection task (accuracy × 100).
true	D15-1177.pdf#75.2	Senseval 3, F1	Soft DD -  Table 4: Different disambiguation choices for the  paraphrase detection task (accuracy × 100).
true	D15-1177.pdf#76.6	Senseval 3, F1	Dynamic  Table 5: Results with average pooling for the para- phrase detection task (accuracy × 100).
true	D15-1177.pdf#77.6	Senseval 3, F1	Table 5: Results with average pooling for the para- phrase detection task (accuracy × 100).
true	D15-1177.pdf#85.3	Senseval 3, F1	F1  Table 6: Cross-model comparison in the para- phrase detection task.
true	D15-1177.pdf#78.6	Senseval 3, F1	Acc .  Table 6: Cross-model comparison in the para- phrase detection task.
true	D15-1177.pdf#58.5	Senseval 2, F1	- - -  Table 2: Results for the word similarity task  (Spearman's ρ × 100).
true	D15-1177.pdf#62.5	Senseval 2, F1	- - -  Table 2: Results for the word similarity task  (Spearman's ρ × 100).
true	D15-1177.pdf#61.8	Senseval 2, F1	Table 2: Results for the word similarity task  (Spearman's ρ × 100).
true	D15-1177.pdf#74.9	Senseval 2, F1	Table 3: Results with different error functions for  the paraphrase detection task (accuracy × 100).
true	D15-1177.pdf#74.3	Senseval 2, F1	Cosine  Table 3: Results with different error functions for  the paraphrase detection task (accuracy × 100).
true	D15-1177.pdf#76.0	Senseval 2, F1	Soft DD  Table 4.
true	D15-1177.pdf#75.2	Senseval 2, F1	Soft DD -  Table 4.
true	D15-1177.pdf#76.0	Senseval 2, F1	Soft DD  Table 4: Different disambiguation choices for the  paraphrase detection task (accuracy × 100).
true	D15-1177.pdf#75.2	Senseval 2, F1	Soft DD -  Table 4: Different disambiguation choices for the  paraphrase detection task (accuracy × 100).
true	D15-1177.pdf#76.6	Senseval 2, F1	Dynamic  Table 5: Results with average pooling for the para- phrase detection task (accuracy × 100).
true	D15-1177.pdf#77.6	Senseval 2, F1	Table 5: Results with average pooling for the para- phrase detection task (accuracy × 100).
true	D15-1177.pdf#85.3	Senseval 2, F1	F1  Table 6: Cross-model comparison in the para- phrase detection task.
true	D15-1177.pdf#78.6	Senseval 2, F1	Acc .  Table 6: Cross-model comparison in the para- phrase detection task.
true	D15-1177.pdf#58.5	SemEval 2013, F1	- - -  Table 2: Results for the word similarity task  (Spearman's ρ × 100).
true	D15-1177.pdf#62.5	SemEval 2013, F1	- - -  Table 2: Results for the word similarity task  (Spearman's ρ × 100).
true	D15-1177.pdf#61.8	SemEval 2013, F1	Table 2: Results for the word similarity task  (Spearman's ρ × 100).
true	D15-1177.pdf#74.9	SemEval 2013, F1	Table 3: Results with different error functions for  the paraphrase detection task (accuracy × 100).
true	D15-1177.pdf#74.3	SemEval 2013, F1	Cosine  Table 3: Results with different error functions for  the paraphrase detection task (accuracy × 100).
true	D15-1177.pdf#76.0	SemEval 2013, F1	Soft DD  Table 4.
true	D15-1177.pdf#75.2	SemEval 2013, F1	Soft DD -  Table 4.
true	D15-1177.pdf#76.0	SemEval 2013, F1	Soft DD  Table 4: Different disambiguation choices for the  paraphrase detection task (accuracy × 100).
true	D15-1177.pdf#75.2	SemEval 2013, F1	Soft DD -  Table 4: Different disambiguation choices for the  paraphrase detection task (accuracy × 100).
true	D15-1177.pdf#76.6	SemEval 2013, F1	Dynamic  Table 5: Results with average pooling for the para- phrase detection task (accuracy × 100).
true	D15-1177.pdf#77.6	SemEval 2013, F1	Table 5: Results with average pooling for the para- phrase detection task (accuracy × 100).
true	D15-1177.pdf#85.3	SemEval 2013, F1	F1  Table 6: Cross-model comparison in the para- phrase detection task.
true	D15-1177.pdf#78.6	SemEval 2013, F1	Acc .  Table 6: Cross-model comparison in the para- phrase detection task.
true	D15-1177.pdf#58.5	SemEval 2015, F1	- - -  Table 2: Results for the word similarity task  (Spearman's ρ × 100).
true	D15-1177.pdf#62.5	SemEval 2015, F1	- - -  Table 2: Results for the word similarity task  (Spearman's ρ × 100).
true	D15-1177.pdf#61.8	SemEval 2015, F1	Table 2: Results for the word similarity task  (Spearman's ρ × 100).
true	D15-1177.pdf#74.9	SemEval 2015, F1	Table 3: Results with different error functions for  the paraphrase detection task (accuracy × 100).
true	D15-1177.pdf#74.3	SemEval 2015, F1	Cosine  Table 3: Results with different error functions for  the paraphrase detection task (accuracy × 100).
true	D15-1177.pdf#76.0	SemEval 2015, F1	Soft DD  Table 4.
true	D15-1177.pdf#75.2	SemEval 2015, F1	Soft DD -  Table 4.
true	D15-1177.pdf#76.0	SemEval 2015, F1	Soft DD  Table 4: Different disambiguation choices for the  paraphrase detection task (accuracy × 100).
true	D15-1177.pdf#75.2	SemEval 2015, F1	Soft DD -  Table 4: Different disambiguation choices for the  paraphrase detection task (accuracy × 100).
true	D15-1177.pdf#76.6	SemEval 2015, F1	Dynamic  Table 5: Results with average pooling for the para- phrase detection task (accuracy × 100).
true	D15-1177.pdf#77.6	SemEval 2015, F1	Table 5: Results with average pooling for the para- phrase detection task (accuracy × 100).
true	D15-1177.pdf#85.3	SemEval 2015, F1	F1  Table 6: Cross-model comparison in the para- phrase detection task.
true	D15-1177.pdf#78.6	SemEval 2015, F1	Acc .  Table 6: Cross-model comparison in the para- phrase detection task.
true	P12-2001.pdf#92.80	Penn Treebank, F1	% ) -  Table 3: The performance of our parsing models on the English and Chinese test sets.
true	P12-2001.pdf#85.58	Penn Treebank, F1	% )  Table 3: The performance of our parsing models on the English and Chinese test sets.
true	P12-2001.pdf#91.86	Penn Treebank, F1	% )  Table 3: The performance of our parsing models on the English and Chinese test sets.
true	P12-2001.pdf#85.60	Penn Treebank, F1	% ) -  Table 3: The performance of our parsing models on the English and Chinese test sets.
true	P12-2001.pdf#83.22	Penn Treebank, F1	% )  Table 3: The performance of our parsing models on the English and Chinese test sets.
true	P12-2001.pdf#89.87	Penn Treebank, F1	% )  Table 3: The performance of our parsing models on the English and Chinese test sets.
true	P12-2001.pdf#85.60	Penn Treebank, F1	Combination % ) -  Table 3: The performance of our parsing models on the English and Chinese test sets.
true	P12-2001.pdf#92.80	Penn Treebank, F1	Self - training F1 ( % )  Table 4: Performance comparison on the English test set
true	P12-1110.pdf#98.26	benchmark Vietnamese dependency treebank VnDT, LAS	Table 5: Final results on CTB-5j
true	P12-1110.pdf#94.64	benchmark Vietnamese dependency treebank VnDT, LAS	Table 5: Final results on CTB-5j
true	P12-1110.pdf#96.22	benchmark Vietnamese dependency treebank VnDT, LAS	Segmentation Model ALL  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#74.92(+2.34‡)	benchmark Vietnamese dependency treebank VnDT, LAS	POS Tagging Dependency OOV  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#75.97(+2.44‡)	benchmark Vietnamese dependency treebank VnDT, LAS	POS Tagging Dependency OOV  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#92.97(+0.63‡)	benchmark Vietnamese dependency treebank VnDT, LAS	POS Tagging Dependency ALL  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#92.30(+0.56‡)	benchmark Vietnamese dependency treebank VnDT, LAS	POS Tagging Dependency ALL  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#61.03(+1.21‡)	benchmark Vietnamese dependency treebank VnDT, LAS	POS Tagging Dependency OOV  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#79.38(+1.06‡)	benchmark Vietnamese dependency treebank VnDT, LAS	Segmentation Model OOV  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#96.90(+0.08‡)	benchmark Vietnamese dependency treebank VnDT, LAS	Segmentation Model ALL  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#67.40(+1.96‡)	benchmark Vietnamese dependency treebank VnDT, LAS	POS Tagging Dependency OOV  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#91.28	benchmark Vietnamese dependency treebank VnDT, LAS	- CTB - 7 Test Tag  Table 6: Final results on CTB-6 and CTB-7
true	P12-1110.pdf#96.07	benchmark Vietnamese dependency treebank VnDT, LAS	- CTB - 7 Test Seg  Table 6: Final results on CTB-6 and CTB-7
true	P12-1110.pdf#75.76	benchmark Vietnamese dependency treebank VnDT, LAS	- CTB - 6 Test Dep  Table 6: Final results on CTB-6 and CTB-7
true	P12-1110.pdf#91.95	benchmark Vietnamese dependency treebank VnDT, LAS	- CTB - 6 Test Tag  Table 6: Final results on CTB-6 and CTB-7
true	P12-1110.pdf#74.58	benchmark Vietnamese dependency treebank VnDT, LAS	- CTB - 7 Test Dep  Table 6: Final results on CTB-6 and CTB-7
true	P12-1110.pdf#96.18	benchmark Vietnamese dependency treebank VnDT, LAS	- CTB - 6 Test Seg  Table 6: Final results on CTB-6 and CTB-7
true	P12-1110.pdf#98.26	benchmark Vietnamese dependency treebank VnDT, UAS	Table 5: Final results on CTB-5j
true	P12-1110.pdf#94.64	benchmark Vietnamese dependency treebank VnDT, UAS	Table 5: Final results on CTB-5j
true	P12-1110.pdf#96.22	benchmark Vietnamese dependency treebank VnDT, UAS	Segmentation Model ALL  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#74.92(+2.34‡)	benchmark Vietnamese dependency treebank VnDT, UAS	POS Tagging Dependency OOV  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#75.97(+2.44‡)	benchmark Vietnamese dependency treebank VnDT, UAS	POS Tagging Dependency OOV  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#92.97(+0.63‡)	benchmark Vietnamese dependency treebank VnDT, UAS	POS Tagging Dependency ALL  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#92.30(+0.56‡)	benchmark Vietnamese dependency treebank VnDT, UAS	POS Tagging Dependency ALL  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#61.03(+1.21‡)	benchmark Vietnamese dependency treebank VnDT, UAS	POS Tagging Dependency OOV  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#79.38(+1.06‡)	benchmark Vietnamese dependency treebank VnDT, UAS	Segmentation Model OOV  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#96.90(+0.08‡)	benchmark Vietnamese dependency treebank VnDT, UAS	Segmentation Model ALL  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#67.40(+1.96‡)	benchmark Vietnamese dependency treebank VnDT, UAS	POS Tagging Dependency OOV  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#91.28	benchmark Vietnamese dependency treebank VnDT, UAS	- CTB - 7 Test Tag  Table 6: Final results on CTB-6 and CTB-7
true	P12-1110.pdf#96.07	benchmark Vietnamese dependency treebank VnDT, UAS	- CTB - 7 Test Seg  Table 6: Final results on CTB-6 and CTB-7
true	P12-1110.pdf#75.76	benchmark Vietnamese dependency treebank VnDT, UAS	- CTB - 6 Test Dep  Table 6: Final results on CTB-6 and CTB-7
true	P12-1110.pdf#91.95	benchmark Vietnamese dependency treebank VnDT, UAS	- CTB - 6 Test Tag  Table 6: Final results on CTB-6 and CTB-7
true	P12-1110.pdf#74.58	benchmark Vietnamese dependency treebank VnDT, UAS	- CTB - 7 Test Dep  Table 6: Final results on CTB-6 and CTB-7
true	P12-1110.pdf#96.18	benchmark Vietnamese dependency treebank VnDT, UAS	- CTB - 6 Test Seg  Table 6: Final results on CTB-6 and CTB-7
true	P12-1110.pdf#98.26	Chinese Treebank 6, F1	Table 5: Final results on CTB-5j
true	P12-1110.pdf#94.64	Chinese Treebank 6, F1	Table 5: Final results on CTB-5j
true	P12-1110.pdf#96.22	Chinese Treebank 6, F1	Segmentation Model ALL  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#74.92(+2.34‡)	Chinese Treebank 6, F1	POS Tagging Dependency OOV  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#75.97(+2.44‡)	Chinese Treebank 6, F1	POS Tagging Dependency OOV  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#92.97(+0.63‡)	Chinese Treebank 6, F1	POS Tagging Dependency ALL  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#92.30(+0.56‡)	Chinese Treebank 6, F1	POS Tagging Dependency ALL  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#61.03(+1.21‡)	Chinese Treebank 6, F1	POS Tagging Dependency OOV  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#79.38(+1.06‡)	Chinese Treebank 6, F1	Segmentation Model OOV  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#96.90(+0.08‡)	Chinese Treebank 6, F1	Segmentation Model ALL  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#67.40(+1.96‡)	Chinese Treebank 6, F1	POS Tagging Dependency OOV  Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five  trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep ( ‡ : p < 0.01).
true	P12-1110.pdf#91.28	Chinese Treebank 6, F1	- CTB - 7 Test Tag  Table 6: Final results on CTB-6 and CTB-7
true	P12-1110.pdf#96.07	Chinese Treebank 6, F1	- CTB - 7 Test Seg  Table 6: Final results on CTB-6 and CTB-7
true	P12-1110.pdf#75.76	Chinese Treebank 6, F1	- CTB - 6 Test Dep  Table 6: Final results on CTB-6 and CTB-7
true	P12-1110.pdf#91.95	Chinese Treebank 6, F1	- CTB - 6 Test Tag  Table 6: Final results on CTB-6 and CTB-7
true	P12-1110.pdf#74.58	Chinese Treebank 6, F1	- CTB - 7 Test Dep  Table 6: Final results on CTB-6 and CTB-7
true	P12-1110.pdf#96.18	Chinese Treebank 6, F1	- CTB - 6 Test Seg  Table 6: Final results on CTB-6 and CTB-7
true	P15-2043.pdf#±0.00104	PKU, F1	- - C r  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.961	PKU, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.962	PKU, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.822	PKU, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.722	PKU, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00126	PKU, F1	- - PKU C p r  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00103	PKU, F1	- - MSR C p  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00118	PKU, F1	- - C r  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.961	PKU, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00108	PKU, F1	- - C r  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00101	PKU, F1	- - MSR C p  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00121	PKU, F1	- - C r  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.973	PKU, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00121	PKU, F1	- - PKU C p r  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.973	PKU, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.965	PKU, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.973	PKU, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00118	PKU, F1	C r  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00126	PKU, F1	PKU C p r  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00121	PKU, F1	C r  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00104	PKU, F1	C r  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00108	PKU, F1	C r  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00121	PKU, F1	PKU C p r  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00103	PKU, F1	MSR C p  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00101	PKU, F1	MSR C p  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00104	MSR, F1	- - C r  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.961	MSR, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.962	MSR, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.822	MSR, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.722	MSR, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00126	MSR, F1	- - PKU C p r  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00103	MSR, F1	- - MSR C p  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00118	MSR, F1	- - C r  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.961	MSR, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00108	MSR, F1	- - C r  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00101	MSR, F1	- - MSR C p  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00121	MSR, F1	- - C r  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.973	MSR, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00121	MSR, F1	- - PKU C p r  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.973	MSR, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.965	MSR, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.973	MSR, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00118	MSR, F1	C r  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00126	MSR, F1	PKU C p r  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00121	MSR, F1	C r  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00104	MSR, F1	C r  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00108	MSR, F1	C r  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00121	MSR, F1	PKU C p r  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00103	MSR, F1	MSR C p  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00101	MSR, F1	MSR C p  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00104	Chinese Treebank 6, F1	- - C r  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.961	Chinese Treebank 6, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.962	Chinese Treebank 6, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.822	Chinese Treebank 6, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.722	Chinese Treebank 6, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00126	Chinese Treebank 6, F1	- - PKU C p r  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00103	Chinese Treebank 6, F1	- - MSR C p  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00118	Chinese Treebank 6, F1	- - C r  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.961	Chinese Treebank 6, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00108	Chinese Treebank 6, F1	- - C r  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00101	Chinese Treebank 6, F1	- - MSR C p  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00121	Chinese Treebank 6, F1	- - C r  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.973	Chinese Treebank 6, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00121	Chinese Treebank 6, F1	- - PKU C p r  Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.973	Chinese Treebank 6, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.965	Chinese Treebank 6, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#0.973	Chinese Treebank 6, F1	Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR  Corpora. Here, "R pseudo " denotes the recall of pseudo-OOV words. "Bakeoff-2005" denotes the best  results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since  we use extra resources and our proposed method replies on the synthetic word parser trained on an  dictionary with internal structure annotated, the results cannot be directly compared with the state-of- the-art systems.
true	P15-2043.pdf#±0.00118	Chinese Treebank 6, F1	C r  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00126	Chinese Treebank 6, F1	PKU C p r  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00121	Chinese Treebank 6, F1	C r  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00104	Chinese Treebank 6, F1	C r  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00108	Chinese Treebank 6, F1	C r  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00121	Chinese Treebank 6, F1	PKU C p r  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00103	Chinese Treebank 6, F1	MSR C p  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	P15-2043.pdf#±0.00101	Chinese Treebank 6, F1	MSR C p  Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.
true	N15-1108.pdf#90.9	Penn Treebank, Accuracy	O ( n ) LP  Table 2: Final Results on English (PTB) test set (sec23).   †The empirical complexities for Charniak and Petrov are  O(n 2.5 ) and O(n 2.4 ), resp.,  ‡but Carreras is exact O(n 4 ).
true	N15-1108.pdf#90.7	Penn Treebank, Accuracy	O ( n ) LR  Table 2: Final Results on English (PTB) test set (sec23).   †The empirical complexities for Charniak and Petrov are  O(n 2.5 ) and O(n 2.4 ), resp.,  ‡but Carreras is exact O(n 4 ).
true	N15-1108.pdf#90.8	Penn Treebank, Accuracy	O ( n ) F1  Table 2: Final Results on English (PTB) test set (sec23).   †The empirical complexities for Charniak and Petrov are  O(n 2.5 ) and O(n 2.4 ), resp.,  ‡but Carreras is exact O(n 4 ).
true	N15-1108.pdf#83.6	Penn Treebank, Accuracy	- LR  Table 3: Results on Chinese (CTB) 5.1 test set.
true	N15-1108.pdf#84.2	Penn Treebank, Accuracy	- LP  Table 3: Results on Chinese (CTB) 5.1 test set.
true	N15-1108.pdf#83.9	Penn Treebank, Accuracy	- F1  Table 3: Results on Chinese (CTB) 5.1 test set.
true	N15-1108.pdf#95.6	Penn Treebank, Accuracy	- POS  Table 3: Results on Chinese (CTB) 5.1 test set.
true	N15-1108.pdf#90.9	Penn Treebank, F1	O ( n ) LP  Table 2: Final Results on English (PTB) test set (sec23).   †The empirical complexities for Charniak and Petrov are  O(n 2.5 ) and O(n 2.4 ), resp.,  ‡but Carreras is exact O(n 4 ).
true	N15-1108.pdf#90.7	Penn Treebank, F1	O ( n ) LR  Table 2: Final Results on English (PTB) test set (sec23).   †The empirical complexities for Charniak and Petrov are  O(n 2.5 ) and O(n 2.4 ), resp.,  ‡but Carreras is exact O(n 4 ).
true	N15-1108.pdf#90.8	Penn Treebank, F1	O ( n ) F1  Table 2: Final Results on English (PTB) test set (sec23).   †The empirical complexities for Charniak and Petrov are  O(n 2.5 ) and O(n 2.4 ), resp.,  ‡but Carreras is exact O(n 4 ).
true	N15-1108.pdf#83.6	Penn Treebank, F1	- LR  Table 3: Results on Chinese (CTB) 5.1 test set.
true	N15-1108.pdf#84.2	Penn Treebank, F1	- LP  Table 3: Results on Chinese (CTB) 5.1 test set.
true	N15-1108.pdf#83.9	Penn Treebank, F1	- F1  Table 3: Results on Chinese (CTB) 5.1 test set.
true	N15-1108.pdf#95.6	Penn Treebank, F1	- POS  Table 3: Results on Chinese (CTB) 5.1 test set.
true	D12-1038.pdf#97.69	Chinese Treebank 6, F1	Accuracy ( F  Table 5: Comparison of the baseline annotation transfor- mation, annotation adaptation and a simple corpus merg- ing strategy.
true	D12-1038.pdf#1.33	Chinese Treebank 6, F1	Time ( s )  Table 5: Comparison of the baseline annotation transfor- mation, annotation adaptation and a simple corpus merg- ing strategy.
true	D12-1038.pdf#1.33	Chinese Treebank 6, F1	Time ( s )  Table 5: Comparison of the baseline annotation transfor- mation, annotation adaptation and a simple corpus merg- ing strategy.
true	D14-1082.pdf#92.2	Penn Treebank, LAS	535 Dev UAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#91.0	Penn Treebank, LAS	535 Dev LAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#92.0	Penn Treebank, LAS	560 Test UAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#92.0	Penn Treebank, LAS	535 Test UAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#90.7	Penn Treebank, LAS	535 Test LAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#89.6	Penn Treebank, LAS	448 LAS  Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.
true	D14-1082.pdf#91.8	Penn Treebank, LAS	448 UAS  Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.
true	D14-1082.pdf#89.7	Penn Treebank, LAS	448 LAS  Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.
true	D14-1082.pdf#92.0	Penn Treebank, LAS	448 UAS  Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.
true	D14-1082.pdf#82.4	Penn Treebank, LAS	393 LAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#84.0	Penn Treebank, LAS	393 UAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#83.9	Penn Treebank, LAS	393 UAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#84.0	Penn Treebank, LAS	420 UAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#82.4	Penn Treebank, LAS	393 LAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#92.2	Penn Treebank, F1	535 Dev UAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#91.0	Penn Treebank, F1	535 Dev LAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#92.0	Penn Treebank, F1	560 Test UAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#92.0	Penn Treebank, F1	535 Test UAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#90.7	Penn Treebank, F1	535 Test LAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#89.6	Penn Treebank, F1	448 LAS  Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.
true	D14-1082.pdf#91.8	Penn Treebank, F1	448 UAS  Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.
true	D14-1082.pdf#89.7	Penn Treebank, F1	448 LAS  Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.
true	D14-1082.pdf#92.0	Penn Treebank, F1	448 UAS  Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.
true	D14-1082.pdf#82.4	Penn Treebank, F1	393 LAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#84.0	Penn Treebank, F1	393 UAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#83.9	Penn Treebank, F1	393 UAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#84.0	Penn Treebank, F1	420 UAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#82.4	Penn Treebank, F1	393 LAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#92.2	Penn Treebank, POS	535 Dev UAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#91.0	Penn Treebank, POS	535 Dev LAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#92.0	Penn Treebank, POS	560 Test UAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#92.0	Penn Treebank, POS	535 Test UAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#90.7	Penn Treebank, POS	535 Test LAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#89.6	Penn Treebank, POS	448 LAS  Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.
true	D14-1082.pdf#91.8	Penn Treebank, POS	448 UAS  Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.
true	D14-1082.pdf#89.7	Penn Treebank, POS	448 LAS  Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.
true	D14-1082.pdf#92.0	Penn Treebank, POS	448 UAS  Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.
true	D14-1082.pdf#82.4	Penn Treebank, POS	393 LAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#84.0	Penn Treebank, POS	393 UAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#83.9	Penn Treebank, POS	393 UAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#84.0	Penn Treebank, POS	420 UAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#82.4	Penn Treebank, POS	393 LAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#92.2	Penn Treebank, UAS	535 Dev UAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#91.0	Penn Treebank, UAS	535 Dev LAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#92.0	Penn Treebank, UAS	560 Test UAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#92.0	Penn Treebank, UAS	535 Test UAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#90.7	Penn Treebank, UAS	535 Test LAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#89.6	Penn Treebank, UAS	448 LAS  Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.
true	D14-1082.pdf#91.8	Penn Treebank, UAS	448 UAS  Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.
true	D14-1082.pdf#89.7	Penn Treebank, UAS	448 LAS  Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.
true	D14-1082.pdf#92.0	Penn Treebank, UAS	448 UAS  Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.
true	D14-1082.pdf#82.4	Penn Treebank, UAS	393 LAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#84.0	Penn Treebank, UAS	393 UAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#83.9	Penn Treebank, UAS	393 UAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#84.0	Penn Treebank, UAS	420 UAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#82.4	Penn Treebank, UAS	393 LAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#92.2	benchmark Vietnamese dependency treebank VnDT, UAS	535 Dev UAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#91.0	benchmark Vietnamese dependency treebank VnDT, UAS	535 Dev LAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#92.0	benchmark Vietnamese dependency treebank VnDT, UAS	560 Test UAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#92.0	benchmark Vietnamese dependency treebank VnDT, UAS	535 Test UAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#90.7	benchmark Vietnamese dependency treebank VnDT, UAS	535 Test LAS  Table 4: Accuracy and parsing speed on PTB +  CoNLL dependencies.
true	D14-1082.pdf#89.6	benchmark Vietnamese dependency treebank VnDT, UAS	448 LAS  Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.
true	D14-1082.pdf#91.8	benchmark Vietnamese dependency treebank VnDT, UAS	448 UAS  Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.
true	D14-1082.pdf#89.7	benchmark Vietnamese dependency treebank VnDT, UAS	448 LAS  Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.
true	D14-1082.pdf#92.0	benchmark Vietnamese dependency treebank VnDT, UAS	448 UAS  Table 5: Accuracy and parsing speed on PTB +  Stanford dependencies.
true	D14-1082.pdf#82.4	benchmark Vietnamese dependency treebank VnDT, UAS	393 LAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#84.0	benchmark Vietnamese dependency treebank VnDT, UAS	393 UAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#83.9	benchmark Vietnamese dependency treebank VnDT, UAS	393 UAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#84.0	benchmark Vietnamese dependency treebank VnDT, UAS	420 UAS  Table 6: Accuracy and parsing speed on CTB.
true	D14-1082.pdf#82.4	benchmark Vietnamese dependency treebank VnDT, UAS	393 LAS  Table 6: Accuracy and parsing speed on CTB.
true	D10-1044.pdf#23.76	Text8, Number of params	NIST nst08 - -  Table 2: Results, for EMEA/EP translation into English  (fren) and French (enfr); and for NIST Chinese to En- glish translation with NIST06 and NIST08 evaluation  sets. Numbers are BLEU scores.
true	D10-1044.pdf#37.01	Text8, Number of params	EMEA / EP fren - -  Table 2: Results, for EMEA/EP translation into English  (fren) and French (enfr); and for NIST Chinese to En- glish translation with NIST06 and NIST08 evaluation  sets. Numbers are BLEU scores.
true	D10-1044.pdf#33.90	Text8, Number of params	EMEA / EP enfr - -  Table 2: Results, for EMEA/EP translation into English  (fren) and French (enfr); and for NIST Chinese to En- glish translation with NIST06 and NIST08 evaluation  sets. Numbers are BLEU scores.
true	D10-1044.pdf#30.04	Text8, Number of params	NIST nst06 - -  Table 2: Results, for EMEA/EP translation into English  (fren) and French (enfr); and for NIST Chinese to En- glish translation with NIST06 and NIST08 evaluation  sets. Numbers are BLEU scores.
true	P15-1165.pdf#84.69	Penn Treebank, UAS	MULTI - SOURCE→TARGET es  Table 4: POS tagging (accuracies), K12: KLEMENTIEV, C14: CHANDAR. Parameters tuned on devel- opment data: σ = 0.01, δ = 160. Iterations not tuned (i = 10). Averages do not include Swedish, for  comparability.
true	P15-1165.pdf#0.90	Penn Treebank, UAS	MULTI - SOURCE→TARGET de CHANDAR  Table 4: POS tagging (accuracies), K12: KLEMENTIEV, C14: CHANDAR. Parameters tuned on devel- opment data: σ = 0.01, δ = 160. Iterations not tuned (i = 10). Averages do not include Swedish, for  comparability.
true	P15-1165.pdf#0.49	Penn Treebank, UAS	MULTI - SOURCE→TARGET de INVERTED  Table 4: POS tagging (accuracies), K12: KLEMENTIEV, C14: CHANDAR. Parameters tuned on devel- opment data: σ = 0.01, δ = 160. Iterations not tuned (i = 10). Averages do not include Swedish, for  comparability.
true	P15-1165.pdf#81.18	Penn Treebank, UAS	EN→TARGET de  Table 4: POS tagging (accuracies), K12: KLEMENTIEV, C14: CHANDAR. Parameters tuned on devel- opment data: σ = 0.01, δ = 160. Iterations not tuned (i = 10). Averages do not include Swedish, for  comparability.
true	P15-1165.pdf#0.90	Penn Treebank, UAS	CHANDAR  Table 3: Document classification results (F 1 - scores)
true	P15-1165.pdf#0.49	Penn Treebank, UAS	INVERTED  Table 3: Document classification results (F 1 - scores)
true	P15-1165.pdf#67.49	Penn Treebank, UAS	MULTI - SOURCE→TARGET UAS sv  Table 5: Dependency parsing results on the Universal Treebanks (unlabeled and labeled attachment  scores). Parameters tuned on development data: σ = 0.005, δ = 20, i = 3.
true	P15-1165.pdf#49.32	Penn Treebank, UAS	MULTI - SOURCE→TARGET LAS de  Table 5: Dependency parsing results on the Universal Treebanks (unlabeled and labeled attachment  scores). Parameters tuned on development data: σ = 0.005, δ = 20, i = 3.
true	P15-1165.pdf#56.80	Penn Treebank, UAS	MULTI - SOURCE→TARGET UAS de  Table 5: Dependency parsing results on the Universal Treebanks (unlabeled and labeled attachment  scores). Parameters tuned on development data: σ = 0.005, δ = 20, i = 3.
true	P15-1165.pdf#64.03	Penn Treebank, UAS	MULTI - SOURCE→TARGET UAS es  Table 5: Dependency parsing results on the Universal Treebanks (unlabeled and labeled attachment  scores). Parameters tuned on development data: σ = 0.005, δ = 20, i = 3.
true	P15-1165.pdf#66.22	Penn Treebank, UAS	MULTI - SOURCE→TARGET UAS fr  Table 5: Dependency parsing results on the Universal Treebanks (unlabeled and labeled attachment  scores). Parameters tuned on development data: σ = 0.005, δ = 20, i = 3.
true	P15-1165.pdf#57.86	Penn Treebank, UAS	MULTI - SOURCE→TARGET LAS sv  Table 5: Dependency parsing results on the Universal Treebanks (unlabeled and labeled attachment  scores). Parameters tuned on development data: σ = 0.005, δ = 20, i = 3.
true	P15-1165.pdf#56.79	Penn Treebank, UAS	MULTI - SOURCE→TARGET LAS fr  Table 5: Dependency parsing results on the Universal Treebanks (unlabeled and labeled attachment  scores). Parameters tuned on development data: σ = 0.005, δ = 20, i = 3.
true	P15-1165.pdf#55.03	Penn Treebank, UAS	MULTI - SOURCE→TARGET LAS es  Table 5: Dependency parsing results on the Universal Treebanks (unlabeled and labeled attachment  scores). Parameters tuned on development data: σ = 0.005, δ = 20, i = 3.
true	P15-1165.pdf#0.25	Penn Treebank, UAS	INVERTED  Table 7: Word alignment results (P @1). S=sure (certain) alignments. P=possible alignments.
true	P15-1165.pdf#0.25	Penn Treebank, UAS	INVERTED  Table 7: Word alignment results (P @1). S=sure (certain) alignments. P=possible alignments.
true	P15-1165.pdf#0.41	Penn Treebank, UAS	INVERTED  Table 7: Word alignment results (P @1). S=sure (certain) alignments. P=possible alignments.
true	P15-1165.pdf#0.25	Penn Treebank, UAS	CHANDAR  Table 7: Word alignment results (P @1). S=sure (certain) alignments. P=possible alignments.
true	P15-1165.pdf#0.53	Penn Treebank, UAS	INVERTED  Table 7: Word alignment results (P @1). S=sure (certain) alignments. P=possible alignments.
true	P11-1127.pdf#52.5	WMT 2014 EN-FR, BLEU	Approach dev TER  Table 2: Performance from best single system max- derivation decoding (Best MAX), the best single system  minimum Bayes risk decoding (Best MBR) and mini- mum Bayes risk system combination (MBR-SC) combin- ing three systems.
true	P11-1127.pdf#32.3	WMT 2014 EN-FR, BLEU	Approach dev BLEU  Table 2: Performance from best single system max- derivation decoding (Best MAX), the best single system  minimum Bayes risk decoding (Best MBR) and mini- mum Bayes risk system combination (MBR-SC) combin- ing three systems.
true	P11-1127.pdf#52.3	WMT 2014 EN-FR, BLEU	Approach test TER  Table 2: Performance from best single system max- derivation decoding (Best MAX), the best single system  minimum Bayes risk decoding (Best MBR) and mini- mum Bayes risk system combination (MBR-SC) combin- ing three systems.
true	P11-1127.pdf#32.8	WMT 2014 EN-FR, BLEU	Approach test BLEU  Table 2: Performance from best single system max- derivation decoding (Best MAX), the best single system  minimum Bayes risk decoding (Best MBR) and mini- mum Bayes risk system combination (MBR-SC) combin- ing three systems.
true	P11-1127.pdf#52.3	WMT 2014 EN-FR, BLEU	TER  Table 3: Results on the test set for different setups of  minimum Bayes risk system combination.
true	P11-1127.pdf#32.8	WMT 2014 EN-FR, BLEU	BLEU  Table 3: Results on the test set for different setups of  minimum Bayes risk system combination.
true	P11-1127.pdf#52.5	WMT 2014 EN-DE, BLEU	Approach dev TER  Table 2: Performance from best single system max- derivation decoding (Best MAX), the best single system  minimum Bayes risk decoding (Best MBR) and mini- mum Bayes risk system combination (MBR-SC) combin- ing three systems.
true	P11-1127.pdf#32.3	WMT 2014 EN-DE, BLEU	Approach dev BLEU  Table 2: Performance from best single system max- derivation decoding (Best MAX), the best single system  minimum Bayes risk decoding (Best MBR) and mini- mum Bayes risk system combination (MBR-SC) combin- ing three systems.
true	P11-1127.pdf#52.3	WMT 2014 EN-DE, BLEU	Approach test TER  Table 2: Performance from best single system max- derivation decoding (Best MAX), the best single system  minimum Bayes risk decoding (Best MBR) and mini- mum Bayes risk system combination (MBR-SC) combin- ing three systems.
true	P11-1127.pdf#32.8	WMT 2014 EN-DE, BLEU	Approach test BLEU  Table 2: Performance from best single system max- derivation decoding (Best MAX), the best single system  minimum Bayes risk decoding (Best MBR) and mini- mum Bayes risk system combination (MBR-SC) combin- ing three systems.
true	P11-1127.pdf#52.3	WMT 2014 EN-DE, BLEU	TER  Table 3: Results on the test set for different setups of  minimum Bayes risk system combination.
true	P11-1127.pdf#32.8	WMT 2014 EN-DE, BLEU	BLEU  Table 3: Results on the test set for different setups of  minimum Bayes risk system combination.
true	P10-1075.pdf#45.2%	New York Times Corpus, P@10%	5  Table 1: MaltParser results for Talbanken, for select values (b = below, a = above threshold (Thr.))
true	P10-1075.pdf#36.9%	New York Times Corpus, P@10%	5  Table 1: MaltParser results for Talbanken, for select values (b = below, a = above threshold (Thr.))
true	P10-1075.pdf#46.4%	New York Times Corpus, P@10%	5  Table 1: MaltParser results for Talbanken, for select values (b = below, a = above threshold (Thr.))
true	P10-1075.pdf#49.9%	New York Times Corpus, P@10%	5  Table 1: MaltParser results for Talbanken, for select values (b = below, a = above threshold (Thr.))
true	P10-1075.pdf#46.5%	New York Times Corpus, P@10%	5  Table 1: MaltParser results for Talbanken, for select values (b = below, a = above threshold (Thr.))
true	P10-1075.pdf#43.9%	New York Times Corpus, P@10%	5  Table 1: MaltParser results for Talbanken, for select values (b = below, a = above threshold (Thr.))
true	P10-1075.pdf#44.8%	New York Times Corpus, P@10%	5  Table 1: MaltParser results for Talbanken, for select values (b = below, a = above threshold (Thr.))
true	P10-1075.pdf#44.3%	New York Times Corpus, P@10%	5  Table 1: MaltParser results for Talbanken, for select values (b = below, a = above threshold (Thr.))
true	P14-1113.pdf#80.29	New York Times Corpus, P@10%	F ( % )  Table 3: Comparison of the proposed method with  existing methods in the test set.
true	P14-1113.pdf#73.74	New York Times Corpus, P@10%	F ( % )  Table 3: Comparison of the proposed method with  existing methods in the test set.
true	P14-1113.pdf#65.17	New York Times Corpus, P@10%	Table 5: Performance on the out-of-CilinE data in  the test set.
true	P14-1113.pdf#19.29	New York Times Corpus, P@10%	Table 5: Performance on the out-of-CilinE data in  the test set.
true	P14-1113.pdf#60.62	New York Times Corpus, P@10%	Table 5: Performance on the out-of-CilinE data in  the test set.
true	P14-1113.pdf#69.13	New York Times Corpus, P@10%	Table 5: Performance on the out-of-CilinE data in  the test set.
true	P14-1113.pdf#61.65	New York Times Corpus, P@10%	Table 5: Performance on the out-of-CilinE data in  the test set.
true	P14-1113.pdf#31.12	New York Times Corpus, P@10%	Table 5: Performance on the out-of-CilinE data in  the test set.
true	P14-1113.pdf#71.16	New York Times Corpus, P@10%	Table 5: Performance on the out-of-CilinE data in  the test set.
true	P14-1113.pdf#52.80	New York Times Corpus, P@10%	Table 5: Performance on the out-of-CilinE data in  the test set.
true	P14-1113.pdf#80.39	New York Times Corpus, P@10%	Table 5: Performance on the out-of-CilinE data in  the test set.
true	N15-2013.pdf#4.1	SearchQA, Unigram Acc	Experiments ( Fader on 30 , 370 , 994  Table 1: Examples of features used for relation extraction for "When was Mariah Carey born? Mariah  Carey was born 27 March 1970"
true	N15-2013.pdf#4	SearchQA, Unigram Acc	in total ) . ( Fader on 30 , 370 , 994  Table 1: Examples of features used for relation extraction for "When was Mariah Carey born? Mariah  Carey was born 27 March 1970"
true	P14-1077.pdf#52.8	DBpedia, Error	Table 2 : Results of the highest F1 score on all three datasets . Chinese F1 ( % )  Table 2: Results of the highest F1 score on all three datasets.  DBpedia  Riedel  Chinese
true	P14-1077.pdf#38.3	DBpedia, Error	Table 2 : Results of the highest F1 score on all three datasets . DBpedia F1 ( % )  Table 2: Results of the highest F1 score on all three datasets.  DBpedia  Riedel  Chinese
true	P14-1077.pdf#28.2	DBpedia, Error	Table 2 : Results of the highest F1 score on all three datasets . Riedel F1 ( % )  Table 2: Results of the highest F1 score on all three datasets.  DBpedia  Riedel  Chinese
true	P14-1077.pdf#52.8	New York Times Corpus, P@10%	Table 2 : Results of the highest F1 score on all three datasets . Chinese F1 ( % )  Table 2: Results of the highest F1 score on all three datasets.  DBpedia  Riedel  Chinese
true	P14-1077.pdf#38.3	New York Times Corpus, P@10%	Table 2 : Results of the highest F1 score on all three datasets . DBpedia F1 ( % )  Table 2: Results of the highest F1 score on all three datasets.  DBpedia  Riedel  Chinese
true	P14-1077.pdf#28.2	New York Times Corpus, P@10%	Table 2 : Results of the highest F1 score on all three datasets . Riedel F1 ( % )  Table 2: Results of the highest F1 score on all three datasets.  DBpedia  Riedel  Chinese
true	P11-1035.pdf#0.67	New York Times Corpus, P@10%	
true	P11-1035.pdf#0.72	New York Times Corpus, P@10%	
true	P11-1035.pdf#0.48	New York Times Corpus, P@10%	
true	P11-1035.pdf#0.65	New York Times Corpus, P@10%	
true	P11-1035.pdf#0.65	New York Times Corpus, P@10%	
true	P11-1035.pdf#0.61	New York Times Corpus, P@10%	
true	P11-1035.pdf#0.65	New York Times Corpus, P@10%	
true	P11-1035.pdf#0.50	New York Times Corpus, P@10%	
true	P11-1035.pdf#0.81	New York Times Corpus, P@10%	
true	P11-1035.pdf#0.89N/A	New York Times Corpus, P@10%	
true	P11-1035.pdf#0.74	New York Times Corpus, P@10%	
true	P11-1035.pdf#0.77N/A	New York Times Corpus, P@10%	
true	P11-1035.pdf#0.60	New York Times Corpus, P@10%	
true	P11-1035.pdf#0.90	New York Times Corpus, P@10%	
true	P11-1035.pdf#0.48	New York Times Corpus, P@10%	
true	P11-1035.pdf#0.66	New York Times Corpus, P@10%	
true	P15-2052.pdf#88.21%	SemEval 2013, F1	BINARY  Table 3: Average ten-fold cross-validation classification ac- curacy for different feature sets on two tasks. "Local" refers  to all feature sets described in Section 4.1. BINARY distin- guishes generic and referential ni, FOUR-WAY distinguishes  between generic and three referential senses.
true	P15-2052.pdf#81.45%	SemEval 2013, F1	FOUR - WAY  Table 3: Average ten-fold cross-validation classification ac- curacy for different feature sets on two tasks. "Local" refers  to all feature sets described in Section 4.1. BINARY distin- guishes generic and referential ni, FOUR-WAY distinguishes  between generic and three referential senses.
true	P15-2052.pdf#88.21%	SemEval 2007, F1	BINARY  Table 3: Average ten-fold cross-validation classification ac- curacy for different feature sets on two tasks. "Local" refers  to all feature sets described in Section 4.1. BINARY distin- guishes generic and referential ni, FOUR-WAY distinguishes  between generic and three referential senses.
true	P15-2052.pdf#81.45%	SemEval 2007, F1	FOUR - WAY  Table 3: Average ten-fold cross-validation classification ac- curacy for different feature sets on two tasks. "Local" refers  to all feature sets described in Section 4.1. BINARY distin- guishes generic and referential ni, FOUR-WAY distinguishes  between generic and three referential senses.
true	P15-2052.pdf#88.21%	SemEval 2015, F1	BINARY  Table 3: Average ten-fold cross-validation classification ac- curacy for different feature sets on two tasks. "Local" refers  to all feature sets described in Section 4.1. BINARY distin- guishes generic and referential ni, FOUR-WAY distinguishes  between generic and three referential senses.
true	P15-2052.pdf#81.45%	SemEval 2015, F1	FOUR - WAY  Table 3: Average ten-fold cross-validation classification ac- curacy for different feature sets on two tasks. "Local" refers  to all feature sets described in Section 4.1. BINARY distin- guishes generic and referential ni, FOUR-WAY distinguishes  between generic and three referential senses.
true	P15-2052.pdf#88.21%	Senseval 3, F1	BINARY  Table 3: Average ten-fold cross-validation classification ac- curacy for different feature sets on two tasks. "Local" refers  to all feature sets described in Section 4.1. BINARY distin- guishes generic and referential ni, FOUR-WAY distinguishes  between generic and three referential senses.
true	P15-2052.pdf#81.45%	Senseval 3, F1	FOUR - WAY  Table 3: Average ten-fold cross-validation classification ac- curacy for different feature sets on two tasks. "Local" refers  to all feature sets described in Section 4.1. BINARY distin- guishes generic and referential ni, FOUR-WAY distinguishes  between generic and three referential senses.
true	P15-2052.pdf#88.21%	Senseval 2, F1	BINARY  Table 3: Average ten-fold cross-validation classification ac- curacy for different feature sets on two tasks. "Local" refers  to all feature sets described in Section 4.1. BINARY distin- guishes generic and referential ni, FOUR-WAY distinguishes  between generic and three referential senses.
true	P15-2052.pdf#81.45%	Senseval 2, F1	FOUR - WAY  Table 3: Average ten-fold cross-validation classification ac- curacy for different feature sets on two tasks. "Local" refers  to all feature sets described in Section 4.1. BINARY distin- guishes generic and referential ni, FOUR-WAY distinguishes  between generic and three referential senses.
true	P13-1047.pdf#29.51	New York Times Corpus, P@10%	Multi - task F1 ( aux ) - - - - - - - - - - - -  Table 2: Performance of precision, recall and F 1 for 4 Level 1 relation classes. "-" indicates N.A.
true	P13-1047.pdf#31.53	New York Times Corpus, P@10%	Multi - task F1 ( aux ) - - -  Table 2: Performance of precision, recall and F 1 for 4 Level 1 relation classes. "-" indicates N.A.
true	P13-1047.pdf#47.52	New York Times Corpus, P@10%	Multi - task F1 ( aux ) - - - - - -  Table 2: Performance of precision, recall and F 1 for 4 Level 1 relation classes. "-" indicates N.A.
true	P13-1047.pdf#70.01	New York Times Corpus, P@10%	Multi - task F1 ( aux ) - - - - - - - - -  Table 2: Performance of precision, recall and F 1 for 4 Level 1 relation classes. "-" indicates N.A.
true	P13-1047.pdf#35.94	New York Times Corpus, P@10%	Multi - task F 1 ( aux ) - -  Table 3: Performance of precision, recall and F 1 for 10 Level 2 relation types. "-" indicates 0.00.
true	P13-1047.pdf#32.47	New York Times Corpus, P@10%	Multi - task F 1 ( aux ) - -  Table 3: Performance of precision, recall and F 1 for 10 Level 2 relation types. "-" indicates 0.00.
true	P13-1047.pdf#38.73	New York Times Corpus, P@10%	Multi - task F 1 ( aux ) - -  Table 3: Performance of precision, recall and F 1 for 10 Level 2 relation types. "-" indicates 0.00.
true	P13-1047.pdf#29.77	New York Times Corpus, P@10%	Multi - task F 1 ( aux ) -  Table 3: Performance of precision, recall and F 1 for 10 Level 2 relation types. "-" indicates 0.00.
true	P13-1047.pdf#22.43	New York Times Corpus, P@10%	Multi - task F 1 ( aux )  Table 3: Performance of precision, recall and F 1 for 10 Level 2 relation types. "-" indicates 0.00.
true	P13-1047.pdf#47.00	New York Times Corpus, P@10%	Multi - task F 1 ( aux ) -  Table 3: Performance of precision, recall and F 1 for 10 Level 2 relation types. "-" indicates 0.00.
true	P11-1087.pdf#70.1	Penn Treebank, Accuracy	1HMM - LM - -  Table 2: Many-to-1 accuracy across a range of languages, comparing our model with mkcls and the best published  result ( Berg-Kirkpatrick et al. (2010) and  † Lee et al.
true	P11-1087.pdf#78.8	Penn Treebank, Accuracy	1HMM - LM - - - 94 , 386 195 , 069 - 206 , 678  Table 2: Many-to-1 accuracy across a range of languages, comparing our model with mkcls and the best published  result ( Berg-Kirkpatrick et al. (2010) and  † Lee et al.
true	P11-1087.pdf#78.5	Penn Treebank, Accuracy	1HMM - LM - - - 94 , 386 195 , 069 -  Table 2: Many-to-1 accuracy across a range of languages, comparing our model with mkcls and the best published  result ( Berg-Kirkpatrick et al. (2010) and  † Lee et al.
true	P11-1087.pdf#70.4	Penn Treebank, Accuracy	1HMM - LM - - - 94 , 386  Table 2: Many-to-1 accuracy across a range of languages, comparing our model with mkcls and the best published  result ( Berg-Kirkpatrick et al. (2010) and  † Lee et al.
true	P11-1087.pdf#67.5	Penn Treebank, Accuracy	1HMM - LM  Table 2: Many-to-1 accuracy across a range of languages, comparing our model with mkcls and the best published  result ( Berg-Kirkpatrick et al. (2010) and  † Lee et al.
true	P11-1087.pdf#76.2	Penn Treebank, Accuracy	1HMM - LM - - -  Table 2: Many-to-1 accuracy across a range of languages, comparing our model with mkcls and the best published  result ( Berg-Kirkpatrick et al. (2010) and  † Lee et al.
true	P11-1087.pdf#73.2	Penn Treebank, Accuracy	1HMM - LM -  Table 2: Many-to-1 accuracy across a range of languages, comparing our model with mkcls and the best published  result ( Berg-Kirkpatrick et al. (2010) and  † Lee et al.
true	P11-1087.pdf#73.0	Penn Treebank, Accuracy	1HMM - LM - - - 94 , 386 195 , 069  Table 2: Many-to-1 accuracy across a range of languages, comparing our model with mkcls and the best published  result ( Berg-Kirkpatrick et al. (2010) and  † Lee et al.
true	P11-1087.pdf#68.6	Penn Treebank, Accuracy	1HMM - LM - - - 94 , 386 195 , 069 - 206 , 678 89 , 334  Table 2: Many-to-1 accuracy across a range of languages, comparing our model with mkcls and the best published  result ( Berg-Kirkpatrick et al. (2010) and  † Lee et al.
true	P12-1026.pdf#95.02%	Chinese Treebank 6, F1	MKCLS  Table 4: Tagging accuracies with different features. S:  supervised segmentation; SS: semi-supervised segmenta- tion.
true	P12-1026.pdf#95.02%	Penn Treebank, Accuracy	MKCLS  Table 4: Tagging accuracies with different features. S:  supervised segmentation; SS: semi-supervised segmenta- tion.
true	P12-1026.pdf#95.02%	Penn Treebank, Number of params	MKCLS  Table 4: Tagging accuracies with different features. S:  supervised segmentation; SS: semi-supervised segmenta- tion.
true	P13-1034.pdf#0.587	SST-2, Accuracy	MNB - FM  Table 4: F1, training size in parentheses
true	P13-1034.pdf#0.677	SST-2, Accuracy	MNB - FM  Table 4: F1, training size in parentheses
true	P13-1034.pdf#0.774	SST-2, Accuracy	NBEM LProp Logist .  Table 4: F1, training size in parentheses
true	P13-1034.pdf#0.542	SST-2, Accuracy	MNB - FM  Table 4: F1, training size in parentheses
true	P13-1034.pdf#0.713	SST-2, Accuracy	NBEM LProp Logist .  Table 4: F1, training size in parentheses
true	P13-1034.pdf#0.336	SST-2, Accuracy	Table 4: F1, training size in parentheses
true	P13-1034.pdf#0.494	SST-2, Accuracy	MNB - FM  Table 4: F1, training size in parentheses
true	P13-1034.pdf#0.554	SST-2, Accuracy	MNB - FM  Table 4: F1, training size in parentheses
true	P13-1034.pdf#0.729	SST-2, Accuracy	MNB - FM  Table 4: F1, training size in parentheses
true	P13-1034.pdf#0.849	SST-2, Accuracy	L |= 10 MNB - FM MNB - FM  Table 6: RCV1: F1, |D L |= 100
true	P13-1034.pdf#0.677	SST-2, Accuracy	L |= 10 MNB - FM MNB - FM  Table 6: RCV1: F1, |D L |= 100
true	P13-1034.pdf#0.463	SST-2, Accuracy	L |= 10 MNB - FM MNB - FM  Table 6: RCV1: F1, |D L |= 100
true	P13-1034.pdf#0.776	SST-2, Accuracy	L |= 10 MNB - FM MNB - FM  Table 6: RCV1: F1, |D L |= 100
true	P13-1034.pdf#0.306	SST-2, Accuracy	MNB - FM  Table 6: RCV1: F1, |D L |= 100
true	P13-1034.pdf#0.732	SST-2, Accuracy	NBEM Logist .  Table 6: RCV1: F1, |D L |= 100
true	P13-1034.pdf#0.643	SST-2, Accuracy	Table 6: RCV1: F1, |D L |= 100
true	P13-1034.pdf#0.499	SST-2, Accuracy	L |= 10 MNB - FM MNB - FM  Table 6: RCV1: F1, |D L |= 100
true	P13-1034.pdf#0.494	SST-2, Accuracy	MNB - FM  Table 6: RCV1: F1, |D L |= 100
true	P13-1034.pdf#0.572	SST-2, Accuracy	MNB - FM  Table 6: RCV1: F1, |D L |= 100
true	P13-1034.pdf#0.797	SST-2, Accuracy	L |= 10 MNB - FM  Table 6: RCV1: F1, |D L |= 100
true	P13-1034.pdf#0.326	SST-2, Accuracy	NBEM Logist .  Table 6: RCV1: F1, |D L |= 100
true	P13-1034.pdf#0.596	SST-2, Accuracy	NBEM Logist .  Table 10: R-Precision, training size in parentheses
true	P13-1034.pdf#0.757	SST-2, Accuracy	Data SetMNB - FM MNB NBEM LProp Logist .  Table 10: R-Precision, training size in parentheses
true	P13-1034.pdf#0.653	SST-2, Accuracy	Table 10: R-Precision, training size in parentheses
true	P13-1034.pdf#0.512	SST-2, Accuracy	NBEM Logist .  Table 10: R-Precision, training size in parentheses
true	P13-1034.pdf#0.297	SST-2, Accuracy	NBEM Logist .  Table 10: R-Precision, training size in parentheses
true	P13-1034.pdf#0.512	SST-2, Accuracy	Data SetMNB - FM MNB NBEM LProp Logist .  Table 10: R-Precision, training size in parentheses
true	P13-1034.pdf#0.544	SST-2, Accuracy	Data SetMNB - FM MNB NBEM LProp Logist .  Table 10: R-Precision, training size in parentheses
true	P13-1034.pdf#0.829	SST-2, Accuracy	Data SetMNB - FM MNB NBEM LProp Logist .  Table 10: R-Precision, training size in parentheses
true	P13-1034.pdf#0.881	SST-2, Accuracy	Data SetMNB - FM MNB NBEM LProp Logist .  Table 10: R-Precision, training size in parentheses
true	P13-1034.pdf#0.639	SST-2, Accuracy	Data SetMNB - FM MNB NBEM LProp Logist .  Table 10: R-Precision, training size in parentheses
true	P13-1034.pdf#0.792	SST-2, Accuracy	NBEM Logist .  Table 10: R-Precision, training size in parentheses
true	P13-1034.pdf#0.802	SST-2, Accuracy	Data SetMNB - FM MNB NBEM LProp Logist .  Table 10: R-Precision, training size in parentheses
true	P13-1034.pdf#0.631	SST-2, Accuracy	Data SetMNB - FM  Table 10: R-Precision, training size in parentheses
true	P13-1034.pdf#0.689	SST-2, Accuracy	Data SetMNB - FM MNB NBEM LProp Logist .  Table 10: R-Precision, training size in parentheses
true	P13-1034.pdf#0.354	SST-2, Accuracy	Data SetMNB - FM  Table 10: R-Precision, training size in parentheses
true	P13-1034.pdf#0.354	SST-2, Accuracy	Data SetMNB - FM  Table 11: RCV1: R-Precision, D L = 10
true	P13-1034.pdf#0.297	SST-2, Accuracy	NBEM Logist .  Table 11: RCV1: R-Precision, D L = 10
true	P13-1034.pdf#0.596	SST-2, Accuracy	NBEM Logist .  Table 11: RCV1: R-Precision, D L = 10
true	P13-1034.pdf#0.653	SST-2, Accuracy	Table 11: RCV1: R-Precision, D L = 10
true	P13-1034.pdf#0.512	SST-2, Accuracy	NBEM Logist .  Table 11: RCV1: R-Precision, D L = 10
true	P13-1034.pdf#0.792	SST-2, Accuracy	NBEM Logist .  Table 11: RCV1: R-Precision, D L = 10
true	P13-1034.pdf#0.520	SST-2, Accuracy	L = 10 NBEM Logist .  Table 12: RCV1: R-Precision, D L = 100  349
true	P13-1034.pdf#0.809	SST-2, Accuracy	L = 10  Table 12: RCV1: R-Precision, D L = 100  349
true	P13-1034.pdf#0.869	SST-2, Accuracy	L = 10 NBEM Logist .  Table 12: RCV1: R-Precision, D L = 100  349
true	P13-1034.pdf#0.689	SST-2, Accuracy	L = 10 NBEM Logist .  Table 12: RCV1: R-Precision, D L = 100  349
true	P13-1034.pdf#0.782	SST-2, Accuracy	L = 10 MNB - FM  Table 12: RCV1: R-Precision, D L = 100  349
true	P13-1034.pdf#0.498	SST-2, Accuracy	L = 10 NBEM Logist .  Table 12: RCV1: R-Precision, D L = 100  349
true	P11-1048.pdf#97.09	Penn Treebank, Accuracy	1 . 3 - 3 . 6 Iteration 1 20 20 Oracle Max F - score LR  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#87.38	Penn Treebank, Accuracy	seen less than k times . Condition k ( dictionary cutoff ) k Viterbi F - score LF  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#97.65	Penn Treebank, Accuracy	1 . 3 - 3 . 6 Parameter k ( dictionary cutoff ) k Oracle Max F - score LF  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#87.17	Penn Treebank, Accuracy	1 . 3 - 3 . 6 Parameter k ( dictionary cutoff ) k Viterbi F - score LR  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#87.83	Penn Treebank, Accuracy	seen less than k times . Condition k ( dictionary cutoff ) k Viterbi F - score LP  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#98.21	Penn Treebank, Accuracy	1 . 3 - 3 . 6 Iteration 1 20 150 Oracle Max F - score LP  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#97.09	Penn Treebank, Accuracy	1 . 3 - 3 . 6 Oracle Max F - score LR  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#87.17	Penn Treebank, Accuracy	1 . 3 - 3 . 6 Viterbi F - score LR  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#98.21	Penn Treebank, Accuracy	1 . 3 - 3 . 6 Oracle Max F - score LP  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#87.38	Penn Treebank, Accuracy	Viterbi F - score LF  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#87.83	Penn Treebank, Accuracy	Viterbi F - score LP  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#97.65	Penn Treebank, Accuracy	1 . 3 - 3 . 6 Oracle Max F - score LF  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#94.82	Penn Treebank, Accuracy	Training 7 20 section 23 ( test ) Reverse ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#94.73	Penn Treebank, Accuracy	Training 2 20 section 00 ( dev ) Reverse ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#88.35	Penn Treebank, Accuracy	Training Iteration 1 150 section 00 ( dev ) Reverse LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#94.60	Penn Treebank, Accuracy	Training 5 20 section 23 ( test ) AST ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#94.44	Penn Treebank, Accuracy	Training Parameter k section 00 ( dev ) AST ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#87.71	Penn Treebank, Accuracy	Training Parameter k section 00 ( dev ) AST LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#88.20	Penn Treebank, Accuracy	Training 3 20 section 23 ( test ) AST LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.68	Penn Treebank, Accuracy	Training 7 20 section 23 ( test ) Reverse UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.32	Penn Treebank, Accuracy	Training Parameter k section 00 ( dev ) AST UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#88.80	Penn Treebank, Accuracy	Training 6 20 section 23 ( test ) Reverse LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.28	Penn Treebank, Accuracy	Training 4 20 section 23 ( test ) AST UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.72	Penn Treebank, Accuracy	Training Iteration 1 20 section 00 ( dev ) Reverse UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#97.09	Penn Treebank, POS	1 . 3 - 3 . 6 Iteration 1 20 20 Oracle Max F - score LR  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#87.38	Penn Treebank, POS	seen less than k times . Condition k ( dictionary cutoff ) k Viterbi F - score LF  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#97.65	Penn Treebank, POS	1 . 3 - 3 . 6 Parameter k ( dictionary cutoff ) k Oracle Max F - score LF  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#87.17	Penn Treebank, POS	1 . 3 - 3 . 6 Parameter k ( dictionary cutoff ) k Viterbi F - score LR  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#87.83	Penn Treebank, POS	seen less than k times . Condition k ( dictionary cutoff ) k Viterbi F - score LP  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#98.21	Penn Treebank, POS	1 . 3 - 3 . 6 Iteration 1 20 150 Oracle Max F - score LP  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#97.09	Penn Treebank, POS	1 . 3 - 3 . 6 Oracle Max F - score LR  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#87.17	Penn Treebank, POS	1 . 3 - 3 . 6 Viterbi F - score LR  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#98.21	Penn Treebank, POS	1 . 3 - 3 . 6 Oracle Max F - score LP  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#87.38	Penn Treebank, POS	Viterbi F - score LF  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#87.83	Penn Treebank, POS	Viterbi F - score LP  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#97.65	Penn Treebank, POS	1 . 3 - 3 . 6 Oracle Max F - score LF  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#94.82	Penn Treebank, POS	Training 7 20 section 23 ( test ) Reverse ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#94.73	Penn Treebank, POS	Training 2 20 section 00 ( dev ) Reverse ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#88.35	Penn Treebank, POS	Training Iteration 1 150 section 00 ( dev ) Reverse LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#94.60	Penn Treebank, POS	Training 5 20 section 23 ( test ) AST ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#94.44	Penn Treebank, POS	Training Parameter k section 00 ( dev ) AST ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#87.71	Penn Treebank, POS	Training Parameter k section 00 ( dev ) AST LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#88.20	Penn Treebank, POS	Training 3 20 section 23 ( test ) AST LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.68	Penn Treebank, POS	Training 7 20 section 23 ( test ) Reverse UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.32	Penn Treebank, POS	Training Parameter k section 00 ( dev ) AST UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#88.80	Penn Treebank, POS	Training 6 20 section 23 ( test ) Reverse LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.28	Penn Treebank, POS	Training 4 20 section 23 ( test ) AST UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.72	Penn Treebank, POS	Training Iteration 1 20 section 00 ( dev ) Reverse UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#97.09	Penn Treebank, UAS	1 . 3 - 3 . 6 Iteration 1 20 20 Oracle Max F - score LR  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#87.38	Penn Treebank, UAS	seen less than k times . Condition k ( dictionary cutoff ) k Viterbi F - score LF  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#97.65	Penn Treebank, UAS	1 . 3 - 3 . 6 Parameter k ( dictionary cutoff ) k Oracle Max F - score LF  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#87.17	Penn Treebank, UAS	1 . 3 - 3 . 6 Parameter k ( dictionary cutoff ) k Viterbi F - score LR  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#87.83	Penn Treebank, UAS	seen less than k times . Condition k ( dictionary cutoff ) k Viterbi F - score LP  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#98.21	Penn Treebank, UAS	1 . 3 - 3 . 6 Iteration 1 20 150 Oracle Max F - score LP  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#97.09	Penn Treebank, UAS	1 . 3 - 3 . 6 Oracle Max F - score LR  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#87.17	Penn Treebank, UAS	1 . 3 - 3 . 6 Viterbi F - score LR  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#98.21	Penn Treebank, UAS	1 . 3 - 3 . 6 Oracle Max F - score LP  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#87.38	Penn Treebank, UAS	Viterbi F - score LF  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#87.83	Penn Treebank, UAS	Viterbi F - score LP  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#97.65	Penn Treebank, UAS	1 . 3 - 3 . 6 Oracle Max F - score LF  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#94.82	Penn Treebank, UAS	Training 7 20 section 23 ( test ) Reverse ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#94.73	Penn Treebank, UAS	Training 2 20 section 00 ( dev ) Reverse ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#88.35	Penn Treebank, UAS	Training Iteration 1 150 section 00 ( dev ) Reverse LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#94.60	Penn Treebank, UAS	Training 5 20 section 23 ( test ) AST ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#94.44	Penn Treebank, UAS	Training Parameter k section 00 ( dev ) AST ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#87.71	Penn Treebank, UAS	Training Parameter k section 00 ( dev ) AST LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#88.20	Penn Treebank, UAS	Training 3 20 section 23 ( test ) AST LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.68	Penn Treebank, UAS	Training 7 20 section 23 ( test ) Reverse UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.32	Penn Treebank, UAS	Training Parameter k section 00 ( dev ) AST UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#88.80	Penn Treebank, UAS	Training 6 20 section 23 ( test ) Reverse LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.28	Penn Treebank, UAS	Training 4 20 section 23 ( test ) AST UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.72	Penn Treebank, UAS	Training Iteration 1 20 section 00 ( dev ) Reverse UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#97.09	Penn Treebank, Number of params	1 . 3 - 3 . 6 Iteration 1 20 20 Oracle Max F - score LR  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#87.38	Penn Treebank, Number of params	seen less than k times . Condition k ( dictionary cutoff ) k Viterbi F - score LF  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#97.65	Penn Treebank, Number of params	1 . 3 - 3 . 6 Parameter k ( dictionary cutoff ) k Oracle Max F - score LF  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#87.17	Penn Treebank, Number of params	1 . 3 - 3 . 6 Parameter k ( dictionary cutoff ) k Viterbi F - score LR  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#87.83	Penn Treebank, Number of params	seen less than k times . Condition k ( dictionary cutoff ) k Viterbi F - score LP  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#98.21	Penn Treebank, Number of params	1 . 3 - 3 . 6 Iteration 1 20 150 Oracle Max F - score LP  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#97.09	Penn Treebank, Number of params	1 . 3 - 3 . 6 Oracle Max F - score LR  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#87.17	Penn Treebank, Number of params	1 . 3 - 3 . 6 Viterbi F - score LR  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#98.21	Penn Treebank, Number of params	1 . 3 - 3 . 6 Oracle Max F - score LP  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#87.38	Penn Treebank, Number of params	Viterbi F - score LF  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#87.83	Penn Treebank, Number of params	Viterbi F - score LP  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#97.65	Penn Treebank, Number of params	1 . 3 - 3 . 6 Oracle Max F - score LF  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#94.82	Penn Treebank, Number of params	Training 7 20 section 23 ( test ) Reverse ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#94.73	Penn Treebank, Number of params	Training 2 20 section 00 ( dev ) Reverse ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#88.35	Penn Treebank, Number of params	Training Iteration 1 150 section 00 ( dev ) Reverse LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#94.60	Penn Treebank, Number of params	Training 5 20 section 23 ( test ) AST ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#94.44	Penn Treebank, Number of params	Training Parameter k section 00 ( dev ) AST ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#87.71	Penn Treebank, Number of params	Training Parameter k section 00 ( dev ) AST LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#88.20	Penn Treebank, Number of params	Training 3 20 section 23 ( test ) AST LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.68	Penn Treebank, Number of params	Training 7 20 section 23 ( test ) Reverse UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.32	Penn Treebank, Number of params	Training Parameter k section 00 ( dev ) AST UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#88.80	Penn Treebank, Number of params	Training 6 20 section 23 ( test ) Reverse LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.28	Penn Treebank, Number of params	Training 4 20 section 23 ( test ) AST UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.72	Penn Treebank, Number of params	Training Iteration 1 20 section 00 ( dev ) Reverse UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#97.09	Penn Treebank, LAS	1 . 3 - 3 . 6 Iteration 1 20 20 Oracle Max F - score LR  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#87.38	Penn Treebank, LAS	seen less than k times . Condition k ( dictionary cutoff ) k Viterbi F - score LF  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#97.65	Penn Treebank, LAS	1 . 3 - 3 . 6 Parameter k ( dictionary cutoff ) k Oracle Max F - score LF  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#87.17	Penn Treebank, LAS	1 . 3 - 3 . 6 Parameter k ( dictionary cutoff ) k Viterbi F - score LR  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#87.83	Penn Treebank, LAS	seen less than k times . Condition k ( dictionary cutoff ) k Viterbi F - score LP  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#98.21	Penn Treebank, LAS	1 . 3 - 3 . 6 Iteration 1 20 150 Oracle Max F - score LP  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#97.09	Penn Treebank, LAS	1 . 3 - 3 . 6 Oracle Max F - score LR  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#87.17	Penn Treebank, LAS	1 . 3 - 3 . 6 Viterbi F - score LR  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#98.21	Penn Treebank, LAS	1 . 3 - 3 . 6 Oracle Max F - score LP  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#87.38	Penn Treebank, LAS	Viterbi F - score LF  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#87.83	Penn Treebank, LAS	Viterbi F - score LP  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#97.65	Penn Treebank, LAS	1 . 3 - 3 . 6 Oracle Max F - score LF  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#94.82	Penn Treebank, LAS	Training 7 20 section 23 ( test ) Reverse ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#94.73	Penn Treebank, LAS	Training 2 20 section 00 ( dev ) Reverse ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#88.35	Penn Treebank, LAS	Training Iteration 1 150 section 00 ( dev ) Reverse LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#94.60	Penn Treebank, LAS	Training 5 20 section 23 ( test ) AST ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#94.44	Penn Treebank, LAS	Training Parameter k section 00 ( dev ) AST ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#87.71	Penn Treebank, LAS	Training Parameter k section 00 ( dev ) AST LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#88.20	Penn Treebank, LAS	Training 3 20 section 23 ( test ) AST LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.68	Penn Treebank, LAS	Training 7 20 section 23 ( test ) Reverse UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.32	Penn Treebank, LAS	Training Parameter k section 00 ( dev ) AST UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#88.80	Penn Treebank, LAS	Training 6 20 section 23 ( test ) Reverse LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.28	Penn Treebank, LAS	Training 4 20 section 23 ( test ) AST UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.72	Penn Treebank, LAS	Training Iteration 1 20 section 00 ( dev ) Reverse UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#97.09	CCGBank, Accuracy	1 . 3 - 3 . 6 Iteration 1 20 20 Oracle Max F - score LR  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#87.38	CCGBank, Accuracy	seen less than k times . Condition k ( dictionary cutoff ) k Viterbi F - score LF  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#97.65	CCGBank, Accuracy	1 . 3 - 3 . 6 Parameter k ( dictionary cutoff ) k Oracle Max F - score LF  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#87.17	CCGBank, Accuracy	1 . 3 - 3 . 6 Parameter k ( dictionary cutoff ) k Viterbi F - score LR  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#87.83	CCGBank, Accuracy	seen less than k times . Condition k ( dictionary cutoff ) k Viterbi F - score LP  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#98.21	CCGBank, Accuracy	1 . 3 - 3 . 6 Iteration 1 20 150 Oracle Max F - score LP  Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.  Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words  seen less than k times.
true	P11-1048.pdf#97.09	CCGBank, Accuracy	1 . 3 - 3 . 6 Oracle Max F - score LR  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#87.17	CCGBank, Accuracy	1 . 3 - 3 . 6 Viterbi F - score LR  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#98.21	CCGBank, Accuracy	1 . 3 - 3 . 6 Oracle Max F - score LP  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#87.38	CCGBank, Accuracy	Viterbi F - score LF  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#87.83	CCGBank, Accuracy	Viterbi F - score LP  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#97.65	CCGBank, Accuracy	1 . 3 - 3 . 6 Oracle Max F - score LF  Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle  F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the  the number of lexical categories per word used (from first to last parsing attempt).
true	P11-1048.pdf#94.82	CCGBank, Accuracy	Training 7 20 section 23 ( test ) Reverse ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#94.73	CCGBank, Accuracy	Training 2 20 section 00 ( dev ) Reverse ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#88.35	CCGBank, Accuracy	Training Iteration 1 150 section 00 ( dev ) Reverse LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#94.60	CCGBank, Accuracy	Training 5 20 section 23 ( test ) AST ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#94.44	CCGBank, Accuracy	Training Parameter k section 00 ( dev ) AST ST -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#87.71	CCGBank, Accuracy	Training Parameter k section 00 ( dev ) AST LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#88.20	CCGBank, Accuracy	Training 3 20 section 23 ( test ) AST LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.68	CCGBank, Accuracy	Training 7 20 section 23 ( test ) Reverse UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.32	CCGBank, Accuracy	Training Parameter k section 00 ( dev ) AST UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#88.80	CCGBank, Accuracy	Training 6 20 section 23 ( test ) Reverse LF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.28	CCGBank, Accuracy	Training 4 20 section 23 ( test ) AST UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-1048.pdf#93.72	CCGBank, Accuracy	Training Iteration 1 20 section 00 ( dev ) Reverse UF -  Table 3: Beam step function used for training (cf. Table 1).
true	P11-2027.pdf#0.4670	WMT 2014 EN-FR, BLEU	Fluency  Table 2: Pearson's correlation coefficients (com- puted at the system level) between automatic met- rics and human-generated scores
true	P11-2027.pdf#0.4232	WMT 2014 EN-FR, BLEU	Fluency  Table 2: Pearson's correlation coefficients (com- puted at the system level) between automatic met- rics and human-generated scores
true	P11-2027.pdf#0.4516	WMT 2014 EN-FR, BLEU	H Mean  Table 2: Pearson's correlation coefficients (com- puted at the system level) between automatic met- rics and human-generated scores
true	P11-2027.pdf#0.3361	WMT 2014 EN-FR, BLEU	Fluency  Table 3: Pearson's correlation coefficients (com- puted at the sentence level) between automatic  metrics and human-generated scores
true	P11-2027.pdf#0.3486	WMT 2014 EN-FR, BLEU	H Mean  Table 3: Pearson's correlation coefficients (com- puted at the sentence level) between automatic  metrics and human-generated scores
true	P11-2027.pdf#0.3220	WMT 2014 EN-FR, BLEU	Fluency  Table 3: Pearson's correlation coefficients (com- puted at the sentence level) between automatic  metrics and human-generated scores
true	P11-2027.pdf#52.83%	WMT 2014 EN-FR, BLEU	Best  Table 4: Percentage of cases in which each auto- matic metric is able to predict the best, the worst,  and both ranked sentence translations
true	P11-2027.pdf#58.03%	WMT 2014 EN-FR, BLEU	Worst  Table 4: Percentage of cases in which each auto- matic metric is able to predict the best, the worst,  and both ranked sentence translations
true	P11-2027.pdf#39.85%	WMT 2014 EN-FR, BLEU	Both  Table 4: Percentage of cases in which each auto- matic metric is able to predict the best, the worst,  and both ranked sentence translations
true	P11-2027.pdf#0.4670	WMT 2014 EN-DE, BLEU	Fluency  Table 2: Pearson's correlation coefficients (com- puted at the system level) between automatic met- rics and human-generated scores
true	P11-2027.pdf#0.4232	WMT 2014 EN-DE, BLEU	Fluency  Table 2: Pearson's correlation coefficients (com- puted at the system level) between automatic met- rics and human-generated scores
true	P11-2027.pdf#0.4516	WMT 2014 EN-DE, BLEU	H Mean  Table 2: Pearson's correlation coefficients (com- puted at the system level) between automatic met- rics and human-generated scores
true	P11-2027.pdf#0.3361	WMT 2014 EN-DE, BLEU	Fluency  Table 3: Pearson's correlation coefficients (com- puted at the sentence level) between automatic  metrics and human-generated scores
true	P11-2027.pdf#0.3486	WMT 2014 EN-DE, BLEU	H Mean  Table 3: Pearson's correlation coefficients (com- puted at the sentence level) between automatic  metrics and human-generated scores
true	P11-2027.pdf#0.3220	WMT 2014 EN-DE, BLEU	Fluency  Table 3: Pearson's correlation coefficients (com- puted at the sentence level) between automatic  metrics and human-generated scores
true	P11-2027.pdf#52.83%	WMT 2014 EN-DE, BLEU	Best  Table 4: Percentage of cases in which each auto- matic metric is able to predict the best, the worst,  and both ranked sentence translations
true	P11-2027.pdf#58.03%	WMT 2014 EN-DE, BLEU	Worst  Table 4: Percentage of cases in which each auto- matic metric is able to predict the best, the worst,  and both ranked sentence translations
true	P11-2027.pdf#39.85%	WMT 2014 EN-DE, BLEU	Both  Table 4: Percentage of cases in which each auto- matic metric is able to predict the best, the worst,  and both ranked sentence translations
true	P12-2075.pdf#0.85	VLSP 2013 word segmentation shared task, F1	proposal , based on normalized VBE with maximization at MSR Evaluation on the Second Bakeoff data with  Table 1: Evaluation on the Second Bakeoff data with  Wang et al.'s (2011) settings. "Worst" and "best" give the  range of the reported results with differents values of the  parameter in Wang et al.'s system. VBE > 0 correspond  to a cut whenever BE is raising. nVBE corresponds to our  proposal, based on normalized VBE with maximization at  word boundaries. Recall that the topline is around 0.85
true	P12-2075.pdf#0.85	VLSP 2013 word segmentation shared task, F1	proposal , based on normalized VBE with maximization at MSR Evaluation on the Second Bakeoff data with  Table 2: Per word-length details of our results with our  nVBE algorithm and setting 4. Recall that the toplines  are respectively 0.85, 0.81, 0.85 and 0.59 (see Section 3)
true	P12-2075.pdf#0.85	Chinese Treebank 6, F1	proposal , based on normalized VBE with maximization at MSR Evaluation on the Second Bakeoff data with  Table 1: Evaluation on the Second Bakeoff data with  Wang et al.'s (2011) settings. "Worst" and "best" give the  range of the reported results with differents values of the  parameter in Wang et al.'s system. VBE > 0 correspond  to a cut whenever BE is raising. nVBE corresponds to our  proposal, based on normalized VBE with maximization at  word boundaries. Recall that the topline is around 0.85
true	P12-2075.pdf#0.85	Chinese Treebank 6, F1	proposal , based on normalized VBE with maximization at MSR Evaluation on the Second Bakeoff data with  Table 2: Per word-length details of our results with our  nVBE algorithm and setting 4. Recall that the toplines  are respectively 0.85, 0.81, 0.85 and 0.59 (see Section 3)
true	P14-1043.pdf#83.34†	Penn Treebank, UAS	Chinese UAS -  Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.
true	P14-1043.pdf#93.50†	Penn Treebank, UAS	English UAS -  Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.
true	P14-1043.pdf#93.78†‡	Penn Treebank, UAS	English UAS -  Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.
true	P14-1043.pdf#92.85	Penn Treebank, UAS	English UAS  Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.
true	P14-1043.pdf#82.46	Penn Treebank, UAS	Chinese UAS -  Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.
true	P14-1043.pdf#84.26†‡	Penn Treebank, UAS	Chinese UAS -  Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.
true	P14-1043.pdf#83.34†	benchmark Vietnamese dependency treebank VnDT, LAS	Chinese UAS -  Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.
true	P14-1043.pdf#93.50†	benchmark Vietnamese dependency treebank VnDT, LAS	English UAS -  Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.
true	P14-1043.pdf#93.78†‡	benchmark Vietnamese dependency treebank VnDT, LAS	English UAS -  Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.
true	P14-1043.pdf#92.85	benchmark Vietnamese dependency treebank VnDT, LAS	English UAS  Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.
true	P14-1043.pdf#82.46	benchmark Vietnamese dependency treebank VnDT, LAS	Chinese UAS -  Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.
true	P14-1043.pdf#84.26†‡	benchmark Vietnamese dependency treebank VnDT, LAS	Chinese UAS -  Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.
true	P14-1043.pdf#83.34†	benchmark Vietnamese dependency treebank VnDT, UAS	Chinese UAS -  Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.
true	P14-1043.pdf#93.50†	benchmark Vietnamese dependency treebank VnDT, UAS	English UAS -  Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.
true	P14-1043.pdf#93.78†‡	benchmark Vietnamese dependency treebank VnDT, UAS	English UAS -  Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.
true	P14-1043.pdf#92.85	benchmark Vietnamese dependency treebank VnDT, UAS	English UAS  Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.
true	P14-1043.pdf#82.46	benchmark Vietnamese dependency treebank VnDT, UAS	Chinese UAS -  Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.
true	P14-1043.pdf#84.26†‡	benchmark Vietnamese dependency treebank VnDT, UAS	Chinese UAS -  Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.
true	P15-2065.pdf#0.567	IMDb, Accuracy	: Clustering Accuracy with Pure Data . L11  Table 2: Clustering Accuracy with Noise.
true	P15-2065.pdf#0.586	IMDb, Accuracy	: Clustering Accuracy with Pure Data .  Table 2: Clustering Accuracy with Noise.
true	P15-2065.pdf#0.556	IMDb, Accuracy	: Clustering Accuracy with Pure Data . L11  Table 2: Clustering Accuracy with Noise.
true	P15-2065.pdf#0.574	IMDb, Accuracy	: Clustering Accuracy with Pure Data . L11  Table 2: Clustering Accuracy with Noise.
true	P15-2065.pdf#0.575	IMDb, Accuracy	: Clustering Accuracy with Pure Data . L11  Table 2: Clustering Accuracy with Noise.
true	P15-2065.pdf#0.574	IMDb, Accuracy	: Clustering Accuracy with Pure Data . L11  Table 2: Clustering Accuracy with Noise.
true	P15-2065.pdf#0.546	IMDb, Accuracy	L11  Table 3: Clustering Accuracy Contrast.
true	P15-2065.pdf#0.547	IMDb, Accuracy	Table 3: Clustering Accuracy Contrast.
true	P15-2065.pdf#0.543	IMDb, Accuracy	L11  Table 3: Clustering Accuracy Contrast.
true	P15-2065.pdf#0.536	IMDb, Accuracy	L11  Table 3: Clustering Accuracy Contrast.
true	P15-2065.pdf#0.543	IMDb, Accuracy	L11  Table 3: Clustering Accuracy Contrast.
true	P15-2065.pdf#0.536	IMDb, Accuracy	L11  Table 3: Clustering Accuracy Contrast.
true	D13-1043.pdf#0.92	New York Times Corpus, P@10%	PENN - 100 P  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.51	New York Times Corpus, P@10%	NYT - 500 F  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.62	New York Times Corpus, P@10%	WEB - 500 F  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.39	New York Times Corpus, P@10%	NYT - 500 R  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.02	New York Times Corpus, P@10%	PENN - 100 Time  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.62	New York Times Corpus, P@10%	PENN - 100 F  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.46	New York Times Corpus, P@10%	WEB - 500 R  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.98	New York Times Corpus, P@10%	WEB - 500 P  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.51	New York Times Corpus, P@10%	PENN - 100 R  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.39	New York Times Corpus, P@10%	NYT - 500 R  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.62	New York Times Corpus, P@10%	PENN - 100 F  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.02	New York Times Corpus, P@10%	NYT - 500 Time  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.01	New York Times Corpus, P@10%	WEB - 500 Time  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.78	New York Times Corpus, P@10%	NYT - 500 P  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.56	New York Times Corpus, P@10%	Method NYT n - ary F  Table 3: Results for n-ary relations.
true	D13-1043.pdf#0.95	New York Times Corpus, P@10%	Method NYT n - ary P  Table 3: Results for n-ary relations.
true	D13-1043.pdf#0.11	New York Times Corpus, P@10%	Method NYT n - ary Time  Table 3: Results for n-ary relations.
true	D13-1043.pdf#0.40	New York Times Corpus, P@10%	Method NYT n - ary R  Table 3: Results for n-ary relations.
true	D13-1043.pdf#0.92	Penn Treebank, Accuracy	PENN - 100 P  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.51	Penn Treebank, Accuracy	NYT - 500 F  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.62	Penn Treebank, Accuracy	WEB - 500 F  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.39	Penn Treebank, Accuracy	NYT - 500 R  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.02	Penn Treebank, Accuracy	PENN - 100 Time  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.62	Penn Treebank, Accuracy	PENN - 100 F  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.46	Penn Treebank, Accuracy	WEB - 500 R  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.98	Penn Treebank, Accuracy	WEB - 500 P  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.51	Penn Treebank, Accuracy	PENN - 100 R  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.39	Penn Treebank, Accuracy	NYT - 500 R  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.62	Penn Treebank, Accuracy	PENN - 100 F  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.02	Penn Treebank, Accuracy	NYT - 500 Time  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.01	Penn Treebank, Accuracy	WEB - 500 Time  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.78	Penn Treebank, Accuracy	NYT - 500 P  Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per  sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
true	D13-1043.pdf#0.56	Penn Treebank, Accuracy	Method NYT n - ary F  Table 3: Results for n-ary relations.
true	D13-1043.pdf#0.95	Penn Treebank, Accuracy	Method NYT n - ary P  Table 3: Results for n-ary relations.
true	D13-1043.pdf#0.11	Penn Treebank, Accuracy	Method NYT n - ary Time  Table 3: Results for n-ary relations.
true	D13-1043.pdf#0.40	Penn Treebank, Accuracy	Method NYT n - ary R  Table 3: Results for n-ary relations.
true	P13-2031.pdf#96.72	Chinese Treebank 6, F1	7  Table 2: F-score (%) results of five CWS models  on CTB-5, CTB-6 and CTB-7.
true	P13-2031.pdf#96.33	Chinese Treebank 6, F1	7  Table 2: F-score (%) results of five CWS models  on CTB-5, CTB-6 and CTB-7.
true	P13-2031.pdf#98.27	Chinese Treebank 6, F1	7  Table 2: F-score (%) results of five CWS models  on CTB-5, CTB-6 and CTB-7.
true	P10-2018.pdf#80.46	Senseval 2, F1	Table 1 : Features for the Structured Model Sp  Table 1: Features for the Structured Model
true	P10-2018.pdf#80.80	Senseval 2, F1	Table 1 : Features for the Structured Model Avg .  Table 1: Features for the Structured Model
true	P10-2018.pdf#78.69	Senseval 2, F1	Table 1 : Features for the Structured Model Jp  Table 1: Features for the Structured Model
true	P10-2018.pdf#78.60	Senseval 2, F1	Table 1 : Features for the Structured Model Ch  Table 1: Features for the Structured Model
true	P10-2018.pdf#85.94	Senseval 2, F1	Table 1 : Features for the Structured Model Cz  Table 1: Features for the Structured Model
true	P10-2018.pdf#85.63	Senseval 2, F1	Table 1 : Features for the Structured Model En  Table 1: Features for the Structured Model
true	P10-2018.pdf#79.71	Senseval 2, F1	Table 1 : Features for the Structured Model Ge  Table 1: Features for the Structured Model
true	P10-2018.pdf#80.32	Senseval 2, F1	Table 1 : Features for the Structured Model Ca  Table 1: Features for the Structured Model
true	P10-2018.pdf#80.46	Senseval 2, F1	Table 1 : Features for the Structured Model Sp  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#80.32	Senseval 2, F1	Table 1 : Features for the Structured Model Ca  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#85.63	Senseval 2, F1	Table 1 : Features for the Structured Model En  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#79.71	Senseval 2, F1	Table 1 : Features for the Structured Model Ge  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#78.69	Senseval 2, F1	Table 1 : Features for the Structured Model Jp  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#78.60	Senseval 2, F1	Table 1 : Features for the Structured Model Ch  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#80.80	Senseval 2, F1	Table 1 : Features for the Structured Model Avg .  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#85.94	Senseval 2, F1	Table 1 : Features for the Structured Model Cz  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#80.46	Senseval 3, F1	Table 1 : Features for the Structured Model Sp  Table 1: Features for the Structured Model
true	P10-2018.pdf#80.80	Senseval 3, F1	Table 1 : Features for the Structured Model Avg .  Table 1: Features for the Structured Model
true	P10-2018.pdf#78.69	Senseval 3, F1	Table 1 : Features for the Structured Model Jp  Table 1: Features for the Structured Model
true	P10-2018.pdf#78.60	Senseval 3, F1	Table 1 : Features for the Structured Model Ch  Table 1: Features for the Structured Model
true	P10-2018.pdf#85.94	Senseval 3, F1	Table 1 : Features for the Structured Model Cz  Table 1: Features for the Structured Model
true	P10-2018.pdf#85.63	Senseval 3, F1	Table 1 : Features for the Structured Model En  Table 1: Features for the Structured Model
true	P10-2018.pdf#79.71	Senseval 3, F1	Table 1 : Features for the Structured Model Ge  Table 1: Features for the Structured Model
true	P10-2018.pdf#80.32	Senseval 3, F1	Table 1 : Features for the Structured Model Ca  Table 1: Features for the Structured Model
true	P10-2018.pdf#80.46	Senseval 3, F1	Table 1 : Features for the Structured Model Sp  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#80.32	Senseval 3, F1	Table 1 : Features for the Structured Model Ca  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#85.63	Senseval 3, F1	Table 1 : Features for the Structured Model En  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#79.71	Senseval 3, F1	Table 1 : Features for the Structured Model Ge  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#78.69	Senseval 3, F1	Table 1 : Features for the Structured Model Jp  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#78.60	Senseval 3, F1	Table 1 : Features for the Structured Model Ch  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#80.80	Senseval 3, F1	Table 1 : Features for the Structured Model Avg .  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#85.94	Senseval 3, F1	Table 1 : Features for the Structured Model Cz  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#80.46	SemEval 2007, F1	Table 1 : Features for the Structured Model Sp  Table 1: Features for the Structured Model
true	P10-2018.pdf#80.80	SemEval 2007, F1	Table 1 : Features for the Structured Model Avg .  Table 1: Features for the Structured Model
true	P10-2018.pdf#78.69	SemEval 2007, F1	Table 1 : Features for the Structured Model Jp  Table 1: Features for the Structured Model
true	P10-2018.pdf#78.60	SemEval 2007, F1	Table 1 : Features for the Structured Model Ch  Table 1: Features for the Structured Model
true	P10-2018.pdf#85.94	SemEval 2007, F1	Table 1 : Features for the Structured Model Cz  Table 1: Features for the Structured Model
true	P10-2018.pdf#85.63	SemEval 2007, F1	Table 1 : Features for the Structured Model En  Table 1: Features for the Structured Model
true	P10-2018.pdf#79.71	SemEval 2007, F1	Table 1 : Features for the Structured Model Ge  Table 1: Features for the Structured Model
true	P10-2018.pdf#80.32	SemEval 2007, F1	Table 1 : Features for the Structured Model Ca  Table 1: Features for the Structured Model
true	P10-2018.pdf#80.46	SemEval 2007, F1	Table 1 : Features for the Structured Model Sp  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#80.32	SemEval 2007, F1	Table 1 : Features for the Structured Model Ca  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#85.63	SemEval 2007, F1	Table 1 : Features for the Structured Model En  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#79.71	SemEval 2007, F1	Table 1 : Features for the Structured Model Ge  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#78.69	SemEval 2007, F1	Table 1 : Features for the Structured Model Jp  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#78.60	SemEval 2007, F1	Table 1 : Features for the Structured Model Ch  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#80.80	SemEval 2007, F1	Table 1 : Features for the Structured Model Avg .  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#85.94	SemEval 2007, F1	Table 1 : Features for the Structured Model Cz  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#80.46	SemEval 2015, F1	Table 1 : Features for the Structured Model Sp  Table 1: Features for the Structured Model
true	P10-2018.pdf#80.80	SemEval 2015, F1	Table 1 : Features for the Structured Model Avg .  Table 1: Features for the Structured Model
true	P10-2018.pdf#78.69	SemEval 2015, F1	Table 1 : Features for the Structured Model Jp  Table 1: Features for the Structured Model
true	P10-2018.pdf#78.60	SemEval 2015, F1	Table 1 : Features for the Structured Model Ch  Table 1: Features for the Structured Model
true	P10-2018.pdf#85.94	SemEval 2015, F1	Table 1 : Features for the Structured Model Cz  Table 1: Features for the Structured Model
true	P10-2018.pdf#85.63	SemEval 2015, F1	Table 1 : Features for the Structured Model En  Table 1: Features for the Structured Model
true	P10-2018.pdf#79.71	SemEval 2015, F1	Table 1 : Features for the Structured Model Ge  Table 1: Features for the Structured Model
true	P10-2018.pdf#80.32	SemEval 2015, F1	Table 1 : Features for the Structured Model Ca  Table 1: Features for the Structured Model
true	P10-2018.pdf#80.46	SemEval 2015, F1	Table 1 : Features for the Structured Model Sp  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#80.32	SemEval 2015, F1	Table 1 : Features for the Structured Model Ca  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#85.63	SemEval 2015, F1	Table 1 : Features for the Structured Model En  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#79.71	SemEval 2015, F1	Table 1 : Features for the Structured Model Ge  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#78.69	SemEval 2015, F1	Table 1 : Features for the Structured Model Jp  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#78.60	SemEval 2015, F1	Table 1 : Features for the Structured Model Ch  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#80.80	SemEval 2015, F1	Table 1 : Features for the Structured Model Avg .  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#85.94	SemEval 2015, F1	Table 1 : Features for the Structured Model Cz  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#80.46	SemEval 2013, F1	Table 1 : Features for the Structured Model Sp  Table 1: Features for the Structured Model
true	P10-2018.pdf#80.80	SemEval 2013, F1	Table 1 : Features for the Structured Model Avg .  Table 1: Features for the Structured Model
true	P10-2018.pdf#78.69	SemEval 2013, F1	Table 1 : Features for the Structured Model Jp  Table 1: Features for the Structured Model
true	P10-2018.pdf#78.60	SemEval 2013, F1	Table 1 : Features for the Structured Model Ch  Table 1: Features for the Structured Model
true	P10-2018.pdf#85.94	SemEval 2013, F1	Table 1 : Features for the Structured Model Cz  Table 1: Features for the Structured Model
true	P10-2018.pdf#85.63	SemEval 2013, F1	Table 1 : Features for the Structured Model En  Table 1: Features for the Structured Model
true	P10-2018.pdf#79.71	SemEval 2013, F1	Table 1 : Features for the Structured Model Ge  Table 1: Features for the Structured Model
true	P10-2018.pdf#80.32	SemEval 2013, F1	Table 1 : Features for the Structured Model Ca  Table 1: Features for the Structured Model
true	P10-2018.pdf#80.46	SemEval 2013, F1	Table 1 : Features for the Structured Model Sp  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#80.32	SemEval 2013, F1	Table 1 : Features for the Structured Model Ca  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#85.63	SemEval 2013, F1	Table 1 : Features for the Structured Model En  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#79.71	SemEval 2013, F1	Table 1 : Features for the Structured Model Ge  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#78.69	SemEval 2013, F1	Table 1 : Features for the Structured Model Jp  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#78.60	SemEval 2013, F1	Table 1 : Features for the Structured Model Ch  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#80.80	SemEval 2013, F1	Table 1 : Features for the Structured Model Avg .  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	P10-2018.pdf#85.94	SemEval 2013, F1	Table 1 : Features for the Structured Model Cz  Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
true	D11-1022.pdf#87.04	Penn Treebank, UAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#93.50(+0.46)	Penn Treebank, UAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#89.46(+0.66)	Penn Treebank, UAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#92.68(+0.23)	Penn Treebank, UAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#91.36	Penn Treebank, UAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#85.81	Penn Treebank, UAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#91.89	Penn Treebank, UAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#91.89(+0.60)	Penn Treebank, UAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#93.03	Penn Treebank, UAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#77.55	Penn Treebank, UAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#81.12	Penn Treebank, UAS	Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#93.72(+0.10)	Penn Treebank, UAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#91.86(+0.16)	Penn Treebank, UAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#93.04	Penn Treebank, UAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#86.95(+0.86)	Penn Treebank, UAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#87.04	Penn Treebank, POS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#93.50(+0.46)	Penn Treebank, POS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#89.46(+0.66)	Penn Treebank, POS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#92.68(+0.23)	Penn Treebank, POS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#91.36	Penn Treebank, POS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#85.81	Penn Treebank, POS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#91.89	Penn Treebank, POS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#91.89(+0.60)	Penn Treebank, POS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#93.03	Penn Treebank, POS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#77.55	Penn Treebank, POS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#81.12	Penn Treebank, POS	Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#93.72(+0.10)	Penn Treebank, POS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#91.86(+0.16)	Penn Treebank, POS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#93.04	Penn Treebank, POS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#86.95(+0.86)	Penn Treebank, POS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#87.04	Penn Treebank, F1	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#93.50(+0.46)	Penn Treebank, F1	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#89.46(+0.66)	Penn Treebank, F1	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#92.68(+0.23)	Penn Treebank, F1	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#91.36	Penn Treebank, F1	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#85.81	Penn Treebank, F1	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#91.89	Penn Treebank, F1	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#91.89(+0.60)	Penn Treebank, F1	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#93.03	Penn Treebank, F1	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#77.55	Penn Treebank, F1	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#81.12	Penn Treebank, F1	Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#93.72(+0.10)	Penn Treebank, F1	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#91.86(+0.16)	Penn Treebank, F1	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#93.04	Penn Treebank, F1	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#86.95(+0.86)	Penn Treebank, F1	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#87.04	Penn Treebank, LAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#93.50(+0.46)	Penn Treebank, LAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#89.46(+0.66)	Penn Treebank, LAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#92.68(+0.23)	Penn Treebank, LAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#91.36	Penn Treebank, LAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#85.81	Penn Treebank, LAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#91.89	Penn Treebank, LAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#91.89(+0.60)	Penn Treebank, LAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#93.03	Penn Treebank, LAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#77.55	Penn Treebank, LAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#81.12	Penn Treebank, LAS	Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#93.72(+0.10)	Penn Treebank, LAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#91.86(+0.16)	Penn Treebank, LAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#93.04	Penn Treebank, LAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D11-1022.pdf#86.95(+0.86)	Penn Treebank, LAS	Full  Table 2: Unlabeled attachment scores, excluding punc- tuation. In the second column, [Ma08] denotes Martins  et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]  is Martins et al. (2010), and [Ko10] is
true	D12-1004.pdf#64.1	New York Times Corpus, P@10%	341 by Subject  Table 3: Performance by subject after T-test feature  selection in different confidence levels.
true	D12-1004.pdf#75.6	New York Times Corpus, P@10%	32 by Subject  Table 3: Performance by subject after T-test feature  selection in different confidence levels.
true	D12-1004.pdf#71.6	New York Times Corpus, P@10%	44 by Subject  Table 3: Performance by subject after T-test feature  selection in different confidence levels.
true	D12-1004.pdf#65.4	New York Times Corpus, P@10%	169 by Subject  Table 3: Performance by subject after T-test feature  selection in different confidence levels.
true	D12-1004.pdf#82.4	New York Times Corpus, P@10%	F ( % )  Table 4: Performance on best feature-set by feature ranking using signal to noise
true	D12-1004.pdf#75.0	New York Times Corpus, P@10%	P ( % )  Table 4: Performance on best feature-set by feature ranking using signal to noise
true	D12-1004.pdf#74.6	New York Times Corpus, P@10%	Macro - F ( % )  Table 4: Performance on best feature-set by feature ranking using signal to noise
true	D12-1004.pdf#76.9	New York Times Corpus, P@10%	Accuracy ( % )  Table 4: Performance on best feature-set by feature ranking using signal to noise
true	D12-1004.pdf#91.3	New York Times Corpus, P@10%	R ( % )  Table 4: Performance on best feature-set by feature ranking using signal to noise
true	D12-1004.pdf#72.5	New York Times Corpus, P@10%	T - test ( 0 . 05 )  Table 6: Accuracy per emotion by different feature-sets
true	D12-1004.pdf#71.8	New York Times Corpus, P@10%	T - test ( 0 . 001 )  Table 6: Accuracy per emotion by different feature-sets
true	D12-1004.pdf#70.7	New York Times Corpus, P@10%	T - test ( 0 . 001 )  Table 6: Accuracy per emotion by different feature-sets
true	D12-1004.pdf#70.7	New York Times Corpus, P@10%	T - test ( 0 . 05 )  Table 6: Accuracy per emotion by different feature-sets
true	D15-1161.pdf#72.10	VLSP 2013 POS tagging shared task, Accuracy	Sentiment Analysis  Table 1: Results for unsupervised POS induction, supervised POS tagging and Sentiment Analysis (one  per column) using different types of embeddings (one per row).
true	D15-1161.pdf#54.00	VLSP 2013 POS tagging shared task, Accuracy	POS Induction  Table 1: Results for unsupervised POS induction, supervised POS tagging and Sentiment Analysis (one  per column) using different types of embeddings (one per row).
true	D15-1161.pdf#97.40	VLSP 2013 POS tagging shared task, Accuracy	POS Tagging  Table 1: Results for unsupervised POS induction, supervised POS tagging and Sentiment Analysis (one  per column) using different types of embeddings (one per row).
true	P15-2077.pdf#54.3	New York Times Corpus, P@10%	1 , 131 %good pos .  Table 1: Selection of examples.
true	P15-2077.pdf#15.0	New York Times Corpus, P@10%	1 , 131 %bad neg .  Table 1: Selection of examples.
true	P15-2077.pdf#54.3	New York Times Corpus, P@10%	1 , 131 %good pos .  Table 1.  Concerning the method we propose,
true	P15-2077.pdf#15.0	New York Times Corpus, P@10%	1 , 131 %bad neg .  Table 1.  Concerning the method we propose,
true	P15-2077.pdf#+1.2	New York Times Corpus, P@10%	R - prec .  Table 2: Evaluation of our initial thesaurus and its  reranked versions (values = percentages).
true	P15-2077.pdf#+1.5	New York Times Corpus, P@10%	P@10  Table 2: Evaluation of our initial thesaurus and its  reranked versions (values = percentages).
true	P15-2077.pdf#+4.7	New York Times Corpus, P@10%	P@1  Table 2: Evaluation of our initial thesaurus and its  reranked versions (values = percentages).
true	P15-2077.pdf#+2.2	New York Times Corpus, P@10%	P@5  Table 2: Evaluation of our initial thesaurus and its  reranked versions (values = percentages).
true	P15-2077.pdf#+1.5	New York Times Corpus, P@10%	P@10  Table 2: Evaluation of our initial thesaurus and its  reranked versions (values = percentages).
true	P15-2077.pdf#+1.4	New York Times Corpus, P@10%	MAP  Table 2: Evaluation of our initial thesaurus and its  reranked versions (values = percentages).
true	P15-1098.pdf#0.577	IMDb, Accuracy	Yelp 2013 Acc  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#1.030	IMDb, Accuracy	IMDB MAE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.596	IMDb, Accuracy	Yelp 2013 Acc  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.800	IMDb, Accuracy	Yelp 2014 RMSE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.464	IMDb, Accuracy	Yelp 2013 MAE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.784	IMDb, Accuracy	Yelp 2013 RMSE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.447	IMDb, Accuracy	Yelp 2014 MAE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.405	IMDb, Accuracy	IMDB Acc  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.435	IMDb, Accuracy	IMDB Acc  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.979	IMDb, Accuracy	IMDB MAE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.585	IMDb, Accuracy	Yelp 2014 Acc  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#1.602	IMDb, Accuracy	IMDB RMSE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.764	IMDb, Accuracy	Yelp 2014 RMSE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#1.629	IMDb, Accuracy	IMDB RMSE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.804	IMDb, Accuracy	Yelp 2013 RMSE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.478	IMDb, Accuracy	Yelp 2014 MAE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.608	IMDb, Accuracy	Yelp 2014 Acc  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.485	IMDb, Accuracy	Yelp 2013 MAE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.55	IMDb, Accuracy	unk UP IMDB Acc  Table 3: Influence of user and product representations. For user k and product j, u k and p j are their  continuous vector representations, U k and P j are their continuous matrix representations (see
true	P15-1098.pdf#0.35	IMDb, Accuracy	full UP IMDB Acc  Table 3: Influence of user and product representations. For user k and product j, u k and p j are their  continuous vector representations, U k and P j are their continuous matrix representations (see
true	P15-1098.pdf#0.45	IMDb, Accuracy	full UP IMDB Acc  Table 3: Influence of user and product representations. For user k and product j, u k and p j are their  continuous vector representations, U k and P j are their continuous matrix representations (see
true	P15-1098.pdf#0.6	IMDb, Accuracy	no UP IMDB Acc  Table 3: Influence of user and product representations. For user k and product j, u k and p j are their  continuous vector representations, U k and P j are their continuous matrix representations (see
true	P15-1098.pdf#0.4	IMDb, Accuracy	full UP IMDB Acc  Table 3: Influence of user and product representations. For user k and product j, u k and p j are their  continuous vector representations, U k and P j are their continuous matrix representations (see
true	P15-1098.pdf#0.5	IMDb, Accuracy	full UP IMDB Acc  Table 3: Influence of user and product representations. For user k and product j, u k and p j are their  continuous vector representations, U k and P j are their continuous matrix representations (see
true	P15-1098.pdf#0.65	IMDb, Accuracy	IMDB Acc  Table 3: Influence of user and product representations. For user k and product j, u k and p j are their  continuous vector representations, U k and P j are their continuous matrix representations (see
true	P15-1098.pdf#0.577	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2013 Acc  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#1.030	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	IMDB MAE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.596	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2013 Acc  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.800	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2014 RMSE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.464	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2013 MAE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.784	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2013 RMSE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.447	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2014 MAE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.405	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	IMDB Acc  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.435	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	IMDB Acc  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.979	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	IMDB MAE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.585	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2014 Acc  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#1.602	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	IMDB RMSE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.764	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2014 RMSE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#1.629	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	IMDB RMSE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.804	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2013 RMSE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.478	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2014 MAE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.608	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2014 Acc  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.485	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2013 MAE  Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets. Evaluation metrics are  accuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better). Our full model is  UPNN (full).
true	P15-1098.pdf#0.55	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	unk UP IMDB Acc  Table 3: Influence of user and product representations. For user k and product j, u k and p j are their  continuous vector representations, U k and P j are their continuous matrix representations (see
true	P15-1098.pdf#0.35	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	full UP IMDB Acc  Table 3: Influence of user and product representations. For user k and product j, u k and p j are their  continuous vector representations, U k and P j are their continuous matrix representations (see
true	P15-1098.pdf#0.45	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	full UP IMDB Acc  Table 3: Influence of user and product representations. For user k and product j, u k and p j are their  continuous vector representations, U k and P j are their continuous matrix representations (see
true	P15-1098.pdf#0.6	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	no UP IMDB Acc  Table 3: Influence of user and product representations. For user k and product j, u k and p j are their  continuous vector representations, U k and P j are their continuous matrix representations (see
true	P15-1098.pdf#0.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	full UP IMDB Acc  Table 3: Influence of user and product representations. For user k and product j, u k and p j are their  continuous vector representations, U k and P j are their continuous matrix representations (see
true	P15-1098.pdf#0.5	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	full UP IMDB Acc  Table 3: Influence of user and product representations. For user k and product j, u k and p j are their  continuous vector representations, U k and P j are their continuous matrix representations (see
true	P15-1098.pdf#0.65	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	IMDB Acc  Table 3: Influence of user and product representations. For user k and product j, u k and p j are their  continuous vector representations, U k and P j are their continuous matrix representations (see
true	P14-1131.pdf#52.9%	Text8, Number of params	one - synonym P@10  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#0.48	Text8, Number of params	extended MRR  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#23.5%	Text8, Number of params	one - synonym P@1  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#44.1%	Text8, Number of params	extended P@1  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#0.32	Text8, Number of params	one - synonym MRR  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#61.8%	Text8, Number of params	one - synonym P@10  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#61.8%	Text8, Number of params	one - synonym P@10  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#20.6%	Text8, Number of params	one - synonym P@1  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#73.5%	Text8, Number of params	extended P@10  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#32.6%	Text8, Number of params	extended P@1  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#61.4%	Text8, Number of params	P@1  Table 4: Results for bilingual lexicon extraction  (TS1000 EN → DE). Best result in each column  in bold.
true	P14-1131.pdf#84.0%	Text8, Number of params	P@10  Table 4: Results for bilingual lexicon extraction  (TS1000 EN → DE). Best result in each column  in bold.
true	P14-1131.pdf#0.05,one-	Text8, Number of params	because the English graph contains a large number synonym extraction ( 68 word pairs ) PPR+cos SimRank CoSimRank Typed CoSimRank of more SimRank is at  Table 5: Execution times in minutes for CoSim- Rank and the baselines. Best result in each column  in bold.
true	P14-1131.pdf#73.6%	Text8, Number of params	P@10  Table 6: Results for bilingual lexicon extraction  (TS774 DE → EN). Best result in each column in  bold.
true	P14-1131.pdf#43.8%	Text8, Number of params	P@1  Table 6: Results for bilingual lexicon extraction  (TS774 DE → EN). Best result in each column in  bold.
true	P14-1131.pdf#52.9%	New York Times Corpus, P@10%	one - synonym P@10  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#0.48	New York Times Corpus, P@10%	extended MRR  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#23.5%	New York Times Corpus, P@10%	one - synonym P@1  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#44.1%	New York Times Corpus, P@10%	extended P@1  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#0.32	New York Times Corpus, P@10%	one - synonym MRR  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#61.8%	New York Times Corpus, P@10%	one - synonym P@10  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#61.8%	New York Times Corpus, P@10%	one - synonym P@10  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#20.6%	New York Times Corpus, P@10%	one - synonym P@1  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#73.5%	New York Times Corpus, P@10%	extended P@10  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#32.6%	New York Times Corpus, P@10%	extended P@1  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#61.4%	New York Times Corpus, P@10%	P@1  Table 4: Results for bilingual lexicon extraction  (TS1000 EN → DE). Best result in each column  in bold.
true	P14-1131.pdf#84.0%	New York Times Corpus, P@10%	P@10  Table 4: Results for bilingual lexicon extraction  (TS1000 EN → DE). Best result in each column  in bold.
true	P14-1131.pdf#0.05,one-	New York Times Corpus, P@10%	because the English graph contains a large number synonym extraction ( 68 word pairs ) PPR+cos SimRank CoSimRank Typed CoSimRank of more SimRank is at  Table 5: Execution times in minutes for CoSim- Rank and the baselines. Best result in each column  in bold.
true	P14-1131.pdf#73.6%	New York Times Corpus, P@10%	P@10  Table 6: Results for bilingual lexicon extraction  (TS774 DE → EN). Best result in each column in  bold.
true	P14-1131.pdf#43.8%	New York Times Corpus, P@10%	P@1  Table 6: Results for bilingual lexicon extraction  (TS774 DE → EN). Best result in each column in  bold.
true	P14-1131.pdf#52.9%	Text8, Bit per Character (BPC)	one - synonym P@10  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#0.48	Text8, Bit per Character (BPC)	extended MRR  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#23.5%	Text8, Bit per Character (BPC)	one - synonym P@1  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#44.1%	Text8, Bit per Character (BPC)	extended P@1  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#0.32	Text8, Bit per Character (BPC)	one - synonym MRR  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#61.8%	Text8, Bit per Character (BPC)	one - synonym P@10  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#61.8%	Text8, Bit per Character (BPC)	one - synonym P@10  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#20.6%	Text8, Bit per Character (BPC)	one - synonym P@1  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#73.5%	Text8, Bit per Character (BPC)	extended P@10  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#32.6%	Text8, Bit per Character (BPC)	extended P@1  Table 2: Results for synonym extraction on TS68.  Best result in each column in bold.
true	P14-1131.pdf#61.4%	Text8, Bit per Character (BPC)	P@1  Table 4: Results for bilingual lexicon extraction  (TS1000 EN → DE). Best result in each column  in bold.
true	P14-1131.pdf#84.0%	Text8, Bit per Character (BPC)	P@10  Table 4: Results for bilingual lexicon extraction  (TS1000 EN → DE). Best result in each column  in bold.
true	P14-1131.pdf#0.05,one-	Text8, Bit per Character (BPC)	because the English graph contains a large number synonym extraction ( 68 word pairs ) PPR+cos SimRank CoSimRank Typed CoSimRank of more SimRank is at  Table 5: Execution times in minutes for CoSim- Rank and the baselines. Best result in each column  in bold.
true	P14-1131.pdf#73.6%	Text8, Bit per Character (BPC)	P@10  Table 6: Results for bilingual lexicon extraction  (TS774 DE → EN). Best result in each column in  bold.
true	P14-1131.pdf#43.8%	Text8, Bit per Character (BPC)	P@1  Table 6: Results for bilingual lexicon extraction  (TS774 DE → EN). Best result in each column in  bold.
true	D12-1131.pdf#≥0.8	benchmark Vietnamese dependency treebank VnDT, UAS	Table 1: Exploratory statistics for constraint selection.  The table shows the percentage of context types for which  the probability of the most frequent head tag is at least p.  Head in Context refers to the subset of contexts where the  most frequent head is within the context itself. Numbers  are based on Section 22 of the Wall Street Journal and are  given for contexts that appear at least 10 times.
true	D12-1131.pdf#0.05,all	benchmark Vietnamese dependency treebank VnDT, UAS	parser of McDonald et al . ( 2005 ) . ST is a self - training model based on Reichart and Rappoport ( 2007 ) . Model is the English data is from the WSJ . Bulgarian ,  Table 4: POS tagging accuracy. Stanford POS tagger refers to the maximum entropy trigram tagger of
true	D12-1131.pdf#≥0.8	Penn Treebank, UAS	Table 1: Exploratory statistics for constraint selection.  The table shows the percentage of context types for which  the probability of the most frequent head tag is at least p.  Head in Context refers to the subset of contexts where the  most frequent head is within the context itself. Numbers  are based on Section 22 of the Wall Street Journal and are  given for contexts that appear at least 10 times.
true	D12-1131.pdf#0.05,all	Penn Treebank, UAS	parser of McDonald et al . ( 2005 ) . ST is a self - training model based on Reichart and Rappoport ( 2007 ) . Model is the English data is from the WSJ . Bulgarian ,  Table 4: POS tagging accuracy. Stanford POS tagger refers to the maximum entropy trigram tagger of
true	D12-1074.pdf#78.9	New York Times Corpus, P@10%	ACE Results English Unlabeled F1  Table 1: Relation Extraction Results. Models using hidden constituency syntax provide significant gains over the  syntactically-uniformed baseline model in both languages, but the advantages of the latent syntax were mitigated on  the smaller Chinese data set.
true	D12-1074.pdf#46.8	New York Times Corpus, P@10%	ACE Results Chinese Labeled F1  Table 1: Relation Extraction Results. Models using hidden constituency syntax provide significant gains over the  syntactically-uniformed baseline model in both languages, but the advantages of the latent syntax were mitigated on  the smaller Chinese data set.
true	D12-1074.pdf#48.3	New York Times Corpus, P@10%	ACE Results Chinese Unlabeled F1  Table 1: Relation Extraction Results. Models using hidden constituency syntax provide significant gains over the  syntactically-uniformed baseline model in both languages, but the advantages of the latent syntax were mitigated on  the smaller Chinese data set.
true	D12-1074.pdf#77.4	New York Times Corpus, P@10%	ACE Results English Labeled F1  Table 1: Relation Extraction Results. Models using hidden constituency syntax provide significant gains over the  syntactically-uniformed baseline model in both languages, but the advantages of the latent syntax were mitigated on  the smaller Chinese data set.
true	D12-1074.pdf#96.52	New York Times Corpus, P@10%	Unlabeled F1  Table 2: SRL Results. The hidden model excels on the unlabeled prediction results, often besting the scores obtained  using the parses distributed with the CoNLL data sets. These gains did not always translate to the labeled task where  poor sense prediction hindered absolute performance.
true	D12-1074.pdf#81.25	New York Times Corpus, P@10%	Unlabeled F1  Table 2: SRL Results. The hidden model excels on the unlabeled prediction results, often besting the scores obtained  using the parses distributed with the CoNLL data sets. These gains did not always translate to the labeled task where  poor sense prediction hindered absolute performance.
true	D12-1074.pdf#77.97	New York Times Corpus, P@10%	Labeled F1  Table 2: SRL Results. The hidden model excels on the unlabeled prediction results, often besting the scores obtained  using the parses distributed with the CoNLL data sets. These gains did not always translate to the labeled task where  poor sense prediction hindered absolute performance.
true	D12-1074.pdf#69.59	New York Times Corpus, P@10%	Labeled F1  Table 2: SRL Results. The hidden model excels on the unlabeled prediction results, often besting the scores obtained  using the parses distributed with the CoNLL data sets. These gains did not always translate to the labeled task where  poor sense prediction hindered absolute performance.
true	D12-1074.pdf#89.00	New York Times Corpus, P@10%	Unlabeled F1  Table 2: SRL Results. The hidden model excels on the unlabeled prediction results, often besting the scores obtained  using the parses distributed with the CoNLL data sets. These gains did not always translate to the labeled task where  poor sense prediction hindered absolute performance.
true	D12-1074.pdf#88.91	New York Times Corpus, P@10%	Unlabeled F1  Table 2: SRL Results. The hidden model excels on the unlabeled prediction results, often besting the scores obtained  using the parses distributed with the CoNLL data sets. These gains did not always translate to the labeled task where  poor sense prediction hindered absolute performance.
true	D12-1074.pdf#67.26	New York Times Corpus, P@10%	Labeled F1  Table 2: SRL Results. The hidden model excels on the unlabeled prediction results, often besting the scores obtained  using the parses distributed with the CoNLL data sets. These gains did not always translate to the labeled task where  poor sense prediction hindered absolute performance.
true	D12-1074.pdf#90.85	New York Times Corpus, P@10%	Unlabeled F1  Table 2: SRL Results. The hidden model excels on the unlabeled prediction results, often besting the scores obtained  using the parses distributed with the CoNLL data sets. These gains did not always translate to the labeled task where  poor sense prediction hindered absolute performance.
true	D12-1074.pdf#68.93	New York Times Corpus, P@10%	Labeled F1  Table 2: SRL Results. The hidden model excels on the unlabeled prediction results, often besting the scores obtained  using the parses distributed with the CoNLL data sets. These gains did not always translate to the labeled task where  poor sense prediction hindered absolute performance.
true	D12-1074.pdf#78.55	New York Times Corpus, P@10%	Labeled F1  Table 2: SRL Results. The hidden model excels on the unlabeled prediction results, often besting the scores obtained  using the parses distributed with the CoNLL data sets. These gains did not always translate to the labeled task where  poor sense prediction hindered absolute performance.
true	D12-1074.pdf#74.84	New York Times Corpus, P@10%	Labeled F1  Table 2: SRL Results. The hidden model excels on the unlabeled prediction results, often besting the scores obtained  using the parses distributed with the CoNLL data sets. These gains did not always translate to the labeled task where  poor sense prediction hindered absolute performance.
true	D12-1074.pdf#87.69	New York Times Corpus, P@10%	Unlabeled F1  Table 2: SRL Results. The hidden model excels on the unlabeled prediction results, often besting the scores obtained  using the parses distributed with the CoNLL data sets. These gains did not always translate to the labeled task where  poor sense prediction hindered absolute performance.
true	D12-1074.pdf#97.31	New York Times Corpus, P@10%	Unlabeled F1  Table 2: SRL Results. The hidden model excels on the unlabeled prediction results, often besting the scores obtained  using the parses distributed with the CoNLL data sets. These gains did not always translate to the labeled task where  poor sense prediction hindered absolute performance.
true	D12-1074.pdf#65.65	New York Times Corpus, P@10%	Labeled F1  Table 2: SRL Results. The hidden model excels on the unlabeled prediction results, often besting the scores obtained  using the parses distributed with the CoNLL data sets. These gains did not always translate to the labeled task where  poor sense prediction hindered absolute performance.
true	D15-1305.pdf#95.05%	IMDb, Accuracy	( Mesnil et al . , 2014 ) Accuracy rate  Table 1: The performance of our method com- pared with other approaches on the IMDB dataset.
true	D15-1305.pdf#99.20%	IMDb, Accuracy	( Le and Mikolov , 2014 ) Sci  Table 2.  Compared with  Confidence-weighted (
true	D15-1305.pdf#95.47%	IMDb, Accuracy	( Le and Mikolov , 2014 ) Comp  Table 2.  Compared with  Confidence-weighted (
true	P13-2087.pdf#81.15	IMDb, Accuracy	A . Re - embeddings ( our method ) Number of training examples 5K  Table 1: Classification accuracy for the sentiment task (IMDB  movie review dataset (Maas et al., 2011)). Subtable A compares  performance of the re-embedded vocabulary, induced from a  given source embedding. Subtable B contains a set of base- lines: X-w/o re-embedding indicates using a source embedding  X directly without re-embedding.
true	P13-2087.pdf#79.30	IMDb, Accuracy	B . Baselines Number of training examples 5K  Table 1: Classification accuracy for the sentiment task (IMDB  movie review dataset (Maas et al., 2011)). Subtable A compares  performance of the re-embedded vocabulary, induced from a  given source embedding. Subtable B contains a set of base- lines: X-w/o re-embedding indicates using a source embedding  X directly without re-embedding.
true	P13-2087.pdf#86.15	IMDb, Accuracy	A . Re - embeddings ( our method ) Number of training examples 20K  Table 1: Classification accuracy for the sentiment task (IMDB  movie review dataset (Maas et al., 2011)). Subtable A compares  performance of the re-embedded vocabulary, induced from a  given source embedding. Subtable B contains a set of base- lines: X-w/o re-embedding indicates using a source embedding  X directly without re-embedding.
true	P13-2087.pdf#85.15	IMDb, Accuracy	B . Baselines Number of training examples 5K  Table 1: Classification accuracy for the sentiment task (IMDB  movie review dataset (Maas et al., 2011)). Subtable A compares  performance of the re-embedded vocabulary, induced from a  given source embedding. Subtable B contains a set of base- lines: X-w/o re-embedding indicates using a source embedding  X directly without re-embedding.
true	P13-2087.pdf#68.17	IMDb, Accuracy	B . Baselines Number of training examples 5K  Table 1: Classification accuracy for the sentiment task (IMDB  movie review dataset (Maas et al., 2011)). Subtable A compares  performance of the re-embedded vocabulary, induced from a  given source embedding. Subtable B contains a set of base- lines: X-w/o re-embedding indicates using a source embedding  X directly without re-embedding.
true	P13-2087.pdf#72.72	IMDb, Accuracy	B . Baselines Number of training examples 5K  Table 1: Classification accuracy for the sentiment task (IMDB  movie review dataset (Maas et al., 2011)). Subtable A compares  performance of the re-embedded vocabulary, induced from a  given source embedding. Subtable B contains a set of base- lines: X-w/o re-embedding indicates using a source embedding  X directly without re-embedding.
true	P13-2087.pdf#86.15	IMDb, Accuracy	B . Baselines Number of training examples 20K  Table 1: Classification accuracy for the sentiment task (IMDB  movie review dataset (Maas et al., 2011)). Subtable A compares  performance of the re-embedded vocabulary, induced from a  given source embedding. Subtable B contains a set of base- lines: X-w/o re-embedding indicates using a source embedding  X directly without re-embedding.
true	P13-2087.pdf#80.25	IMDb, Accuracy	A . Re - embeddings ( our method ) Number of training examples 5K  Table 1: Classification accuracy for the sentiment task (IMDB  movie review dataset (Maas et al., 2011)). Subtable A compares  performance of the re-embedded vocabulary, induced from a  given source embedding. Subtable B contains a set of base- lines: X-w/o re-embedding indicates using a source embedding  X directly without re-embedding.
true	P13-2087.pdf#79.34	IMDb, Accuracy	A . Re - embeddings ( our method ) Number of training examples 5K  Table 1: Classification accuracy for the sentiment task (IMDB  movie review dataset (Maas et al., 2011)). Subtable A compares  performance of the re-embedded vocabulary, induced from a  given source embedding. Subtable B contains a set of base- lines: X-w/o re-embedding indicates using a source embedding  X directly without re-embedding.
true	P13-2087.pdf#74.80	IMDb, Accuracy	A . Re - embeddings ( our method ) Number of training examples 5K  Table 1: Classification accuracy for the sentiment task (IMDB  movie review dataset (Maas et al., 2011)). Subtable A compares  performance of the re-embedded vocabulary, induced from a  given source embedding. Subtable B contains a set of base- lines: X-w/o re-embedding indicates using a source embedding  X directly without re-embedding.
true	P13-2087.pdf#85.28	IMDb, Accuracy	A . Re - embeddings ( our method ) Number of training examples 5K  Table 1: Classification accuracy for the sentiment task (IMDB  movie review dataset (Maas et al., 2011)). Subtable A compares  performance of the re-embedded vocabulary, induced from a  given source embedding. Subtable B contains a set of base- lines: X-w/o re-embedding indicates using a source embedding  X directly without re-embedding.
true	P13-2087.pdf#73.38	IMDb, Accuracy	B . Baselines Number of training examples 5K  Table 1: Classification accuracy for the sentiment task (IMDB  movie review dataset (Maas et al., 2011)). Subtable A compares  performance of the re-embedded vocabulary, induced from a  given source embedding. Subtable B contains a set of base- lines: X-w/o re-embedding indicates using a source embedding  X directly without re-embedding.
true	P10-1050.pdf#1600	CCGBank, Accuracy	Joint tagging 850  Table 1: Decoding speed (sent./sec).
true	P10-1050.pdf#2400	CCGBank, Accuracy	300 Joint tagging  Table 4: Comparison with beam search (sent./sec).
true	D14-1122.pdf#0.669	benchmark Vietnamese dependency treebank VnDT, UAS	Dev .  Table 1: Comparing our Weibo parser to other  baselines (UAS). The off-the-shelf Stanford parser  uses its attached Xinhua and Chinese factored  models, which are trained on external Chinese  treebank of newswire data. MaltParser was trained  on the same in-domain data as our proposed ap- proach. * indicates p < .001 comparing to the  MaltParser.
true	D14-1122.pdf#0.675*	benchmark Vietnamese dependency treebank VnDT, UAS	Our methods - ProPPR Test  Table 1: Comparing our Weibo parser to other  baselines (UAS). The off-the-shelf Stanford parser  uses its attached Xinhua and Chinese factored  models, which are trained on external Chinese  treebank of newswire data. MaltParser was trained  on the same in-domain data as our proposed ap- proach. * indicates p < .001 comparing to the  MaltParser.
true	D11-1140.pdf#81.9	Text8, Number of params	Rec .  Table 1: Performance on the Atis development set.
true	D11-1140.pdf#87.3	Text8, Number of params	Exact Match  Table 1: Performance on the Atis development set.
true	D11-1140.pdf#82.0	Text8, Number of params	F1  Table 1: Performance on the Atis development set.
true	D11-1140.pdf#95.1	Text8, Number of params	Partial Match  Table 2: Performance on the Atis test set.
true	D11-1140.pdf#85.2	Text8, Number of params	Exact Match  Table 2: Performance on the Atis test set.
true	D11-1140.pdf#96.7	Text8, Number of params	Partial Match  Table 2: Performance on the Atis test set.
true	D11-1140.pdf#95.9	Text8, Number of params	Partial Match  Table 2: Performance on the Atis test set.
true	D11-1140.pdf#85.8	Text8, Number of params	Exact Match  Table 2: Performance on the Atis test set.
true	D11-1140.pdf#84.6	Text8, Number of params	Exact Match  Table 2: Performance on the Atis test set.
true	D11-1140.pdf#96.3	Text8, Number of params	Pre .  Table 3: Exact match accuracy on the Geo880 test set.
true	D11-1140.pdf#88.8	Text8, Number of params	Labelled Logical Forms F1  Table 3: Exact match accuracy on the Geo880 test set.
true	D11-1140.pdf#91.1	Text8, Number of params	Labelled Logical Forms Rec .  Table 3: Exact match accuracy on the Geo880 test set.
true	D11-1140.pdf#88.6	Text8, Number of params	Labelled Logical Forms Rec .  Table 3: Exact match accuracy on the Geo880 test set.
true	D11-1140.pdf#90.4	Text8, Number of params	Spanish Pre . Turkish  Table 4: Exact-match accuracy on the Geo250 data set.
true	D11-1140.pdf#83.7	Text8, Number of params	F1  Table 4: Exact-match accuracy on the Geo250 data set.
true	D11-1140.pdf#85.6	Text8, Number of params	Rec .  Table 4: Exact-match accuracy on the Geo250 data set.
true	D11-1140.pdf#85.8	Text8, Number of params	Spanish  Table 4: Exact-match accuracy on the Geo250 data set.
true	D11-1140.pdf#92.5	Text8, Number of params	Spanish  Table 4: Exact-match accuracy on the Geo250 data set.
true	D11-1140.pdf#72.5	Text8, Number of params	Rec . Rec .  Table 4: Exact-match accuracy on the Geo250 data set.
true	D11-1140.pdf#91.8	Text8, Number of params	English  Table 4: Exact-match accuracy on the Geo250 data set.
true	D11-1140.pdf#83.7	Text8, Number of params	Rec .  Table 4: Exact-match accuracy on the Geo250 data set.
true	D11-1140.pdf#78.1	Text8, Number of params	F1 Turkish  Table 4: Exact-match accuracy on the Geo250 data set.
true	D11-1140.pdf#85.8	Text8, Number of params	F1 Japanese  Table 4: Exact-match accuracy on the Geo250 data set.
true	D11-1140.pdf#90.1	Text8, Number of params	English Pre . Japanese  Table 4: Exact-match accuracy on the Geo250 data set.
true	D11-1140.pdf#83.2	Text8, Number of params	Rec . Rec .  Table 4: Exact-match accuracy on the Geo250 data set.
true	P14-2131.pdf#62.6	Penn Treebank, UAS	SIM  Table 2: Intrinsic evaluation of representations. SIM column  has Spearman's ρ × 100 for 353-pair word similarity dataset.  M-1 is our unsupervised POS tagging metric. For BROWN,  M-1 is simply many-to-one accuracy of the clusters. Best  score in each column is bold.
true	P14-2131.pdf#89.3	Penn Treebank, UAS	Table 2: Intrinsic evaluation of representations. SIM column  has Spearman's ρ × 100 for 353-pair word similarity dataset.  M-1 is our unsupervised POS tagging metric. For BROWN,  M-1 is simply many-to-one accuracy of the clusters. Best  score in each column is bold.
true	P14-2131.pdf#92.69	Penn Treebank, UAS	Test  Table 5: Full results with bit string features (UAS on WSJ).
true	P14-2131.pdf#92.69	Penn Treebank, UAS	Test  Table 5: Full results with bit string features (UAS on WSJ).
true	P14-2131.pdf#93.33	Penn Treebank, UAS	Dev  Table 5: Full results with bit string features (UAS on WSJ).
true	P14-2131.pdf#81.9	Penn Treebank, UAS	eml  Table 6: Main UAS test results on Web treebanks. Here,  ans=answers, eml=email, nwg=newsgroup, rev=reviews,  blog=weblog, BR=BROWN, Avg=Macro-average.
true	P14-2131.pdf#83.7	Penn Treebank, UAS	ans  Table 6: Main UAS test results on Web treebanks. Here,  ans=answers, eml=email, nwg=newsgroup, rev=reviews,  blog=weblog, BR=BROWN, Avg=Macro-average.
true	P14-2131.pdf#84.3	Penn Treebank, UAS	Avg  Table 6: Main UAS test results on Web treebanks. Here,  ans=answers, eml=email, nwg=newsgroup, rev=reviews,  blog=weblog, BR=BROWN, Avg=Macro-average.
true	P14-2131.pdf#85.2	Penn Treebank, UAS	nwg  Table 6: Main UAS test results on Web treebanks. Here,  ans=answers, eml=email, nwg=newsgroup, rev=reviews,  blog=weblog, BR=BROWN, Avg=Macro-average.
true	P14-2131.pdf#85.0	Penn Treebank, UAS	rev  Table 6: Main UAS test results on Web treebanks. Here,  ans=answers, eml=email, nwg=newsgroup, rev=reviews,  blog=weblog, BR=BROWN, Avg=Macro-average.
true	P14-2131.pdf#85.2	Penn Treebank, UAS	nwg  Table 6: Main UAS test results on Web treebanks. Here,  ans=answers, eml=email, nwg=newsgroup, rev=reviews,  blog=weblog, BR=BROWN, Avg=Macro-average.
true	P14-2131.pdf#85.2	Penn Treebank, UAS	nwg  Table 6: Main UAS test results on Web treebanks. Here,  ans=answers, eml=email, nwg=newsgroup, rev=reviews,  blog=weblog, BR=BROWN, Avg=Macro-average.
true	P14-2131.pdf#86.1	Penn Treebank, UAS	blog  Table 6: Main UAS test results on Web treebanks. Here,  ans=answers, eml=email, nwg=newsgroup, rev=reviews,  blog=weblog, BR=BROWN, Avg=Macro-average.
true	P14-2131.pdf#62.6	benchmark Vietnamese dependency treebank VnDT, UAS	SIM  Table 2: Intrinsic evaluation of representations. SIM column  has Spearman's ρ × 100 for 353-pair word similarity dataset.  M-1 is our unsupervised POS tagging metric. For BROWN,  M-1 is simply many-to-one accuracy of the clusters. Best  score in each column is bold.
true	P14-2131.pdf#89.3	benchmark Vietnamese dependency treebank VnDT, UAS	Table 2: Intrinsic evaluation of representations. SIM column  has Spearman's ρ × 100 for 353-pair word similarity dataset.  M-1 is our unsupervised POS tagging metric. For BROWN,  M-1 is simply many-to-one accuracy of the clusters. Best  score in each column is bold.
true	P14-2131.pdf#92.69	benchmark Vietnamese dependency treebank VnDT, UAS	Test  Table 5: Full results with bit string features (UAS on WSJ).
true	P14-2131.pdf#92.69	benchmark Vietnamese dependency treebank VnDT, UAS	Test  Table 5: Full results with bit string features (UAS on WSJ).
true	P14-2131.pdf#93.33	benchmark Vietnamese dependency treebank VnDT, UAS	Dev  Table 5: Full results with bit string features (UAS on WSJ).
true	P14-2131.pdf#81.9	benchmark Vietnamese dependency treebank VnDT, UAS	eml  Table 6: Main UAS test results on Web treebanks. Here,  ans=answers, eml=email, nwg=newsgroup, rev=reviews,  blog=weblog, BR=BROWN, Avg=Macro-average.
true	P14-2131.pdf#83.7	benchmark Vietnamese dependency treebank VnDT, UAS	ans  Table 6: Main UAS test results on Web treebanks. Here,  ans=answers, eml=email, nwg=newsgroup, rev=reviews,  blog=weblog, BR=BROWN, Avg=Macro-average.
true	P14-2131.pdf#84.3	benchmark Vietnamese dependency treebank VnDT, UAS	Avg  Table 6: Main UAS test results on Web treebanks. Here,  ans=answers, eml=email, nwg=newsgroup, rev=reviews,  blog=weblog, BR=BROWN, Avg=Macro-average.
true	P14-2131.pdf#85.2	benchmark Vietnamese dependency treebank VnDT, UAS	nwg  Table 6: Main UAS test results on Web treebanks. Here,  ans=answers, eml=email, nwg=newsgroup, rev=reviews,  blog=weblog, BR=BROWN, Avg=Macro-average.
true	P14-2131.pdf#85.0	benchmark Vietnamese dependency treebank VnDT, UAS	rev  Table 6: Main UAS test results on Web treebanks. Here,  ans=answers, eml=email, nwg=newsgroup, rev=reviews,  blog=weblog, BR=BROWN, Avg=Macro-average.
true	P14-2131.pdf#85.2	benchmark Vietnamese dependency treebank VnDT, UAS	nwg  Table 6: Main UAS test results on Web treebanks. Here,  ans=answers, eml=email, nwg=newsgroup, rev=reviews,  blog=weblog, BR=BROWN, Avg=Macro-average.
true	P14-2131.pdf#85.2	benchmark Vietnamese dependency treebank VnDT, UAS	nwg  Table 6: Main UAS test results on Web treebanks. Here,  ans=answers, eml=email, nwg=newsgroup, rev=reviews,  blog=weblog, BR=BROWN, Avg=Macro-average.
true	P14-2131.pdf#86.1	benchmark Vietnamese dependency treebank VnDT, UAS	blog  Table 6: Main UAS test results on Web treebanks. Here,  ans=answers, eml=email, nwg=newsgroup, rev=reviews,  blog=weblog, BR=BROWN, Avg=Macro-average.
true	D15-1082.pdf#51.8	FB15K-237, H@1	Hits@10 ( % ) Raw  Table 2: Evaluation results on entity prediction.
true	D15-1082.pdf#84.6	FB15K-237, H@1	Hits@10 ( % ) Filter  Table 2: Evaluation results on entity prediction.
true	D15-1082.pdf#91.0	FB15K-237, H@1	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#60.9	FB15K-237, H@1	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#3.3	FB15K-237, H@1	prediction . tion paths . As compared with TransE , the inferior Metric  Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#1.7	FB15K-237, H@1	prediction . tion paths . As compared with TransE , the inferior Mean Rank Raw  Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#69.7	FB15K-237, H@1	prediction . tion paths . As compared with TransE , the inferior Hits@1 ( % ) Raw  Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#88.7	FB15K-237, H@1	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#86.1	FB15K-237, H@1	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#94.0	FB15K-237, H@1	prediction . tion paths . As compared with TransE , the inferior Hits@1 ( % ) Filter  Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#91.2	FB15K-237, H@1	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#1.2	FB15K-237, H@1	prediction . tion paths . As compared with TransE , the inferior Filter  Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#74.0	FB15K-237, H@1	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#90.4	FB15K-237, H@1	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#92.8	FB15K-237, H@1	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#51.8	FB15K-237, MRR	Hits@10 ( % ) Raw  Table 2: Evaluation results on entity prediction.
true	D15-1082.pdf#84.6	FB15K-237, MRR	Hits@10 ( % ) Filter  Table 2: Evaluation results on entity prediction.
true	D15-1082.pdf#91.0	FB15K-237, MRR	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#60.9	FB15K-237, MRR	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#3.3	FB15K-237, MRR	prediction . tion paths . As compared with TransE , the inferior Metric  Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#1.7	FB15K-237, MRR	prediction . tion paths . As compared with TransE , the inferior Mean Rank Raw  Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#69.7	FB15K-237, MRR	prediction . tion paths . As compared with TransE , the inferior Hits@1 ( % ) Raw  Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#88.7	FB15K-237, MRR	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#86.1	FB15K-237, MRR	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#94.0	FB15K-237, MRR	prediction . tion paths . As compared with TransE , the inferior Hits@1 ( % ) Filter  Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#91.2	FB15K-237, MRR	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#1.2	FB15K-237, MRR	prediction . tion paths . As compared with TransE , the inferior Filter  Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#74.0	FB15K-237, MRR	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#90.4	FB15K-237, MRR	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#92.8	FB15K-237, MRR	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#51.8	WN18RR, MRR	Hits@10 ( % ) Raw  Table 2: Evaluation results on entity prediction.
true	D15-1082.pdf#84.6	WN18RR, MRR	Hits@10 ( % ) Filter  Table 2: Evaluation results on entity prediction.
true	D15-1082.pdf#91.0	WN18RR, MRR	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#60.9	WN18RR, MRR	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#3.3	WN18RR, MRR	prediction . tion paths . As compared with TransE , the inferior Metric  Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#1.7	WN18RR, MRR	prediction . tion paths . As compared with TransE , the inferior Mean Rank Raw  Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#69.7	WN18RR, MRR	prediction . tion paths . As compared with TransE , the inferior Hits@1 ( % ) Raw  Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#88.7	WN18RR, MRR	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#86.1	WN18RR, MRR	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#94.0	WN18RR, MRR	prediction . tion paths . As compared with TransE , the inferior Hits@1 ( % ) Filter  Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#91.2	WN18RR, MRR	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#1.2	WN18RR, MRR	prediction . tion paths . As compared with TransE , the inferior Filter  Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#74.0	WN18RR, MRR	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#90.4	WN18RR, MRR	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#92.8	WN18RR, MRR	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#51.8	FB15K-237, H@10	Hits@10 ( % ) Raw  Table 2: Evaluation results on entity prediction.
true	D15-1082.pdf#84.6	FB15K-237, H@10	Hits@10 ( % ) Filter  Table 2: Evaluation results on entity prediction.
true	D15-1082.pdf#91.0	FB15K-237, H@10	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#60.9	FB15K-237, H@10	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#3.3	FB15K-237, H@10	prediction . tion paths . As compared with TransE , the inferior Metric  Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#1.7	FB15K-237, H@10	prediction . tion paths . As compared with TransE , the inferior Mean Rank Raw  Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#69.7	FB15K-237, H@10	prediction . tion paths . As compared with TransE , the inferior Hits@1 ( % ) Raw  Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#88.7	FB15K-237, H@10	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#86.1	FB15K-237, H@10	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#94.0	FB15K-237, H@10	prediction . tion paths . As compared with TransE , the inferior Hits@1 ( % ) Filter  Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#91.2	FB15K-237, H@10	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#1.2	FB15K-237, H@10	prediction . tion paths . As compared with TransE , the inferior Filter  Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#74.0	FB15K-237, H@10	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#90.4	FB15K-237, H@10	Table 4: Evaluation results on relation prediction.
true	D15-1082.pdf#92.8	FB15K-237, H@10	Table 4: Evaluation results on relation prediction.
true	P10-1124.pdf#41.2	New York Times Corpus, P@10%	Not using a development set Edges F1  Table 1: Results for all experiments
true	P10-1124.pdf#66.2	New York Times Corpus, P@10%	Using a development set Propositions F1  Table 1: Results for all experiments
true	P10-1124.pdf#62.4	New York Times Corpus, P@10%	Not using a development set Propositions F1  Table 1: Results for all experiments
true	P10-1124.pdf#43.8	New York Times Corpus, P@10%	Using a development set Edges F1  Table 1: Results for all experiments
true	P15-1042.pdf#79.40%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	music  Table 2: Performance comparisons on the  NLP&CC 2013 CLSC dataset.
true	P15-1042.pdf#81.60%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	DVD  Table 2: Performance comparisons on the  NLP&CC 2013 CLSC dataset.
true	P15-1042.pdf#81.05%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	book  Table 2: Performance comparisons on the  NLP&CC 2013 CLSC dataset.
true	P15-1042.pdf#80.68%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Average  Table 2: Performance comparisons on the  NLP&CC 2013 CLSC dataset.
true	D14-1042.pdf#10.85	SemEval 2013, F1	F1 points  Table 2: Results on SemEval-2007 coarse-grained (verbs as clause heads)
true	D14-1042.pdf#78.52	SemEval 2013, F1	Verbs ( clause heads ) F1  Table 2: Results on SemEval-2007 coarse-grained (verbs as clause heads)
true	D14-1042.pdf#81.18	SemEval 2013, F1	Verbs ( clause heads ) F1  Table 2: Results on SemEval-2007 coarse-grained (verbs as clause heads)
true	D14-1042.pdf#3.46	SemEval 2013, F1	MFS F1 points  Table 3:
true	D14-1042.pdf#7.85	SemEval 2013, F1	SimpleExtLesk ( DKPro ) F1 points  Table 3:
true	D14-1042.pdf#61.17	SemEval 2013, F1	SimpleExtLesk ( DKPro ) F1  Table 3:
true	D14-1042.pdf#7.93	SemEval 2013, F1	SimpleExtLesk ( DKPro ) F1 points  Table 3:
true	D14-1042.pdf#73.70	SemEval 2013, F1	SimpleExtLesk ( DKPro ) P  Table 3:
true	D14-1042.pdf#52.28	SemEval 2013, F1	SimpleExtLesk ( DKPro ) R  Table 3:
true	D14-1042.pdf#+3.90	SemEval 2013, F1	SimpleExtLesk ( DKPro ) with MFS back - off F1 points  Table 3:
true	D14-1042.pdf#3.35	SemEval 2013, F1	MFS F1 points  Table 3:
true	D14-1042.pdf#3.78	SemEval 2013, F1	SimpleExtLesk ( DKPro ) with MFS back - off F1 points  Table 3:
true	D14-1042.pdf#10.85	SemEval 2007, F1	F1 points  Table 2: Results on SemEval-2007 coarse-grained (verbs as clause heads)
true	D14-1042.pdf#78.52	SemEval 2007, F1	Verbs ( clause heads ) F1  Table 2: Results on SemEval-2007 coarse-grained (verbs as clause heads)
true	D14-1042.pdf#81.18	SemEval 2007, F1	Verbs ( clause heads ) F1  Table 2: Results on SemEval-2007 coarse-grained (verbs as clause heads)
true	D14-1042.pdf#3.46	SemEval 2007, F1	MFS F1 points  Table 3:
true	D14-1042.pdf#7.85	SemEval 2007, F1	SimpleExtLesk ( DKPro ) F1 points  Table 3:
true	D14-1042.pdf#61.17	SemEval 2007, F1	SimpleExtLesk ( DKPro ) F1  Table 3:
true	D14-1042.pdf#7.93	SemEval 2007, F1	SimpleExtLesk ( DKPro ) F1 points  Table 3:
true	D14-1042.pdf#73.70	SemEval 2007, F1	SimpleExtLesk ( DKPro ) P  Table 3:
true	D14-1042.pdf#52.28	SemEval 2007, F1	SimpleExtLesk ( DKPro ) R  Table 3:
true	D14-1042.pdf#+3.90	SemEval 2007, F1	SimpleExtLesk ( DKPro ) with MFS back - off F1 points  Table 3:
true	D14-1042.pdf#3.35	SemEval 2007, F1	MFS F1 points  Table 3:
true	D14-1042.pdf#3.78	SemEval 2007, F1	SimpleExtLesk ( DKPro ) with MFS back - off F1 points  Table 3:
true	D14-1042.pdf#10.85	Senseval 2, F1	F1 points  Table 2: Results on SemEval-2007 coarse-grained (verbs as clause heads)
true	D14-1042.pdf#78.52	Senseval 2, F1	Verbs ( clause heads ) F1  Table 2: Results on SemEval-2007 coarse-grained (verbs as clause heads)
true	D14-1042.pdf#81.18	Senseval 2, F1	Verbs ( clause heads ) F1  Table 2: Results on SemEval-2007 coarse-grained (verbs as clause heads)
true	D14-1042.pdf#3.46	Senseval 2, F1	MFS F1 points  Table 3:
true	D14-1042.pdf#7.85	Senseval 2, F1	SimpleExtLesk ( DKPro ) F1 points  Table 3:
true	D14-1042.pdf#61.17	Senseval 2, F1	SimpleExtLesk ( DKPro ) F1  Table 3:
true	D14-1042.pdf#7.93	Senseval 2, F1	SimpleExtLesk ( DKPro ) F1 points  Table 3:
true	D14-1042.pdf#73.70	Senseval 2, F1	SimpleExtLesk ( DKPro ) P  Table 3:
true	D14-1042.pdf#52.28	Senseval 2, F1	SimpleExtLesk ( DKPro ) R  Table 3:
true	D14-1042.pdf#+3.90	Senseval 2, F1	SimpleExtLesk ( DKPro ) with MFS back - off F1 points  Table 3:
true	D14-1042.pdf#3.35	Senseval 2, F1	MFS F1 points  Table 3:
true	D14-1042.pdf#3.78	Senseval 2, F1	SimpleExtLesk ( DKPro ) with MFS back - off F1 points  Table 3:
true	D14-1042.pdf#10.85	Senseval 3, F1	F1 points  Table 2: Results on SemEval-2007 coarse-grained (verbs as clause heads)
true	D14-1042.pdf#78.52	Senseval 3, F1	Verbs ( clause heads ) F1  Table 2: Results on SemEval-2007 coarse-grained (verbs as clause heads)
true	D14-1042.pdf#81.18	Senseval 3, F1	Verbs ( clause heads ) F1  Table 2: Results on SemEval-2007 coarse-grained (verbs as clause heads)
true	D14-1042.pdf#3.46	Senseval 3, F1	MFS F1 points  Table 3:
true	D14-1042.pdf#7.85	Senseval 3, F1	SimpleExtLesk ( DKPro ) F1 points  Table 3:
true	D14-1042.pdf#61.17	Senseval 3, F1	SimpleExtLesk ( DKPro ) F1  Table 3:
true	D14-1042.pdf#7.93	Senseval 3, F1	SimpleExtLesk ( DKPro ) F1 points  Table 3:
true	D14-1042.pdf#73.70	Senseval 3, F1	SimpleExtLesk ( DKPro ) P  Table 3:
true	D14-1042.pdf#52.28	Senseval 3, F1	SimpleExtLesk ( DKPro ) R  Table 3:
true	D14-1042.pdf#+3.90	Senseval 3, F1	SimpleExtLesk ( DKPro ) with MFS back - off F1 points  Table 3:
true	D14-1042.pdf#3.35	Senseval 3, F1	MFS F1 points  Table 3:
true	D14-1042.pdf#3.78	Senseval 3, F1	SimpleExtLesk ( DKPro ) with MFS back - off F1 points  Table 3:
true	D14-1042.pdf#10.85	SemEval 2015, F1	F1 points  Table 2: Results on SemEval-2007 coarse-grained (verbs as clause heads)
true	D14-1042.pdf#78.52	SemEval 2015, F1	Verbs ( clause heads ) F1  Table 2: Results on SemEval-2007 coarse-grained (verbs as clause heads)
true	D14-1042.pdf#81.18	SemEval 2015, F1	Verbs ( clause heads ) F1  Table 2: Results on SemEval-2007 coarse-grained (verbs as clause heads)
true	D14-1042.pdf#3.46	SemEval 2015, F1	MFS F1 points  Table 3:
true	D14-1042.pdf#7.85	SemEval 2015, F1	SimpleExtLesk ( DKPro ) F1 points  Table 3:
true	D14-1042.pdf#61.17	SemEval 2015, F1	SimpleExtLesk ( DKPro ) F1  Table 3:
true	D14-1042.pdf#7.93	SemEval 2015, F1	SimpleExtLesk ( DKPro ) F1 points  Table 3:
true	D14-1042.pdf#73.70	SemEval 2015, F1	SimpleExtLesk ( DKPro ) P  Table 3:
true	D14-1042.pdf#52.28	SemEval 2015, F1	SimpleExtLesk ( DKPro ) R  Table 3:
true	D14-1042.pdf#+3.90	SemEval 2015, F1	SimpleExtLesk ( DKPro ) with MFS back - off F1 points  Table 3:
true	D14-1042.pdf#3.35	SemEval 2015, F1	MFS F1 points  Table 3:
true	D14-1042.pdf#3.78	SemEval 2015, F1	SimpleExtLesk ( DKPro ) with MFS back - off F1 points  Table 3:
true	D14-1131.pdf#3	WMT 2014 EN-FR, BLEU	Beam search 10  Table 3: Beam search and cube pruning search er- rors (out of 3,003 test samples) by beam size using  LMs of order 3 to 5 (d = 4).
true	D14-1131.pdf#4	WMT 2014 EN-FR, BLEU	Beam search 10 10  Table 3: Beam search and cube pruning search er- rors (out of 3,003 test samples) by beam size using  LMs of order 3 to 5 (d = 4).
true	D14-1131.pdf#2	WMT 2014 EN-FR, BLEU	Beam search  Table 3: Beam search and cube pruning search er- rors (out of 3,003 test samples) by beam size using  LMs of order 3 to 5 (d = 4).
true	D14-1131.pdf#3	WMT 2014 EN-DE, BLEU	Beam search 10  Table 3: Beam search and cube pruning search er- rors (out of 3,003 test samples) by beam size using  LMs of order 3 to 5 (d = 4).
true	D14-1131.pdf#4	WMT 2014 EN-DE, BLEU	Beam search 10 10  Table 3: Beam search and cube pruning search er- rors (out of 3,003 test samples) by beam size using  LMs of order 3 to 5 (d = 4).
true	D14-1131.pdf#2	WMT 2014 EN-DE, BLEU	Beam search  Table 3: Beam search and cube pruning search er- rors (out of 3,003 test samples) by beam size using  LMs of order 3 to 5 (d = 4).
true	D11-1035.pdf#0.4170	WMT 2014 EN-FR, BLEU	TESLA - M  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.6028	WMT 2014 EN-FR, BLEU	TER  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.4409	WMT 2014 EN-FR, BLEU	( a ) The French - English task TESLA - F TESLA - F  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.4070	WMT 2014 EN-FR, BLEU	( b ) The Spanish - English task TESLA - F TESLA - F TESLA - F  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.6329	WMT 2014 EN-FR, BLEU	( b ) The Spanish - English task TER TER TER  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.5667	WMT 2014 EN-FR, BLEU	( a ) The French - English task BLEU BLEU  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.4511	WMT 2014 EN-FR, BLEU	( a ) The French - English task TESLA - M TESLA - M  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.3784	WMT 2014 EN-FR, BLEU	( b ) The Spanish - English task TESLA - M TESLA - M TESLA - M  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.5239	WMT 2014 EN-FR, BLEU	BLEU  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.5725	WMT 2014 EN-FR, BLEU	( a ) The French - English task TER TER  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.4963	WMT 2014 EN-FR, BLEU	( b ) The Spanish - English task BLEU BLEU BLEU  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.4963	WMT 2014 EN-FR, BLEU	( b ) The Spanish - English task BLEU BLEU BLEU  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.4224	WMT 2014 EN-FR, BLEU	TESLA - F  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.01,exceptthe	WMT 2014 EN-FR, BLEU	- TESLA - F  Table 7: Pairwise system comparisons. Each cell shows the proportion of time the system tuned on A is  preferred over the system tuned on B, and the proportion of time the opposite happens. Notice that the upper  right half of each table is the mirror image of the lower left half.
true	D11-1035.pdf#0.01,except	WMT 2014 EN-FR, BLEU	- TESLA - F TESLA - F  Table 7: Pairwise system comparisons. Each cell shows the proportion of time the system tuned on A is  preferred over the system tuned on B, and the proportion of time the opposite happens. Notice that the upper  right half of each table is the mirror image of the lower left half.
true	D11-1035.pdf#0.01,except	WMT 2014 EN-FR, BLEU	- TESLA - F TESLA - F TESLA - F  Table 7: Pairwise system comparisons. Each cell shows the proportion of time the system tuned on A is  preferred over the system tuned on B, and the proportion of time the opposite happens. Notice that the upper  right half of each table is the mirror image of the lower left half.
true	D11-1035.pdf#0.4170	WMT 2014 EN-DE, BLEU	TESLA - M  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.6028	WMT 2014 EN-DE, BLEU	TER  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.4409	WMT 2014 EN-DE, BLEU	( a ) The French - English task TESLA - F TESLA - F  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.4070	WMT 2014 EN-DE, BLEU	( b ) The Spanish - English task TESLA - F TESLA - F TESLA - F  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.6329	WMT 2014 EN-DE, BLEU	( b ) The Spanish - English task TER TER TER  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.5667	WMT 2014 EN-DE, BLEU	( a ) The French - English task BLEU BLEU  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.4511	WMT 2014 EN-DE, BLEU	( a ) The French - English task TESLA - M TESLA - M  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.3784	WMT 2014 EN-DE, BLEU	( b ) The Spanish - English task TESLA - M TESLA - M TESLA - M  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.5239	WMT 2014 EN-DE, BLEU	BLEU  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.5725	WMT 2014 EN-DE, BLEU	( a ) The French - English task TER TER  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.4963	WMT 2014 EN-DE, BLEU	( b ) The Spanish - English task BLEU BLEU BLEU  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.4963	WMT 2014 EN-DE, BLEU	( b ) The Spanish - English task BLEU BLEU BLEU  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.4224	WMT 2014 EN-DE, BLEU	TESLA - F  Table 4: Automatic evaluation scores
true	D11-1035.pdf#0.01,exceptthe	WMT 2014 EN-DE, BLEU	- TESLA - F  Table 7: Pairwise system comparisons. Each cell shows the proportion of time the system tuned on A is  preferred over the system tuned on B, and the proportion of time the opposite happens. Notice that the upper  right half of each table is the mirror image of the lower left half.
true	D11-1035.pdf#0.01,except	WMT 2014 EN-DE, BLEU	- TESLA - F TESLA - F  Table 7: Pairwise system comparisons. Each cell shows the proportion of time the system tuned on A is  preferred over the system tuned on B, and the proportion of time the opposite happens. Notice that the upper  right half of each table is the mirror image of the lower left half.
true	D11-1035.pdf#0.01,except	WMT 2014 EN-DE, BLEU	- TESLA - F TESLA - F TESLA - F  Table 7: Pairwise system comparisons. Each cell shows the proportion of time the system tuned on A is  preferred over the system tuned on B, and the proportion of time the opposite happens. Notice that the upper  right half of each table is the mirror image of the lower left half.
true	P12-1052.pdf#90.15	Penn Treebank, UAS	AF  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#92.66	Penn Treebank, UAS	LR  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#91.51	Penn Treebank, UAS	LR  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#92.09	Penn Treebank, UAS	LF  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#94.00	Penn Treebank, UAS	LP  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#91.47	Penn Treebank, UAS	AF  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#92.67	Penn Treebank, UAS	LP  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#93.32	Penn Treebank, UAS	LF  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#87.21	Penn Treebank, UAS	Test data no hashing  Table 5: Reranked parser accuracy; labeled F-score using  gold POS tags, with and without dependency hashing
true	P12-1052.pdf#87.15	Penn Treebank, UAS	Test data hashing  Table 5: Reranked parser accuracy; labeled F-score using  gold POS tags, with and without dependency hashing
true	P12-1052.pdf#90.15	Penn Treebank, POS	AF  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#92.66	Penn Treebank, POS	LR  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#91.51	Penn Treebank, POS	LR  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#92.09	Penn Treebank, POS	LF  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#94.00	Penn Treebank, POS	LP  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#91.47	Penn Treebank, POS	AF  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#92.67	Penn Treebank, POS	LP  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#93.32	Penn Treebank, POS	LF  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#87.21	Penn Treebank, POS	Test data no hashing  Table 5: Reranked parser accuracy; labeled F-score using  gold POS tags, with and without dependency hashing
true	P12-1052.pdf#87.15	Penn Treebank, POS	Test data hashing  Table 5: Reranked parser accuracy; labeled F-score using  gold POS tags, with and without dependency hashing
true	P12-1052.pdf#90.15	Penn Treebank, LAS	AF  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#92.66	Penn Treebank, LAS	LR  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#91.51	Penn Treebank, LAS	LR  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#92.09	Penn Treebank, LAS	LF  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#94.00	Penn Treebank, LAS	LP  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#91.47	Penn Treebank, LAS	AF  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#92.67	Penn Treebank, LAS	LP  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#93.32	Penn Treebank, LAS	LF  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#87.21	Penn Treebank, LAS	Test data no hashing  Table 5: Reranked parser accuracy; labeled F-score using  gold POS tags, with and without dependency hashing
true	P12-1052.pdf#87.15	Penn Treebank, LAS	Test data hashing  Table 5: Reranked parser accuracy; labeled F-score using  gold POS tags, with and without dependency hashing
true	P12-1052.pdf#90.15	Penn Treebank, Accuracy	AF  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#92.66	Penn Treebank, Accuracy	LR  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#91.51	Penn Treebank, Accuracy	LR  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#92.09	Penn Treebank, Accuracy	LF  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#94.00	Penn Treebank, Accuracy	LP  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#91.47	Penn Treebank, Accuracy	AF  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#92.67	Penn Treebank, Accuracy	LP  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#93.32	Penn Treebank, Accuracy	LF  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#87.21	Penn Treebank, Accuracy	Test data no hashing  Table 5: Reranked parser accuracy; labeled F-score using  gold POS tags, with and without dependency hashing
true	P12-1052.pdf#87.15	Penn Treebank, Accuracy	Test data hashing  Table 5: Reranked parser accuracy; labeled F-score using  gold POS tags, with and without dependency hashing
true	P12-1052.pdf#90.15	CCGBank, Accuracy	AF  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#92.66	CCGBank, Accuracy	LR  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#91.51	CCGBank, Accuracy	LR  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#92.09	CCGBank, Accuracy	LF  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#94.00	CCGBank, Accuracy	LP  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#91.47	CCGBank, Accuracy	AF  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#92.67	CCGBank, Accuracy	LP  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#93.32	CCGBank, Accuracy	LF  Table 4: Oracle precision, recall, and F-score on gold and  auto POS tags for the C&C n-best parser. 
true	P12-1052.pdf#87.21	CCGBank, Accuracy	Test data no hashing  Table 5: Reranked parser accuracy; labeled F-score using  gold POS tags, with and without dependency hashing
true	P12-1052.pdf#87.15	CCGBank, Accuracy	Test data hashing  Table 5: Reranked parser accuracy; labeled F-score using  gold POS tags, with and without dependency hashing
true	P12-1029.pdf#0.3735‡	SemEval 2015, F1	9445 RB03 - - - -  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.3126‡	SemEval 2015, F1	9445 TERC8 - - - -  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.2762‡	SemEval 2015, F1	9445 TREC7 - - - -  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.4019	SemEval 2015, F1	RB04  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.3735‡	SemEval 2007, F1	9445 RB03 - - - -  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.3126‡	SemEval 2007, F1	9445 TERC8 - - - -  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.2762‡	SemEval 2007, F1	9445 TREC7 - - - -  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.4019	SemEval 2007, F1	RB04  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.3735‡	Text8, Bit per Character (BPC)	9445 RB03 - - - -  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.3126‡	Text8, Bit per Character (BPC)	9445 TERC8 - - - -  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.2762‡	Text8, Bit per Character (BPC)	9445 TREC7 - - - -  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.4019	Text8, Bit per Character (BPC)	RB04  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.3735‡	Text8, Number of params	9445 RB03 - - - -  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.3126‡	Text8, Number of params	9445 TERC8 - - - -  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.2762‡	Text8, Number of params	9445 TREC7 - - - -  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.4019	Text8, Number of params	RB04  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.3735‡	Senseval 3, F1	9445 RB03 - - - -  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.3126‡	Senseval 3, F1	9445 TERC8 - - - -  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.2762‡	Senseval 3, F1	9445 TREC7 - - - -  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.4019	Senseval 3, F1	RB04  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.3735‡	SemEval 2013, F1	9445 RB03 - - - -  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.3126‡	SemEval 2013, F1	9445 TERC8 - - - -  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.2762‡	SemEval 2013, F1	9445 TREC7 - - - -  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.4019	SemEval 2013, F1	RB04  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.3735‡	Senseval 2, F1	9445 RB03 - - - -  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.3126‡	Senseval 2, F1	9445 TERC8 - - - -  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.2762‡	Senseval 2, F1	9445 TREC7 - - - -  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	P12-1029.pdf#0.4019	Senseval 2, F1	RB04  Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the  next row shows the performance of the baseline method, and the rest rows are the results of our method with different  settings. Single dagger (  † ) and double dagger (  ‡ ) indicate statistically significant improvement over Stem prf at the  95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
true	D12-1078.pdf#50.58	Text8, Number of params	dev8+dialog  Table 5. MT performance of symmetrization  methods on IWSLT data set. The results in bold  type are significantly better than the performance  of IDG.
true	D12-1078.pdf#49.85	Text8, Number of params	dev9  Table 5. MT performance of symmetrization  methods on IWSLT data set. The results in bold  type are significantly better than the performance  of IDG.
true	D12-1078.pdf#37.07	Text8, Number of params	3 , 376K NIST ' 05  Table 6. MT performance of symmetrization  methods on NIST data. The results in bold type are  significantly better than the performance of IDG.
true	D12-1078.pdf#25.67	Text8, Number of params	3 , 376K NIST ' 08  Table 6. MT performance of symmetrization  methods on NIST data. The results in bold type are  significantly better than the performance of IDG.
true	D12-1078.pdf#25.30	Text8, Number of params	3 , 376K dev9 574K ( +1 . 41 ) ( +2 . 66 ) 591K ( +2 . 95 ) NIST ' 08  Table 3. MT performance of parser re-training  strategies on IWSLT data set. The results in  bold type are significantly better than the  baseline.
true	D12-1078.pdf#50.58	Text8, Number of params	dev8+dialog  Table 3. MT performance of parser re-training  strategies on IWSLT data set. The results in  bold type are significantly better than the  baseline.
true	D12-1078.pdf#24.87	Text8, Number of params	572K dev9 574K ( +1 . 41 ) ( +2 . 66 ) 591K ( +2 . 95 ) NIST ' 08  Table 3. MT performance of parser re-training  strategies on IWSLT data set. The results in  bold type are significantly better than the  baseline.
true	D12-1078.pdf#36.79	Text8, Number of params	3 , 376K dev8+dialog SST ( +1 . 46 ) ( +2 . 17 ) FA - PR ( +2 . 73 ) NIST ' 05  Table 3. MT performance of parser re-training  strategies on IWSLT data set. The results in  bold type are significantly better than the  baseline.
true	D12-1078.pdf#37.98	Text8, Number of params	3 , 376K dev8+dialog SST ( +1 . 46 ) ( +2 . 17 ) FA - PR ( +2 . 73 ) NIST ' 03  Table 3. MT performance of parser re-training  strategies on IWSLT data set. The results in  bold type are significantly better than the  baseline.
true	D12-1078.pdf#37.57	Text8, Number of params	572K dev8+dialog SST ( +1 . 46 ) ( +2 . 17 ) FA - PR ( +2 . 73 ) NIST ' 03  Table 3. MT performance of parser re-training  strategies on IWSLT data set. The results in  bold type are significantly better than the  baseline.
true	D12-1078.pdf#36.44	Text8, Number of params	572K dev8+dialog SST ( +1 . 46 ) ( +2 . 17 ) FA - PR ( +2 . 73 ) NIST ' 05  Table 3. MT performance of parser re-training  strategies on IWSLT data set. The results in  bold type are significantly better than the  baseline.
true	D12-1078.pdf#49.85	Text8, Number of params	dev9  Table 3. MT performance of parser re-training  strategies on IWSLT data set. The results in  bold type are significantly better than the  baseline.
true	D12-1078.pdf#50.58	Text8, Number of params	dev8+dialog  Table 4. MT performance of parser re-training  strategies on NIST data set. The results in bold  type are significantly better than the baseline.
true	D12-1078.pdf#36.44	Text8, Number of params	572K dev8+dialog SST ( +1 . 46 ) ( +2 . 17 ) FA - PR ( +2 . 73 ) NIST ' 05  Table 4. MT performance of parser re-training  strategies on NIST data set. The results in bold  type are significantly better than the baseline.
true	D12-1078.pdf#25.30	Text8, Number of params	3 , 376K dev9 574K ( +1 . 41 ) ( +2 . 66 ) 591K ( +2 . 95 ) NIST ' 08  Table 4. MT performance of parser re-training  strategies on NIST data set. The results in bold  type are significantly better than the baseline.
true	D12-1078.pdf#49.85	Text8, Number of params	dev9  Table 4. MT performance of parser re-training  strategies on NIST data set. The results in bold  type are significantly better than the baseline.
true	D12-1078.pdf#24.87	Text8, Number of params	572K dev9 574K ( +1 . 41 ) ( +2 . 66 ) 591K ( +2 . 95 ) NIST ' 08  Table 4. MT performance of parser re-training  strategies on NIST data set. The results in bold  type are significantly better than the baseline.
true	D12-1078.pdf#36.79	Text8, Number of params	3 , 376K dev8+dialog SST ( +1 . 46 ) ( +2 . 17 ) FA - PR ( +2 . 73 ) NIST ' 05  Table 4. MT performance of parser re-training  strategies on NIST data set. The results in bold  type are significantly better than the baseline.
true	D12-1078.pdf#37.57	Text8, Number of params	572K dev8+dialog SST ( +1 . 46 ) ( +2 . 17 ) FA - PR ( +2 . 73 ) NIST ' 03  Table 4. MT performance of parser re-training  strategies on NIST data set. The results in bold  type are significantly better than the baseline.
true	D12-1078.pdf#37.98	Text8, Number of params	3 , 376K dev8+dialog SST ( +1 . 46 ) ( +2 . 17 ) FA - PR ( +2 . 73 ) NIST ' 03  Table 4. MT performance of parser re-training  strategies on NIST data set. The results in bold  type are significantly better than the baseline.
true	D12-1078.pdf#50.58	Text8, Number of params	Rules 
true	D12-1078.pdf#49.85	Text8, Number of params	Rules 
true	D12-1078.pdf#36.44	Text8, Number of params	NIST ' 05  Table 8. MT performance of the new methods  on NIST data set. The results in bold type are  significantly better than the baseline.
true	D12-1078.pdf#37.57	Text8, Number of params	NIST ' 03  Table 8. MT performance of the new methods  on NIST data set. The results in bold type are  significantly better than the baseline.
true	D12-1078.pdf#24.87	Text8, Number of params	NIST ' 08  Table 8. MT performance of the new methods  on NIST data set. The results in bold type are  significantly better than the baseline.
true	D12-1078.pdf#38.15	Text8, Number of params	3 , 376K NIST ' 03  Table 8. MT performance of the new methods  on NIST data set. The results in bold type are  significantly better than the baseline.
true	D12-1078.pdf#50.58	Text8, Bit per Character (BPC)	dev8+dialog  Table 5. MT performance of symmetrization  methods on IWSLT data set. The results in bold  type are significantly better than the performance  of IDG.
true	D12-1078.pdf#49.85	Text8, Bit per Character (BPC)	dev9  Table 5. MT performance of symmetrization  methods on IWSLT data set. The results in bold  type are significantly better than the performance  of IDG.
true	D12-1078.pdf#37.07	Text8, Bit per Character (BPC)	3 , 376K NIST ' 05  Table 6. MT performance of symmetrization  methods on NIST data. The results in bold type are  significantly better than the performance of IDG.
true	D12-1078.pdf#25.67	Text8, Bit per Character (BPC)	3 , 376K NIST ' 08  Table 6. MT performance of symmetrization  methods on NIST data. The results in bold type are  significantly better than the performance of IDG.
true	D12-1078.pdf#25.30	Text8, Bit per Character (BPC)	3 , 376K dev9 574K ( +1 . 41 ) ( +2 . 66 ) 591K ( +2 . 95 ) NIST ' 08  Table 3. MT performance of parser re-training  strategies on IWSLT data set. The results in  bold type are significantly better than the  baseline.
true	D12-1078.pdf#50.58	Text8, Bit per Character (BPC)	dev8+dialog  Table 3. MT performance of parser re-training  strategies on IWSLT data set. The results in  bold type are significantly better than the  baseline.
true	D12-1078.pdf#24.87	Text8, Bit per Character (BPC)	572K dev9 574K ( +1 . 41 ) ( +2 . 66 ) 591K ( +2 . 95 ) NIST ' 08  Table 3. MT performance of parser re-training  strategies on IWSLT data set. The results in  bold type are significantly better than the  baseline.
true	D12-1078.pdf#36.79	Text8, Bit per Character (BPC)	3 , 376K dev8+dialog SST ( +1 . 46 ) ( +2 . 17 ) FA - PR ( +2 . 73 ) NIST ' 05  Table 3. MT performance of parser re-training  strategies on IWSLT data set. The results in  bold type are significantly better than the  baseline.
true	D12-1078.pdf#37.98	Text8, Bit per Character (BPC)	3 , 376K dev8+dialog SST ( +1 . 46 ) ( +2 . 17 ) FA - PR ( +2 . 73 ) NIST ' 03  Table 3. MT performance of parser re-training  strategies on IWSLT data set. The results in  bold type are significantly better than the  baseline.
true	D12-1078.pdf#37.57	Text8, Bit per Character (BPC)	572K dev8+dialog SST ( +1 . 46 ) ( +2 . 17 ) FA - PR ( +2 . 73 ) NIST ' 03  Table 3. MT performance of parser re-training  strategies on IWSLT data set. The results in  bold type are significantly better than the  baseline.
true	D12-1078.pdf#36.44	Text8, Bit per Character (BPC)	572K dev8+dialog SST ( +1 . 46 ) ( +2 . 17 ) FA - PR ( +2 . 73 ) NIST ' 05  Table 3. MT performance of parser re-training  strategies on IWSLT data set. The results in  bold type are significantly better than the  baseline.
true	D12-1078.pdf#49.85	Text8, Bit per Character (BPC)	dev9  Table 3. MT performance of parser re-training  strategies on IWSLT data set. The results in  bold type are significantly better than the  baseline.
true	D12-1078.pdf#50.58	Text8, Bit per Character (BPC)	dev8+dialog  Table 4. MT performance of parser re-training  strategies on NIST data set. The results in bold  type are significantly better than the baseline.
true	D12-1078.pdf#36.44	Text8, Bit per Character (BPC)	572K dev8+dialog SST ( +1 . 46 ) ( +2 . 17 ) FA - PR ( +2 . 73 ) NIST ' 05  Table 4. MT performance of parser re-training  strategies on NIST data set. The results in bold  type are significantly better than the baseline.
true	D12-1078.pdf#25.30	Text8, Bit per Character (BPC)	3 , 376K dev9 574K ( +1 . 41 ) ( +2 . 66 ) 591K ( +2 . 95 ) NIST ' 08  Table 4. MT performance of parser re-training  strategies on NIST data set. The results in bold  type are significantly better than the baseline.
true	D12-1078.pdf#49.85	Text8, Bit per Character (BPC)	dev9  Table 4. MT performance of parser re-training  strategies on NIST data set. The results in bold  type are significantly better than the baseline.
true	D12-1078.pdf#24.87	Text8, Bit per Character (BPC)	572K dev9 574K ( +1 . 41 ) ( +2 . 66 ) 591K ( +2 . 95 ) NIST ' 08  Table 4. MT performance of parser re-training  strategies on NIST data set. The results in bold  type are significantly better than the baseline.
true	D12-1078.pdf#36.79	Text8, Bit per Character (BPC)	3 , 376K dev8+dialog SST ( +1 . 46 ) ( +2 . 17 ) FA - PR ( +2 . 73 ) NIST ' 05  Table 4. MT performance of parser re-training  strategies on NIST data set. The results in bold  type are significantly better than the baseline.
true	D12-1078.pdf#37.57	Text8, Bit per Character (BPC)	572K dev8+dialog SST ( +1 . 46 ) ( +2 . 17 ) FA - PR ( +2 . 73 ) NIST ' 03  Table 4. MT performance of parser re-training  strategies on NIST data set. The results in bold  type are significantly better than the baseline.
true	D12-1078.pdf#37.98	Text8, Bit per Character (BPC)	3 , 376K dev8+dialog SST ( +1 . 46 ) ( +2 . 17 ) FA - PR ( +2 . 73 ) NIST ' 03  Table 4. MT performance of parser re-training  strategies on NIST data set. The results in bold  type are significantly better than the baseline.
true	D12-1078.pdf#50.58	Text8, Bit per Character (BPC)	Rules 
true	D12-1078.pdf#49.85	Text8, Bit per Character (BPC)	Rules 
true	D12-1078.pdf#36.44	Text8, Bit per Character (BPC)	NIST ' 05  Table 8. MT performance of the new methods  on NIST data set. The results in bold type are  significantly better than the baseline.
true	D12-1078.pdf#37.57	Text8, Bit per Character (BPC)	NIST ' 03  Table 8. MT performance of the new methods  on NIST data set. The results in bold type are  significantly better than the baseline.
true	D12-1078.pdf#24.87	Text8, Bit per Character (BPC)	NIST ' 08  Table 8. MT performance of the new methods  on NIST data set. The results in bold type are  significantly better than the baseline.
true	D12-1078.pdf#38.15	Text8, Bit per Character (BPC)	3 , 376K NIST ' 03  Table 8. MT performance of the new methods  on NIST data set. The results in bold type are  significantly better than the baseline.
true	P15-1095.pdf#62.2	LDC2014T12, F1 on Newswire	F 1  Table 3: Results on two AMR datasets for JAMR  and our NER++ embedded in the JAMR SRL++  component. Note that recall is consistently higher  across both datasets, with only a small loss in pre- cision.
true	P15-1095.pdf#62.3	LDC2014T12, F1 on Newswire	F 1  Table 3: Results on two AMR datasets for JAMR  and our NER++ embedded in the JAMR SRL++  component. Note that recall is consistently higher  across both datasets, with only a small loss in pre- cision.
true	P15-1095.pdf#59.0	LDC2014T12, F1 on Newswire	R  Table 3: Results on two AMR datasets for JAMR  and our NER++ embedded in the JAMR SRL++  component. Note that recall is consistently higher  across both datasets, with only a small loss in pre- cision.
true	P15-1095.pdf#67.1	LDC2014T12, F1 on Newswire	Table 3: Results on two AMR datasets for JAMR  and our NER++ embedded in the JAMR SRL++  component. Note that recall is consistently higher  across both datasets, with only a small loss in pre- cision.
true	P15-1095.pdf#58.3	LDC2014T12, F1 on Newswire	R  Table 3: Results on two AMR datasets for JAMR  and our NER++ embedded in the JAMR SRL++  component. Note that recall is consistently higher  across both datasets, with only a small loss in pre- cision.
true	P15-1095.pdf#66.9	LDC2014T12, F1 on Newswire	P  Table 3: Results on two AMR datasets for JAMR  and our NER++ embedded in the JAMR SRL++  component. Note that recall is consistently higher  across both datasets, with only a small loss in pre- cision.
true	P15-1095.pdf#62.2	LDC2014T12, F1 on Full	F 1  Table 3: Results on two AMR datasets for JAMR  and our NER++ embedded in the JAMR SRL++  component. Note that recall is consistently higher  across both datasets, with only a small loss in pre- cision.
true	P15-1095.pdf#62.3	LDC2014T12, F1 on Full	F 1  Table 3: Results on two AMR datasets for JAMR  and our NER++ embedded in the JAMR SRL++  component. Note that recall is consistently higher  across both datasets, with only a small loss in pre- cision.
true	P15-1095.pdf#59.0	LDC2014T12, F1 on Full	R  Table 3: Results on two AMR datasets for JAMR  and our NER++ embedded in the JAMR SRL++  component. Note that recall is consistently higher  across both datasets, with only a small loss in pre- cision.
true	P15-1095.pdf#67.1	LDC2014T12, F1 on Full	Table 3: Results on two AMR datasets for JAMR  and our NER++ embedded in the JAMR SRL++  component. Note that recall is consistently higher  across both datasets, with only a small loss in pre- cision.
true	P15-1095.pdf#58.3	LDC2014T12, F1 on Full	R  Table 3: Results on two AMR datasets for JAMR  and our NER++ embedded in the JAMR SRL++  component. Note that recall is consistently higher  across both datasets, with only a small loss in pre- cision.
true	P15-1095.pdf#66.9	LDC2014T12, F1 on Full	P  Table 3: Results on two AMR datasets for JAMR  and our NER++ embedded in the JAMR SRL++  component. Note that recall is consistently higher  across both datasets, with only a small loss in pre- cision.
true	P14-2122.pdf#0.807	WMT 2014 EN-DE, BLEU	Accuracy MSR 17 m  Table 3: Results on Monolingual Corpora.
true	P14-2122.pdf#0.774	WMT 2014 EN-DE, BLEU	Accuracy CHILD . 17 m - - - 3 s  Table 3: Results on Monolingual Corpora.
true	P14-2122.pdf#31.73±	WMT 2014 EN-DE, BLEU	BLEU PatentMT9  Table 4: Results on Bilingual Corpora.
true	P14-2122.pdf#31.68±	WMT 2014 EN-DE, BLEU	BLEU OpenMT06  Table 4: Results on Bilingual Corpora.
true	P14-2122.pdf#0.25	WMT 2014 EN-DE, BLEU	BLEU OpenMT06  Table 4: Results on Bilingual Corpora.
true	P14-2122.pdf#0.05	WMT 2014 EN-DE, BLEU	BLEU PatentMT9  Table 4: Results on Bilingual Corpora.
true	P14-2122.pdf#0.807	Chinese Treebank 6, F1	Accuracy MSR 17 m  Table 3: Results on Monolingual Corpora.
true	P14-2122.pdf#0.774	Chinese Treebank 6, F1	Accuracy CHILD . 17 m - - - 3 s  Table 3: Results on Monolingual Corpora.
true	P14-2122.pdf#31.73±	Chinese Treebank 6, F1	BLEU PatentMT9  Table 4: Results on Bilingual Corpora.
true	P14-2122.pdf#31.68±	Chinese Treebank 6, F1	BLEU OpenMT06  Table 4: Results on Bilingual Corpora.
true	P14-2122.pdf#0.25	Chinese Treebank 6, F1	BLEU OpenMT06  Table 4: Results on Bilingual Corpora.
true	P14-2122.pdf#0.05	Chinese Treebank 6, F1	BLEU PatentMT9  Table 4: Results on Bilingual Corpora.
true	P14-2122.pdf#0.807	WMT 2014 EN-FR, BLEU	Accuracy MSR 17 m  Table 3: Results on Monolingual Corpora.
true	P14-2122.pdf#0.774	WMT 2014 EN-FR, BLEU	Accuracy CHILD . 17 m - - - 3 s  Table 3: Results on Monolingual Corpora.
true	P14-2122.pdf#31.73±	WMT 2014 EN-FR, BLEU	BLEU PatentMT9  Table 4: Results on Bilingual Corpora.
true	P14-2122.pdf#31.68±	WMT 2014 EN-FR, BLEU	BLEU OpenMT06  Table 4: Results on Bilingual Corpora.
true	P14-2122.pdf#0.25	WMT 2014 EN-FR, BLEU	BLEU OpenMT06  Table 4: Results on Bilingual Corpora.
true	P14-2122.pdf#0.05	WMT 2014 EN-FR, BLEU	BLEU PatentMT9  Table 4: Results on Bilingual Corpora.
true	P14-1070.pdf#87.5	Penn Treebank, UAS	STRUCTURED REPRESENTATION LAS irreg  Table 3: Baseline and best results for the four MWE+parsing architectures on the dev set (see text for  statistical significance evaluation). The UAS for the structured representation is the same as the one for  the labeled representation, and is not repeated.
true	P14-1070.pdf#76.5	Penn Treebank, UAS	FLAT REPRESENTATION FTM irreg  Table 3: Baseline and best results for the four MWE+parsing architectures on the dev set (see text for  statistical significance evaluation). The UAS for the structured representation is the same as the one for  the labeled representation, and is not repeated.
true	P14-1070.pdf#85.2	Penn Treebank, UAS	FLAT REPRESENTATION LAS irreg  Table 3: Baseline and best results for the four MWE+parsing architectures on the dev set (see text for  statistical significance evaluation). The UAS for the structured representation is the same as the one for  the labeled representation, and is not repeated.
true	P14-1070.pdf#85.3	Penn Treebank, UAS	LABELED REPRES . LAS reg  Table 3: Baseline and best results for the four MWE+parsing architectures on the dev set (see text for  statistical significance evaluation). The UAS for the structured representation is the same as the one for  the labeled representation, and is not repeated.
true	P14-1070.pdf#88.8	Penn Treebank, UAS	FLAT REPRESENTATION UAS irreg  Table 3: Baseline and best results for the four MWE+parsing architectures on the dev set (see text for  statistical significance evaluation). The UAS for the structured representation is the same as the one for  the labeled representation, and is not repeated.
true	P14-1070.pdf#79.0	Penn Treebank, UAS	FLAT REPRESENTATION FUM irreg  Table 3: Baseline and best results for the four MWE+parsing architectures on the dev set (see text for  statistical significance evaluation). The UAS for the structured representation is the same as the one for  the labeled representation, and is not repeated.
true	P14-1070.pdf#82.6	Penn Treebank, UAS	STRUCTURED REPRESENTATION FTM irreg  Table 3: Baseline and best results for the four MWE+parsing architectures on the dev set (see text for  statistical significance evaluation). The UAS for the structured representation is the same as the one for  the labeled representation, and is not repeated.
true	P14-1070.pdf#89.8	Penn Treebank, UAS	LABELED REPRES . UAS reg  Table 3: Baseline and best results for the four MWE+parsing architectures on the dev set (see text for  statistical significance evaluation). The UAS for the structured representation is the same as the one for  the labeled representation, and is not repeated.
true	P14-1070.pdf#85.4	Penn Treebank, UAS	STRUCTURED REPRESENTATION FUM irreg  Table 3: Baseline and best results for the four MWE+parsing architectures on the dev set (see text for  statistical significance evaluation). The UAS for the structured representation is the same as the one for  the labeled representation, and is not repeated.
true	P14-1070.pdf#88.30	Penn Treebank, UAS	DEV UAS  Table 5: Comparison on dev set of our best archi- tecture with reg/irregular MWE distinction (first  row), with the single-parser architectures of (Con- stant et al., 2013) (Const13) and (Björkelund et  al., 2013) (Bjork13). Uniform joint is our reimple- mentation of Const13 joint, enhanced with mwe- features and switch.
true	P14-1070.pdf#88.81	Penn Treebank, UAS	DEV UAS  Table 5: Comparison on dev set of our best archi- tecture with reg/irregular MWE distinction (first  row), with the single-parser architectures of (Con- stant et al., 2013) (Const13) and (Björkelund et  al., 2013) (Bjork13). Uniform joint is our reimple- mentation of Const13 joint, enhanced with mwe- features and switch.
true	P14-1070.pdf#84.91	Penn Treebank, UAS	TEST LAS  Table 5: Comparison on dev set of our best archi- tecture with reg/irregular MWE distinction (first  row), with the single-parser architectures of (Con- stant et al., 2013) (Const13) and (Björkelund et  al., 2013) (Bjork13). Uniform joint is our reimple- mentation of Const13 joint, enhanced with mwe- features and switch.
true	P14-1070.pdf#88.35	Penn Treebank, UAS	TEST UAS  Table 5: Comparison on dev set of our best archi- tecture with reg/irregular MWE distinction (first  row), with the single-parser architectures of (Con- stant et al., 2013) (Const13) and (Björkelund et  al., 2013) (Bjork13). Uniform joint is our reimple- mentation of Const13 joint, enhanced with mwe- features and switch.
true	P14-1070.pdf#84.37	Penn Treebank, UAS	TEST LAS  Table 5: Comparison on dev set of our best archi- tecture with reg/irregular MWE distinction (first  row), with the single-parser architectures of (Con- stant et al., 2013) (Const13) and (Björkelund et  al., 2013) (Bjork13). Uniform joint is our reimple- mentation of Const13 joint, enhanced with mwe- features and switch.
true	P14-1070.pdf#84.84	Penn Treebank, UAS	DEV LAS  Table 5: Comparison on dev set of our best archi- tecture with reg/irregular MWE distinction (first  row), with the single-parser architectures of (Con- stant et al., 2013) (Const13) and (Björkelund et  al., 2013) (Bjork13). Uniform joint is our reimple- mentation of Const13 joint, enhanced with mwe- features and switch.
true	P14-1070.pdf#87.87	Penn Treebank, UAS	TEST UAS  Table 5: Comparison on dev set of our best archi- tecture with reg/irregular MWE distinction (first  row), with the single-parser architectures of (Con- stant et al., 2013) (Const13) and (Björkelund et  al., 2013) (Bjork13). Uniform joint is our reimple- mentation of Const13 joint, enhanced with mwe- features and switch.
true	P14-1070.pdf#85.42	Penn Treebank, UAS	DEV LAS  Table 5: Comparison on dev set of our best archi- tecture with reg/irregular MWE distinction (first  row), with the single-parser architectures of (Con- stant et al., 2013) (Const13) and (Björkelund et  al., 2013) (Bjork13). Uniform joint is our reimple- mentation of Const13 joint, enhanced with mwe- features and switch.
true	P15-1122.pdf#78.84	New York Times Corpus, P@10%	F%  Table 5: Performance of NI Detection
true	P15-1122.pdf#72.71	New York Times Corpus, P@10%	R%  Table 5: Performance of NI Detection
true	P15-1122.pdf#69.93	New York Times Corpus, P@10%	R%  Table 6: Performance of DNI Identification
true	P15-1122.pdf#68.88	New York Times Corpus, P@10%	F%  Table 6: Performance of DNI Identification
true	P15-1122.pdf#67.86	New York Times Corpus, P@10%	R%  Table 6: Performance of DNI Identification
true	P15-1122.pdf#39.75	New York Times Corpus, P@10%	Win3 F%  Table 7: Results on golden DNI
true	P15-1122.pdf#37.07	New York Times Corpus, P@10%	Win4 F%  Table 7: Results on golden DNI
true	P15-1122.pdf#39.06	New York Times Corpus, P@10%	Win2 F%  Table 7: Results on golden DNI
true	P15-1122.pdf#40.53	New York Times Corpus, P@10%	NI resolution by about 11% in terms of F - score . , it shows that the errors caused detection P%  Table 8: Performance of NI resolution for our models
true	P15-1122.pdf#28.13	New York Times Corpus, P@10%	NI resolution by about 11% in terms of F - score . , it shows that the errors caused automatic F%  Table 8: Performance of NI resolution for our models
true	P15-1122.pdf#21.54	New York Times Corpus, P@10%	NI resolution by about 11% in terms of F - score . , it shows that the errors caused and R%  Table 8: Performance of NI resolution for our models
true	D14-1107.pdf#493	CCGBank, Accuracy	99 Speed ( sentences / second ) Parser 52 58  Table 3: Effect of our optimizations of parsing  speed.
true	D14-1107.pdf#343	CCGBank, Accuracy	Speed ( sentences / second ) Tagger  Table 3: Effect of our optimizations of parsing  speed.
true	D14-1107.pdf#186	CCGBank, Accuracy	99 Speed ( sentences / second ) Total 45 49  Table 3: Effect of our optimizations of parsing  speed.
true	D10-1071.pdf#9.4	Senseval 2, F1	E1  Table 5: Percent error rates of alternative approaches.  Note: The results reported are 10 fold cross validation  test accuracies and no parameters have been tuned on  them. We used same train-test splits for all the datasets.
true	D10-1071.pdf#3.8	Senseval 2, F1	E2  Table 5: Percent error rates of alternative approaches.  Note: The results reported are 10 fold cross validation  test accuracies and no parameters have been tuned on  them. We used same train-test splits for all the datasets.
true	D10-1071.pdf#9.4	SemEval 2015, F1	E1  Table 5: Percent error rates of alternative approaches.  Note: The results reported are 10 fold cross validation  test accuracies and no parameters have been tuned on  them. We used same train-test splits for all the datasets.
true	D10-1071.pdf#3.8	SemEval 2015, F1	E2  Table 5: Percent error rates of alternative approaches.  Note: The results reported are 10 fold cross validation  test accuracies and no parameters have been tuned on  them. We used same train-test splits for all the datasets.
true	D10-1071.pdf#9.4	Senseval 3, F1	E1  Table 5: Percent error rates of alternative approaches.  Note: The results reported are 10 fold cross validation  test accuracies and no parameters have been tuned on  them. We used same train-test splits for all the datasets.
true	D10-1071.pdf#3.8	Senseval 3, F1	E2  Table 5: Percent error rates of alternative approaches.  Note: The results reported are 10 fold cross validation  test accuracies and no parameters have been tuned on  them. We used same train-test splits for all the datasets.
true	D10-1071.pdf#9.4	SemEval 2013, F1	E1  Table 5: Percent error rates of alternative approaches.  Note: The results reported are 10 fold cross validation  test accuracies and no parameters have been tuned on  them. We used same train-test splits for all the datasets.
true	D10-1071.pdf#3.8	SemEval 2013, F1	E2  Table 5: Percent error rates of alternative approaches.  Note: The results reported are 10 fold cross validation  test accuracies and no parameters have been tuned on  them. We used same train-test splits for all the datasets.
true	D10-1071.pdf#9.4	SemEval 2007, F1	E1  Table 5: Percent error rates of alternative approaches.  Note: The results reported are 10 fold cross validation  test accuracies and no parameters have been tuned on  them. We used same train-test splits for all the datasets.
true	D10-1071.pdf#3.8	SemEval 2007, F1	E2  Table 5: Percent error rates of alternative approaches.  Note: The results reported are 10 fold cross validation  test accuracies and no parameters have been tuned on  them. We used same train-test splits for all the datasets.
true	P11-3009.pdf#59.6%	Senseval 2, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#72.6%	Senseval 2, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#65.1%	Senseval 2, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#58.4%	Senseval 2, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#75.3%	Senseval 2, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#59.6%	SemEval 2013, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#72.6%	SemEval 2013, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#65.1%	SemEval 2013, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#58.4%	SemEval 2013, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#75.3%	SemEval 2013, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#59.6%	Senseval 3, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#72.6%	Senseval 3, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#65.1%	Senseval 3, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#58.4%	Senseval 3, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#75.3%	Senseval 3, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#59.6%	SemEval 2007, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#72.6%	SemEval 2007, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#65.1%	SemEval 2007, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#58.4%	SemEval 2007, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#75.3%	SemEval 2007, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#59.6%	SemEval 2015, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#72.6%	SemEval 2015, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#65.1%	SemEval 2015, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#58.4%	SemEval 2015, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-3009.pdf#75.3%	SemEval 2015, F1	Accuracy  Table 3: Disambiguation of temporal-contrastive connectives.
true	P11-1057.pdf#43.75%	Senseval 2, F1	1000 2250 2250+2250 )  Table 6: Reduction in annotation cost achieved using Bilingual Bootstrapping
true	P11-1057.pdf#33.33%	Senseval 2, F1	Table 6: Reduction in annotation cost achieved using Bilingual Bootstrapping
true	P11-1014.pdf#87.70	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	K  Table 3: Cross-domain sentiment classification accuracy.
true	P11-1014.pdf#85.18	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	K  Table 3: Cross-domain sentiment classification accuracy.
true	P11-1014.pdf#83.63	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	E  Table 3: Cross-domain sentiment classification accuracy.
true	P11-1014.pdf#78.77	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	D  Table 3: Cross-domain sentiment classification accuracy.
true	P11-1014.pdf#77.73	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	B  Table 3: Cross-domain sentiment classification accuracy.
true	P11-1014.pdf#84.40	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	E  Table 3: Cross-domain sentiment classification accuracy.
true	P11-1014.pdf#82.40	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	D  Table 3: Cross-domain sentiment classification accuracy.
true	P11-1014.pdf#80.40	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	B  Table 3: Cross-domain sentiment classification accuracy.
true	D15-1165.pdf#37.4	WMT 2014 EN-FR, BLEU	eval11  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#31.6	WMT 2014 EN-FR, BLEU	test  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#37.7	WMT 2014 EN-FR, BLEU	eval11  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#35.5	WMT 2014 EN-FR, BLEU	dev  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#33.0	WMT 2014 EN-FR, BLEU	test  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#32.2	WMT 2014 EN-FR, BLEU	test  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#34.2	WMT 2014 EN-FR, BLEU	dev  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#32.2	WMT 2014 EN-FR, BLEU	test  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#33.0	WMT 2014 EN-FR, BLEU	test  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#34.6	WMT 2014 EN-FR, BLEU	dev  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#35.0	WMT 2014 EN-FR, BLEU	dev  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#37.1	WMT 2014 EN-FR, BLEU	eval11  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#31.6	WMT 2014 EN-FR, BLEU	test  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#32.8	WMT 2014 EN-FR, BLEU	test  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#36.6	WMT 2014 EN-FR, BLEU	eval11  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#18.8	WMT 2014 EN-FR, BLEU	test1  Table 4: Results measured in BLEU for the BOLT  Chinese→English task.
true	D15-1165.pdf#18.8	WMT 2014 EN-FR, BLEU	test1  Table 4: Results measured in BLEU for the BOLT  Chinese→English task.
true	D15-1165.pdf#17.6	WMT 2014 EN-FR, BLEU	test2  Table 4: Results measured in BLEU for the BOLT  Chinese→English task.
true	D15-1165.pdf#19.3	WMT 2014 EN-FR, BLEU	test1  Table 4: Results measured in BLEU for the BOLT  Chinese→English task.
true	D15-1165.pdf#17.6	WMT 2014 EN-FR, BLEU	test2  Table 4: Results measured in BLEU for the BOLT  Chinese→English task.
true	D15-1165.pdf#18.4	WMT 2014 EN-FR, BLEU	test2  Table 4: Results measured in BLEU for the BOLT  Chinese→English task.
true	D15-1165.pdf#17.4	WMT 2014 EN-FR, BLEU	test2  Table 4: Results measured in BLEU for the BOLT  Chinese→English task.
true	D15-1165.pdf#18.8	WMT 2014 EN-FR, BLEU	test1  Table 4: Results measured in BLEU for the BOLT  Chinese→English task.
true	D15-1165.pdf#28.6	WMT 2014 EN-FR, BLEU	newstest 2013  Table 5: Results measured in BLEU for the WMT  German→English task.
true	D15-1165.pdf#28.1	WMT 2014 EN-FR, BLEU	newstest 2013  Table 5: Results measured in BLEU for the WMT  German→English task.
true	D15-1165.pdf#29.4	WMT 2014 EN-FR, BLEU	newstest 2015  Table 5: Results measured in BLEU for the WMT  German→English task.
true	D15-1165.pdf#29.9	WMT 2014 EN-FR, BLEU	newstest 2015  Table 5: Results measured in BLEU for the WMT  German→English task.
true	D15-1165.pdf#28.7	WMT 2014 EN-FR, BLEU	newstest 2013  Table 5: Results measured in BLEU for the WMT  German→English task.
true	D15-1165.pdf#29.7	WMT 2014 EN-FR, BLEU	newstest 2015  Table 5: Results measured in BLEU for the WMT  German→English task.
true	D15-1165.pdf#28.6	WMT 2014 EN-FR, BLEU	newstest 2014  Table 5: Results measured in BLEU for the WMT  German→English task.
true	D15-1165.pdf#37.4	WMT 2014 EN-DE, BLEU	eval11  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#31.6	WMT 2014 EN-DE, BLEU	test  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#37.7	WMT 2014 EN-DE, BLEU	eval11  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#35.5	WMT 2014 EN-DE, BLEU	dev  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#33.0	WMT 2014 EN-DE, BLEU	test  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#32.2	WMT 2014 EN-DE, BLEU	test  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#34.2	WMT 2014 EN-DE, BLEU	dev  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#32.2	WMT 2014 EN-DE, BLEU	test  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#33.0	WMT 2014 EN-DE, BLEU	test  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#34.6	WMT 2014 EN-DE, BLEU	dev  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#35.0	WMT 2014 EN-DE, BLEU	dev  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#37.1	WMT 2014 EN-DE, BLEU	eval11  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#31.6	WMT 2014 EN-DE, BLEU	test  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#32.8	WMT 2014 EN-DE, BLEU	test  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#36.6	WMT 2014 EN-DE, BLEU	eval11  Table 3: Results measured in BLEU for the IWSLT  German→English task.
true	D15-1165.pdf#18.8	WMT 2014 EN-DE, BLEU	test1  Table 4: Results measured in BLEU for the BOLT  Chinese→English task.
true	D15-1165.pdf#18.8	WMT 2014 EN-DE, BLEU	test1  Table 4: Results measured in BLEU for the BOLT  Chinese→English task.
true	D15-1165.pdf#17.6	WMT 2014 EN-DE, BLEU	test2  Table 4: Results measured in BLEU for the BOLT  Chinese→English task.
true	D15-1165.pdf#19.3	WMT 2014 EN-DE, BLEU	test1  Table 4: Results measured in BLEU for the BOLT  Chinese→English task.
true	D15-1165.pdf#17.6	WMT 2014 EN-DE, BLEU	test2  Table 4: Results measured in BLEU for the BOLT  Chinese→English task.
true	D15-1165.pdf#18.4	WMT 2014 EN-DE, BLEU	test2  Table 4: Results measured in BLEU for the BOLT  Chinese→English task.
true	D15-1165.pdf#17.4	WMT 2014 EN-DE, BLEU	test2  Table 4: Results measured in BLEU for the BOLT  Chinese→English task.
true	D15-1165.pdf#18.8	WMT 2014 EN-DE, BLEU	test1  Table 4: Results measured in BLEU for the BOLT  Chinese→English task.
true	D15-1165.pdf#28.6	WMT 2014 EN-DE, BLEU	newstest 2013  Table 5: Results measured in BLEU for the WMT  German→English task.
true	D15-1165.pdf#28.1	WMT 2014 EN-DE, BLEU	newstest 2013  Table 5: Results measured in BLEU for the WMT  German→English task.
true	D15-1165.pdf#29.4	WMT 2014 EN-DE, BLEU	newstest 2015  Table 5: Results measured in BLEU for the WMT  German→English task.
true	D15-1165.pdf#29.9	WMT 2014 EN-DE, BLEU	newstest 2015  Table 5: Results measured in BLEU for the WMT  German→English task.
true	D15-1165.pdf#28.7	WMT 2014 EN-DE, BLEU	newstest 2013  Table 5: Results measured in BLEU for the WMT  German→English task.
true	D15-1165.pdf#29.7	WMT 2014 EN-DE, BLEU	newstest 2015  Table 5: Results measured in BLEU for the WMT  German→English task.
true	D15-1165.pdf#28.6	WMT 2014 EN-DE, BLEU	newstest 2014  Table 5: Results measured in BLEU for the WMT  German→English task.
true	P10-1132.pdf#0.636	VLSP 2013 POS tagging shared task, Accuracy	100 Fine k=13 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#0.663	VLSP 2013 POS tagging shared task, Accuracy	100 Fine k=34 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#59.1	VLSP 2013 POS tagging shared task, Accuracy	60 Fine k=13 Prototype % k=17 Clark µ  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#0.652	VLSP 2013 POS tagging shared task, Accuracy	100 Fine k=13 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#0.795	VLSP 2013 POS tagging shared task, Accuracy	100 Fine k=13 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#53.7	VLSP 2013 POS tagging shared task, Accuracy	60 Fine k=13 Clark σ k=17 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#54.9	VLSP 2013 POS tagging shared task, Accuracy	100 Fine k=13 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#71.6	VLSP 2013 POS tagging shared task, Accuracy	Fine k=34 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#0.590	VLSP 2013 POS tagging shared task, Accuracy	100 Coarse k=13 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#0.819	VLSP 2013 POS tagging shared task, Accuracy	100 Fine k=13 Clark σ k=17 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#53.7	VLSP 2013 POS tagging shared task, Accuracy	100 Coarse k=13 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#63.5	VLSP 2013 POS tagging shared task, Accuracy	90 Fine k=34 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#64.7	VLSP 2013 POS tagging shared task, Accuracy	98 Fine k=13 Prototype % k=17 Clark µ  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#61.0	VLSP 2013 POS tagging shared task, Accuracy	Fine k=13 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#60.0	VLSP 2013 POS tagging shared task, Accuracy	90 Fine k=13 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#0.785	VLSP 2013 POS tagging shared task, Accuracy	100 Coarse k=13 Clark σ k=26 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#66.1	VLSP 2013 POS tagging shared task, Accuracy	90 Coarse k=13 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#58.8	VLSP 2013 POS tagging shared task, Accuracy	100 Fine k=34 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#0.553	VLSP 2013 POS tagging shared task, Accuracy	100 Fine k=13 Prototype % k=17 Clark µ  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#67.5	VLSP 2013 POS tagging shared task, Accuracy	90 Fine k=34 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#50.7	VLSP 2013 POS tagging shared task, Accuracy	99 Coarse k=13 Clark σ k=26 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#0.675	VLSP 2013 POS tagging shared task, Accuracy	100 Coarse k=13 Clark σ k=26 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#0.677	VLSP 2013 POS tagging shared task, Accuracy	100 Fine k=34 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#48.0	VLSP 2013 POS tagging shared task, Accuracy	99 Fine k=13 Clark σ k=17 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#0.596	VLSP 2013 POS tagging shared task, Accuracy	100 Coarse k=13 Clark σ k=26 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#63.2	VLSP 2013 POS tagging shared task, Accuracy	60 Coarse k=13 Clark σ k=26 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#58.1	VLSP 2013 POS tagging shared task, Accuracy	90 Coarse k=13 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#0.809	VLSP 2013 POS tagging shared task, Accuracy	100 Fine k=34 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#0.841	VLSP 2013 POS tagging shared task, Accuracy	100 Coarse k=13 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#68.2	VLSP 2013 POS tagging shared task, Accuracy	98 Coarse k=13 Clark σ k=26 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#0.646	VLSP 2013 POS tagging shared task, Accuracy	100 Fine k=13 Clark σ k=17 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#55.5	VLSP 2013 POS tagging shared task, Accuracy	90 Fine k=13 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#0.667	VLSP 2013 POS tagging shared task, Accuracy	99 Fine k=13 Clark σ k=17 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#70.0	VLSP 2013 POS tagging shared task, Accuracy	Coarse k=13 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#0.608	VLSP 2013 POS tagging shared task, Accuracy	100 Fine k=34 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#56.0	VLSP 2013 POS tagging shared task, Accuracy	60 Coarse k=13 Clark σ k=26 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#0.542	VLSP 2013 POS tagging shared task, Accuracy	100 Fine k=13 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#1.052	VLSP 2013 POS tagging shared task, Accuracy	100 Coarse k=13 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#0.484	VLSP 2013 POS tagging shared task, Accuracy	100 Coarse k=13 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#0.640	VLSP 2013 POS tagging shared task, Accuracy	99 Coarse k=13 Clark σ k=26 Prototype Tagger  Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark's average score (µ),
true	P10-1132.pdf#63.5	VLSP 2013 POS tagging shared task, Accuracy	B+M 1 - to - 1  Table 2: A comparison of partial versions of the model in
true	P10-1132.pdf#71.6	VLSP 2013 POS tagging shared task, Accuracy	B M - to - 1  Table 2: A comparison of partial versions of the model in
true	P10-1132.pdf#0.636	VLSP 2013 POS tagging shared task, Accuracy	F  Table 2: A comparison of partial versions of the model in
true	P10-1132.pdf#2.00	VLSP 2013 POS tagging shared task, Accuracy	F VI  Table 2: A comparison of partial versions of the model in
true	P10-1132.pdf#61.0	VLSP 2013 POS tagging shared task, Accuracy	F  Table 2: A comparison of partial versions of the model in
true	P10-1132.pdf#0.677	VLSP 2013 POS tagging shared task, Accuracy	F ( I=1 ) V  Table 2: A comparison of partial versions of the model in
true	P10-1132.pdf#0.652	VLSP 2013 POS tagging shared task, Accuracy	F  Table 2: A comparison of partial versions of the model in
true	P10-1132.pdf#60.0	VLSP 2013 POS tagging shared task, Accuracy	F  Table 2: A comparison of partial versions of the model in
true	P10-1132.pdf#0.809	VLSP 2013 POS tagging shared task, Accuracy	Excluding Punctuation NVI  Table 3: Comparison between the iHMM: PY-fixed model (Van Gael et al., 2009) and ours with various punctuation assign-
true	P10-1132.pdf#67.5	VLSP 2013 POS tagging shared task, Accuracy	Excluding Punctuation M - to - 1  Table 3: Comparison between the iHMM: PY-fixed model (Van Gael et al., 2009) and ours with various punctuation assign-
true	P10-1132.pdf#0.677	VLSP 2013 POS tagging shared task, Accuracy	Including Punctuation V  Table 3: Comparison between the iHMM: PY-fixed model (Van Gael et al., 2009) and ours with various punctuation assign-
true	P10-1132.pdf#71.6	VLSP 2013 POS tagging shared task, Accuracy	Perfect Punctuation M - to - 1  Table 3: Comparison between the iHMM: PY-fixed model (Van Gael et al., 2009) and ours with various punctuation assign-
true	P10-1132.pdf#71.6	VLSP 2013 POS tagging shared task, Accuracy	Including Punctuation M - to - 1  Table 3: Comparison between the iHMM: PY-fixed model (Van Gael et al., 2009) and ours with various punctuation assign-
true	P10-1132.pdf#63.5	VLSP 2013 POS tagging shared task, Accuracy	Including Punctuation 1 - to - 1  Table 3: Comparison between the iHMM: PY-fixed model (Van Gael et al., 2009) and ours with various punctuation assign-
true	P10-1132.pdf#0.679	VLSP 2013 POS tagging shared task, Accuracy	Perfect Punctuation V  Table 3: Comparison between the iHMM: PY-fixed model (Van Gael et al., 2009) and ours with various punctuation assign-
true	P10-1132.pdf#0.663	VLSP 2013 POS tagging shared task, Accuracy	Including Punctuation NVI  Table 3: Comparison between the iHMM: PY-fixed model (Van Gael et al., 2009) and ours with various punctuation assign-
true	P10-1132.pdf#0.659	VLSP 2013 POS tagging shared task, Accuracy	Perfect Punctuation NVI  Table 3: Comparison between the iHMM: PY-fixed model (Van Gael et al., 2009) and ours with various punctuation assign-
true	P10-1132.pdf#58.8	VLSP 2013 POS tagging shared task, Accuracy	Excluding Punctuation 1 - to - 1  Table 3: Comparison between the iHMM: PY-fixed model (Van Gael et al., 2009) and ours with various punctuation assign-
true	P10-1132.pdf#0.608	VLSP 2013 POS tagging shared task, Accuracy	Excluding Punctuation V  Table 3: Comparison between the iHMM: PY-fixed model (Van Gael et al., 2009) and ours with various punctuation assign-
true	P10-1132.pdf#63.9	VLSP 2013 POS tagging shared task, Accuracy	Perfect Punctuation 1 - to - 1  Table 3: Comparison between the iHMM: PY-fixed model (Van Gael et al., 2009) and ours with various punctuation assign-
true	N10-1085.pdf#77.6%	Penn Treebank, Number of params	Type Recall  Table 5: Cross-domain generalization.
true	N10-1085.pdf#94.4%	Penn Treebank, Number of params	Testing Corpus Token Precision  Table 5: Cross-domain generalization.
true	N10-1085.pdf#77.9%	Penn Treebank, Number of params	Token Accuracy  Table 5: Cross-domain generalization.
true	N10-1085.pdf#77.2%	Penn Treebank, Number of params	Token Accuracy  Table 5: Cross-domain generalization.
true	N10-1085.pdf#87.0%	Penn Treebank, Number of params	Type Precision  Table 5: Cross-domain generalization.
true	N10-1085.pdf#86.7%	Penn Treebank, Number of params	Type Recall Token F - measure  Table 5: Cross-domain generalization.
true	N10-1085.pdf#77.9%	Penn Treebank, Number of params	Type F - measure  Table 5: Cross-domain generalization.
true	N10-1085.pdf#76.5%	Penn Treebank, Number of params	Type F - measure  Table 5: Cross-domain generalization.
true	N10-1085.pdf#86.7%(0.9)	Penn Treebank, Number of params	Training Corpora Token Accuracy  Table 5: Cross-domain generalization.
true	N10-1085.pdf#76.4%	Penn Treebank, Number of params	Type Recall  Table 5: Cross-domain generalization.
true	N10-1085.pdf#86.7%(0.9)	Penn Treebank, Number of params	Type Precision Token Recall  Table 5: Cross-domain generalization.
true	N10-1085.pdf#82.4%	Penn Treebank, Number of params	Type Precision  Table 5: Cross-domain generalization.
true	N10-1085.pdf#77.3%	Penn Treebank, Number of params	Type Recall  Table 5: Cross-domain generalization.
true	N10-1085.pdf#77.4%	Penn Treebank, Number of params	Type F - measure  Table 5: Cross-domain generalization.
true	N10-1085.pdf#76.4%	Penn Treebank, Number of params	Token Accuracy  Table 5: Cross-domain generalization.
true	N10-1085.pdf#94.2%	Penn Treebank, Number of params	Type Precision  Table 5: Cross-domain generalization.
true	N10-1085.pdf#86.7%(0.9)	Penn Treebank, Number of params	Token Accuracy  Table 6: Full NP ordering accuracies; averages and standard deviations over a 10-fold cross validation. To  compare directly with Mitchell
true	N10-1085.pdf#86.7%	Penn Treebank, Number of params	Token F - measure  Table 6: Full NP ordering accuracies; averages and standard deviations over a 10-fold cross validation. To  compare directly with Mitchell
true	N10-1085.pdf#94.4%	Penn Treebank, Number of params	Token Precision  Table 6: Full NP ordering accuracies; averages and standard deviations over a 10-fold cross validation. To  compare directly with Mitchell
true	N10-1085.pdf#86.7%(0.9)	Penn Treebank, Number of params	Token Recall  Table 6: Full NP ordering accuracies; averages and standard deviations over a 10-fold cross validation. To  compare directly with Mitchell
true	N10-1085.pdf#77.6%	Penn Treebank, Test perplexity	Type Recall  Table 5: Cross-domain generalization.
true	N10-1085.pdf#94.4%	Penn Treebank, Test perplexity	Testing Corpus Token Precision  Table 5: Cross-domain generalization.
true	N10-1085.pdf#77.9%	Penn Treebank, Test perplexity	Token Accuracy  Table 5: Cross-domain generalization.
true	N10-1085.pdf#77.2%	Penn Treebank, Test perplexity	Token Accuracy  Table 5: Cross-domain generalization.
true	N10-1085.pdf#87.0%	Penn Treebank, Test perplexity	Type Precision  Table 5: Cross-domain generalization.
true	N10-1085.pdf#86.7%	Penn Treebank, Test perplexity	Type Recall Token F - measure  Table 5: Cross-domain generalization.
true	N10-1085.pdf#77.9%	Penn Treebank, Test perplexity	Type F - measure  Table 5: Cross-domain generalization.
true	N10-1085.pdf#76.5%	Penn Treebank, Test perplexity	Type F - measure  Table 5: Cross-domain generalization.
true	N10-1085.pdf#86.7%(0.9)	Penn Treebank, Test perplexity	Training Corpora Token Accuracy  Table 5: Cross-domain generalization.
true	N10-1085.pdf#76.4%	Penn Treebank, Test perplexity	Type Recall  Table 5: Cross-domain generalization.
true	N10-1085.pdf#86.7%(0.9)	Penn Treebank, Test perplexity	Type Precision Token Recall  Table 5: Cross-domain generalization.
true	N10-1085.pdf#82.4%	Penn Treebank, Test perplexity	Type Precision  Table 5: Cross-domain generalization.
true	N10-1085.pdf#77.3%	Penn Treebank, Test perplexity	Type Recall  Table 5: Cross-domain generalization.
true	N10-1085.pdf#77.4%	Penn Treebank, Test perplexity	Type F - measure  Table 5: Cross-domain generalization.
true	N10-1085.pdf#76.4%	Penn Treebank, Test perplexity	Token Accuracy  Table 5: Cross-domain generalization.
true	N10-1085.pdf#94.2%	Penn Treebank, Test perplexity	Type Precision  Table 5: Cross-domain generalization.
true	N10-1085.pdf#86.7%(0.9)	Penn Treebank, Test perplexity	Token Accuracy  Table 6: Full NP ordering accuracies; averages and standard deviations over a 10-fold cross validation. To  compare directly with Mitchell
true	N10-1085.pdf#86.7%	Penn Treebank, Test perplexity	Token F - measure  Table 6: Full NP ordering accuracies; averages and standard deviations over a 10-fold cross validation. To  compare directly with Mitchell
true	N10-1085.pdf#94.4%	Penn Treebank, Test perplexity	Token Precision  Table 6: Full NP ordering accuracies; averages and standard deviations over a 10-fold cross validation. To  compare directly with Mitchell
true	N10-1085.pdf#86.7%(0.9)	Penn Treebank, Test perplexity	Token Recall  Table 6: Full NP ordering accuracies; averages and standard deviations over a 10-fold cross validation. To  compare directly with Mitchell
true	N10-1085.pdf#77.6%	Penn Treebank, Validation perplexity	Type Recall  Table 5: Cross-domain generalization.
true	N10-1085.pdf#94.4%	Penn Treebank, Validation perplexity	Testing Corpus Token Precision  Table 5: Cross-domain generalization.
true	N10-1085.pdf#77.9%	Penn Treebank, Validation perplexity	Token Accuracy  Table 5: Cross-domain generalization.
true	N10-1085.pdf#77.2%	Penn Treebank, Validation perplexity	Token Accuracy  Table 5: Cross-domain generalization.
true	N10-1085.pdf#87.0%	Penn Treebank, Validation perplexity	Type Precision  Table 5: Cross-domain generalization.
true	N10-1085.pdf#86.7%	Penn Treebank, Validation perplexity	Type Recall Token F - measure  Table 5: Cross-domain generalization.
true	N10-1085.pdf#77.9%	Penn Treebank, Validation perplexity	Type F - measure  Table 5: Cross-domain generalization.
true	N10-1085.pdf#76.5%	Penn Treebank, Validation perplexity	Type F - measure  Table 5: Cross-domain generalization.
true	N10-1085.pdf#86.7%(0.9)	Penn Treebank, Validation perplexity	Training Corpora Token Accuracy  Table 5: Cross-domain generalization.
true	N10-1085.pdf#76.4%	Penn Treebank, Validation perplexity	Type Recall  Table 5: Cross-domain generalization.
true	N10-1085.pdf#86.7%(0.9)	Penn Treebank, Validation perplexity	Type Precision Token Recall  Table 5: Cross-domain generalization.
true	N10-1085.pdf#82.4%	Penn Treebank, Validation perplexity	Type Precision  Table 5: Cross-domain generalization.
true	N10-1085.pdf#77.3%	Penn Treebank, Validation perplexity	Type Recall  Table 5: Cross-domain generalization.
true	N10-1085.pdf#77.4%	Penn Treebank, Validation perplexity	Type F - measure  Table 5: Cross-domain generalization.
true	N10-1085.pdf#76.4%	Penn Treebank, Validation perplexity	Token Accuracy  Table 5: Cross-domain generalization.
true	N10-1085.pdf#94.2%	Penn Treebank, Validation perplexity	Type Precision  Table 5: Cross-domain generalization.
true	N10-1085.pdf#86.7%(0.9)	Penn Treebank, Validation perplexity	Token Accuracy  Table 6: Full NP ordering accuracies; averages and standard deviations over a 10-fold cross validation. To  compare directly with Mitchell
true	N10-1085.pdf#86.7%	Penn Treebank, Validation perplexity	Token F - measure  Table 6: Full NP ordering accuracies; averages and standard deviations over a 10-fold cross validation. To  compare directly with Mitchell
true	N10-1085.pdf#94.4%	Penn Treebank, Validation perplexity	Token Precision  Table 6: Full NP ordering accuracies; averages and standard deviations over a 10-fold cross validation. To  compare directly with Mitchell
true	N10-1085.pdf#86.7%(0.9)	Penn Treebank, Validation perplexity	Token Recall  Table 6: Full NP ordering accuracies; averages and standard deviations over a 10-fold cross validation. To  compare directly with Mitchell
true	D14-1183.pdf#:0.1	Text8, Number of params	) . MT06  Table 3: BLEU scores for Arabic-English transla- tion systems with different reordering models (*:  better than the lexicalized model with at least 95%  statistical significance).
true	N15-3009.pdf#0	Penn Treebank, F1	Failure Parser ( 
true	N15-3009.pdf#0	Penn Treebank, Accuracy	Failure Parser ( 
true	P13-1004.pdf#0.6680	WMT 2014 EN-DE, BLEU	MAE  Table 1: Single-task learning results on the  WMT12 dataset, trained and evaluated against  the weighted averaged response variable. µ is a  baseline which predicts the training mean, SVM  uses the same system as the WMT12 QE task, and  the remainder are GP regression models with dif- ferent kernels (all include additive noise).
true	P13-1004.pdf#0.8098	WMT 2014 EN-DE, BLEU	RMSE  Table 1: Single-task learning results on the  WMT12 dataset, trained and evaluated against  the weighted averaged response variable. µ is a  baseline which predicts the training mean, SVM  uses the same system as the WMT12 QE task, and  the remainder are GP regression models with dif- ferent kernels (all include additive noise).
true	P13-1004.pdf#0.8448	WMT 2014 EN-DE, BLEU	RMSE  Table 2: Results on the WMT12 dataset, trained  and evaluated over all three annotator's judge- ments. Shown above are the training mean base- line µ, single-task learning approaches, and multi- task learning models, with the columns showing  macro average error rates over all three response  values. All systems use a squared exponential  ARD kernel in a product with the named task- kernel, and with added noise (per-task noise is de- noted {N}, otherwise has shared noise).
true	P13-1004.pdf#0.6966	WMT 2014 EN-DE, BLEU	MAE  Table 2: Results on the WMT12 dataset, trained  and evaluated over all three annotator's judge- ments. Shown above are the training mean base- line µ, single-task learning approaches, and multi- task learning models, with the columns showing  macro average error rates over all three response  values. All systems use a squared exponential  ARD kernel in a product with the named task- kernel, and with added noise (per-task noise is de- noted {N}, otherwise has shared noise).
true	P14-1129.pdf#−1.82	WMT 2014 EN-FR, BLEU	Arabic BOLT Val  Table 1: Comparison of neural network likelihood  for various α values. log(P
true	P14-1129.pdf#−1.83	WMT 2014 EN-FR, BLEU	Arabic BOLT Val log ( P ( x ) )  Table 1: Comparison of neural network likelihood  for various α values. log(P
true	P14-1129.pdf#−1.91	WMT 2014 EN-FR, BLEU	Arabic BOLT Val log ( P ( x ) )  Table 1: Comparison of neural network likelihood  for various α values. log(P
true	P14-1129.pdf#−1.81	WMT 2014 EN-FR, BLEU	Arabic BOLT Val log ( P ( x ) )  Table 1: Comparison of neural network likelihood  for various α values. log(P
true	P14-1129.pdf#−1.82	WMT 2014 EN-DE, BLEU	Arabic BOLT Val  Table 1: Comparison of neural network likelihood  for various α values. log(P
true	P14-1129.pdf#−1.83	WMT 2014 EN-DE, BLEU	Arabic BOLT Val log ( P ( x ) )  Table 1: Comparison of neural network likelihood  for various α values. log(P
true	P14-1129.pdf#−1.91	WMT 2014 EN-DE, BLEU	Arabic BOLT Val log ( P ( x ) )  Table 1: Comparison of neural network likelihood  for various α values. log(P
true	P14-1129.pdf#−1.81	WMT 2014 EN-DE, BLEU	Arabic BOLT Val log ( P ( x ) )  Table 1: Comparison of neural network likelihood  for various α values. log(P
true	D10-1102.pdf#0.001(orp<0.05).	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SVM sle + Prox . Feat . w / Prior + Prox . Feat . w / Prior - For Movie Reviews , the SVM baseline accuracy is 88 . 56% . A  Table 1: Summary of the experimental results for the Movie Reviews (top) and U.S. Congressional Floor Debates  (bottom) datasets using SVM sle , SVM sle w/ Prior and SVM sle  f s with and without proximity features.
true	P10-3018.pdf#90.14	Penn Treebank, UAS	due different MAXENT package used . and our baseline accuracy is Malt UAS  Table 2: Comparison of NA and PA with previous  best results for Hindi
true	P10-3018.pdf#75.87	Penn Treebank, UAS	due different MAXENT package used . and our baseline accuracy is MST+MAXENT LS  Table 2: Comparison of NA and PA with previous  best results for Hindi
true	P10-3018.pdf#74.74	Penn Treebank, UAS	due different MAXENT package used . and our baseline accuracy is Malt LAS  Table 2: Comparison of NA and PA with previous  best results for Hindi
true	P10-3018.pdf#73.36	Penn Treebank, UAS	due different MAXENT package used . and our baseline accuracy is MST+MAXENT LAS  Table 2: Comparison of NA and PA with previous  best results for Hindi
true	P10-3018.pdf#91.26	Penn Treebank, UAS	due different MAXENT package used . and our baseline accuracy is MST+MAXENT UAS  Table 2: Comparison of NA and PA with previous  best results for Hindi
true	P10-3018.pdf#76.56	Penn Treebank, UAS	due different MAXENT package used . and our baseline accuracy is Malt LS  Table 2: Comparison of NA and PA with previous  best results for Hindi
true	P10-3018.pdf#83.69	Penn Treebank, UAS	LS  Table 3: Comparison of NA and PA with previous  best results for Czech
true	P10-3018.pdf#76.32	Penn Treebank, UAS	LAS  Table 3: Comparison of NA and PA with previous  best results for Czech
true	P10-3018.pdf#83.40	Penn Treebank, UAS	LS  Table 3: Comparison of NA and PA with previous  best results for Czech
true	P10-3018.pdf#82.92	Penn Treebank, UAS	UAS  Table 3: Comparison of NA and PA with previous  best results for Czech
true	P10-3018.pdf#75.97	Penn Treebank, UAS	LAS  Table 3: Comparison of NA and PA with previous  best results for Czech
true	P10-3018.pdf#82.92	Penn Treebank, UAS	UAS  Table 3: Comparison of NA and PA with previous  best results for Czech
true	P10-3018.pdf#90.14	benchmark Vietnamese dependency treebank VnDT, UAS	due different MAXENT package used . and our baseline accuracy is Malt UAS  Table 2: Comparison of NA and PA with previous  best results for Hindi
true	P10-3018.pdf#75.87	benchmark Vietnamese dependency treebank VnDT, UAS	due different MAXENT package used . and our baseline accuracy is MST+MAXENT LS  Table 2: Comparison of NA and PA with previous  best results for Hindi
true	P10-3018.pdf#74.74	benchmark Vietnamese dependency treebank VnDT, UAS	due different MAXENT package used . and our baseline accuracy is Malt LAS  Table 2: Comparison of NA and PA with previous  best results for Hindi
true	P10-3018.pdf#73.36	benchmark Vietnamese dependency treebank VnDT, UAS	due different MAXENT package used . and our baseline accuracy is MST+MAXENT LAS  Table 2: Comparison of NA and PA with previous  best results for Hindi
true	P10-3018.pdf#91.26	benchmark Vietnamese dependency treebank VnDT, UAS	due different MAXENT package used . and our baseline accuracy is MST+MAXENT UAS  Table 2: Comparison of NA and PA with previous  best results for Hindi
true	P10-3018.pdf#76.56	benchmark Vietnamese dependency treebank VnDT, UAS	due different MAXENT package used . and our baseline accuracy is Malt LS  Table 2: Comparison of NA and PA with previous  best results for Hindi
true	P10-3018.pdf#83.69	benchmark Vietnamese dependency treebank VnDT, UAS	LS  Table 3: Comparison of NA and PA with previous  best results for Czech
true	P10-3018.pdf#76.32	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 3: Comparison of NA and PA with previous  best results for Czech
true	P10-3018.pdf#83.40	benchmark Vietnamese dependency treebank VnDT, UAS	LS  Table 3: Comparison of NA and PA with previous  best results for Czech
true	P10-3018.pdf#82.92	benchmark Vietnamese dependency treebank VnDT, UAS	UAS  Table 3: Comparison of NA and PA with previous  best results for Czech
true	P10-3018.pdf#75.97	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 3: Comparison of NA and PA with previous  best results for Czech
true	P10-3018.pdf#82.92	benchmark Vietnamese dependency treebank VnDT, UAS	UAS  Table 3: Comparison of NA and PA with previous  best results for Czech
true	P10-3003.pdf#57.4	Senseval 2, F1	Senseval - 3 All Words data set Senseval - 2 All Words data set all  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#70.1	Senseval 2, F1	Senseval - 2 All Words data set adv  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#38.90	Senseval 2, F1	Senseval - 2 All Words data set verb  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#64.1	Senseval 2, F1	Senseval - 3 All Words data set Senseval - 2 All Words data set noun  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#61.99	Senseval 2, F1	Senseval - 2 All Words data set adj  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#62.6	Senseval 2, F1	Senseval - 3 All Words data set Senseval - 2 All Words data set adj  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#58.83	Senseval 2, F1	Senseval - 2 All Words data set all  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#58.6	Senseval 2, F1	Senseval - 2 All Words data set all  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#46.9	Senseval 2, F1	Senseval - 3 All Words data set Senseval - 2 All Words data set verb  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#70.40	Senseval 2, F1	Senseval - 2 All Words data set noun  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#62.72	Senseval 2, F1	Senseval - 2 All Words data set adj  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#61.42	Senseval 2, F1	Senseval - 2 All Words data set adj  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#57.4	Senseval 3, F1	Senseval - 3 All Words data set Senseval - 2 All Words data set all  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#70.1	Senseval 3, F1	Senseval - 2 All Words data set adv  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#38.90	Senseval 3, F1	Senseval - 2 All Words data set verb  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#64.1	Senseval 3, F1	Senseval - 3 All Words data set Senseval - 2 All Words data set noun  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#61.99	Senseval 3, F1	Senseval - 2 All Words data set adj  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#62.6	Senseval 3, F1	Senseval - 3 All Words data set Senseval - 2 All Words data set adj  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#58.83	Senseval 3, F1	Senseval - 2 All Words data set all  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#58.6	Senseval 3, F1	Senseval - 2 All Words data set all  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#46.9	Senseval 3, F1	Senseval - 3 All Words data set Senseval - 2 All Words data set verb  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#70.40	Senseval 3, F1	Senseval - 2 All Words data set noun  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#62.72	Senseval 3, F1	Senseval - 2 All Words data set adj  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P10-3003.pdf#61.42	Senseval 3, F1	Senseval - 2 All Words data set adj  Table 1: Evaluation results on Senseval-2 and  Senseval-3 data-set of all words task.
true	P14-1136v2.pdf#46.15	Penn Treebank, LAS	Development Data SEMAFOR LEXICON Rare SEMAFOR LEXICON Unseen  Table 2: Frame identification results for FrameNet. See  §5.6.
true	P14-1136v2.pdf#73.10	Penn Treebank, LAS	Development Data FULL LEXICON Ambiguous FULL LEXICON Ambiguous  Table 2: Frame identification results for FrameNet. See  §5.6.
true	P14-1136v2.pdf#90.78	Penn Treebank, LAS	Development Data SEMAFOR LEXICON All  Table 2: Frame identification results for FrameNet. See  §5.6.
true	P14-1136v2.pdf#88.93	Penn Treebank, LAS	Development Data FULL LEXICON Rare FULL LEXICON Rare  Table 2: Frame identification results for FrameNet. See  §5.6.
true	P14-1136v2.pdf#76.43	Penn Treebank, LAS	Development Data SEMAFOR LEXICON Ambiguous  Table 2: Frame identification results for FrameNet. See  §5.6.
true	P14-1136v2.pdf#86.49	Penn Treebank, LAS	Development Data SEMAFOR LEXICON All SEMAFOR LEXICON All  Table 2: Frame identification results for FrameNet. See  §5.6.
true	P14-1136v2.pdf#73.39	Penn Treebank, LAS	Development Data SEMAFOR LEXICON All SEMAFOR LEXICON Ambiguous  Table 2: Frame identification results for FrameNet. See  §5.6.
true	P14-1136v2.pdf#90.18	Penn Treebank, LAS	Development Data FULL LEXICON Rare  Table 2: Frame identification results for FrameNet. See  §5.6.
true	P14-1136v2.pdf#85.22	Penn Treebank, LAS	Development Data SEMAFOR LEXICON Ambiguous SEMAFOR LEXICON Rare  Table 2: Frame identification results for FrameNet. See  §5.6.
true	P14-1136v2.pdf#88.41	Penn Treebank, LAS	Development Data FULL LEXICON All FULL LEXICON All  Table 2: Frame identification results for FrameNet. See  §5.6.
true	P14-1136v2.pdf#90.18	Penn Treebank, LAS	Development Data SEMAFOR LEXICON Rare  Table 2: Frame identification results for FrameNet. See  §5.6.
true	P14-1136v2.pdf#90.90	Penn Treebank, LAS	Development Data FULL LEXICON All  Table 2: Frame identification results for FrameNet. See  §5.6.
true	P14-1136v2.pdf#76.83	Penn Treebank, LAS	Development Data FULL LEXICON Ambiguous  Table 2: Frame identification results for FrameNet. See  §5.6.
true	P14-1136v2.pdf#73.00	Penn Treebank, LAS	SEMAFOR LEXICON Precision  Table 3: Full structure prediction results for FrameNet; this reports frame and argument identification performance jointly. We  skip LOG-LINEAR EMBEDDING because it underperforms all other models by a large margin.
true	P14-1136v2.pdf#64.53	Penn Treebank, LAS	FULL LEXICON Recall  Table 3: Full structure prediction results for FrameNet; this reports frame and argument identification performance jointly. We  skip LOG-LINEAR EMBEDDING because it underperforms all other models by a large margin.
true	P14-1136v2.pdf#64.87	Penn Treebank, LAS	SEMAFOR LEXICON Recall  Table 3: Full structure prediction results for FrameNet; this reports frame and argument identification performance jointly. We  skip LOG-LINEAR EMBEDDING because it underperforms all other models by a large margin.
true	P14-1136v2.pdf#70.76	Penn Treebank, LAS	FULL LEXICON F1  Table 3: Full structure prediction results for FrameNet; this reports frame and argument identification performance jointly. We  skip LOG-LINEAR EMBEDDING because it underperforms all other models by a large margin.
true	P14-1136v2.pdf#64.51	Penn Treebank, LAS	SEMAFOR LEXICON Recall  Table 3: Full structure prediction results for FrameNet; this reports frame and argument identification performance jointly. We  skip LOG-LINEAR EMBEDDING because it underperforms all other models by a large margin.
true	P14-1136v2.pdf#78.33	Penn Treebank, LAS	SEMAFOR LEXICON Precision  Table 3: Full structure prediction results for FrameNet; this reports frame and argument identification performance jointly. We  skip LOG-LINEAR EMBEDDING because it underperforms all other models by a large margin.
true	P14-1136v2.pdf#70.75	Penn Treebank, LAS	SEMAFOR LEXICON F1  Table 3: Full structure prediction results for FrameNet; this reports frame and argument identification performance jointly. We  skip LOG-LINEAR EMBEDDING because it underperforms all other models by a large margin.
true	P14-1136v2.pdf#78.33	Penn Treebank, LAS	FULL LEXICON Precision  Table 3: Full structure prediction results for FrameNet; this reports frame and argument identification performance jointly. We  skip LOG-LINEAR EMBEDDING because it underperforms all other models by a large margin.
true	P14-1136v2.pdf#69.91	Penn Treebank, LAS	FULL LEXICON F1  Table 3: Full structure prediction results for FrameNet; this reports frame and argument identification performance jointly. We  skip LOG-LINEAR EMBEDDING because it underperforms all other models by a large margin.
true	P14-1136v2.pdf#68.69	Penn Treebank, LAS	SEMAFOR LEXICON F1  Table 3: Full structure prediction results for FrameNet; this reports frame and argument identification performance jointly. We  skip LOG-LINEAR EMBEDDING because it underperforms all other models by a large margin.
true	P14-1136v2.pdf#66.02	Penn Treebank, LAS	FULL LEXICON Recall  Table 3: Full structure prediction results for FrameNet; this reports frame and argument identification performance jointly. We  skip LOG-LINEAR EMBEDDING because it underperforms all other models by a large margin.
true	P14-1136v2.pdf#74.29	Penn Treebank, LAS	FULL LEXICON Precision  Table 3: Full structure prediction results for FrameNet; this reports frame and argument identification performance jointly. We  skip LOG-LINEAR EMBEDDING because it underperforms all other models by a large margin.
true	P14-1136v2.pdf#93.73	Penn Treebank, LAS	Rare  Table 4: Frame identification accuracy results for PropBank.  The model and the column names have the same semantics  as Table 2.
true	P14-1136v2.pdf#92.07	Penn Treebank, LAS	Dev data ↑ ↓ Test data Ambiguous Ambiguous  Table 4: Frame identification accuracy results for PropBank.  The model and the column names have the same semantics  as Table 2.
true	P14-1136v2.pdf#94.74	Penn Treebank, LAS	Dev data ↑ ↓ Test data All All  Table 4: Frame identification accuracy results for PropBank.  The model and the column names have the same semantics  as Table 2.
true	P14-1136v2.pdf#91.52	Penn Treebank, LAS	Ambiguous  Table 4: Frame identification accuracy results for PropBank.  The model and the column names have the same semantics  as Table 2.
true	P14-1136v2.pdf#91.32	Penn Treebank, LAS	Dev data ↑ ↓ Test data Rare Rare  Table 4: Frame identification accuracy results for PropBank.  The model and the column names have the same semantics  as Table 2.
true	P14-1136v2.pdf#94.79	Penn Treebank, LAS	All  Table 4: Frame identification accuracy results for PropBank.  The model and the column names have the same semantics  as Table 2.
true	P14-1136v2.pdf#80.06	Penn Treebank, LAS	P  Table 5: Full frame-structure prediction results for Propbank.  This is a metric that takes into account frames and arguments  together. See  §5.7 for more details.
true	P14-1136v2.pdf#79.65	Penn Treebank, LAS	Dev data ↑ ↓ Test data F1 F1  Table 5: Full frame-structure prediction results for Propbank.  This is a metric that takes into account frames and arguments  together. See  §5.7 for more details.
true	P14-1136v2.pdf#81.55	Penn Treebank, LAS	Dev data ↑ ↓ Test data P P  Table 5: Full frame-structure prediction results for Propbank.  This is a metric that takes into account frames and arguments  together. See  §5.7 for more details.
true	P14-1136v2.pdf#77.83	Penn Treebank, LAS	Dev data ↑ ↓ Test data R R  Table 5: Full frame-structure prediction results for Propbank.  This is a metric that takes into account frames and arguments  together. See  §5.7 for more details.
true	P14-1136v2.pdf#77.84	Penn Treebank, LAS	F1  Table 5: Full frame-structure prediction results for Propbank.  This is a metric that takes into account frames and arguments  together. See  §5.7 for more details.
true	P14-1136v2.pdf#75.74	Penn Treebank, LAS	R  Table 5: Full frame-structure prediction results for Propbank.  This is a metric that takes into account frames and arguments  together. See  §5.7 for more details.
true	P14-1136v2.pdf#71.50	Penn Treebank, LAS	R  Table 6: Argument only evaluation (semantic role labeling  metrics) using the CoNLL 2005 shared task evaluation script  (Carreras and M` arquez, 2005). Results from Punyakanok et  al. (2008) are taken from Table 11 of that paper.
true	P14-1136v2.pdf#75.11	Penn Treebank, LAS	Dev data ↑ ↓ Test data R R  Table 6: Argument only evaluation (semantic role labeling  metrics) using the CoNLL 2005 shared task evaluation script  (Carreras and M` arquez, 2005). Results from Punyakanok et  al. (2008) are taken from Table 11 of that paper.
true	P14-1136v2.pdf#77.29	Penn Treebank, LAS	P  Table 6: Argument only evaluation (semantic role labeling  metrics) using the CoNLL 2005 shared task evaluation script  (Carreras and M` arquez, 2005). Results from Punyakanok et  al. (2008) are taken from Table 11 of that paper.
true	P14-1136v2.pdf#77.23	Penn Treebank, LAS	Dev data ↑ ↓ Test data F1 F1  Table 6: Argument only evaluation (semantic role labeling  metrics) using the CoNLL 2005 shared task evaluation script  (Carreras and M` arquez, 2005). Results from Punyakanok et  al. (2008) are taken from Table 11 of that paper.
true	P14-1136v2.pdf#79.47	Penn Treebank, LAS	Dev data ↑ ↓ Test data P P  Table 6: Argument only evaluation (semantic role labeling  metrics) using the CoNLL 2005 shared task evaluation script  (Carreras and M` arquez, 2005). Results from Punyakanok et  al. (2008) are taken from Table 11 of that paper.
true	P14-1136v2.pdf#74.28	Penn Treebank, LAS	F1  Table 6: Argument only evaluation (semantic role labeling  metrics) using the CoNLL 2005 shared task evaluation script  (Carreras and M` arquez, 2005). Results from Punyakanok et  al. (2008) are taken from Table 11 of that paper.
true	D11-1016.pdf#3	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	T . Polarity  Table 1: Mapping of combination of polarities and inten- sities from MPQA dataset to our ordinal sentiment scale.
true	D11-1016.pdf#3	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	T . Polarity  Table 1.
true	N10-1121.pdf#62.61*†	New York Times Corpus, P@10%	F1  Table 7: Results of kernel combinations (  *  : significantly  better than best SKs;  † : significantly better than best TKs;  all convolution kernels are significantly better than VK).
true	N10-1121.pdf#59.22	New York Times Corpus, P@10%	Prec .  Table 7: Results of kernel combinations (  *  : significantly  better than best SKs;  † : significantly better than best TKs;  all convolution kernels are significantly better than VK).
true	N10-1121.pdf#68.36	New York Times Corpus, P@10%	Rec .  Table 7: Results of kernel combinations (  *  : significantly  better than best SKs;  † : significantly better than best TKs;  all convolution kernels are significantly better than VK).
true	N10-1121.pdf#94.53	New York Times Corpus, P@10%	Acc .  Table 7: Results of kernel combinations (  *  : significantly  better than best SKs;  † : significantly better than best TKs;  all convolution kernels are significantly better than VK).
true	N10-1121.pdf#59.81	New York Times Corpus, P@10%	i∈{P RED , SEM } Rec .  Table 5: Results of the different sequence kernels.
true	N10-1121.pdf#58.70	New York Times Corpus, P@10%	i∈{P RED , SEM } F1  Table 5: Results of the different sequence kernels.
true	N10-1121.pdf#94.21	New York Times Corpus, P@10%	i∈{P RED , SEM } Acc .  Table 5: Results of the different sequence kernels.
true	N10-1121.pdf#57.64	New York Times Corpus, P@10%	i∈{P RED , SEM } Prec .  Table 5: Results of the different sequence kernels.
true	N10-1121.pdf#69.96	New York Times Corpus, P@10%	A i = CON ST , j = P AS Rec .  Table 6: Results of the different tree kernels.
true	N10-1121.pdf#94.30	New York Times Corpus, P@10%	k∈{P RED , SEM } A i = CON ST , j = P AS Acc .  Table 6: Results of the different tree kernels.
true	N10-1121.pdf#68.36	New York Times Corpus, P@10%	B i = CON ST AU G , j = P AS AU G Rec .  Table 6: Results of the different tree kernels.
true	N10-1121.pdf#61.67	New York Times Corpus, P@10%	B i = CON ST AU G , j = P AS AU G F1  Table 6: Results of the different tree kernels.
true	N10-1121.pdf#60.40	New York Times Corpus, P@10%	A i = CON ST AU G , j = P AS AU G F1  Table 6: Results of the different tree kernels.
true	N10-1121.pdf#57.95	New York Times Corpus, P@10%	k∈{P RED , SEM } A i = CON ST , j = P AS Prec .  Table 6: Results of the different tree kernels.
true	N10-1121.pdf#58.11	New York Times Corpus, P@10%	B i = CON ST AU G , j = P AS AU G Prec .  Table 6: Results of the different tree kernels.
true	N10-1121.pdf#94.36	New York Times Corpus, P@10%	k∈{P RED , SEM } B i = CON ST AU G , j = P AS AU G Acc .  Table 6: Results of the different tree kernels.
true	N15-1175.pdf#31.3	WMT 2014 EN-DE, BLEU	test  Table 1: Statistics for the bilingual training data of the IWSLT 2013 German→English, the DARPA BOLT  Chinese→English and the WMT 2014 German→English tasks.
true	N15-1175.pdf#31.6	WMT 2014 EN-DE, BLEU	test  Table 1: Statistics for the bilingual training data of the IWSLT 2013 German→English, the DARPA BOLT  Chinese→English and the WMT 2014 German→English tasks.
true	N15-1175.pdf#18.7	WMT 2014 EN-DE, BLEU	df  Table 3: Results for the BOLT Chinese→English task  in BLEU [%] on the discussion forum test set (df), the  mixed web test set and NIST MT06. The baseline is our  BOLT evaluation system and contains a recurrent neural  LM. We compare with (Setiawan and Zhou, 2013) who  applied maximum expected BLEU training with growth  transformation (GT). Note that the number of features re- ported by Setiawan and Zhou (2013) is artificially blown  up due to renormalization.
true	N15-1175.pdf#40.5	WMT 2014 EN-DE, BLEU	MT06  Table 3: Results for the BOLT Chinese→English task  in BLEU [%] on the discussion forum test set (df), the  mixed web test set and NIST MT06. The baseline is our  BOLT evaluation system and contains a recurrent neural  LM. We compare with (Setiawan and Zhou, 2013) who  applied maximum expected BLEU training with growth  transformation (GT). Note that the number of features re- ported by Setiawan and Zhou (2013) is artificially blown  up due to renormalization.
true	N15-1175.pdf#34.8	WMT 2014 EN-DE, BLEU	web  Table 3: Results for the BOLT Chinese→English task  in BLEU [%] on the discussion forum test set (df), the  mixed web test set and NIST MT06. The baseline is our  BOLT evaluation system and contains a recurrent neural  LM. We compare with (Setiawan and Zhou, 2013) who  applied maximum expected BLEU training with growth  transformation (GT). Note that the number of features re- ported by Setiawan and Zhou (2013) is artificially blown  up due to renormalization.
true	N15-1175.pdf#28.9	WMT 2014 EN-DE, BLEU	newstest2013  Table 4: Results for the WMT German→English task in  BLEU [%]. The baseline contains a recurrent neural LM.  We compare with the best single system that is reported  on matrix.statmt.org, which was submitted by the  Unversity of Edinburgh.
true	N15-1175.pdf#31.3	WMT 2014 EN-FR, BLEU	test  Table 1: Statistics for the bilingual training data of the IWSLT 2013 German→English, the DARPA BOLT  Chinese→English and the WMT 2014 German→English tasks.
true	N15-1175.pdf#31.6	WMT 2014 EN-FR, BLEU	test  Table 1: Statistics for the bilingual training data of the IWSLT 2013 German→English, the DARPA BOLT  Chinese→English and the WMT 2014 German→English tasks.
true	N15-1175.pdf#18.7	WMT 2014 EN-FR, BLEU	df  Table 3: Results for the BOLT Chinese→English task  in BLEU [%] on the discussion forum test set (df), the  mixed web test set and NIST MT06. The baseline is our  BOLT evaluation system and contains a recurrent neural  LM. We compare with (Setiawan and Zhou, 2013) who  applied maximum expected BLEU training with growth  transformation (GT). Note that the number of features re- ported by Setiawan and Zhou (2013) is artificially blown  up due to renormalization.
true	N15-1175.pdf#40.5	WMT 2014 EN-FR, BLEU	MT06  Table 3: Results for the BOLT Chinese→English task  in BLEU [%] on the discussion forum test set (df), the  mixed web test set and NIST MT06. The baseline is our  BOLT evaluation system and contains a recurrent neural  LM. We compare with (Setiawan and Zhou, 2013) who  applied maximum expected BLEU training with growth  transformation (GT). Note that the number of features re- ported by Setiawan and Zhou (2013) is artificially blown  up due to renormalization.
true	N15-1175.pdf#34.8	WMT 2014 EN-FR, BLEU	web  Table 3: Results for the BOLT Chinese→English task  in BLEU [%] on the discussion forum test set (df), the  mixed web test set and NIST MT06. The baseline is our  BOLT evaluation system and contains a recurrent neural  LM. We compare with (Setiawan and Zhou, 2013) who  applied maximum expected BLEU training with growth  transformation (GT). Note that the number of features re- ported by Setiawan and Zhou (2013) is artificially blown  up due to renormalization.
true	N15-1175.pdf#28.9	WMT 2014 EN-FR, BLEU	newstest2013  Table 4: Results for the WMT German→English task in  BLEU [%]. The baseline contains a recurrent neural LM.  We compare with the best single system that is reported  on matrix.statmt.org, which was submitted by the  Unversity of Edinburgh.
true	D13-1108.pdf#37.68*	New York Times Corpus, P@10%	MT04  Table 1: Statistics of the extracted rules on training data and the BLEU scores (%) on the test sets of different systems.  The "+" denotes that the rules are composed of syntactic translation rules and bilingual phrases (32.5M). The "*"  denotes that the results are significantly better than all the other systems (p<0.01).
true	D13-1108.pdf#36.29	New York Times Corpus, P@10%	Average  Table 1: Statistics of the extracted rules on training data and the BLEU scores (%) on the test sets of different systems.  The "+" denotes that the rules are composed of syntactic translation rules and bilingual phrases (32.5M). The "*"  denotes that the results are significantly better than all the other systems (p<0.01).
true	D13-1108.pdf#35.57*	New York Times Corpus, P@10%	MT03  Table 1: Statistics of the extracted rules on training data and the BLEU scores (%) on the test sets of different systems.  The "+" denotes that the rules are composed of syntactic translation rules and bilingual phrases (32.5M). The "*"  denotes that the results are significantly better than all the other systems (p<0.01).
true	D13-1108.pdf#35.62*	New York Times Corpus, P@10%	MT05  Table 1: Statistics of the extracted rules on training data and the BLEU scores (%) on the test sets of different systems.  The "+" denotes that the rules are composed of syntactic translation rules and bilingual phrases (32.5M). The "*"  denotes that the results are significantly better than all the other systems (p<0.01).
true	D10-1092.pdf#0.947	WMT 2014 EN-DE, BLEU	man judgments ( Spm = Spearman , Prs = Pearson ) Table 1 : NTCIR - 7 Meta - evaluation : correlation with hu - Adequacy Spm  Table 1: NTCIR-7 Meta-evaluation: correlation with hu- man judgments (Spm = Spearman, Prs = Pearson)
true	D10-1092.pdf#0.947	WMT 2014 EN-FR, BLEU	man judgments ( Spm = Spearman , Prs = Pearson ) Table 1 : NTCIR - 7 Meta - evaluation : correlation with hu - Adequacy Spm  Table 1: NTCIR-7 Meta-evaluation: correlation with hu- man judgments (Spm = Spearman, Prs = Pearson)
true	P14-5014.pdf#32.99	WMT 2014 EN-DE, BLEU	-  Table 1: Scores
true	P14-5014.pdf#33.61	WMT 2014 EN-DE, BLEU	Table 1: Scores
true	P14-5014.pdf#42.79	WMT 2014 EN-DE, BLEU	Table 1: Scores
true	P14-5014.pdf#29.00	WMT 2014 EN-DE, BLEU	-  Table 1: Scores
true	D11-1031.pdf#87.76	Penn Treebank, UAS	section 23 ( test ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.63	Penn Treebank, UAS	section 00 ( dev ) UP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.16	Penn Treebank, UAS	section 00 ( dev ) LR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.93	Penn Treebank, UAS	section 00 ( dev ) LP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.06	Penn Treebank, UAS	section 23 ( test ) UF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#92.86	Penn Treebank, UAS	section 23 ( test ) UR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.66	Penn Treebank, UAS	section 23 ( test ) UP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.34	Penn Treebank, UAS	section 00 ( dev ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.34	Penn Treebank, UAS	section 00 ( dev ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.51	Penn Treebank, UAS	section 23 ( test ) LR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#92.80	Penn Treebank, UAS	section 00 ( dev ) UR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#88.34	Penn Treebank, UAS	section 23 ( test ) LP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.05	Penn Treebank, UAS	section 00 ( dev ) UF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#94.39	Penn Treebank, UAS	section 00 ( dev ) AST ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.12	Penn Treebank, UAS	section 00 ( dev ) Reverse LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#87.67	Penn Treebank, UAS	section 00 ( dev ) AST LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.28	Penn Treebank, UAS	section 23 ( test ) AST UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.46	Penn Treebank, UAS	section 00 ( dev ) Reverse ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.50	Penn Treebank, UAS	section 23 ( test ) AST ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.58	Penn Treebank, UAS	section 23 ( test ) Reverse LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.10	Penn Treebank, UAS	section 23 ( test ) AST LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.23	Penn Treebank, UAS	section 00 ( dev ) AST UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.53	Penn Treebank, UAS	section 23 ( test ) Reverse ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.57	Penn Treebank, UAS	section 23 ( test ) Reverse UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.52	Penn Treebank, UAS	section 00 ( dev ) Reverse UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.48	Penn Treebank, UAS	section 23 ( test ) AST UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.98	Penn Treebank, UAS	section 23 ( test ) Reverse UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.88	Penn Treebank, UAS	section 00 ( dev ) Reverse UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.71	Penn Treebank, UAS	section 23 ( test ) AST ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.40	Penn Treebank, UAS	section 00 ( dev ) AST UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#88.47	Penn Treebank, UAS	section 23 ( test ) AST LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#95.01	Penn Treebank, UAS	section 23 ( test ) Reverse ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#87.90	Penn Treebank, UAS	section 00 ( dev ) AST LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.52	Penn Treebank, UAS	section 00 ( dev ) AST ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#89.25	Penn Treebank, UAS	section 23 ( test ) Reverse LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.79	Penn Treebank, UAS	section 00 ( dev ) Reverse ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#88.58	Penn Treebank, UAS	section 00 ( dev ) Reverse LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#92.44	Penn Treebank, UAS	section 23 ( test ) UR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.43	Penn Treebank, UAS	section 00 ( dev ) UR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#93.16	Penn Treebank, UAS	section 00 ( dev ) UP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.07	Penn Treebank, UAS	section 00 ( dev ) LP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.39	Penn Treebank, UAS	section 00 ( dev ) LR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.76	Penn Treebank, UAS	section 23 ( test ) UF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.90	Penn Treebank, UAS	section 23 ( test ) LR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.50	Penn Treebank, UAS	section 23 ( test ) LP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.20	Penn Treebank, UAS	section 23 ( test ) LF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.79	Penn Treebank, UAS	section 00 ( dev ) UF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#93.08	Penn Treebank, UAS	section 23 ( test ) UP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.73	Penn Treebank, UAS	section 00 ( dev ) LF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.76	Penn Treebank, POS	section 23 ( test ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.63	Penn Treebank, POS	section 00 ( dev ) UP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.16	Penn Treebank, POS	section 00 ( dev ) LR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.93	Penn Treebank, POS	section 00 ( dev ) LP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.06	Penn Treebank, POS	section 23 ( test ) UF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#92.86	Penn Treebank, POS	section 23 ( test ) UR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.66	Penn Treebank, POS	section 23 ( test ) UP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.34	Penn Treebank, POS	section 00 ( dev ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.34	Penn Treebank, POS	section 00 ( dev ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.51	Penn Treebank, POS	section 23 ( test ) LR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#92.80	Penn Treebank, POS	section 00 ( dev ) UR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#88.34	Penn Treebank, POS	section 23 ( test ) LP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.05	Penn Treebank, POS	section 00 ( dev ) UF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#94.39	Penn Treebank, POS	section 00 ( dev ) AST ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.12	Penn Treebank, POS	section 00 ( dev ) Reverse LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#87.67	Penn Treebank, POS	section 00 ( dev ) AST LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.28	Penn Treebank, POS	section 23 ( test ) AST UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.46	Penn Treebank, POS	section 00 ( dev ) Reverse ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.50	Penn Treebank, POS	section 23 ( test ) AST ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.58	Penn Treebank, POS	section 23 ( test ) Reverse LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.10	Penn Treebank, POS	section 23 ( test ) AST LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.23	Penn Treebank, POS	section 00 ( dev ) AST UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.53	Penn Treebank, POS	section 23 ( test ) Reverse ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.57	Penn Treebank, POS	section 23 ( test ) Reverse UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.52	Penn Treebank, POS	section 00 ( dev ) Reverse UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.48	Penn Treebank, POS	section 23 ( test ) AST UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.98	Penn Treebank, POS	section 23 ( test ) Reverse UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.88	Penn Treebank, POS	section 00 ( dev ) Reverse UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.71	Penn Treebank, POS	section 23 ( test ) AST ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.40	Penn Treebank, POS	section 00 ( dev ) AST UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#88.47	Penn Treebank, POS	section 23 ( test ) AST LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#95.01	Penn Treebank, POS	section 23 ( test ) Reverse ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#87.90	Penn Treebank, POS	section 00 ( dev ) AST LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.52	Penn Treebank, POS	section 00 ( dev ) AST ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#89.25	Penn Treebank, POS	section 23 ( test ) Reverse LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.79	Penn Treebank, POS	section 00 ( dev ) Reverse ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#88.58	Penn Treebank, POS	section 00 ( dev ) Reverse LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#92.44	Penn Treebank, POS	section 23 ( test ) UR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.43	Penn Treebank, POS	section 00 ( dev ) UR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#93.16	Penn Treebank, POS	section 00 ( dev ) UP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.07	Penn Treebank, POS	section 00 ( dev ) LP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.39	Penn Treebank, POS	section 00 ( dev ) LR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.76	Penn Treebank, POS	section 23 ( test ) UF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.90	Penn Treebank, POS	section 23 ( test ) LR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.50	Penn Treebank, POS	section 23 ( test ) LP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.20	Penn Treebank, POS	section 23 ( test ) LF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.79	Penn Treebank, POS	section 00 ( dev ) UF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#93.08	Penn Treebank, POS	section 23 ( test ) UP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.73	Penn Treebank, POS	section 00 ( dev ) LF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.76	CCGBank, Accuracy	section 23 ( test ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.63	CCGBank, Accuracy	section 00 ( dev ) UP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.16	CCGBank, Accuracy	section 00 ( dev ) LR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.93	CCGBank, Accuracy	section 00 ( dev ) LP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.06	CCGBank, Accuracy	section 23 ( test ) UF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#92.86	CCGBank, Accuracy	section 23 ( test ) UR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.66	CCGBank, Accuracy	section 23 ( test ) UP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.34	CCGBank, Accuracy	section 00 ( dev ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.34	CCGBank, Accuracy	section 00 ( dev ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.51	CCGBank, Accuracy	section 23 ( test ) LR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#92.80	CCGBank, Accuracy	section 00 ( dev ) UR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#88.34	CCGBank, Accuracy	section 23 ( test ) LP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.05	CCGBank, Accuracy	section 00 ( dev ) UF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#94.39	CCGBank, Accuracy	section 00 ( dev ) AST ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.12	CCGBank, Accuracy	section 00 ( dev ) Reverse LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#87.67	CCGBank, Accuracy	section 00 ( dev ) AST LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.28	CCGBank, Accuracy	section 23 ( test ) AST UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.46	CCGBank, Accuracy	section 00 ( dev ) Reverse ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.50	CCGBank, Accuracy	section 23 ( test ) AST ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.58	CCGBank, Accuracy	section 23 ( test ) Reverse LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.10	CCGBank, Accuracy	section 23 ( test ) AST LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.23	CCGBank, Accuracy	section 00 ( dev ) AST UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.53	CCGBank, Accuracy	section 23 ( test ) Reverse ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.57	CCGBank, Accuracy	section 23 ( test ) Reverse UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.52	CCGBank, Accuracy	section 00 ( dev ) Reverse UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.48	CCGBank, Accuracy	section 23 ( test ) AST UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.98	CCGBank, Accuracy	section 23 ( test ) Reverse UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.88	CCGBank, Accuracy	section 00 ( dev ) Reverse UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.71	CCGBank, Accuracy	section 23 ( test ) AST ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.40	CCGBank, Accuracy	section 00 ( dev ) AST UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#88.47	CCGBank, Accuracy	section 23 ( test ) AST LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#95.01	CCGBank, Accuracy	section 23 ( test ) Reverse ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#87.90	CCGBank, Accuracy	section 00 ( dev ) AST LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.52	CCGBank, Accuracy	section 00 ( dev ) AST ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#89.25	CCGBank, Accuracy	section 23 ( test ) Reverse LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.79	CCGBank, Accuracy	section 00 ( dev ) Reverse ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#88.58	CCGBank, Accuracy	section 00 ( dev ) Reverse LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#92.44	CCGBank, Accuracy	section 23 ( test ) UR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.43	CCGBank, Accuracy	section 00 ( dev ) UR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#93.16	CCGBank, Accuracy	section 00 ( dev ) UP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.07	CCGBank, Accuracy	section 00 ( dev ) LP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.39	CCGBank, Accuracy	section 00 ( dev ) LR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.76	CCGBank, Accuracy	section 23 ( test ) UF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.90	CCGBank, Accuracy	section 23 ( test ) LR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.50	CCGBank, Accuracy	section 23 ( test ) LP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.20	CCGBank, Accuracy	section 23 ( test ) LF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.79	CCGBank, Accuracy	section 00 ( dev ) UF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#93.08	CCGBank, Accuracy	section 23 ( test ) UP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.73	CCGBank, Accuracy	section 00 ( dev ) LF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.76	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.63	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) UP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.16	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) LR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.93	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) LP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.06	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) UF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#92.86	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) UR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.66	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) UP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.34	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.34	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.51	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) LR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#92.80	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) UR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#88.34	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) LP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.05	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) UF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#94.39	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) AST ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.12	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) Reverse LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#87.67	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) AST LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.28	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) AST UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.46	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) Reverse ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.50	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) AST ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.58	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) Reverse LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.10	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) AST LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.23	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) AST UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.53	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) Reverse ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.57	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) Reverse UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.52	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) Reverse UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.48	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) AST UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.98	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) Reverse UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.88	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) Reverse UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.71	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) AST ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.40	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) AST UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#88.47	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) AST LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#95.01	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) Reverse ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#87.90	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) AST LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.52	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) AST ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#89.25	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) Reverse LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.79	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) Reverse ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#88.58	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) Reverse LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#92.44	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) UR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.43	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) UR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#93.16	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) UP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.07	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) LP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.39	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) LR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.76	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) UF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.90	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) LR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.50	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) LP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.20	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) LF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.79	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) UF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#93.08	benchmark Vietnamese dependency treebank VnDT, UAS	section 23 ( test ) UP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.73	benchmark Vietnamese dependency treebank VnDT, UAS	section 00 ( dev ) LF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.76	Penn Treebank, F1	section 23 ( test ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.63	Penn Treebank, F1	section 00 ( dev ) UP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.16	Penn Treebank, F1	section 00 ( dev ) LR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.93	Penn Treebank, F1	section 00 ( dev ) LP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.06	Penn Treebank, F1	section 23 ( test ) UF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#92.86	Penn Treebank, F1	section 23 ( test ) UR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.66	Penn Treebank, F1	section 23 ( test ) UP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.34	Penn Treebank, F1	section 00 ( dev ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.34	Penn Treebank, F1	section 00 ( dev ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.51	Penn Treebank, F1	section 23 ( test ) LR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#92.80	Penn Treebank, F1	section 00 ( dev ) UR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#88.34	Penn Treebank, F1	section 23 ( test ) LP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.05	Penn Treebank, F1	section 00 ( dev ) UF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#94.39	Penn Treebank, F1	section 00 ( dev ) AST ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.12	Penn Treebank, F1	section 00 ( dev ) Reverse LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#87.67	Penn Treebank, F1	section 00 ( dev ) AST LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.28	Penn Treebank, F1	section 23 ( test ) AST UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.46	Penn Treebank, F1	section 00 ( dev ) Reverse ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.50	Penn Treebank, F1	section 23 ( test ) AST ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.58	Penn Treebank, F1	section 23 ( test ) Reverse LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.10	Penn Treebank, F1	section 23 ( test ) AST LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.23	Penn Treebank, F1	section 00 ( dev ) AST UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.53	Penn Treebank, F1	section 23 ( test ) Reverse ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.57	Penn Treebank, F1	section 23 ( test ) Reverse UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.52	Penn Treebank, F1	section 00 ( dev ) Reverse UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.48	Penn Treebank, F1	section 23 ( test ) AST UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.98	Penn Treebank, F1	section 23 ( test ) Reverse UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.88	Penn Treebank, F1	section 00 ( dev ) Reverse UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.71	Penn Treebank, F1	section 23 ( test ) AST ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.40	Penn Treebank, F1	section 00 ( dev ) AST UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#88.47	Penn Treebank, F1	section 23 ( test ) AST LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#95.01	Penn Treebank, F1	section 23 ( test ) Reverse ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#87.90	Penn Treebank, F1	section 00 ( dev ) AST LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.52	Penn Treebank, F1	section 00 ( dev ) AST ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#89.25	Penn Treebank, F1	section 23 ( test ) Reverse LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.79	Penn Treebank, F1	section 00 ( dev ) Reverse ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#88.58	Penn Treebank, F1	section 00 ( dev ) Reverse LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#92.44	Penn Treebank, F1	section 23 ( test ) UR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.43	Penn Treebank, F1	section 00 ( dev ) UR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#93.16	Penn Treebank, F1	section 00 ( dev ) UP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.07	Penn Treebank, F1	section 00 ( dev ) LP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.39	Penn Treebank, F1	section 00 ( dev ) LR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.76	Penn Treebank, F1	section 23 ( test ) UF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.90	Penn Treebank, F1	section 23 ( test ) LR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.50	Penn Treebank, F1	section 23 ( test ) LP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.20	Penn Treebank, F1	section 23 ( test ) LF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.79	Penn Treebank, F1	section 00 ( dev ) UF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#93.08	Penn Treebank, F1	section 23 ( test ) UP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.73	Penn Treebank, F1	section 00 ( dev ) LF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.76	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.63	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) UP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.16	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) LR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.93	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) LP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.06	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) UF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#92.86	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) UR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.66	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) UP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.34	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.34	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.51	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) LR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#92.80	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) UR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#88.34	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) LP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.05	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) UF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#94.39	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) AST ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.12	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) Reverse LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#87.67	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) AST LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.28	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) AST UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.46	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) Reverse ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.50	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) AST ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.58	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) Reverse LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.10	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) AST LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.23	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) AST UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.53	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) Reverse ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.57	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) Reverse UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.52	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) Reverse UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.48	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) AST UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.98	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) Reverse UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.88	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) Reverse UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.71	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) AST ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.40	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) AST UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#88.47	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) AST LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#95.01	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) Reverse ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#87.90	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) AST LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.52	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) AST ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#89.25	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) Reverse LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.79	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) Reverse ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#88.58	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) Reverse LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#92.44	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) UR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.43	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) UR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#93.16	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) UP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.07	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) LP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.39	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) LR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.76	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) UF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.90	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) LR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.50	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) LP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.20	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) LF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.79	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) UF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#93.08	benchmark Vietnamese dependency treebank VnDT, LAS	section 23 ( test ) UP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.73	benchmark Vietnamese dependency treebank VnDT, LAS	section 00 ( dev ) LF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.76	Penn Treebank, LAS	section 23 ( test ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.63	Penn Treebank, LAS	section 00 ( dev ) UP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.16	Penn Treebank, LAS	section 00 ( dev ) LR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.93	Penn Treebank, LAS	section 00 ( dev ) LP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.06	Penn Treebank, LAS	section 23 ( test ) UF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#92.86	Penn Treebank, LAS	section 23 ( test ) UR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.66	Penn Treebank, LAS	section 23 ( test ) UP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.34	Penn Treebank, LAS	section 00 ( dev ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.34	Penn Treebank, LAS	section 00 ( dev ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.51	Penn Treebank, LAS	section 23 ( test ) LR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#92.80	Penn Treebank, LAS	section 00 ( dev ) UR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#88.34	Penn Treebank, LAS	section 23 ( test ) LP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.05	Penn Treebank, LAS	section 00 ( dev ) UF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#94.39	Penn Treebank, LAS	section 00 ( dev ) AST ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.12	Penn Treebank, LAS	section 00 ( dev ) Reverse LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#87.67	Penn Treebank, LAS	section 00 ( dev ) AST LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.28	Penn Treebank, LAS	section 23 ( test ) AST UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.46	Penn Treebank, LAS	section 00 ( dev ) Reverse ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.50	Penn Treebank, LAS	section 23 ( test ) AST ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.58	Penn Treebank, LAS	section 23 ( test ) Reverse LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.10	Penn Treebank, LAS	section 23 ( test ) AST LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.23	Penn Treebank, LAS	section 00 ( dev ) AST UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.53	Penn Treebank, LAS	section 23 ( test ) Reverse ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.57	Penn Treebank, LAS	section 23 ( test ) Reverse UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.52	Penn Treebank, LAS	section 00 ( dev ) Reverse UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.48	Penn Treebank, LAS	section 23 ( test ) AST UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.98	Penn Treebank, LAS	section 23 ( test ) Reverse UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.88	Penn Treebank, LAS	section 00 ( dev ) Reverse UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.71	Penn Treebank, LAS	section 23 ( test ) AST ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.40	Penn Treebank, LAS	section 00 ( dev ) AST UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#88.47	Penn Treebank, LAS	section 23 ( test ) AST LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#95.01	Penn Treebank, LAS	section 23 ( test ) Reverse ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#87.90	Penn Treebank, LAS	section 00 ( dev ) AST LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.52	Penn Treebank, LAS	section 00 ( dev ) AST ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#89.25	Penn Treebank, LAS	section 23 ( test ) Reverse LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.79	Penn Treebank, LAS	section 00 ( dev ) Reverse ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#88.58	Penn Treebank, LAS	section 00 ( dev ) Reverse LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#92.44	Penn Treebank, LAS	section 23 ( test ) UR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.43	Penn Treebank, LAS	section 00 ( dev ) UR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#93.16	Penn Treebank, LAS	section 00 ( dev ) UP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.07	Penn Treebank, LAS	section 00 ( dev ) LP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.39	Penn Treebank, LAS	section 00 ( dev ) LR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.76	Penn Treebank, LAS	section 23 ( test ) UF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.90	Penn Treebank, LAS	section 23 ( test ) LR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.50	Penn Treebank, LAS	section 23 ( test ) LP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.20	Penn Treebank, LAS	section 23 ( test ) LF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.79	Penn Treebank, LAS	section 00 ( dev ) UF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#93.08	Penn Treebank, LAS	section 23 ( test ) UP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.73	Penn Treebank, LAS	section 00 ( dev ) LF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.76	Penn Treebank, Accuracy	section 23 ( test ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.63	Penn Treebank, Accuracy	section 00 ( dev ) UP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.16	Penn Treebank, Accuracy	section 00 ( dev ) LR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.93	Penn Treebank, Accuracy	section 00 ( dev ) LP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.06	Penn Treebank, Accuracy	section 23 ( test ) UF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#92.86	Penn Treebank, Accuracy	section 23 ( test ) UR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.66	Penn Treebank, Accuracy	section 23 ( test ) UP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.34	Penn Treebank, Accuracy	section 00 ( dev ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.34	Penn Treebank, Accuracy	section 00 ( dev ) LF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#87.51	Penn Treebank, Accuracy	section 23 ( test ) LR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#92.80	Penn Treebank, Accuracy	section 00 ( dev ) UR  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#88.34	Penn Treebank, Accuracy	section 23 ( test ) LP  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#93.05	Penn Treebank, Accuracy	section 00 ( dev ) UF  Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable  precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).  Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
true	D11-1031.pdf#94.39	Penn Treebank, Accuracy	section 00 ( dev ) AST ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.12	Penn Treebank, Accuracy	section 00 ( dev ) Reverse LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#87.67	Penn Treebank, Accuracy	section 00 ( dev ) AST LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.28	Penn Treebank, Accuracy	section 23 ( test ) AST UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.46	Penn Treebank, Accuracy	section 00 ( dev ) Reverse ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.50	Penn Treebank, Accuracy	section 23 ( test ) AST ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.58	Penn Treebank, Accuracy	section 23 ( test ) Reverse LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#88.10	Penn Treebank, Accuracy	section 23 ( test ) AST LF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.23	Penn Treebank, Accuracy	section 00 ( dev ) AST UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#94.53	Penn Treebank, Accuracy	section 23 ( test ) Reverse ST  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.57	Penn Treebank, Accuracy	section 23 ( test ) Reverse UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.52	Penn Treebank, Accuracy	section 00 ( dev ) Reverse UF  Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and  unlabelled F-measure (LF/UF) and supertag accuracy (ST).
true	D11-1031.pdf#93.48	Penn Treebank, Accuracy	section 23 ( test ) AST UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.98	Penn Treebank, Accuracy	section 23 ( test ) Reverse UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.88	Penn Treebank, Accuracy	section 00 ( dev ) Reverse UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.71	Penn Treebank, Accuracy	section 23 ( test ) AST ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#93.40	Penn Treebank, Accuracy	section 00 ( dev ) AST UF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#88.47	Penn Treebank, Accuracy	section 23 ( test ) AST LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#95.01	Penn Treebank, Accuracy	section 23 ( test ) Reverse ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#87.90	Penn Treebank, Accuracy	section 00 ( dev ) AST LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.52	Penn Treebank, Accuracy	section 00 ( dev ) AST ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#89.25	Penn Treebank, Accuracy	section 23 ( test ) Reverse LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#94.79	Penn Treebank, Accuracy	section 00 ( dev ) Reverse ST  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#88.58	Penn Treebank, Accuracy	section 00 ( dev ) Reverse LF  Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as  parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
true	D11-1031.pdf#92.44	Penn Treebank, Accuracy	section 23 ( test ) UR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.43	Penn Treebank, Accuracy	section 00 ( dev ) UR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#93.16	Penn Treebank, Accuracy	section 00 ( dev ) UP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.07	Penn Treebank, Accuracy	section 00 ( dev ) LP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.39	Penn Treebank, Accuracy	section 00 ( dev ) LR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.76	Penn Treebank, Accuracy	section 23 ( test ) UF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.90	Penn Treebank, Accuracy	section 23 ( test ) LR  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.50	Penn Treebank, Accuracy	section 23 ( test ) LP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#87.20	Penn Treebank, Accuracy	section 23 ( test ) LF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#92.79	Penn Treebank, Accuracy	section 00 ( dev ) UF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#93.08	Penn Treebank, Accuracy	section 23 ( test ) UP  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D11-1031.pdf#86.73	Penn Treebank, Accuracy	section 00 ( dev ) LF  Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
true	D15-1280.pdf#10	IMDb, Accuracy	100 SST - 1 100  Table 2: Hyper-parameter settings for the LSTM  and MT-LSTM.
true	D15-1280.pdf#10	IMDb, Accuracy	100 SST - 2 100  Table 2: Hyper-parameter settings for the LSTM  and MT-LSTM.
true	D15-1280.pdf#10−5	IMDb, Accuracy	100 IMDB 100  Table 2: Hyper-parameter settings for the LSTM  and MT-LSTM.
true	D15-1280.pdf#10	IMDb, Accuracy	100 QC 100  Table 2: Hyper-parameter settings for the LSTM  and MT-LSTM.
true	D15-1280.pdf#94.4	IMDb, Accuracy	- QC - - -  Table 3: Results of our MT-LSTM model against state-of-the-art neural models. All the results without  marks are reported in the corresponding paper.
true	D15-1280.pdf#88.1	IMDb, Accuracy	- SST - 2 - - -  Table 3: Results of our MT-LSTM model against state-of-the-art neural models. All the results without  marks are reported in the corresponding paper.
true	D15-1280.pdf#92.1	IMDb, Accuracy	- IMDB - - -  Table 3: Results of our MT-LSTM model against state-of-the-art neural models. All the results without  marks are reported in the corresponding paper.
true	D15-1280.pdf#49.1	IMDb, Accuracy	- SST - 1 - - -  Table 3: Results of our MT-LSTM model against state-of-the-art neural models. All the results without  marks are reported in the corresponding paper.
true	P13-2110.pdf#83.42	Chinese Treebank 6, F1	F  Table 4: Lattice-based framework evaluation.
true	P13-2110.pdf#94.40	Chinese Treebank 6, F1	F  Table 4: Lattice-based framework evaluation.
true	D14-1029.pdf#+0.4	Text8, Number of params	MT08 Web Bleu  Table 3: Results.
true	D14-1029.pdf#-0.8	Text8, Number of params	MT08 Newswire Ter − Bleu 2  Table 3: Results.
true	D14-1029.pdf#+0.3	Text8, Number of params	GALE Web Bleu  Table 3: Results.
true	D14-1029.pdf#-1.0	Text8, Number of params	GALE Web Ter  Table 3: Results.
true	D14-1029.pdf#-1.0	Text8, Number of params	MT08 Web Ter − Bleu 2  Table 3: Results.
true	D14-1029.pdf#+1.1	Text8, Number of params	MT08 Newswire Bleu  Table 3: Results.
true	D14-1029.pdf#-1.3	Text8, Number of params	MT08 Newswire Ter  Table 3: Results.
true	D14-1029.pdf#-0.6	Text8, Number of params	GALE Web Ter − Bleu 2  Table 3: Results.
true	D14-1029.pdf#-1.6	Text8, Number of params	MT08 Web Ter  Table 3: Results.
true	D14-1046.pdf#0.47(0.94)	SemEval 2013, F1	R  Table 4: Results for WSA of nouns with domain filtering.
true	D14-1046.pdf#0.77(0.67)	SemEval 2013, F1	P  Table 4: Results for WSA of nouns with domain filtering.
true	D14-1046.pdf#0.51(na)	SemEval 2013, F1	F1  Table 4: Results for WSA of nouns with domain filtering.
true	D14-1046.pdf#0.87(0.77)	SemEval 2013, F1	P  Table 4: Results for WSA of nouns with domain filtering.
true	D14-1046.pdf#0.47(0.94)	SemEval 2015, F1	R  Table 4: Results for WSA of nouns with domain filtering.
true	D14-1046.pdf#0.77(0.67)	SemEval 2015, F1	P  Table 4: Results for WSA of nouns with domain filtering.
true	D14-1046.pdf#0.51(na)	SemEval 2015, F1	F1  Table 4: Results for WSA of nouns with domain filtering.
true	D14-1046.pdf#0.87(0.77)	SemEval 2015, F1	P  Table 4: Results for WSA of nouns with domain filtering.
true	D14-1046.pdf#0.47(0.94)	SemEval 2007, F1	R  Table 4: Results for WSA of nouns with domain filtering.
true	D14-1046.pdf#0.77(0.67)	SemEval 2007, F1	P  Table 4: Results for WSA of nouns with domain filtering.
true	D14-1046.pdf#0.51(na)	SemEval 2007, F1	F1  Table 4: Results for WSA of nouns with domain filtering.
true	D14-1046.pdf#0.87(0.77)	SemEval 2007, F1	P  Table 4: Results for WSA of nouns with domain filtering.
true	D14-1046.pdf#0.47(0.94)	Senseval 3, F1	R  Table 4: Results for WSA of nouns with domain filtering.
true	D14-1046.pdf#0.77(0.67)	Senseval 3, F1	P  Table 4: Results for WSA of nouns with domain filtering.
true	D14-1046.pdf#0.51(na)	Senseval 3, F1	F1  Table 4: Results for WSA of nouns with domain filtering.
true	D14-1046.pdf#0.87(0.77)	Senseval 3, F1	P  Table 4: Results for WSA of nouns with domain filtering.
true	D14-1046.pdf#0.47(0.94)	Senseval 2, F1	R  Table 4: Results for WSA of nouns with domain filtering.
true	D14-1046.pdf#0.77(0.67)	Senseval 2, F1	P  Table 4: Results for WSA of nouns with domain filtering.
true	D14-1046.pdf#0.51(na)	Senseval 2, F1	F1  Table 4: Results for WSA of nouns with domain filtering.
true	D14-1046.pdf#0.87(0.77)	Senseval 2, F1	P  Table 4: Results for WSA of nouns with domain filtering.
true	P15-2143.pdf#91.3	VLSP 2013 POS tagging shared task, Accuracy	Labeling O  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#58.7	VLSP 2013 POS tagging shared task, Accuracy	Tagging VM  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#81.2	VLSP 2013 POS tagging shared task, Accuracy	Labeling V  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#10.5	VLSP 2013 POS tagging shared task, Accuracy	Parsing LF1  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#82.7	VLSP 2013 POS tagging shared task, Accuracy	Labeling O  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#26.6	VLSP 2013 POS tagging shared task, Accuracy	Parsing LF1  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#50.1	VLSP 2013 POS tagging shared task, Accuracy	Tagging VM  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#82.6	VLSP 2013 POS tagging shared task, Accuracy	Labeling O  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#66.0	VLSP 2013 POS tagging shared task, Accuracy	Tagging M - 1  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#66.8	VLSP 2013 POS tagging shared task, Accuracy	Tagging M - 1  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#85.6	VLSP 2013 POS tagging shared task, Accuracy	Labeling N  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#94.4	VLSP 2013 POS tagging shared task, Accuracy	Labeling N  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#48.7	VLSP 2013 POS tagging shared task, Accuracy	Labeling V  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#19.2	VLSP 2013 POS tagging shared task, Accuracy	ST  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#39.9	VLSP 2013 POS tagging shared task, Accuracy	This  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#44.4	VLSP 2013 POS tagging shared task, Accuracy	ST  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#31.9	VLSP 2013 POS tagging shared task, Accuracy	ST  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#34.2	VLSP 2013 POS tagging shared task, Accuracy	This  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#43.8	VLSP 2013 POS tagging shared task, Accuracy	This  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#33.2	VLSP 2013 POS tagging shared task, Accuracy	ST  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#55.5	VLSP 2013 POS tagging shared task, Accuracy	This  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#45.8	VLSP 2013 POS tagging shared task, Accuracy	This  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#47.2	VLSP 2013 POS tagging shared task, Accuracy	This  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#31.1	VLSP 2013 POS tagging shared task, Accuracy	This  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#91.3	CCGBank, Accuracy	Labeling O  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#58.7	CCGBank, Accuracy	Tagging VM  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#81.2	CCGBank, Accuracy	Labeling V  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#10.5	CCGBank, Accuracy	Parsing LF1  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#82.7	CCGBank, Accuracy	Labeling O  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#26.6	CCGBank, Accuracy	Parsing LF1  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#50.1	CCGBank, Accuracy	Tagging VM  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#82.6	CCGBank, Accuracy	Labeling O  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#66.0	CCGBank, Accuracy	Tagging M - 1  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#66.8	CCGBank, Accuracy	Tagging M - 1  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#85.6	CCGBank, Accuracy	Labeling N  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#94.4	CCGBank, Accuracy	Labeling N  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#48.7	CCGBank, Accuracy	Labeling V  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#19.2	CCGBank, Accuracy	ST  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#39.9	CCGBank, Accuracy	This  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#44.4	CCGBank, Accuracy	ST  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#31.9	CCGBank, Accuracy	ST  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#34.2	CCGBank, Accuracy	This  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#43.8	CCGBank, Accuracy	This  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#33.2	CCGBank, Accuracy	ST  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#55.5	CCGBank, Accuracy	This  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#45.8	CCGBank, Accuracy	This  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#47.2	CCGBank, Accuracy	This  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#31.1	CCGBank, Accuracy	This  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#91.3	benchmark Vietnamese dependency treebank VnDT, UAS	Labeling O  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#58.7	benchmark Vietnamese dependency treebank VnDT, UAS	Tagging VM  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#81.2	benchmark Vietnamese dependency treebank VnDT, UAS	Labeling V  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#10.5	benchmark Vietnamese dependency treebank VnDT, UAS	Parsing LF1  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#82.7	benchmark Vietnamese dependency treebank VnDT, UAS	Labeling O  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#26.6	benchmark Vietnamese dependency treebank VnDT, UAS	Parsing LF1  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#50.1	benchmark Vietnamese dependency treebank VnDT, UAS	Tagging VM  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#82.6	benchmark Vietnamese dependency treebank VnDT, UAS	Labeling O  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#66.0	benchmark Vietnamese dependency treebank VnDT, UAS	Tagging M - 1  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#66.8	benchmark Vietnamese dependency treebank VnDT, UAS	Tagging M - 1  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#85.6	benchmark Vietnamese dependency treebank VnDT, UAS	Labeling N  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#94.4	benchmark Vietnamese dependency treebank VnDT, UAS	Labeling N  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#48.7	benchmark Vietnamese dependency treebank VnDT, UAS	Labeling V  Table 1: Tagging evaluation (M-1, VM, N/V/O  Recall) and directed labeled CCG-Dependency  performance (LF1) as compared to the use of gold  POS tags (Gold) for three clustering algorithms.
true	P15-2143.pdf#19.2	benchmark Vietnamese dependency treebank VnDT, UAS	ST  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#39.9	benchmark Vietnamese dependency treebank VnDT, UAS	This  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#44.4	benchmark Vietnamese dependency treebank VnDT, UAS	ST  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#31.9	benchmark Vietnamese dependency treebank VnDT, UAS	ST  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#34.2	benchmark Vietnamese dependency treebank VnDT, UAS	This  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#43.8	benchmark Vietnamese dependency treebank VnDT, UAS	This  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#33.2	benchmark Vietnamese dependency treebank VnDT, UAS	ST  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#55.5	benchmark Vietnamese dependency treebank VnDT, UAS	This  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#45.8	benchmark Vietnamese dependency treebank VnDT, UAS	This  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#47.2	benchmark Vietnamese dependency treebank VnDT, UAS	This  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	P15-2143.pdf#31.1	benchmark Vietnamese dependency treebank VnDT, UAS	This  Table 3: Tagging VM and N/V/O Recall along- side Directed Accuracy for our approach and the  best shared task baseline. Additionally, we pro- vide results for length 15 to compare to previ- ously published results ([ST]: Best of the PAS- CAL joint tag/dependency induction shared task  systems; [BH]: Bisk and Hockenmaier (2013).
true	D10-1075.pdf#74.3	SemEval 2013, F1	FE + yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#86.5	SemEval 2013, F1	S+T yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#83.3	SemEval 2013, F1	FE + yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#83.3	SemEval 2013, F1	S+T yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#80.1	SemEval 2013, F1	FE + yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#86.5	SemEval 2013, F1	T . F1  Table 3: Comparisons between different NER systems. P.F1  and T.F1 represent the phrase-level and token-level F1 score,  respectively. We use "Cluster?" to indicate if cluster features  are used and use "TGT?" to indicate if target labeled data is  used. Previous systems often only use one class of adaptation  algorithms. Using both adaptation aspects makes our system  perform significantly better than FM09 and RR09.
true	D10-1075.pdf#84.1	SemEval 2013, F1	P . F1  Table 3: Comparisons between different NER systems. P.F1  and T.F1 represent the phrase-level and token-level F1 score,  respectively. We use "Cluster?" to indicate if cluster features  are used and use "TGT?" to indicate if target labeled data is  used. Previous systems often only use one class of adaptation  algorithms. Using both adaptation aspects makes our system  perform significantly better than FM09 and RR09.
true	D10-1075.pdf#51.3	SemEval 2013, F1	FE + yes Accuracy  Table 4: Preposition Sense Disambiguation. We mark the best  accuracy in a row using the bold font and underline the runner  up. Note that both adding cluster features and adding source la- beled data help the performance significantly. Moreover, adding  clusters also changes the behavior of the labeled adaptation al- gorithms.
true	D10-1075.pdf#52.0	SemEval 2013, F1	S+T yes Accuracy  Table 4: Preposition Sense Disambiguation. We mark the best  accuracy in a row using the bold font and underline the runner  up. Note that both adding cluster features and adding source la- beled data help the performance significantly. Moreover, adding  clusters also changes the behavior of the labeled adaptation al- gorithms.
true	D10-1075.pdf#62.1	SemEval 2013, F1	S+T yes Accuracy  Table 4: Preposition Sense Disambiguation. We mark the best  accuracy in a row using the bold font and underline the runner  up. Note that both adding cluster features and adding source la- beled data help the performance significantly. Moreover, adding  clusters also changes the behavior of the labeled adaptation al- gorithms.
true	D10-1075.pdf#61.2	SemEval 2013, F1	S+T yes Accuracy  Table 4: Preposition Sense Disambiguation. We mark the best  accuracy in a row using the bold font and underline the runner  up. Note that both adding cluster features and adding source la- beled data help the performance significantly. Moreover, adding  clusters also changes the behavior of the labeled adaptation al- gorithms.
true	D10-1075.pdf#74.3	SemEval 2007, F1	FE + yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#86.5	SemEval 2007, F1	S+T yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#83.3	SemEval 2007, F1	FE + yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#83.3	SemEval 2007, F1	S+T yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#80.1	SemEval 2007, F1	FE + yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#86.5	SemEval 2007, F1	T . F1  Table 3: Comparisons between different NER systems. P.F1  and T.F1 represent the phrase-level and token-level F1 score,  respectively. We use "Cluster?" to indicate if cluster features  are used and use "TGT?" to indicate if target labeled data is  used. Previous systems often only use one class of adaptation  algorithms. Using both adaptation aspects makes our system  perform significantly better than FM09 and RR09.
true	D10-1075.pdf#84.1	SemEval 2007, F1	P . F1  Table 3: Comparisons between different NER systems. P.F1  and T.F1 represent the phrase-level and token-level F1 score,  respectively. We use "Cluster?" to indicate if cluster features  are used and use "TGT?" to indicate if target labeled data is  used. Previous systems often only use one class of adaptation  algorithms. Using both adaptation aspects makes our system  perform significantly better than FM09 and RR09.
true	D10-1075.pdf#51.3	SemEval 2007, F1	FE + yes Accuracy  Table 4: Preposition Sense Disambiguation. We mark the best  accuracy in a row using the bold font and underline the runner  up. Note that both adding cluster features and adding source la- beled data help the performance significantly. Moreover, adding  clusters also changes the behavior of the labeled adaptation al- gorithms.
true	D10-1075.pdf#52.0	SemEval 2007, F1	S+T yes Accuracy  Table 4: Preposition Sense Disambiguation. We mark the best  accuracy in a row using the bold font and underline the runner  up. Note that both adding cluster features and adding source la- beled data help the performance significantly. Moreover, adding  clusters also changes the behavior of the labeled adaptation al- gorithms.
true	D10-1075.pdf#62.1	SemEval 2007, F1	S+T yes Accuracy  Table 4: Preposition Sense Disambiguation. We mark the best  accuracy in a row using the bold font and underline the runner  up. Note that both adding cluster features and adding source la- beled data help the performance significantly. Moreover, adding  clusters also changes the behavior of the labeled adaptation al- gorithms.
true	D10-1075.pdf#61.2	SemEval 2007, F1	S+T yes Accuracy  Table 4: Preposition Sense Disambiguation. We mark the best  accuracy in a row using the bold font and underline the runner  up. Note that both adding cluster features and adding source la- beled data help the performance significantly. Moreover, adding  clusters also changes the behavior of the labeled adaptation al- gorithms.
true	D10-1075.pdf#74.3	SemEval 2015, F1	FE + yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#86.5	SemEval 2015, F1	S+T yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#83.3	SemEval 2015, F1	FE + yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#83.3	SemEval 2015, F1	S+T yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#80.1	SemEval 2015, F1	FE + yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#86.5	SemEval 2015, F1	T . F1  Table 3: Comparisons between different NER systems. P.F1  and T.F1 represent the phrase-level and token-level F1 score,  respectively. We use "Cluster?" to indicate if cluster features  are used and use "TGT?" to indicate if target labeled data is  used. Previous systems often only use one class of adaptation  algorithms. Using both adaptation aspects makes our system  perform significantly better than FM09 and RR09.
true	D10-1075.pdf#84.1	SemEval 2015, F1	P . F1  Table 3: Comparisons between different NER systems. P.F1  and T.F1 represent the phrase-level and token-level F1 score,  respectively. We use "Cluster?" to indicate if cluster features  are used and use "TGT?" to indicate if target labeled data is  used. Previous systems often only use one class of adaptation  algorithms. Using both adaptation aspects makes our system  perform significantly better than FM09 and RR09.
true	D10-1075.pdf#51.3	SemEval 2015, F1	FE + yes Accuracy  Table 4: Preposition Sense Disambiguation. We mark the best  accuracy in a row using the bold font and underline the runner  up. Note that both adding cluster features and adding source la- beled data help the performance significantly. Moreover, adding  clusters also changes the behavior of the labeled adaptation al- gorithms.
true	D10-1075.pdf#52.0	SemEval 2015, F1	S+T yes Accuracy  Table 4: Preposition Sense Disambiguation. We mark the best  accuracy in a row using the bold font and underline the runner  up. Note that both adding cluster features and adding source la- beled data help the performance significantly. Moreover, adding  clusters also changes the behavior of the labeled adaptation al- gorithms.
true	D10-1075.pdf#62.1	SemEval 2015, F1	S+T yes Accuracy  Table 4: Preposition Sense Disambiguation. We mark the best  accuracy in a row using the bold font and underline the runner  up. Note that both adding cluster features and adding source la- beled data help the performance significantly. Moreover, adding  clusters also changes the behavior of the labeled adaptation al- gorithms.
true	D10-1075.pdf#61.2	SemEval 2015, F1	S+T yes Accuracy  Table 4: Preposition Sense Disambiguation. We mark the best  accuracy in a row using the bold font and underline the runner  up. Note that both adding cluster features and adding source la- beled data help the performance significantly. Moreover, adding  clusters also changes the behavior of the labeled adaptation al- gorithms.
true	D10-1075.pdf#74.3	Senseval 3, F1	FE + yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#86.5	Senseval 3, F1	S+T yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#83.3	Senseval 3, F1	FE + yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#83.3	Senseval 3, F1	S+T yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#80.1	Senseval 3, F1	FE + yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#86.5	Senseval 3, F1	T . F1  Table 3: Comparisons between different NER systems. P.F1  and T.F1 represent the phrase-level and token-level F1 score,  respectively. We use "Cluster?" to indicate if cluster features  are used and use "TGT?" to indicate if target labeled data is  used. Previous systems often only use one class of adaptation  algorithms. Using both adaptation aspects makes our system  perform significantly better than FM09 and RR09.
true	D10-1075.pdf#84.1	Senseval 3, F1	P . F1  Table 3: Comparisons between different NER systems. P.F1  and T.F1 represent the phrase-level and token-level F1 score,  respectively. We use "Cluster?" to indicate if cluster features  are used and use "TGT?" to indicate if target labeled data is  used. Previous systems often only use one class of adaptation  algorithms. Using both adaptation aspects makes our system  perform significantly better than FM09 and RR09.
true	D10-1075.pdf#51.3	Senseval 3, F1	FE + yes Accuracy  Table 4: Preposition Sense Disambiguation. We mark the best  accuracy in a row using the bold font and underline the runner  up. Note that both adding cluster features and adding source la- beled data help the performance significantly. Moreover, adding  clusters also changes the behavior of the labeled adaptation al- gorithms.
true	D10-1075.pdf#52.0	Senseval 3, F1	S+T yes Accuracy  Table 4: Preposition Sense Disambiguation. We mark the best  accuracy in a row using the bold font and underline the runner  up. Note that both adding cluster features and adding source la- beled data help the performance significantly. Moreover, adding  clusters also changes the behavior of the labeled adaptation al- gorithms.
true	D10-1075.pdf#62.1	Senseval 3, F1	S+T yes Accuracy  Table 4: Preposition Sense Disambiguation. We mark the best  accuracy in a row using the bold font and underline the runner  up. Note that both adding cluster features and adding source la- beled data help the performance significantly. Moreover, adding  clusters also changes the behavior of the labeled adaptation al- gorithms.
true	D10-1075.pdf#61.2	Senseval 3, F1	S+T yes Accuracy  Table 4: Preposition Sense Disambiguation. We mark the best  accuracy in a row using the bold font and underline the runner  up. Note that both adding cluster features and adding source la- beled data help the performance significantly. Moreover, adding  clusters also changes the behavior of the labeled adaptation al- gorithms.
true	D10-1075.pdf#74.3	Senseval 2, F1	FE + yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#86.5	Senseval 2, F1	S+T yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#83.3	Senseval 2, F1	FE + yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#83.3	Senseval 2, F1	S+T yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#80.1	Senseval 2, F1	FE + yes Token F1  Table 2: NER Experiments. We bold face the best accuracy  in a row and underline the runner up. Both unlabeled adapta- tion algorithms (adding cluster features) and labeled adaptation  algorithm (using source labeled data) help the performance sig- nificantly. Moreover, adding cluster-like features also changes  the behavior of the labeled adaptation algorithms. Note that  after adding cluster features, S+T becomes quite competitive  with (or slightly better than) the FE + approach. The size of  MUC7 develop set is roughly 20% of the size of the MUC7  training set.
true	D10-1075.pdf#86.5	Senseval 2, F1	T . F1  Table 3: Comparisons between different NER systems. P.F1  and T.F1 represent the phrase-level and token-level F1 score,  respectively. We use "Cluster?" to indicate if cluster features  are used and use "TGT?" to indicate if target labeled data is  used. Previous systems often only use one class of adaptation  algorithms. Using both adaptation aspects makes our system  perform significantly better than FM09 and RR09.
true	D10-1075.pdf#84.1	Senseval 2, F1	P . F1  Table 3: Comparisons between different NER systems. P.F1  and T.F1 represent the phrase-level and token-level F1 score,  respectively. We use "Cluster?" to indicate if cluster features  are used and use "TGT?" to indicate if target labeled data is  used. Previous systems often only use one class of adaptation  algorithms. Using both adaptation aspects makes our system  perform significantly better than FM09 and RR09.
true	D10-1075.pdf#51.3	Senseval 2, F1	FE + yes Accuracy  Table 4: Preposition Sense Disambiguation. We mark the best  accuracy in a row using the bold font and underline the runner  up. Note that both adding cluster features and adding source la- beled data help the performance significantly. Moreover, adding  clusters also changes the behavior of the labeled adaptation al- gorithms.
true	D10-1075.pdf#52.0	Senseval 2, F1	S+T yes Accuracy  Table 4: Preposition Sense Disambiguation. We mark the best  accuracy in a row using the bold font and underline the runner  up. Note that both adding cluster features and adding source la- beled data help the performance significantly. Moreover, adding  clusters also changes the behavior of the labeled adaptation al- gorithms.
true	D10-1075.pdf#62.1	Senseval 2, F1	S+T yes Accuracy  Table 4: Preposition Sense Disambiguation. We mark the best  accuracy in a row using the bold font and underline the runner  up. Note that both adding cluster features and adding source la- beled data help the performance significantly. Moreover, adding  clusters also changes the behavior of the labeled adaptation al- gorithms.
true	D10-1075.pdf#61.2	Senseval 2, F1	S+T yes Accuracy  Table 4: Preposition Sense Disambiguation. We mark the best  accuracy in a row using the bold font and underline the runner  up. Note that both adding cluster features and adding source la- beled data help the performance significantly. Moreover, adding  clusters also changes the behavior of the labeled adaptation al- gorithms.
true	D10-1032.pdf#49.7%	WikiText-103, Test perplexity	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	WikiText-103, Test perplexity	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	CNN / Daily Mail (Non-anonymized version), ROUGE-L	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	CNN / Daily Mail (Non-anonymized version), ROUGE-L	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	SemEval 2015, F1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	SemEval 2015, F1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Hutter Prize, Bit per Character (BPC)	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Hutter Prize, Bit per Character (BPC)	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	CNN / Daily Mail (Anonymized version), ROUGE-2	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	CNN / Daily Mail (Anonymized version), ROUGE-2	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	DBpedia, Error	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	DBpedia, Error	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Gigaword, ROUGE-1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Gigaword, ROUGE-1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	LDC2015E86, Smatch	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	LDC2015E86, Smatch	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	TREC, Error	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	TREC, Error	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	DUC 2004 Task 1, ROUGE-1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	DUC 2004 Task 1, ROUGE-1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Hutter Prize, Number of params	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Hutter Prize, Number of params	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	VLSP 2013 word segmentation shared task, F1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	VLSP 2013 word segmentation shared task, F1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	WN18RR, H@1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	WN18RR, H@1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	CNN / Daily Mail (Anonymized version), ROUGE-1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	CNN / Daily Mail (Anonymized version), ROUGE-1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Penn Treebank, F1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Penn Treebank, F1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	CNN / Daily Mail (Non-anonymized version), ROUGE-2	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	CNN / Daily Mail (Non-anonymized version), ROUGE-2	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Penn Treebank, Accuracy	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Penn Treebank, Accuracy	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	New York Times Corpus, P@10%	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	New York Times Corpus, P@10%	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Senseval 2, F1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Senseval 2, F1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	SemEval 2018, MRR	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	SemEval 2018, MRR	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	WN18RR, MRR	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	WN18RR, MRR	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	DUC 2004 Task 1, ROUGE-L	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	DUC 2004 Task 1, ROUGE-L	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Penn Treebank, Bit per Character (BPC)	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Penn Treebank, Bit per Character (BPC)	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	1B Words / Google Billion Word benchmark, Test perplexity	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	1B Words / Google Billion Word benchmark, Test perplexity	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Penn Treebank, Number of params	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Penn Treebank, Number of params	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Penn Treebank, Test perplexity	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Penn Treebank, Test perplexity	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	SemEval 2018, P@5	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	SemEval 2018, P@5	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	SemEval-2010 Task 8, F1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	SemEval-2010 Task 8, F1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	DUC 2004 Task 1, ROUGE-2	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	DUC 2004 Task 1, ROUGE-2	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	SQuAD, EM	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	SQuAD, EM	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	SemEval 2013, F1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	SemEval 2013, F1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	FB15K-237, H@10	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	FB15K-237, H@10	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Ontonotes v5 (English), F1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Ontonotes v5 (English), F1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	CNN / Daily Mail (Non-anonymized version), ROUGE-1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	CNN / Daily Mail (Non-anonymized version), ROUGE-1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	IMDb, Accuracy	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	IMDb, Accuracy	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Gigaword, ROUGE-2	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Gigaword, ROUGE-2	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	CNN / Daily Mail (Non-anonymized version), METEOR	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	CNN / Daily Mail (Non-anonymized version), METEOR	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Senseval 3, F1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Senseval 3, F1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Penn Treebank, POS	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Penn Treebank, POS	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	SemEval 2018, MAP	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	SemEval 2018, MAP	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Penn Treebank, LAS	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Penn Treebank, LAS	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Gigaword, ROUGE-L	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Gigaword, ROUGE-L	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	SemEval 2007, F1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	SemEval 2007, F1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	PKU, F1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	PKU, F1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	LDC2014T12, F1 on Newswire	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	LDC2014T12, F1 on Newswire	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	CCGBank, Accuracy	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	CCGBank, Accuracy	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	FB15K-237, MRR	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	FB15K-237, MRR	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Penn Treebank, F1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Penn Treebank, F1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	SUBJ, Accuracy	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	SUBJ, Accuracy	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	benchmark Vietnamese dependency treebank VnDT, UAS	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	benchmark Vietnamese dependency treebank VnDT, UAS	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	WN18RR, H@10	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	WN18RR, H@10	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	SearchQA, N-gram F1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	SearchQA, N-gram F1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	WikiText-2, Test perplexity	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	WikiText-2, Test perplexity	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	WikiText-2, Validation perplexity	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	WikiText-2, Validation perplexity	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	WMT 2014 EN-FR, BLEU	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	WMT 2014 EN-FR, BLEU	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	AG News, Error	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	AG News, Error	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Penn Treebank, Validation perplexity	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Penn Treebank, Validation perplexity	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Text8, Number of params	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Text8, Number of params	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	FB15K-237, H@1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	FB15K-237, H@1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	LDC2014T12, F1 on Full	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	LDC2014T12, F1 on Full	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	SearchQA, Unigram Acc	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	SearchQA, Unigram Acc	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	CoNLL 2003 (English), F1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	CoNLL 2003 (English), F1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	VLSP 2013 POS tagging shared task, Accuracy	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	VLSP 2013 POS tagging shared task, Accuracy	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	MSR, F1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	MSR, F1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	benchmark Vietnamese dependency treebank VnDT, LAS	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	benchmark Vietnamese dependency treebank VnDT, LAS	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Quasar, F1 (Quasar-T)	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Quasar, F1 (Quasar-T)	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	CNN / Daily Mail (Anonymized version), ROUGE-L	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	CNN / Daily Mail (Anonymized version), ROUGE-L	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Text8, Bit per Character (BPC)	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Text8, Bit per Character (BPC)	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	SQuAD, F1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	SQuAD, F1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Quasar, EM (Quasar-T)	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Quasar, EM (Quasar-T)	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Chinese Treebank 6, F1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Chinese Treebank 6, F1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	SST-2, Accuracy	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	SST-2, Accuracy	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	Penn Treebank, UAS	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	Penn Treebank, UAS	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	WikiText-2, Number of params	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	WikiText-2, Number of params	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	WMT 2014 EN-DE, BLEU	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	WMT 2014 EN-DE, BLEU	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#49.7%	VLSP 2016 NER shared task, F1	TypeUnknown  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D10-1032.pdf#58.8%	VLSP 2016 NER shared task, F1	TypeClassifier  Table 3: Performance of our algorithm and of the MFS  baseline where at test time ASF type is known (right),  unknown (left) or given by a simple rule-based classifier  (middle). Our algorithm is superior in all three condi- tions.
true	D11-1118.pdf#47.8	benchmark Vietnamese dependency treebank VnDT, LAS	3 . tags from a flat ( Clark , 2000 ) clustering Viable Unsupervised  Table 1: Directed accuracies for the "less is more" DMV,  trained on WSJ15 (after 40 steps of EM) and evaluated  also against WSJ15, using various lexical categories in  place of gold part-of-speech tags. For each tag-set, we  include its effective number of (non-empty) categories in  WSJ15 and the oracle skylines (supervised performance).
true	D11-1118.pdf#78.0	benchmark Vietnamese dependency treebank VnDT, LAS	Viable Sky  Table 1: Directed accuracies for the "less is more" DMV,  trained on WSJ15 (after 40 steps of EM) and evaluated  also against WSJ15, using various lexical categories in  place of gold part-of-speech tags. For each tag-set, we  include its effective number of (non-empty) categories in  WSJ15 and the oracle skylines (supervised performance).
true	D11-1118.pdf#78.4	benchmark Vietnamese dependency treebank VnDT, LAS	160 Viable Sky  Table 1: Directed accuracies for the "less is more" DMV,  trained on WSJ15 (after 40 steps of EM) and evaluated  also against WSJ15, using various lexical categories in  place of gold part-of-speech tags. For each tag-set, we  include its effective number of (non-empty) categories in  WSJ15 and the oracle skylines (supervised performance).
true	D11-1118.pdf#48.0	benchmark Vietnamese dependency treebank VnDT, LAS	96 Viable Unsupervised  Table 1: Directed accuracies for the "less is more" DMV,  trained on WSJ15 (after 40 steps of EM) and evaluated  also against WSJ15, using various lexical categories in  place of gold part-of-speech tags. For each tag-set, we  include its effective number of (non-empty) categories in  WSJ15 and the oracle skylines (supervised performance).
true	D11-1118.pdf#82.3	benchmark Vietnamese dependency treebank VnDT, LAS	165 Viable Sky  Table 1: Directed accuracies for the "less is more" DMV,  trained on WSJ15 (after 40 steps of EM) and evaluated  also against WSJ15, using various lexical categories in  place of gold part-of-speech tags. For each tag-set, we  include its effective number of (non-empty) categories in  WSJ15 and the oracle skylines (supervised performance).
true	D11-1118.pdf#83.8	benchmark Vietnamese dependency treebank VnDT, LAS	3 . tags from a flat ( Clark , 2000 ) clustering Viable Sky  Table 1: Directed accuracies for the "less is more" DMV,  trained on WSJ15 (after 40 steps of EM) and evaluated  also against WSJ15, using various lexical categories in  place of gold part-of-speech tags. For each tag-set, we  include its effective number of (non-empty) categories in  WSJ15 and the oracle skylines (supervised performance).
true	D11-1118.pdf#47.2	benchmark Vietnamese dependency treebank VnDT, LAS	36 Viable Unsupervised  Table 1: Directed accuracies for the "less is more" DMV,  trained on WSJ15 (after 40 steps of EM) and evaluated  also against WSJ15, using various lexical categories in  place of gold part-of-speech tags. For each tag-set, we  include its effective number of (non-empty) categories in  WSJ15 and the oracle skylines (supervised performance).
true	D11-1118.pdf#97.3	benchmark Vietnamese dependency treebank VnDT, LAS	2 . tagless lexicalized models Viable Sky  Table 1: Directed accuracies for the "less is more" DMV,  trained on WSJ15 (after 40 steps of EM) and evaluated  also against WSJ15, using various lexical categories in  place of gold part-of-speech tags. For each tag-set, we  include its effective number of (non-empty) categories in  WSJ15 and the oracle skylines (supervised performance).
true	D11-1118.pdf#30.7	benchmark Vietnamese dependency treebank VnDT, LAS	176 Viable Unsupervised  Table 1: Directed accuracies for the "less is more" DMV,  trained on WSJ15 (after 40 steps of EM) and evaluated  also against WSJ15, using various lexical categories in  place of gold part-of-speech tags. For each tag-set, we  include its effective number of (non-empty) categories in  WSJ15 and the oracle skylines (supervised performance).
true	D11-1118.pdf#24.5	benchmark Vietnamese dependency treebank VnDT, LAS	176 Viable Sky  Table 1: Directed accuracies for the "less is more" DMV,  trained on WSJ15 (after 40 steps of EM) and evaluated  also against WSJ15, using various lexical categories in  place of gold part-of-speech tags. For each tag-set, we  include its effective number of (non-empty) categories in  WSJ15 and the oracle skylines (supervised performance).
true	D11-1118.pdf#1	benchmark Vietnamese dependency treebank VnDT, LAS	20 % 80 mfc 60 gold 40 none 10  Table 1: hierar- chical). Outside of this range, however, performance  can be substantially worse (see
true	D11-1118.pdf#4	benchmark Vietnamese dependency treebank VnDT, LAS	20 % gold mfc 60 gold 40 none 10  Table 1: hierar- chical). Outside of this range, however, performance  can be substantially worse (see
true	D11-1118.pdf#47.8	benchmark Vietnamese dependency treebank VnDT, UAS	3 . tags from a flat ( Clark , 2000 ) clustering Viable Unsupervised  Table 1: Directed accuracies for the "less is more" DMV,  trained on WSJ15 (after 40 steps of EM) and evaluated  also against WSJ15, using various lexical categories in  place of gold part-of-speech tags. For each tag-set, we  include its effective number of (non-empty) categories in  WSJ15 and the oracle skylines (supervised performance).
true	D11-1118.pdf#78.0	benchmark Vietnamese dependency treebank VnDT, UAS	Viable Sky  Table 1: Directed accuracies for the "less is more" DMV,  trained on WSJ15 (after 40 steps of EM) and evaluated  also against WSJ15, using various lexical categories in  place of gold part-of-speech tags. For each tag-set, we  include its effective number of (non-empty) categories in  WSJ15 and the oracle skylines (supervised performance).
true	D11-1118.pdf#78.4	benchmark Vietnamese dependency treebank VnDT, UAS	160 Viable Sky  Table 1: Directed accuracies for the "less is more" DMV,  trained on WSJ15 (after 40 steps of EM) and evaluated  also against WSJ15, using various lexical categories in  place of gold part-of-speech tags. For each tag-set, we  include its effective number of (non-empty) categories in  WSJ15 and the oracle skylines (supervised performance).
true	D11-1118.pdf#48.0	benchmark Vietnamese dependency treebank VnDT, UAS	96 Viable Unsupervised  Table 1: Directed accuracies for the "less is more" DMV,  trained on WSJ15 (after 40 steps of EM) and evaluated  also against WSJ15, using various lexical categories in  place of gold part-of-speech tags. For each tag-set, we  include its effective number of (non-empty) categories in  WSJ15 and the oracle skylines (supervised performance).
true	D11-1118.pdf#82.3	benchmark Vietnamese dependency treebank VnDT, UAS	165 Viable Sky  Table 1: Directed accuracies for the "less is more" DMV,  trained on WSJ15 (after 40 steps of EM) and evaluated  also against WSJ15, using various lexical categories in  place of gold part-of-speech tags. For each tag-set, we  include its effective number of (non-empty) categories in  WSJ15 and the oracle skylines (supervised performance).
true	D11-1118.pdf#83.8	benchmark Vietnamese dependency treebank VnDT, UAS	3 . tags from a flat ( Clark , 2000 ) clustering Viable Sky  Table 1: Directed accuracies for the "less is more" DMV,  trained on WSJ15 (after 40 steps of EM) and evaluated  also against WSJ15, using various lexical categories in  place of gold part-of-speech tags. For each tag-set, we  include its effective number of (non-empty) categories in  WSJ15 and the oracle skylines (supervised performance).
true	D11-1118.pdf#47.2	benchmark Vietnamese dependency treebank VnDT, UAS	36 Viable Unsupervised  Table 1: Directed accuracies for the "less is more" DMV,  trained on WSJ15 (after 40 steps of EM) and evaluated  also against WSJ15, using various lexical categories in  place of gold part-of-speech tags. For each tag-set, we  include its effective number of (non-empty) categories in  WSJ15 and the oracle skylines (supervised performance).
true	D11-1118.pdf#97.3	benchmark Vietnamese dependency treebank VnDT, UAS	2 . tagless lexicalized models Viable Sky  Table 1: Directed accuracies for the "less is more" DMV,  trained on WSJ15 (after 40 steps of EM) and evaluated  also against WSJ15, using various lexical categories in  place of gold part-of-speech tags. For each tag-set, we  include its effective number of (non-empty) categories in  WSJ15 and the oracle skylines (supervised performance).
true	D11-1118.pdf#30.7	benchmark Vietnamese dependency treebank VnDT, UAS	176 Viable Unsupervised  Table 1: Directed accuracies for the "less is more" DMV,  trained on WSJ15 (after 40 steps of EM) and evaluated  also against WSJ15, using various lexical categories in  place of gold part-of-speech tags. For each tag-set, we  include its effective number of (non-empty) categories in  WSJ15 and the oracle skylines (supervised performance).
true	D11-1118.pdf#24.5	benchmark Vietnamese dependency treebank VnDT, UAS	176 Viable Sky  Table 1: Directed accuracies for the "less is more" DMV,  trained on WSJ15 (after 40 steps of EM) and evaluated  also against WSJ15, using various lexical categories in  place of gold part-of-speech tags. For each tag-set, we  include its effective number of (non-empty) categories in  WSJ15 and the oracle skylines (supervised performance).
true	D11-1118.pdf#1	benchmark Vietnamese dependency treebank VnDT, UAS	20 % 80 mfc 60 gold 40 none 10  Table 1: hierar- chical). Outside of this range, however, performance  can be substantially worse (see
true	D11-1118.pdf#4	benchmark Vietnamese dependency treebank VnDT, UAS	20 % gold mfc 60 gold 40 none 10  Table 1: hierar- chical). Outside of this range, however, performance  can be substantially worse (see
true	P14-2048.pdf#79.57	WMT 2014 EN-DE, BLEU	Linguatec  Table 1: Classifier performance, including the R 2 coefficient describing the correlation with BLEU.
true	P14-2048.pdf#0.978	WMT 2014 EN-DE, BLEU	R 2  Table 1: Classifier performance, including the R 2 coefficient describing the correlation with BLEU.
true	P14-2048.pdf#72.02	WMT 2014 EN-DE, BLEU	Moses  Table 1: Classifier performance, including the R 2 coefficient describing the correlation with BLEU.
true	P14-2048.pdf#63.34	WMT 2014 EN-DE, BLEU	Google  Table 1: Classifier performance, including the R 2 coefficient describing the correlation with BLEU.
true	P14-2048.pdf#78.2	WMT 2014 EN-DE, BLEU	ProMT  Table 1: Classifier performance, including the R 2 coefficient describing the correlation with BLEU.
true	P14-2048.pdf#89.36	WMT 2014 EN-DE, BLEU	Trident  Table 1: Classifier performance, including the R 2 coefficient describing the correlation with BLEU.
true	P14-2048.pdf#80.9	WMT 2014 EN-DE, BLEU	Skycode  Table 1: Classifier performance, including the R 2 coefficient describing the correlation with BLEU.
true	P14-2048.pdf#72.36	WMT 2014 EN-DE, BLEU	Systran  Table 1: Classifier performance, including the R 2 coefficient describing the correlation with BLEU.
true	P14-2048.pdf#79.57	WMT 2014 EN-FR, BLEU	Linguatec  Table 1: Classifier performance, including the R 2 coefficient describing the correlation with BLEU.
true	P14-2048.pdf#0.978	WMT 2014 EN-FR, BLEU	R 2  Table 1: Classifier performance, including the R 2 coefficient describing the correlation with BLEU.
true	P14-2048.pdf#72.02	WMT 2014 EN-FR, BLEU	Moses  Table 1: Classifier performance, including the R 2 coefficient describing the correlation with BLEU.
true	P14-2048.pdf#63.34	WMT 2014 EN-FR, BLEU	Google  Table 1: Classifier performance, including the R 2 coefficient describing the correlation with BLEU.
true	P14-2048.pdf#78.2	WMT 2014 EN-FR, BLEU	ProMT  Table 1: Classifier performance, including the R 2 coefficient describing the correlation with BLEU.
true	P14-2048.pdf#89.36	WMT 2014 EN-FR, BLEU	Trident  Table 1: Classifier performance, including the R 2 coefficient describing the correlation with BLEU.
true	P14-2048.pdf#80.9	WMT 2014 EN-FR, BLEU	Skycode  Table 1: Classifier performance, including the R 2 coefficient describing the correlation with BLEU.
true	P14-2048.pdf#72.36	WMT 2014 EN-FR, BLEU	Systran  Table 1: Classifier performance, including the R 2 coefficient describing the correlation with BLEU.
true	P13-2066.pdf#29.01	Chinese Treebank 6, F1	D2  Table 3: Comparison with related models.
true	P13-2066.pdf#31.69	Chinese Treebank 6, F1	D2  Table 3: Comparison with related models.
true	P13-2066.pdf#25.74	Chinese Treebank 6, F1	D2  Table 3: Comparison with related models.
true	P13-2066.pdf#30.63	Chinese Treebank 6, F1	D2  Table 3: Comparison with related models.
true	P13-2066.pdf#25.74	Chinese Treebank 6, F1	D1  Table 3: Comparison with related models.
true	P12-1105.pdf#2	SUBJ, Accuracy	Table 3 : Intra - and inter - dictionaries inconsistency AL 42 63 27  Table 3: Intra-and inter-dictionaries inconsistency  POS OF QW GI QW AL QW UF QW  Noun 23  119 4  61  0  42  90  140  Verb 66  113 2  67  0  0  63  137  Adj. 90  170 8  48  0  0  27  177  Adv. 61  1  0  0  2  0  69  1  Total 240 403 14 176 2  42  249 455
true	P12-1105.pdf#2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 3 : Intra - and inter - dictionaries inconsistency AL 42 63 27  Table 3: Intra-and inter-dictionaries inconsistency  POS OF QW GI QW AL QW UF QW  Noun 23  119 4  61  0  42  90  140  Verb 66  113 2  67  0  0  63  137  Adj. 90  170 8  48  0  0  27  177  Adv. 61  1  0  0  2  0  69  1  Total 240 403 14 176 2  42  249 455
true	P14-1038.pdf#49.5	New York Times Corpus, P@10%	Entity Mention + Relation ( % ) F 1  Table 2: Overall performance on ACE'05 corpus.
true	P14-1038.pdf#80.8	New York Times Corpus, P@10%	Entity Mention ( % ) F 1  Table 2: Overall performance on ACE'05 corpus.
true	P14-1038.pdf#52.1	New York Times Corpus, P@10%	Relation ( % ) F 1  Table 2: Overall performance on ACE'05 corpus.
true	P14-1038.pdf#79.7	New York Times Corpus, P@10%	- Entity Mention ( % ) F 1  Table 3: 5-fold cross-validation on ACE'04 corpus. Bolded scores indicate highly statistical significant  improvement as measured by paired t-test (p < 0.01)
true	P14-1038.pdf#48.3	New York Times Corpus, P@10%	- Relation ( % ) F 1  Table 3: 5-fold cross-validation on ACE'04 corpus. Bolded scores indicate highly statistical significant  improvement as measured by paired t-test (p < 0.01)
true	P14-1038.pdf#45.3	New York Times Corpus, P@10%	- Entity Mention + Relation ( % ) F 1  Table 3: 5-fold cross-validation on ACE'04 corpus. Bolded scores indicate highly statistical significant  improvement as measured by paired t-test (p < 0.01)
true	D15-1169.pdf#74.0	CCGBank, Accuracy	Brown  Table 1: Comparison with the best single-parser  SRL models on PropBank from CoNLL-2008.  The comparison models are Vickrey and Koller  (2008), Che et al. (2008),
true	D15-1169.pdf#69.2	CCGBank, Accuracy	Brown R  Table 1: Comparison with the best single-parser  SRL models on PropBank from CoNLL-2008.  The comparison models are Vickrey and Koller  (2008), Che et al. (2008),
true	D15-1169.pdf#83.5	CCGBank, Accuracy	PropBank F1  Table 1: Comparison with the best single-parser  SRL models on PropBank from CoNLL-2008.  The comparison models are Vickrey and Koller  (2008), Che et al. (2008),
true	D15-1169.pdf#70.2	CCGBank, Accuracy	Brown F1  Table 1: Comparison with the best single-parser  SRL models on PropBank from CoNLL-2008.  The comparison models are Vickrey and Koller  (2008), Che et al. (2008),
true	D15-1169.pdf#82.2	CCGBank, Accuracy	PropBank R  Table 1: Comparison with the best single-parser  SRL models on PropBank from CoNLL-2008.  The comparison models are Vickrey and Koller  (2008), Che et al. (2008),
true	D15-1169.pdf#87.3	CCGBank, Accuracy	PropBank  Table 1: Comparison with the best single-parser  SRL models on PropBank from CoNLL-2008.  The comparison models are Vickrey and Koller  (2008), Che et al. (2008),
true	D15-1169.pdf#38.3	CCGBank, Accuracy	Sentences  Table 2: Parser speed on PropBank Section 23
true	D15-1169.pdf#83.5	CCGBank, Accuracy	PropBank F1  Table 2: Parser speed on PropBank Section 23
true	D15-1169.pdf#83.5	CCGBank, Accuracy	PropBank F1  Table 2: Parser speed on PropBank Section 23
true	D13-1034.pdf#36.4	Quasar, F1 (Quasar-T)	- Hebrew ( HEB ) R3  Table 2: Morpheme lexicon induction quality. F1-scores for lexicons induced from the most probable parse  of each different dataset under each models.  † 42.4 was obtained by taking the union of R3 and T3 items to  match the way the model used them (see  §6.4).
true	D13-1034.pdf#38.6	Quasar, F1 (Quasar-T)	- Hebrew ( HEB ) Pre  Table 2: Morpheme lexicon induction quality. F1-scores for lexicons induced from the most probable parse  of each different dataset under each models.  † 42.4 was obtained by taking the union of R3 and T3 items to  match the way the model used them (see  §6.4).
true	D13-1034.pdf#51.9	Quasar, F1 (Quasar-T)	- Unvocalised Arabic ( BW ) Suf  Table 2: Morpheme lexicon induction quality. F1-scores for lexicons induced from the most probable parse  of each different dataset under each models.  † 42.4 was obtained by taking the union of R3 and T3 items to  match the way the model used them (see  §6.4).
true	D13-1034.pdf#37.8	Quasar, F1 (Quasar-T)	- Vocalised Arabic ( BW ) Pre  Table 2: Morpheme lexicon induction quality. F1-scores for lexicons induced from the most probable parse  of each different dataset under each models.  † 42.4 was obtained by taking the union of R3 and T3 items to  match the way the model used them (see  §6.4).
true	D13-1034.pdf#53.0	Quasar, F1 (Quasar-T)	- Unvocalised Arabic ( BW ) Pre  Table 2: Morpheme lexicon induction quality. F1-scores for lexicons induced from the most probable parse  of each different dataset under each models.  † 42.4 was obtained by taking the union of R3 and T3 items to  match the way the model used them (see  §6.4).
true	D13-1034.pdf#57.7	Quasar, F1 (Quasar-T)	- Unvocalised Arabic ( BW ) Stem  Table 2: Morpheme lexicon induction quality. F1-scores for lexicons induced from the most probable parse  of each different dataset under each models.  † 42.4 was obtained by taking the union of R3 and T3 items to  match the way the model used them (see  §6.4).
true	D13-1034.pdf#56.6	Quasar, F1 (Quasar-T)	- Hebrew ( HEB ) Suf  Table 2: Morpheme lexicon induction quality. F1-scores for lexicons induced from the most probable parse  of each different dataset under each models.  † 42.4 was obtained by taking the union of R3 and T3 items to  match the way the model used them (see  §6.4).
true	D13-1034.pdf#60.3	Quasar, F1 (Quasar-T)	- Vocalised Arabic ( BW ) Stem  Table 2: Morpheme lexicon induction quality. F1-scores for lexicons induced from the most probable parse  of each different dataset under each models.  † 42.4 was obtained by taking the union of R3 and T3 items to  match the way the model used them (see  §6.4).
true	D13-1034.pdf#47.0	Quasar, F1 (Quasar-T)	- Vocalised Arabic ( BW ) Suf  Table 2: Morpheme lexicon induction quality. F1-scores for lexicons induced from the most probable parse  of each different dataset under each models.  † 42.4 was obtained by taking the union of R3 and T3 items to  match the way the model used them (see  §6.4).
true	D13-1034.pdf#62.4	Quasar, F1 (Quasar-T)	- Hebrew ( HEB ) Stem  Table 2: Morpheme lexicon induction quality. F1-scores for lexicons induced from the most probable parse  of each different dataset under each models.  † 42.4 was obtained by taking the union of R3 and T3 items to  match the way the model used them (see  §6.4).
true	D13-1034.pdf#78.14	Quasar, F1 (Quasar-T)	HEB  Table 3: Segmentation quality in SBF1. The QU
true	D13-1034.pdf#73.66	Quasar, F1 (Quasar-T)	BW  Table 3: Segmentation quality in SBF1. The QU
true	D13-1034.pdf#74.54	Quasar, F1 (Quasar-T)	BW  Table 3: Segmentation quality in SBF1. The QU
true	D13-1034.pdf#44.34	Quasar, F1 (Quasar-T)	QU  Table 3: Segmentation quality in SBF1. The QU
true	P14-1125.pdf#91.96	Chinese Treebank 6, F1	POS  Table 3: Development test results of the character- level arc-standard model on CTB60.
true	P14-1125.pdf#77.49	Chinese Treebank 6, F1	DEP  Table 3: Development test results of the character- level arc-standard model on CTB60.
true	P14-1125.pdf#96.10	Chinese Treebank 6, F1	SEG  Table 3: Development test results of the character- level arc-standard model on CTB60.
true	P14-1125.pdf#95.29	Chinese Treebank 6, F1	WS  Table 3: Development test results of the character- level arc-standard model on CTB60.
true	P14-1125.pdf#95.29	Chinese Treebank 6, F1	WS  Table 3: Development test results of the character- level arc-standard model on CTB60.
true	P14-1125.pdf#96.00	Chinese Treebank 6, F1	SEG  Table 4: Development test results of the character- level arc-eager model on CTB60.
true	P14-1125.pdf#92.01	Chinese Treebank 6, F1	POS  Table 4: Development test results of the character- level arc-eager model on CTB60.
true	P14-1125.pdf#96.16	Chinese Treebank 6, F1	SEG  Table 4: Development test results of the character- level arc-eager model on CTB60.
true	P14-1125.pdf#95.62	Chinese Treebank 6, F1	WS  Table 4: Development test results of the character- level arc-eager model on CTB60.
true	P14-1125.pdf#95.49	Chinese Treebank 6, F1	WS  Table 4: Development test results of the character- level arc-eager model on CTB60.
true	P14-1125.pdf#76.94	Chinese Treebank 6, F1	DEP  Table 4: Development test results of the character- level arc-eager model on CTB60.
true	P14-1125.pdf#77.48	Chinese Treebank 6, F1	DEP  Table 4: Development test results of the character- level arc-eager model on CTB60.
true	P14-1125.pdf#91.74	Chinese Treebank 6, F1	POS  Table 4: Development test results of the character- level arc-eager model on CTB60.
true	P14-1125.pdf#97.84	Chinese Treebank 6, F1	- CTB50 SEG  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#75.76‡	Chinese Treebank 6, F1	- CTB70 DEP  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#82.14‡	Chinese Treebank 6, F1	- CTB50 DEP  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#95.71‡	Chinese Treebank 6, F1	- CTB60 SEG  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#95.16	Chinese Treebank 6, F1	- CTB60 WS  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#94.36‡	Chinese Treebank 6, F1	- CTB50 POS  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#95.08	Chinese Treebank 6, F1	- CTB60 WS  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#90.76‡	Chinese Treebank 6, F1	- CTB70 POS  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#95.77‡	Chinese Treebank 6, F1	The arc - standard models CTB60 SEG  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#95.59‡	Chinese Treebank 6, F1	The arc - standard models CTB70 SEG  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#94.36‡	Chinese Treebank 6, F1	- CTB50 POS  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#95.00	Chinese Treebank 6, F1	- CTB70 WS  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#82.07‡	Chinese Treebank 6, F1	- CTB50 DEP  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#91.51‡	Chinese Treebank 6, F1	- CTB60 POS  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#94.97	Chinese Treebank 6, F1	- CTB70 WS  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#77.09‡	Chinese Treebank 6, F1	- CTB60 DEP  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#97.84	Chinese Treebank 6, F1	- CTB50 SEG  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#75.70‡	Chinese Treebank 6, F1	- CTB70 DEP  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#97.40	Chinese Treebank 6, F1	- CTB50 WS  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#94.62‡	Chinese Treebank 6, F1	- CTB50 POS  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#95.50‡	Chinese Treebank 6, F1	The arc - eager models CTB70 SEG  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#76.99‡	Chinese Treebank 6, F1	- CTB60 DEP  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#91.40‡	Chinese Treebank 6, F1	- CTB60 POS  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#97.49	Chinese Treebank 6, F1	- CTB50 WS  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P14-1125.pdf#90.72‡	Chinese Treebank 6, F1	- CTB70 POS  Table 6: Main results, where the results marked with  ‡ denote that the p-value is less than 0.001 compared  with the pipeline word-based models using pairwise t-test.
true	P15-1033.pdf#85.9	Penn Treebank, UAS	Table 1 : English parsing results ( SD ) Test LAS Dev . set  Table 1: English parsing results (SD)
true	P15-1033.pdf#93.1	Penn Treebank, UAS	Test  Table 1: English parsing results (SD)
true	P15-1033.pdf#93.2	Penn Treebank, UAS	Test  Table 1: English parsing results (SD)
true	P15-1033.pdf#85.7	Penn Treebank, UAS	Table 1 : English parsing results ( SD ) Test LAS Test set  Table 1: English parsing results (SD)
true	P15-1033.pdf#90.9	Penn Treebank, UAS	Test  Table 1: English parsing results (SD)
true	P15-1033.pdf#87.2	Penn Treebank, UAS	Table 1 : English parsing results ( SD ) Test UAS Test set  Table 1: English parsing results (SD)
true	P15-1033.pdf#87.2	Penn Treebank, UAS	Table 1 : English parsing results ( SD ) Test UAS Dev . set  Table 1: English parsing results (SD)
true	P15-1033.pdf#90.9	Penn Treebank, UAS	Test  Table 1: English parsing results (SD)
true	P15-1033.pdf#90.9	Penn Treebank, UAS	Test  Table 2: Chinese parsing results (CTB5)
true	P15-1033.pdf#87.2	Penn Treebank, UAS	Table 1 : English parsing results ( SD ) Test UAS Dev . set  Table 2: Chinese parsing results (CTB5)
true	P15-1033.pdf#85.9	Penn Treebank, UAS	Table 1 : English parsing results ( SD ) Test LAS Dev . set  Table 2: Chinese parsing results (CTB5)
true	P15-1033.pdf#90.9	Penn Treebank, UAS	Test  Table 2: Chinese parsing results (CTB5)
true	P15-1033.pdf#85.7	Penn Treebank, UAS	Table 1 : English parsing results ( SD ) Test LAS Test set  Table 2: Chinese parsing results (CTB5)
true	P15-1033.pdf#93.2	Penn Treebank, UAS	Test  Table 2: Chinese parsing results (CTB5)
true	P15-1033.pdf#87.2	Penn Treebank, UAS	Table 1 : English parsing results ( SD ) Test UAS Test set  Table 2: Chinese parsing results (CTB5)
true	P15-1033.pdf#93.1	Penn Treebank, UAS	Test  Table 2: Chinese parsing results (CTB5)
true	P13-1100.pdf#38.5	DUC 2004 Task 1, ROUGE-2	structured representation and dispersion are com -  Table 1: Performance on DUC 2004.
true	P13-1100.pdf#16.2	DUC 2004 Task 1, ROUGE-2	ROUGE - 2  Table 4. We observe a sim- ilar pattern as with multi-document summariza- tion. The full system using all components out- perform all other parameter settings, achieving the  best ROUGE-1 and ROUGE-2 scores. The table  also shows that incorporating dispersion into the  objective function yields an improvement in sum- marization quality (row 4 versus row 5).
true	P13-1100.pdf#41.6	DUC 2004 Task 1, ROUGE-2	ROUGE - 1  Table 4. We observe a sim- ilar pattern as with multi-document summariza- tion. The full system using all components out- perform all other parameter settings, achieving the  best ROUGE-1 and ROUGE-2 scores. The table  also shows that incorporating dispersion into the  objective function yields an improvement in sum- marization quality (row 4 versus row 5).
true	P13-1100.pdf#16.2	DUC 2004 Task 1, ROUGE-2	components ROUGE - 2  Table 4: Performance with different parameters  (comments).
true	P13-1100.pdf#41.6	DUC 2004 Task 1, ROUGE-2	components ROUGE - 1  Table 4: Performance with different parameters  (comments).
true	P13-1100.pdf#38.5	DUC 2004 Task 1, ROUGE-L	structured representation and dispersion are com -  Table 1: Performance on DUC 2004.
true	P13-1100.pdf#16.2	DUC 2004 Task 1, ROUGE-L	ROUGE - 2  Table 4. We observe a sim- ilar pattern as with multi-document summariza- tion. The full system using all components out- perform all other parameter settings, achieving the  best ROUGE-1 and ROUGE-2 scores. The table  also shows that incorporating dispersion into the  objective function yields an improvement in sum- marization quality (row 4 versus row 5).
true	P13-1100.pdf#41.6	DUC 2004 Task 1, ROUGE-L	ROUGE - 1  Table 4. We observe a sim- ilar pattern as with multi-document summariza- tion. The full system using all components out- perform all other parameter settings, achieving the  best ROUGE-1 and ROUGE-2 scores. The table  also shows that incorporating dispersion into the  objective function yields an improvement in sum- marization quality (row 4 versus row 5).
true	P13-1100.pdf#16.2	DUC 2004 Task 1, ROUGE-L	components ROUGE - 2  Table 4: Performance with different parameters  (comments).
true	P13-1100.pdf#41.6	DUC 2004 Task 1, ROUGE-L	components ROUGE - 1  Table 4: Performance with different parameters  (comments).
true	P13-1100.pdf#38.5	DUC 2004 Task 1, ROUGE-1	structured representation and dispersion are com -  Table 1: Performance on DUC 2004.
true	P13-1100.pdf#16.2	DUC 2004 Task 1, ROUGE-1	ROUGE - 2  Table 4. We observe a sim- ilar pattern as with multi-document summariza- tion. The full system using all components out- perform all other parameter settings, achieving the  best ROUGE-1 and ROUGE-2 scores. The table  also shows that incorporating dispersion into the  objective function yields an improvement in sum- marization quality (row 4 versus row 5).
true	P13-1100.pdf#41.6	DUC 2004 Task 1, ROUGE-1	ROUGE - 1  Table 4. We observe a sim- ilar pattern as with multi-document summariza- tion. The full system using all components out- perform all other parameter settings, achieving the  best ROUGE-1 and ROUGE-2 scores. The table  also shows that incorporating dispersion into the  objective function yields an improvement in sum- marization quality (row 4 versus row 5).
true	P13-1100.pdf#16.2	DUC 2004 Task 1, ROUGE-1	components ROUGE - 2  Table 4: Performance with different parameters  (comments).
true	P13-1100.pdf#41.6	DUC 2004 Task 1, ROUGE-1	components ROUGE - 1  Table 4: Performance with different parameters  (comments).
true	P15-1061.pdf#83.7	SemEval-2010 Task 8, F1	Prec .  Table 2: Comparison of different CR-CNN con- figurations.
true	P15-1061.pdf#84.7	SemEval-2010 Task 8, F1	Rec .  Table 2: Comparison of different CR-CNN con- figurations.
true	P15-1061.pdf#84.1	SemEval-2010 Task 8, F1	F1  Table 2: Comparison of different CR-CNN con- figurations.
true	P15-1061.pdf#84.1	SemEval-2010 Task 8, F1	is effective . Table 3 we present the results embedding The two first lines of results F1  Table 3: Impact of not using an embedding for the  artificial class Other.
true	P15-1061.pdf#84.7	SemEval-2010 Task 8, F1	is effective . Table 3 we present the results of The two first lines of results Rec .  Table 3: Impact of not using an embedding for the  artificial class Other.
true	P15-1061.pdf#83.7	SemEval-2010 Task 8, F1	is effective . Table 3 we present the results omission The two first lines of results Prec .  Table 3: Impact of not using an embedding for the  artificial class Other.
true	P15-1061.pdf#84.1	SemEval-2010 Task 8, F1	Table 4: Comparison of results of CR-CNN and  CNN+Softmax.
true	P15-1061.pdf#84.7	SemEval-2010 Task 8, F1	Table 4: Comparison of results of CR-CNN and  CNN+Softmax.
true	P15-1061.pdf#83.7	SemEval-2010 Task 8, F1	Table 4: Comparison of results of CR-CNN and  CNN+Softmax.
true	P15-1061.pdf#84.1	SemEval-2010 Task 8, F1	Google n - gram , paraphrases , TextRunner F1 POS , prefixes , morphological , WordNet , dependency parse ,  Table 5: Comparison with results published in the literature.
true	P12-2056.pdf#22.04	PKU, F1	Table 3 Alignment evaluation . Precision ( P ) , recall ( R ) , CTB  Table 4 Translation evaluation of WordSys and pro-
true	P12-2056.pdf#22.04	PKU, F1	Table 3 Alignment evaluation . Precision ( P ) , recall ( R ) , ICT  Table 4 Translation evaluation of WordSys and pro-
true	P12-2056.pdf#23.05	PKU, F1	S Table 3 Alignment evaluation . Precision ( P ) , recall ( R ) , ICT  Table 4 Translation evaluation of WordSys and pro-
true	P12-2056.pdf#0.5(FraserandMarcu,2007)	PKU, F1	Table 3 Alignment evaluation . Precision ( P ) , recall ( R ) ,  Table 4 Translation evaluation of WordSys and pro-
true	P12-2056.pdf#21.98	PKU, F1	Table 3 Alignment evaluation . Precision ( P ) , recall ( R ) , PKU  Table 4 Translation evaluation of WordSys and pro-
true	P12-2056.pdf#23.51	PKU, F1	S Table 3 Alignment evaluation . Precision ( P ) , recall ( R ) , PKU  Table 4 Translation evaluation of WordSys and pro-
true	P12-2056.pdf#23.41	PKU, F1	S Table 3 Alignment evaluation . Precision ( P ) , recall ( R ) , CTB  Table 4 Translation evaluation of WordSys and pro-
true	P12-2056.pdf#21.98	PKU, F1	PKU  Table 6 Comparison with other works
true	P12-2056.pdf#22.04	PKU, F1	ICT  Table 6 Comparison with other works
true	P12-2056.pdf#22.04	PKU, F1	CTB  Table 6 Comparison with other works
true	P12-2056.pdf#22.46	PKU, F1	CTB  Table 6 Comparison with other works
true	P12-2056.pdf#21.97	PKU, F1	ICT  Table 6 Comparison with other works
true	P12-2056.pdf#21.87	PKU, F1	PKU  Table 6 Comparison with other works
true	P12-2056.pdf#22.04	Chinese Treebank 6, F1	Table 3 Alignment evaluation . Precision ( P ) , recall ( R ) , CTB  Table 4 Translation evaluation of WordSys and pro-
true	P12-2056.pdf#22.04	Chinese Treebank 6, F1	Table 3 Alignment evaluation . Precision ( P ) , recall ( R ) , ICT  Table 4 Translation evaluation of WordSys and pro-
true	P12-2056.pdf#23.05	Chinese Treebank 6, F1	S Table 3 Alignment evaluation . Precision ( P ) , recall ( R ) , ICT  Table 4 Translation evaluation of WordSys and pro-
true	P12-2056.pdf#0.5(FraserandMarcu,2007)	Chinese Treebank 6, F1	Table 3 Alignment evaluation . Precision ( P ) , recall ( R ) ,  Table 4 Translation evaluation of WordSys and pro-
true	P12-2056.pdf#21.98	Chinese Treebank 6, F1	Table 3 Alignment evaluation . Precision ( P ) , recall ( R ) , PKU  Table 4 Translation evaluation of WordSys and pro-
true	P12-2056.pdf#23.51	Chinese Treebank 6, F1	S Table 3 Alignment evaluation . Precision ( P ) , recall ( R ) , PKU  Table 4 Translation evaluation of WordSys and pro-
true	P12-2056.pdf#23.41	Chinese Treebank 6, F1	S Table 3 Alignment evaluation . Precision ( P ) , recall ( R ) , CTB  Table 4 Translation evaluation of WordSys and pro-
true	P12-2056.pdf#21.98	Chinese Treebank 6, F1	PKU  Table 6 Comparison with other works
true	P12-2056.pdf#22.04	Chinese Treebank 6, F1	ICT  Table 6 Comparison with other works
true	P12-2056.pdf#22.04	Chinese Treebank 6, F1	CTB  Table 6 Comparison with other works
true	P12-2056.pdf#22.46	Chinese Treebank 6, F1	CTB  Table 6 Comparison with other works
true	P12-2056.pdf#21.97	Chinese Treebank 6, F1	ICT  Table 6 Comparison with other works
true	P12-2056.pdf#21.87	Chinese Treebank 6, F1	PKU  Table 6 Comparison with other works
true	P14-1130.pdf#90.38	benchmark Vietnamese dependency treebank VnDT, UAS	hyper - parameter γ . γ=0 . 0 8 Average UAS on CoNLL testsets af - Our full model consistently with word vector  Table 3: Results of adding unsupervised word vec- tors to the tensor. Adding this information yields  consistent improvement for all languages.
true	P14-1130.pdf#91.84	benchmark Vietnamese dependency treebank VnDT, UAS	hyper - parameter γ . γ=0 . 0 4 Average UAS on CoNLL testsets af - ter different epochs .  Table 3: Results of adding unsupervised word vec- tors to the tensor. Adding this information yields  consistent improvement for all languages.
true	P14-1130.pdf#89.86	benchmark Vietnamese dependency treebank VnDT, UAS	hyper - parameter γ . γ=0 . 0 4 Average UAS on CoNLL testsets af - ter different epochs . no word vector  Table 3: Results of adding unsupervised word vec- tors to the tensor. Adding this information yields  consistent improvement for all languages.
true	P14-1130.pdf#90.48	benchmark Vietnamese dependency treebank VnDT, UAS	hyper - parameter γ . γ=0 . 0 8 Average UAS on CoNLL testsets af - Our full model consistently with word vector  Table 3: Results of adding unsupervised word vec- tors to the tensor. Adding this information yields  consistent improvement for all languages.
true	P14-1130.pdf#90.24	benchmark Vietnamese dependency treebank VnDT, UAS	hyper - parameter γ . γ=0 . 0 4 Average UAS on CoNLL testsets af - ter different epochs . no word vector  Table 3: Results of adding unsupervised word vec- tors to the tensor. Adding this information yields  consistent improvement for all languages.
true	P14-1130.pdf#92.07	benchmark Vietnamese dependency treebank VnDT, UAS	hyper - parameter γ . γ=0 . 0 8 Average UAS on CoNLL testsets af - Our full model consistently  Table 3: Results of adding unsupervised word vec- tors to the tensor. Adding this information yields  consistent improvement for all languages.
true	P14-1130.pdf#90.38	Penn Treebank, UAS	hyper - parameter γ . γ=0 . 0 8 Average UAS on CoNLL testsets af - Our full model consistently with word vector  Table 3: Results of adding unsupervised word vec- tors to the tensor. Adding this information yields  consistent improvement for all languages.
true	P14-1130.pdf#91.84	Penn Treebank, UAS	hyper - parameter γ . γ=0 . 0 4 Average UAS on CoNLL testsets af - ter different epochs .  Table 3: Results of adding unsupervised word vec- tors to the tensor. Adding this information yields  consistent improvement for all languages.
true	P14-1130.pdf#89.86	Penn Treebank, UAS	hyper - parameter γ . γ=0 . 0 4 Average UAS on CoNLL testsets af - ter different epochs . no word vector  Table 3: Results of adding unsupervised word vec- tors to the tensor. Adding this information yields  consistent improvement for all languages.
true	P14-1130.pdf#90.48	Penn Treebank, UAS	hyper - parameter γ . γ=0 . 0 8 Average UAS on CoNLL testsets af - Our full model consistently with word vector  Table 3: Results of adding unsupervised word vec- tors to the tensor. Adding this information yields  consistent improvement for all languages.
true	P14-1130.pdf#90.24	Penn Treebank, UAS	hyper - parameter γ . γ=0 . 0 4 Average UAS on CoNLL testsets af - ter different epochs . no word vector  Table 3: Results of adding unsupervised word vec- tors to the tensor. Adding this information yields  consistent improvement for all languages.
true	P14-1130.pdf#92.07	Penn Treebank, UAS	hyper - parameter γ . γ=0 . 0 8 Average UAS on CoNLL testsets af - Our full model consistently  Table 3: Results of adding unsupervised word vec- tors to the tensor. Adding this information yields  consistent improvement for all languages.
true	P14-1130.pdf#90.38	Penn Treebank, POS	hyper - parameter γ . γ=0 . 0 8 Average UAS on CoNLL testsets af - Our full model consistently with word vector  Table 3: Results of adding unsupervised word vec- tors to the tensor. Adding this information yields  consistent improvement for all languages.
true	P14-1130.pdf#91.84	Penn Treebank, POS	hyper - parameter γ . γ=0 . 0 4 Average UAS on CoNLL testsets af - ter different epochs .  Table 3: Results of adding unsupervised word vec- tors to the tensor. Adding this information yields  consistent improvement for all languages.
true	P14-1130.pdf#89.86	Penn Treebank, POS	hyper - parameter γ . γ=0 . 0 4 Average UAS on CoNLL testsets af - ter different epochs . no word vector  Table 3: Results of adding unsupervised word vec- tors to the tensor. Adding this information yields  consistent improvement for all languages.
true	P14-1130.pdf#90.48	Penn Treebank, POS	hyper - parameter γ . γ=0 . 0 8 Average UAS on CoNLL testsets af - Our full model consistently with word vector  Table 3: Results of adding unsupervised word vec- tors to the tensor. Adding this information yields  consistent improvement for all languages.
true	P14-1130.pdf#90.24	Penn Treebank, POS	hyper - parameter γ . γ=0 . 0 4 Average UAS on CoNLL testsets af - ter different epochs . no word vector  Table 3: Results of adding unsupervised word vec- tors to the tensor. Adding this information yields  consistent improvement for all languages.
true	P14-1130.pdf#92.07	Penn Treebank, POS	hyper - parameter γ . γ=0 . 0 8 Average UAS on CoNLL testsets af - Our full model consistently  Table 3: Results of adding unsupervised word vec- tors to the tensor. Adding this information yields  consistent improvement for all languages.
true	P13-1136.pdf#18.46†	Gigaword, ROUGE-1	DUC 2007 R - SU4  Table 5: Query-focused MDS performance comparison: C Rate or compression rate is the proportion of words
true	P13-1136.pdf#11.02*†	Gigaword, ROUGE-1	DUC 2006 R - 2  Table 5: Query-focused MDS performance comparison: C Rate or compression rate is the proportion of words
true	P13-1136.pdf#16.25†	Gigaword, ROUGE-1	DUC 2006 R - SU4  Table 5: Query-focused MDS performance comparison: C Rate or compression rate is the proportion of words
true	P13-1136.pdf#13.49†	Gigaword, ROUGE-1	DUC 2007 R - 2  Table 5: Query-focused MDS performance comparison: C Rate or compression rate is the proportion of words
true	P13-1136.pdf#0.59	Gigaword, ROUGE-1	Rel - F1  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#0.79	Gigaword, ROUGE-1	Uni - Prec  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#0.77	Gigaword, ROUGE-1	Uni - F1  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#0.77	Gigaword, ROUGE-1	Uni - F1  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#0.76	Gigaword, ROUGE-1	Uni - F1  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#0.77	Gigaword, ROUGE-1	Uni - F1  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#0.76	Gigaword, ROUGE-1	Uni - F1  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#0.79	Gigaword, ROUGE-1	Uni - Prec  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#18.46†	Gigaword, ROUGE-2	DUC 2007 R - SU4  Table 5: Query-focused MDS performance comparison: C Rate or compression rate is the proportion of words
true	P13-1136.pdf#11.02*†	Gigaword, ROUGE-2	DUC 2006 R - 2  Table 5: Query-focused MDS performance comparison: C Rate or compression rate is the proportion of words
true	P13-1136.pdf#16.25†	Gigaword, ROUGE-2	DUC 2006 R - SU4  Table 5: Query-focused MDS performance comparison: C Rate or compression rate is the proportion of words
true	P13-1136.pdf#13.49†	Gigaword, ROUGE-2	DUC 2007 R - 2  Table 5: Query-focused MDS performance comparison: C Rate or compression rate is the proportion of words
true	P13-1136.pdf#0.59	Gigaword, ROUGE-2	Rel - F1  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#0.79	Gigaword, ROUGE-2	Uni - Prec  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#0.77	Gigaword, ROUGE-2	Uni - F1  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#0.77	Gigaword, ROUGE-2	Uni - F1  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#0.76	Gigaword, ROUGE-2	Uni - F1  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#0.77	Gigaword, ROUGE-2	Uni - F1  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#0.76	Gigaword, ROUGE-2	Uni - F1  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#0.79	Gigaword, ROUGE-2	Uni - Prec  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#18.46†	Gigaword, ROUGE-L	DUC 2007 R - SU4  Table 5: Query-focused MDS performance comparison: C Rate or compression rate is the proportion of words
true	P13-1136.pdf#11.02*†	Gigaword, ROUGE-L	DUC 2006 R - 2  Table 5: Query-focused MDS performance comparison: C Rate or compression rate is the proportion of words
true	P13-1136.pdf#16.25†	Gigaword, ROUGE-L	DUC 2006 R - SU4  Table 5: Query-focused MDS performance comparison: C Rate or compression rate is the proportion of words
true	P13-1136.pdf#13.49†	Gigaword, ROUGE-L	DUC 2007 R - 2  Table 5: Query-focused MDS performance comparison: C Rate or compression rate is the proportion of words
true	P13-1136.pdf#0.59	Gigaword, ROUGE-L	Rel - F1  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#0.79	Gigaword, ROUGE-L	Uni - Prec  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#0.77	Gigaword, ROUGE-L	Uni - F1  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#0.77	Gigaword, ROUGE-L	Uni - F1  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#0.76	Gigaword, ROUGE-L	Uni - F1  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#0.77	Gigaword, ROUGE-L	Uni - F1  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#0.76	Gigaword, ROUGE-L	Uni - F1  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1136.pdf#0.79	Gigaword, ROUGE-L	Uni - Prec  Table 7: Sentence compression comparison. The true c rate is 69.06% for the test set. Tree-based approaches
true	P13-1039.pdf#26.5*	Gigaword, ROUGE-L	F1  Table 2: Slot induction results on the TAC guided  summarization data set. Asterisks (*) indicate  that the model is statistically significantly differ- ent from PROFINDER in terms of F1 at p < 0.05.
true	P13-1039.pdf#23.7	Gigaword, ROUGE-L	P  Table 2: Slot induction results on the TAC guided  summarization data set. Asterisks (*) indicate  that the model is statistically significantly differ- ent from PROFINDER in terms of F1 at p < 0.05.
true	P13-1039.pdf#36.3	Gigaword, ROUGE-L	R  Table 2: Slot induction results on the TAC guided  summarization data set. Asterisks (*) indicate  that the model is statistically significantly differ- ent from PROFINDER in terms of F1 at p < 0.05.
true	P13-1039.pdf#11.0*	Gigaword, ROUGE-L	− sup .  Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the  model is statistically significantly better than the HMM model without semantics at a 95% confidence  interval, a caretîndicatescaretîndicates that the value is marginally so.
true	P13-1039.pdf#32.8	Gigaword, ROUGE-L	− unsup .  Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the  model is statistically significantly better than the HMM model without semantics at a 95% confidence  interval, a caretîndicatescaretîndicates that the value is marginally so.
true	P13-1039.pdf#6.45	Gigaword, ROUGE-L	unsup .  Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the  model is statistically significantly better than the HMM model without semantics at a 95% confidence  interval, a caretîndicatescaretîndicates that the value is marginally so.
true	P13-1039.pdf#34.3*	Gigaword, ROUGE-L	− sup .  Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the  model is statistically significantly better than the HMM model without semantics at a 95% confidence  interval, a caretîndicatescaretîndicates that the value is marginally so.
true	P13-1039.pdf#7.31*	Gigaword, ROUGE-L	− sup .  Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the  model is statistically significantly better than the HMM model without semantics at a 95% confidence  interval, a caretîndicatescaretîndicates that the value is marginally so.
true	P13-1039.pdf#10.3	Gigaword, ROUGE-L	− unsup .  Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the  model is statistically significantly better than the HMM model without semantics at a 95% confidence  interval, a caretîndicatescaretîndicates that the value is marginally so.
true	P13-1039.pdf#26.5*	Gigaword, ROUGE-1	F1  Table 2: Slot induction results on the TAC guided  summarization data set. Asterisks (*) indicate  that the model is statistically significantly differ- ent from PROFINDER in terms of F1 at p < 0.05.
true	P13-1039.pdf#23.7	Gigaword, ROUGE-1	P  Table 2: Slot induction results on the TAC guided  summarization data set. Asterisks (*) indicate  that the model is statistically significantly differ- ent from PROFINDER in terms of F1 at p < 0.05.
true	P13-1039.pdf#36.3	Gigaword, ROUGE-1	R  Table 2: Slot induction results on the TAC guided  summarization data set. Asterisks (*) indicate  that the model is statistically significantly differ- ent from PROFINDER in terms of F1 at p < 0.05.
true	P13-1039.pdf#11.0*	Gigaword, ROUGE-1	− sup .  Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the  model is statistically significantly better than the HMM model without semantics at a 95% confidence  interval, a caretîndicatescaretîndicates that the value is marginally so.
true	P13-1039.pdf#32.8	Gigaword, ROUGE-1	− unsup .  Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the  model is statistically significantly better than the HMM model without semantics at a 95% confidence  interval, a caretîndicatescaretîndicates that the value is marginally so.
true	P13-1039.pdf#6.45	Gigaword, ROUGE-1	unsup .  Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the  model is statistically significantly better than the HMM model without semantics at a 95% confidence  interval, a caretîndicatescaretîndicates that the value is marginally so.
true	P13-1039.pdf#34.3*	Gigaword, ROUGE-1	− sup .  Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the  model is statistically significantly better than the HMM model without semantics at a 95% confidence  interval, a caretîndicatescaretîndicates that the value is marginally so.
true	P13-1039.pdf#7.31*	Gigaword, ROUGE-1	− sup .  Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the  model is statistically significantly better than the HMM model without semantics at a 95% confidence  interval, a caretîndicatescaretîndicates that the value is marginally so.
true	P13-1039.pdf#10.3	Gigaword, ROUGE-1	− unsup .  Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the  model is statistically significantly better than the HMM model without semantics at a 95% confidence  interval, a caretîndicatescaretîndicates that the value is marginally so.
true	P13-1039.pdf#26.5*	Gigaword, ROUGE-2	F1  Table 2: Slot induction results on the TAC guided  summarization data set. Asterisks (*) indicate  that the model is statistically significantly differ- ent from PROFINDER in terms of F1 at p < 0.05.
true	P13-1039.pdf#23.7	Gigaword, ROUGE-2	P  Table 2: Slot induction results on the TAC guided  summarization data set. Asterisks (*) indicate  that the model is statistically significantly differ- ent from PROFINDER in terms of F1 at p < 0.05.
true	P13-1039.pdf#36.3	Gigaword, ROUGE-2	R  Table 2: Slot induction results on the TAC guided  summarization data set. Asterisks (*) indicate  that the model is statistically significantly differ- ent from PROFINDER in terms of F1 at p < 0.05.
true	P13-1039.pdf#11.0*	Gigaword, ROUGE-2	− sup .  Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the  model is statistically significantly better than the HMM model without semantics at a 95% confidence  interval, a caretîndicatescaretîndicates that the value is marginally so.
true	P13-1039.pdf#32.8	Gigaword, ROUGE-2	− unsup .  Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the  model is statistically significantly better than the HMM model without semantics at a 95% confidence  interval, a caretîndicatescaretîndicates that the value is marginally so.
true	P13-1039.pdf#6.45	Gigaword, ROUGE-2	unsup .  Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the  model is statistically significantly better than the HMM model without semantics at a 95% confidence  interval, a caretîndicatescaretîndicates that the value is marginally so.
true	P13-1039.pdf#34.3*	Gigaword, ROUGE-2	− sup .  Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the  model is statistically significantly better than the HMM model without semantics at a 95% confidence  interval, a caretîndicatescaretîndicates that the value is marginally so.
true	P13-1039.pdf#7.31*	Gigaword, ROUGE-2	− sup .  Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the  model is statistically significantly better than the HMM model without semantics at a 95% confidence  interval, a caretîndicatescaretîndicates that the value is marginally so.
true	P13-1039.pdf#10.3	Gigaword, ROUGE-2	− unsup .  Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the  model is statistically significantly better than the HMM model without semantics at a 95% confidence  interval, a caretîndicatescaretîndicates that the value is marginally so.
true	N15-1028.pdf#44.4	VLSP 2013 POS tagging shared task, Accuracy	128 AN  Table 1: Main results on word and bigram similarity tasks, tuned on 7 development tasks (see text for  details). Shading indicates a result that matches or improves the best linear CCA result; boldface indicates  the best result in a given column. See Section 3.4 for discussion on NN results.
true	N15-1028.pdf#43.3	VLSP 2013 POS tagging shared task, Accuracy	128 Avg  Table 1: Main results on word and bigram similarity tasks, tuned on 7 development tasks (see text for  details). Shading indicates a result that matches or improves the best linear CCA result; boldface indicates  the best result in a given column. See Section 3.4 for discussion on NN results.
true	N15-1028.pdf#41.3	VLSP 2013 POS tagging shared task, Accuracy	384 VN  Table 1: Main results on word and bigram similarity tasks, tuned on 7 development tasks (see text for  details). Shading indicates a result that matches or improves the best linear CCA result; boldface indicates  the best result in a given column. See Section 3.4 for discussion on NN results.
true	N15-1028.pdf#42.3	VLSP 2013 POS tagging shared task, Accuracy	128 SL - 999  Table 1: Main results on word and bigram similarity tasks, tuned on 7 development tasks (see text for  details). Shading indicates a result that matches or improves the best linear CCA result; boldface indicates  the best result in a given column. See Section 3.4 for discussion on NN results.
true	N15-1028.pdf#42.3	VLSP 2013 POS tagging shared task, Accuracy	128 SL - 999  Table 1: Main results on word and bigram similarity tasks, tuned on 7 development tasks (see text for  details). Shading indicates a result that matches or improves the best linear CCA result; boldface indicates  the best result in a given column. See Section 3.4 for discussion on NN results.
true	N15-1028.pdf#70.8	VLSP 2013 POS tagging shared task, Accuracy	384 WS - 353  Table 1: Main results on word and bigram similarity tasks, tuned on 7 development tasks (see text for  details). Shading indicates a result that matches or improves the best linear CCA result; boldface indicates  the best result in a given column. See Section 3.4 for discussion on NN results.
true	N15-1028.pdf#45.9	VLSP 2013 POS tagging shared task, Accuracy	details ) . Shading indicates a result that matches or improves the best linear CCA result ; boldface indicates WS - SIM Main results on word and bigram similarity tasks , tuned on 7 development tasks ( see text for Avg  Table 1: Main results on word and bigram similarity tasks, tuned on 7 development tasks (see text for  details). Shading indicates a result that matches or improves the best linear CCA result; boldface indicates  the best result in a given column. See Section 3.4 for discussion on NN results.
true	N15-1028.pdf#48.1	VLSP 2013 POS tagging shared task, Accuracy	details ) . Shading indicates a result that matches or improves the best linear CCA result ; boldface indicates WS - 353 Main results on word and bigram similarity tasks , tuned on 7 development tasks ( see text for  Table 1: Main results on word and bigram similarity tasks, tuned on 7 development tasks (see text for  details). Shading indicates a result that matches or improves the best linear CCA result; boldface indicates  the best result in a given column. See Section 3.4 for discussion on NN results.
true	N15-1028.pdf#45.1	VLSP 2013 POS tagging shared task, Accuracy	details ) . Shading indicates a result that matches or improves the best linear CCA result ; boldface indicates WS - 353 Main results on word and bigram similarity tasks , tuned on 7 development tasks ( see text for VN  Table 1: Main results on word and bigram similarity tasks, tuned on 7 development tasks (see text for  details). Shading indicates a result that matches or improves the best linear CCA result; boldface indicates  the best result in a given column. See Section 3.4 for discussion on NN results.
true	N15-1028.pdf#48.2	VLSP 2013 POS tagging shared task, Accuracy	640 NN  Table 1: Main results on word and bigram similarity tasks, tuned on 7 development tasks (see text for  details). Shading indicates a result that matches or improves the best linear CCA result; boldface indicates  the best result in a given column. See Section 3.4 for discussion on NN results.
true	N15-1028.pdf#45.5	VLSP 2013 POS tagging shared task, Accuracy	details ) . Shading indicates a result that matches or improves the best linear CCA result ; boldface indicates Embeddings Main results on word and bigram similarity tasks , tuned on 7 development tasks ( see text for AN  Table 1: Main results on word and bigram similarity tasks, tuned on 7 development tasks (see text for  details). Shading indicates a result that matches or improves the best linear CCA result; boldface indicates  the best result in a given column. See Section 3.4 for discussion on NN results.
true	N15-1028.pdf#67.3	VLSP 2013 POS tagging shared task, Accuracy	384 WS - REL  Table 1: Main results on word and bigram similarity tasks, tuned on 7 development tasks (see text for  details). Shading indicates a result that matches or improves the best linear CCA result; boldface indicates  the best result in a given column. See Section 3.4 for discussion on NN results.
true	N15-1028.pdf#75.2	VLSP 2013 POS tagging shared task, Accuracy	384 WS - SIM  Table 1: Main results on word and bigram similarity tasks, tuned on 7 development tasks (see text for  details). Shading indicates a result that matches or improves the best linear CCA result; boldface indicates  the best result in a given column. See Section 3.4 for discussion on NN results.
true	N15-1028.pdf#42.3	VLSP 2013 POS tagging shared task, Accuracy	128 SL - 999  Table 2: Bigram results, tuned on bigram dev sets.
true	N15-1028.pdf#70.8	VLSP 2013 POS tagging shared task, Accuracy	384 WS - 353  Table 2: Bigram results, tuned on bigram dev sets.
true	N15-1028.pdf#45.9	VLSP 2013 POS tagging shared task, Accuracy	details ) . Shading indicates a result that matches or improves the best linear CCA result ; boldface indicates WS - SIM Main results on word and bigram similarity tasks , tuned on 7 development tasks ( see text for Avg  Table 2: Bigram results, tuned on bigram dev sets.
true	N15-1028.pdf#43.3	VLSP 2013 POS tagging shared task, Accuracy	128 Avg  Table 2: Bigram results, tuned on bigram dev sets.
true	N15-1028.pdf#75.2	VLSP 2013 POS tagging shared task, Accuracy	384 WS - SIM  Table 2: Bigram results, tuned on bigram dev sets.
true	N15-1028.pdf#48.2	VLSP 2013 POS tagging shared task, Accuracy	640 NN  Table 2: Bigram results, tuned on bigram dev sets.
true	N15-1028.pdf#44.4	VLSP 2013 POS tagging shared task, Accuracy	128 AN  Table 2: Bigram results, tuned on bigram dev sets.
true	N15-1028.pdf#41.3	VLSP 2013 POS tagging shared task, Accuracy	384 VN  Table 2: Bigram results, tuned on bigram dev sets.
true	N15-1028.pdf#42.3	VLSP 2013 POS tagging shared task, Accuracy	128 SL - 999  Table 2: Bigram results, tuned on bigram dev sets.
true	N15-1028.pdf#45.5	VLSP 2013 POS tagging shared task, Accuracy	details ) . Shading indicates a result that matches or improves the best linear CCA result ; boldface indicates Embeddings Main results on word and bigram similarity tasks , tuned on 7 development tasks ( see text for AN  Table 2: Bigram results, tuned on bigram dev sets.
true	N15-1028.pdf#45.1	VLSP 2013 POS tagging shared task, Accuracy	details ) . Shading indicates a result that matches or improves the best linear CCA result ; boldface indicates WS - 353 Main results on word and bigram similarity tasks , tuned on 7 development tasks ( see text for VN  Table 2: Bigram results, tuned on bigram dev sets.
true	N15-1028.pdf#67.3	VLSP 2013 POS tagging shared task, Accuracy	384 WS - REL  Table 2: Bigram results, tuned on bigram dev sets.
true	N15-1028.pdf#48.1	VLSP 2013 POS tagging shared task, Accuracy	details ) . Shading indicates a result that matches or improves the best linear CCA result ; boldface indicates WS - 353 Main results on word and bigram similarity tasks , tuned on 7 development tasks ( see text for  Table 2: Bigram results, tuned on bigram dev sets.
true	N13-1041.pdf#0.50	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.56	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.68	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	BL - 2  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.63	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	BL - 2  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.94	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.57	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.62	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.38	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.68	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.74	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.94	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UU  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.68	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.99	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.58	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.71	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.52	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.85	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.94	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.68	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.64	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.56	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.60	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.68	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.79	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.70	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.50	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.56	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.68	New York Times Corpus, P@10%	BL - 2  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.63	New York Times Corpus, P@10%	BL - 2  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.94	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.57	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.62	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.38	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.68	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.74	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.94	New York Times Corpus, P@10%	PMF - UU  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.68	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.99	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.58	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.71	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.52	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.85	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.94	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.68	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.64	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.56	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.60	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.68	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.79	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	N13-1041.pdf#0.70	New York Times Corpus, P@10%	PMF - UOM  Table 3: Results on subgroup detection on all the 6  threads. P, E, A and R refer to Purity, Entropy, Accuracy  and RandIndex, respectively.
true	P11-1052.pdf#38.90	DUC 2004 Task 1, ROUGE-1	F j∈S  Table 1: ROUGE-1 recall (R) and F-measure (F) results  (%) on DUC-04. DUC-03 was used as development set.
true	P11-1052.pdf#39.35	DUC 2004 Task 1, ROUGE-1	R j∈S  Table 1: ROUGE-1 recall (R) and F-measure (F) results  (%) on DUC-04. DUC-03 was used as development set.
true	P11-1052.pdf#8.38	DUC 2004 Task 1, ROUGE-1	R  Table 2: ROUGE-2 recall (R) and F-measure (F) results  (%) on DUC-05, where DUC-05 was used as training set.
true	P11-1052.pdf#8.31	DUC 2004 Task 1, ROUGE-1	F  Table 2: ROUGE-2 recall (R) and F-measure (F) results  (%) on DUC-05, where DUC-05 was used as training set.
true	P11-1052.pdf#7.72	DUC 2004 Task 1, ROUGE-1	F  Table 3: ROUGE-2 recall (R) and F-measure (F) results  on DUC-05 (%). We used DUC-06 as training set.
true	P11-1052.pdf#7.82	DUC 2004 Task 1, ROUGE-1	R  Table 3: ROUGE-2 recall (R) and F-measure (F) results  on DUC-05 (%). We used DUC-06 as training set.
true	P11-1052.pdf#9.10	DUC 2004 Task 1, ROUGE-1	R  Table 4: ROUGE-2 recall (R) and F-measure (F) results  (%) on DUC-06, where DUC-05 was used as training set.
true	P11-1052.pdf#12.33	DUC 2004 Task 1, ROUGE-1	Table 5: ROUGE-2 recall (R) and F-measure (F) re- sults (%) on DUC-07. DUC-05 was used as training  set for objective L 1 (S) + λR Q (S). DUC-05 and DUC- 06 were used as training sets for objective
true	P11-1052.pdf#38.90	DUC 2004 Task 1, ROUGE-L	F j∈S  Table 1: ROUGE-1 recall (R) and F-measure (F) results  (%) on DUC-04. DUC-03 was used as development set.
true	P11-1052.pdf#39.35	DUC 2004 Task 1, ROUGE-L	R j∈S  Table 1: ROUGE-1 recall (R) and F-measure (F) results  (%) on DUC-04. DUC-03 was used as development set.
true	P11-1052.pdf#8.38	DUC 2004 Task 1, ROUGE-L	R  Table 2: ROUGE-2 recall (R) and F-measure (F) results  (%) on DUC-05, where DUC-05 was used as training set.
true	P11-1052.pdf#8.31	DUC 2004 Task 1, ROUGE-L	F  Table 2: ROUGE-2 recall (R) and F-measure (F) results  (%) on DUC-05, where DUC-05 was used as training set.
true	P11-1052.pdf#7.72	DUC 2004 Task 1, ROUGE-L	F  Table 3: ROUGE-2 recall (R) and F-measure (F) results  on DUC-05 (%). We used DUC-06 as training set.
true	P11-1052.pdf#7.82	DUC 2004 Task 1, ROUGE-L	R  Table 3: ROUGE-2 recall (R) and F-measure (F) results  on DUC-05 (%). We used DUC-06 as training set.
true	P11-1052.pdf#9.10	DUC 2004 Task 1, ROUGE-L	R  Table 4: ROUGE-2 recall (R) and F-measure (F) results  (%) on DUC-06, where DUC-05 was used as training set.
true	P11-1052.pdf#12.33	DUC 2004 Task 1, ROUGE-L	Table 5: ROUGE-2 recall (R) and F-measure (F) re- sults (%) on DUC-07. DUC-05 was used as training  set for objective L 1 (S) + λR Q (S). DUC-05 and DUC- 06 were used as training sets for objective
true	P11-1052.pdf#38.90	DUC 2004 Task 1, ROUGE-2	F j∈S  Table 1: ROUGE-1 recall (R) and F-measure (F) results  (%) on DUC-04. DUC-03 was used as development set.
true	P11-1052.pdf#39.35	DUC 2004 Task 1, ROUGE-2	R j∈S  Table 1: ROUGE-1 recall (R) and F-measure (F) results  (%) on DUC-04. DUC-03 was used as development set.
true	P11-1052.pdf#8.38	DUC 2004 Task 1, ROUGE-2	R  Table 2: ROUGE-2 recall (R) and F-measure (F) results  (%) on DUC-05, where DUC-05 was used as training set.
true	P11-1052.pdf#8.31	DUC 2004 Task 1, ROUGE-2	F  Table 2: ROUGE-2 recall (R) and F-measure (F) results  (%) on DUC-05, where DUC-05 was used as training set.
true	P11-1052.pdf#7.72	DUC 2004 Task 1, ROUGE-2	F  Table 3: ROUGE-2 recall (R) and F-measure (F) results  on DUC-05 (%). We used DUC-06 as training set.
true	P11-1052.pdf#7.82	DUC 2004 Task 1, ROUGE-2	R  Table 3: ROUGE-2 recall (R) and F-measure (F) results  on DUC-05 (%). We used DUC-06 as training set.
true	P11-1052.pdf#9.10	DUC 2004 Task 1, ROUGE-2	R  Table 4: ROUGE-2 recall (R) and F-measure (F) results  (%) on DUC-06, where DUC-05 was used as training set.
true	P11-1052.pdf#12.33	DUC 2004 Task 1, ROUGE-2	Table 5: ROUGE-2 recall (R) and F-measure (F) re- sults (%) on DUC-07. DUC-05 was used as training  set for objective L 1 (S) + λR Q (S). DUC-05 and DUC- 06 were used as training sets for objective
true	P15-1058.pdf#72.6	Senseval 2, F1	Table 3: Results for nouns on WordNet annotated  datasets.
true	P15-1058.pdf#66.0	Senseval 2, F1	Sem07  Table 3: Results for nouns on WordNet annotated  datasets.
true	P15-1058.pdf#72.8	Senseval 2, F1	Sem13  Table 3: Results for nouns on WordNet annotated  datasets.
true	P15-1058.pdf#71.5	Senseval 2, F1	KORE  Table 4: Results for NEs on Wikipedia annotated  datasets.
true	P15-1058.pdf#85.1	Senseval 2, F1	AIDA  Table 4: Results for NEs on Wikipedia annotated  datasets.
true	P15-1058.pdf#75.4	Senseval 2, F1	- Sem15  Table 5: Results for nouns and NEs on BabelNet  annotated datasets.
true	P15-1058.pdf#71.5	Senseval 2, F1	- Sem13  Table 5: Results for nouns and NEs on BabelNet  annotated datasets.
true	P15-1058.pdf#75.4	Senseval 2, F1	Sem15  Table 6: Detailed results for nouns and NEs on  BabelNet annotated datasets and AIDA CoNLL.
true	P15-1058.pdf#71.5	Senseval 2, F1	Sem13  Table 6: Detailed results for nouns and NEs on  BabelNet annotated datasets and AIDA CoNLL.
true	P15-1058.pdf#85.1	Senseval 2, F1	AIDA  Table 6: Detailed results for nouns and NEs on  BabelNet annotated datasets and AIDA CoNLL.
true	P15-1058.pdf#72.6	SemEval 2007, F1	Table 3: Results for nouns on WordNet annotated  datasets.
true	P15-1058.pdf#66.0	SemEval 2007, F1	Sem07  Table 3: Results for nouns on WordNet annotated  datasets.
true	P15-1058.pdf#72.8	SemEval 2007, F1	Sem13  Table 3: Results for nouns on WordNet annotated  datasets.
true	P15-1058.pdf#71.5	SemEval 2007, F1	KORE  Table 4: Results for NEs on Wikipedia annotated  datasets.
true	P15-1058.pdf#85.1	SemEval 2007, F1	AIDA  Table 4: Results for NEs on Wikipedia annotated  datasets.
true	P15-1058.pdf#75.4	SemEval 2007, F1	- Sem15  Table 5: Results for nouns and NEs on BabelNet  annotated datasets.
true	P15-1058.pdf#71.5	SemEval 2007, F1	- Sem13  Table 5: Results for nouns and NEs on BabelNet  annotated datasets.
true	P15-1058.pdf#75.4	SemEval 2007, F1	Sem15  Table 6: Detailed results for nouns and NEs on  BabelNet annotated datasets and AIDA CoNLL.
true	P15-1058.pdf#71.5	SemEval 2007, F1	Sem13  Table 6: Detailed results for nouns and NEs on  BabelNet annotated datasets and AIDA CoNLL.
true	P15-1058.pdf#85.1	SemEval 2007, F1	AIDA  Table 6: Detailed results for nouns and NEs on  BabelNet annotated datasets and AIDA CoNLL.
true	P15-1058.pdf#72.6	SemEval 2013, F1	Table 3: Results for nouns on WordNet annotated  datasets.
true	P15-1058.pdf#66.0	SemEval 2013, F1	Sem07  Table 3: Results for nouns on WordNet annotated  datasets.
true	P15-1058.pdf#72.8	SemEval 2013, F1	Sem13  Table 3: Results for nouns on WordNet annotated  datasets.
true	P15-1058.pdf#71.5	SemEval 2013, F1	KORE  Table 4: Results for NEs on Wikipedia annotated  datasets.
true	P15-1058.pdf#85.1	SemEval 2013, F1	AIDA  Table 4: Results for NEs on Wikipedia annotated  datasets.
true	P15-1058.pdf#75.4	SemEval 2013, F1	- Sem15  Table 5: Results for nouns and NEs on BabelNet  annotated datasets.
true	P15-1058.pdf#71.5	SemEval 2013, F1	- Sem13  Table 5: Results for nouns and NEs on BabelNet  annotated datasets.
true	P15-1058.pdf#75.4	SemEval 2013, F1	Sem15  Table 6: Detailed results for nouns and NEs on  BabelNet annotated datasets and AIDA CoNLL.
true	P15-1058.pdf#71.5	SemEval 2013, F1	Sem13  Table 6: Detailed results for nouns and NEs on  BabelNet annotated datasets and AIDA CoNLL.
true	P15-1058.pdf#85.1	SemEval 2013, F1	AIDA  Table 6: Detailed results for nouns and NEs on  BabelNet annotated datasets and AIDA CoNLL.
true	P15-1058.pdf#72.6	SemEval 2015, F1	Table 3: Results for nouns on WordNet annotated  datasets.
true	P15-1058.pdf#66.0	SemEval 2015, F1	Sem07  Table 3: Results for nouns on WordNet annotated  datasets.
true	P15-1058.pdf#72.8	SemEval 2015, F1	Sem13  Table 3: Results for nouns on WordNet annotated  datasets.
true	P15-1058.pdf#71.5	SemEval 2015, F1	KORE  Table 4: Results for NEs on Wikipedia annotated  datasets.
true	P15-1058.pdf#85.1	SemEval 2015, F1	AIDA  Table 4: Results for NEs on Wikipedia annotated  datasets.
true	P15-1058.pdf#75.4	SemEval 2015, F1	- Sem15  Table 5: Results for nouns and NEs on BabelNet  annotated datasets.
true	P15-1058.pdf#71.5	SemEval 2015, F1	- Sem13  Table 5: Results for nouns and NEs on BabelNet  annotated datasets.
true	P15-1058.pdf#75.4	SemEval 2015, F1	Sem15  Table 6: Detailed results for nouns and NEs on  BabelNet annotated datasets and AIDA CoNLL.
true	P15-1058.pdf#71.5	SemEval 2015, F1	Sem13  Table 6: Detailed results for nouns and NEs on  BabelNet annotated datasets and AIDA CoNLL.
true	P15-1058.pdf#85.1	SemEval 2015, F1	AIDA  Table 6: Detailed results for nouns and NEs on  BabelNet annotated datasets and AIDA CoNLL.
true	P15-1058.pdf#72.6	Senseval 3, F1	Table 3: Results for nouns on WordNet annotated  datasets.
true	P15-1058.pdf#66.0	Senseval 3, F1	Sem07  Table 3: Results for nouns on WordNet annotated  datasets.
true	P15-1058.pdf#72.8	Senseval 3, F1	Sem13  Table 3: Results for nouns on WordNet annotated  datasets.
true	P15-1058.pdf#71.5	Senseval 3, F1	KORE  Table 4: Results for NEs on Wikipedia annotated  datasets.
true	P15-1058.pdf#85.1	Senseval 3, F1	AIDA  Table 4: Results for NEs on Wikipedia annotated  datasets.
true	P15-1058.pdf#75.4	Senseval 3, F1	- Sem15  Table 5: Results for nouns and NEs on BabelNet  annotated datasets.
true	P15-1058.pdf#71.5	Senseval 3, F1	- Sem13  Table 5: Results for nouns and NEs on BabelNet  annotated datasets.
true	P15-1058.pdf#75.4	Senseval 3, F1	Sem15  Table 6: Detailed results for nouns and NEs on  BabelNet annotated datasets and AIDA CoNLL.
true	P15-1058.pdf#71.5	Senseval 3, F1	Sem13  Table 6: Detailed results for nouns and NEs on  BabelNet annotated datasets and AIDA CoNLL.
true	P15-1058.pdf#85.1	Senseval 3, F1	AIDA  Table 6: Detailed results for nouns and NEs on  BabelNet annotated datasets and AIDA CoNLL.
true	P15-1171.pdf#82.6*	MSR, F1	NA PYHSMM NA  Table 4: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of  Zhikov et al. (2010), and HMM 2 is a product of  word and character HMMs of Chen et al. (2014).   *  is the accuracy decoded with L = 3: it becomes  81.7 with L = 4 as MSR and PKU.
true	P15-1171.pdf#82.1	MSR, F1	NA PYHSMM NA  Table 4: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of  Zhikov et al. (2010), and HMM 2 is a product of  word and character HMMs of Chen et al. (2014).   *  is the accuracy decoded with L = 3: it becomes  81.7 with L = 4 as MSR and PKU.
true	P15-1171.pdf#71.5	MSR, F1	PYHSMM  Table 4: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of  Zhikov et al. (2010), and HMM 2 is a product of  word and character HMMs of Chen et al. (2014).   *  is the accuracy decoded with L = 3: it becomes  81.7 with L = 4 as MSR and PKU.
true	P15-1171.pdf#82.1	MSR, F1	NA BE NA  Table 4: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of  Zhikov et al. (2010), and HMM 2 is a product of  word and character HMMs of Chen et al. (2014).   *  is the accuracy decoded with L = 3: it becomes  81.7 with L = 4 as MSR and PKU.
true	P15-1171.pdf#70.5	MSR, F1	PYHSMM  Table 4: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of  Zhikov et al. (2010), and HMM 2 is a product of  word and character HMMs of Chen et al. (2014).   *  is the accuracy decoded with L = 3: it becomes  81.7 with L = 4 as MSR and PKU.
true	P15-1171.pdf#82.9	MSR, F1	NA PYHSMM  Table 4: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of  Zhikov et al. (2010), and HMM 2 is a product of  word and character HMMs of Chen et al. (2014).   *  is the accuracy decoded with L = 3: it becomes  81.7 with L = 4 as MSR and PKU.
true	P15-1171.pdf#81.6	MSR, F1	NA PYHSMM NA  Table 4: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of  Zhikov et al. (2010), and HMM 2 is a product of  word and character HMMs of Chen et al. (2014).   *  is the accuracy decoded with L = 3: it becomes  81.7 with L = 4 as MSR and PKU.
true	P15-1171.pdf#57.4	MSR, F1	PYHSMM  Table 5: Precision of POS tagging on correctly  segmented words.
true	P15-1171.pdf#50.2	MSR, F1	PYHSMM  Table 5: Precision of POS tagging on correctly  segmented words.
true	P15-1171.pdf#82.6*	PKU, F1	NA PYHSMM NA  Table 4: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of  Zhikov et al. (2010), and HMM 2 is a product of  word and character HMMs of Chen et al. (2014).   *  is the accuracy decoded with L = 3: it becomes  81.7 with L = 4 as MSR and PKU.
true	P15-1171.pdf#82.1	PKU, F1	NA PYHSMM NA  Table 4: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of  Zhikov et al. (2010), and HMM 2 is a product of  word and character HMMs of Chen et al. (2014).   *  is the accuracy decoded with L = 3: it becomes  81.7 with L = 4 as MSR and PKU.
true	P15-1171.pdf#71.5	PKU, F1	PYHSMM  Table 4: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of  Zhikov et al. (2010), and HMM 2 is a product of  word and character HMMs of Chen et al. (2014).   *  is the accuracy decoded with L = 3: it becomes  81.7 with L = 4 as MSR and PKU.
true	P15-1171.pdf#82.1	PKU, F1	NA BE NA  Table 4: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of  Zhikov et al. (2010), and HMM 2 is a product of  word and character HMMs of Chen et al. (2014).   *  is the accuracy decoded with L = 3: it becomes  81.7 with L = 4 as MSR and PKU.
true	P15-1171.pdf#70.5	PKU, F1	PYHSMM  Table 4: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of  Zhikov et al. (2010), and HMM 2 is a product of  word and character HMMs of Chen et al. (2014).   *  is the accuracy decoded with L = 3: it becomes  81.7 with L = 4 as MSR and PKU.
true	P15-1171.pdf#82.9	PKU, F1	NA PYHSMM  Table 4: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of  Zhikov et al. (2010), and HMM 2 is a product of  word and character HMMs of Chen et al. (2014).   *  is the accuracy decoded with L = 3: it becomes  81.7 with L = 4 as MSR and PKU.
true	P15-1171.pdf#81.6	PKU, F1	NA PYHSMM NA  Table 4: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of  Zhikov et al. (2010), and HMM 2 is a product of  word and character HMMs of Chen et al. (2014).   *  is the accuracy decoded with L = 3: it becomes  81.7 with L = 4 as MSR and PKU.
true	P15-1171.pdf#57.4	PKU, F1	PYHSMM  Table 5: Precision of POS tagging on correctly  segmented words.
true	P15-1171.pdf#50.2	PKU, F1	PYHSMM  Table 5: Precision of POS tagging on correctly  segmented words.
true	P15-1171.pdf#82.6*	Chinese Treebank 6, F1	NA PYHSMM NA  Table 4: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of  Zhikov et al. (2010), and HMM 2 is a product of  word and character HMMs of Chen et al. (2014).   *  is the accuracy decoded with L = 3: it becomes  81.7 with L = 4 as MSR and PKU.
true	P15-1171.pdf#82.1	Chinese Treebank 6, F1	NA PYHSMM NA  Table 4: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of  Zhikov et al. (2010), and HMM 2 is a product of  word and character HMMs of Chen et al. (2014).   *  is the accuracy decoded with L = 3: it becomes  81.7 with L = 4 as MSR and PKU.
true	P15-1171.pdf#71.5	Chinese Treebank 6, F1	PYHSMM  Table 4: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of  Zhikov et al. (2010), and HMM 2 is a product of  word and character HMMs of Chen et al. (2014).   *  is the accuracy decoded with L = 3: it becomes  81.7 with L = 4 as MSR and PKU.
true	P15-1171.pdf#82.1	Chinese Treebank 6, F1	NA BE NA  Table 4: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of  Zhikov et al. (2010), and HMM 2 is a product of  word and character HMMs of Chen et al. (2014).   *  is the accuracy decoded with L = 3: it becomes  81.7 with L = 4 as MSR and PKU.
true	P15-1171.pdf#70.5	Chinese Treebank 6, F1	PYHSMM  Table 4: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of  Zhikov et al. (2010), and HMM 2 is a product of  word and character HMMs of Chen et al. (2014).   *  is the accuracy decoded with L = 3: it becomes  81.7 with L = 4 as MSR and PKU.
true	P15-1171.pdf#82.9	Chinese Treebank 6, F1	NA PYHSMM  Table 4: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of  Zhikov et al. (2010), and HMM 2 is a product of  word and character HMMs of Chen et al. (2014).   *  is the accuracy decoded with L = 3: it becomes  81.7 with L = 4 as MSR and PKU.
true	P15-1171.pdf#81.6	Chinese Treebank 6, F1	NA PYHSMM NA  Table 4: Accuracies of unsupervised word seg- mentation. BE is a Branching Entropy method of  Zhikov et al. (2010), and HMM 2 is a product of  word and character HMMs of Chen et al. (2014).   *  is the accuracy decoded with L = 3: it becomes  81.7 with L = 4 as MSR and PKU.
true	P15-1171.pdf#57.4	Chinese Treebank 6, F1	PYHSMM  Table 5: Precision of POS tagging on correctly  segmented words.
true	P15-1171.pdf#50.2	Chinese Treebank 6, F1	PYHSMM  Table 5: Precision of POS tagging on correctly  segmented words.
true	P15-2109.pdf#91.50	Chinese Treebank 6, F1	from Word seg - p  Table 3: Test text word segmentation results  from general-purpose segmenter and top 3 texts.
true	P15-2109.pdf#100.00	Chinese Treebank 6, F1	from Word seg - r OOV  Table 3: Test text word segmentation results  from general-purpose segmenter and top 3 texts.
true	P15-2109.pdf#100.00	Chinese Treebank 6, F1	from Word seg - r OOV  Table 3: Test text word segmentation results  from general-purpose segmenter and top 3 texts.
true	P15-2109.pdf#92.82	Chinese Treebank 6, F1	from Word seg - p  Table 3: Test text word segmentation results  from general-purpose segmenter and top 3 texts.
true	P15-2109.pdf#91.49	Chinese Treebank 6, F1	from Word seg - f  Table 3: Test text word segmentation results  from general-purpose segmenter and top 3 texts.
true	P15-2109.pdf#100.00	Chinese Treebank 6, F1	from Word seg - r OOV  Table 3: Test text word segmentation results  from general-purpose segmenter and top 3 texts.
true	P15-2109.pdf#89.78	Chinese Treebank 6, F1	tion from Word segmenta - f  Table 4: Segmenters' results on test data.  We also find out that the NTAs might be par- ticularly useful to identify OOV words, such as  proper names and neo-logisms. If users frequent- ly put some characters in one segment, this seg- ment may be some new word or the new internet  slang, such as "白富美(white, rich and pretty) ",  " 萌萌哒(very cute)", " 十 动 然 拒 (someone is  moved but refuses to become girl/boyfriend)",  etc.
true	P15-2109.pdf#89.76	Chinese Treebank 6, F1	tion from Word segmenta - r  Table 4: Segmenters' results on test data.  We also find out that the NTAs might be par- ticularly useful to identify OOV words, such as  proper names and neo-logisms. If users frequent- ly put some characters in one segment, this seg- ment may be some new word or the new internet  slang, such as "白富美(white, rich and pretty) ",  " 萌萌哒(very cute)", " 十 动 然 拒 (someone is  moved but refuses to become girl/boyfriend)",  etc.
true	P15-2109.pdf#88.95	Chinese Treebank 6, F1	tion from Word segmenta - p  Table 4: Segmenters' results on test data.  We also find out that the NTAs might be par- ticularly useful to identify OOV words, such as  proper names and neo-logisms. If users frequent- ly put some characters in one segment, this seg- ment may be some new word or the new internet  slang, such as "白富美(white, rich and pretty) ",  " 萌萌哒(very cute)", " 十 动 然 拒 (someone is  moved but refuses to become girl/boyfriend)",  etc.
true	P15-2109.pdf#90.63	Chinese Treebank 6, F1	tion from Word segmenta - r  Table 4: Segmenters' results on test data.  We also find out that the NTAs might be par- ticularly useful to identify OOV words, such as  proper names and neo-logisms. If users frequent- ly put some characters in one segment, this seg- ment may be some new word or the new internet  slang, such as "白富美(white, rich and pretty) ",  " 萌萌哒(very cute)", " 十 动 然 拒 (someone is  moved but refuses to become girl/boyfriend)",  etc.
true	P11-2103.pdf#55.13	SST-2, Accuracy	Stem+Morph  Table 2: Subjectivity results on Stem+Morph+language independent features
true	P11-2103.pdf#55.13	SST-2, Accuracy	+UNIQUE  Table 2: Subjectivity results on Stem+Morph+language independent features
true	P11-2103.pdf#55.13	SST-2, Accuracy	+DOMAIN  Table 2: Subjectivity results on Stem+Morph+language independent features
true	P11-2103.pdf#55.13	SST-2, Accuracy	+ADJ  Table 2: Subjectivity results on Stem+Morph+language independent features
true	P11-2103.pdf#58.38	SST-2, Accuracy	Table 2 : Subjectivity results on Stem+Morph+language independent features +ADJ  Table 3: Sentiment results on Stem+Morph+language independent features
true	P11-2103.pdf#58.38	SST-2, Accuracy	Table 2 : Subjectivity results on Stem+Morph+language independent features +DOMAIN  Table 3: Sentiment results on Stem+Morph+language independent features
true	P11-2103.pdf#58.38	SST-2, Accuracy	Table 2 : Subjectivity results on Stem+Morph+language independent features Stem+Morph  Table 3: Sentiment results on Stem+Morph+language independent features
true	P11-2103.pdf#58.38	SST-2, Accuracy	Table 2 : Subjectivity results on Stem+Morph+language independent features +UNIQUE  Table 3: Sentiment results on Stem+Morph+language independent features
true	P11-2103.pdf#55.13	SUBJ, Accuracy	Stem+Morph  Table 2: Subjectivity results on Stem+Morph+language independent features
true	P11-2103.pdf#55.13	SUBJ, Accuracy	+UNIQUE  Table 2: Subjectivity results on Stem+Morph+language independent features
true	P11-2103.pdf#55.13	SUBJ, Accuracy	+DOMAIN  Table 2: Subjectivity results on Stem+Morph+language independent features
true	P11-2103.pdf#55.13	SUBJ, Accuracy	+ADJ  Table 2: Subjectivity results on Stem+Morph+language independent features
true	P11-2103.pdf#58.38	SUBJ, Accuracy	Table 2 : Subjectivity results on Stem+Morph+language independent features +ADJ  Table 3: Sentiment results on Stem+Morph+language independent features
true	P11-2103.pdf#58.38	SUBJ, Accuracy	Table 2 : Subjectivity results on Stem+Morph+language independent features +DOMAIN  Table 3: Sentiment results on Stem+Morph+language independent features
true	P11-2103.pdf#58.38	SUBJ, Accuracy	Table 2 : Subjectivity results on Stem+Morph+language independent features Stem+Morph  Table 3: Sentiment results on Stem+Morph+language independent features
true	P11-2103.pdf#58.38	SUBJ, Accuracy	Table 2 : Subjectivity results on Stem+Morph+language independent features +UNIQUE  Table 3: Sentiment results on Stem+Morph+language independent features
true	P11-2112.pdf#94.91	Penn Treebank, F1	dev . 3 , 720M 3 , 720M  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#90.90700,000M	Penn Treebank, F1	
true	P11-2112.pdf#94.33	Penn Treebank, F1	N / A dev . 3 , 720M 3 , 720M 27M 1 , 000M N / A 37M dev .  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#94.13	Penn Treebank, F1	N / A dev . 3 , 720M 3 , 720M 27M 1 , 000M N / A 37M dev . 0 43M 43M  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#91.02	Penn Treebank, F1	test 3 , 720M 3 , 720M  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#94.48	Penn Treebank, F1	dev . 3 , 720M 3 , 720M 27M  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#94.223,720M	Penn Treebank, F1	N / A 
true	P11-2112.pdf#93.793,720M	Penn Treebank, F1	N / A 
true	P11-2112.pdf#94.91	benchmark Vietnamese dependency treebank VnDT, UAS	dev . 3 , 720M 3 , 720M  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#90.90700,000M	benchmark Vietnamese dependency treebank VnDT, UAS	
true	P11-2112.pdf#94.33	benchmark Vietnamese dependency treebank VnDT, UAS	N / A dev . 3 , 720M 3 , 720M 27M 1 , 000M N / A 37M dev .  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#94.13	benchmark Vietnamese dependency treebank VnDT, UAS	N / A dev . 3 , 720M 3 , 720M 27M 1 , 000M N / A 37M dev . 0 43M 43M  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#91.02	benchmark Vietnamese dependency treebank VnDT, UAS	test 3 , 720M 3 , 720M  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#94.48	benchmark Vietnamese dependency treebank VnDT, UAS	dev . 3 , 720M 3 , 720M 27M  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#94.223,720M	benchmark Vietnamese dependency treebank VnDT, UAS	N / A 
true	P11-2112.pdf#93.793,720M	benchmark Vietnamese dependency treebank VnDT, UAS	N / A 
true	P11-2112.pdf#94.91	Penn Treebank, POS	dev . 3 , 720M 3 , 720M  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#90.90700,000M	Penn Treebank, POS	
true	P11-2112.pdf#94.33	Penn Treebank, POS	N / A dev . 3 , 720M 3 , 720M 27M 1 , 000M N / A 37M dev .  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#94.13	Penn Treebank, POS	N / A dev . 3 , 720M 3 , 720M 27M 1 , 000M N / A 37M dev . 0 43M 43M  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#91.02	Penn Treebank, POS	test 3 , 720M 3 , 720M  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#94.48	Penn Treebank, POS	dev . 3 , 720M 3 , 720M 27M  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#94.223,720M	Penn Treebank, POS	N / A 
true	P11-2112.pdf#93.793,720M	Penn Treebank, POS	N / A 
true	P11-2112.pdf#94.91	Penn Treebank, LAS	dev . 3 , 720M 3 , 720M  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#90.90700,000M	Penn Treebank, LAS	
true	P11-2112.pdf#94.33	Penn Treebank, LAS	N / A dev . 3 , 720M 3 , 720M 27M 1 , 000M N / A 37M dev .  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#94.13	Penn Treebank, LAS	N / A dev . 3 , 720M 3 , 720M 27M 1 , 000M N / A 37M dev . 0 43M 43M  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#91.02	Penn Treebank, LAS	test 3 , 720M 3 , 720M  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#94.48	Penn Treebank, LAS	dev . 3 , 720M 3 , 720M 27M  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#94.223,720M	Penn Treebank, LAS	N / A 
true	P11-2112.pdf#93.793,720M	Penn Treebank, LAS	N / A 
true	P11-2112.pdf#94.91	Penn Treebank, UAS	dev . 3 , 720M 3 , 720M  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#90.90700,000M	Penn Treebank, UAS	
true	P11-2112.pdf#94.33	Penn Treebank, UAS	N / A dev . 3 , 720M 3 , 720M 27M 1 , 000M N / A 37M dev .  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#94.13	Penn Treebank, UAS	N / A dev . 3 , 720M 3 , 720M 27M 1 , 000M N / A 37M dev . 0 43M 43M  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#91.02	Penn Treebank, UAS	test 3 , 720M 3 , 720M  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#94.48	Penn Treebank, UAS	dev . 3 , 720M 3 , 720M 27M  Table 1: Comparison with previous top-line systems on  test data. (
true	P11-2112.pdf#94.223,720M	Penn Treebank, UAS	N / A 
true	P11-2112.pdf#93.793,720M	Penn Treebank, UAS	N / A 
true	D15-1302.pdf#496	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#207	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#154	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- 77  Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#152	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- 77  Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#106	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#460	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- 77  Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#315	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- 77  Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#467	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	3416 2484  Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#436	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- 77  Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#197	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#519	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- 77  Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#873	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- 77  Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#180	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#486	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#512	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#651	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	3416  Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#792	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#924	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#227	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#165	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	81 50  Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#194	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- 77  Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#222	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#334	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- 77  Table 2: Number of opinion expressions at the sentence level of the datasets.
true	D15-1302.pdf#.836	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Avg  Table 3: Inter-annotator agreement. F 1 on sentence-level aspect identification (higher is better). sMAE M  on sentence-level sentiment agreement (only on matching aspects, lower is better).
true	D15-1302.pdf#.790	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Avg  Table 3: Inter-annotator agreement. F 1 on sentence-level aspect identification (higher is better). sMAE M  on sentence-level sentiment agreement (only on matching aspects, lower is better).
true	D15-1302.pdf#.188	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Avg  Table 3: Inter-annotator agreement. F 1 on sentence-level aspect identification (higher is better). sMAE M  on sentence-level sentiment agreement (only on matching aspects, lower is better).
true	D15-1302.pdf#.161	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Avg  Table 3: Inter-annotator agreement. F 1 on sentence-level aspect identification (higher is better). sMAE M  on sentence-level sentiment agreement (only on matching aspects, lower is better).
true	D15-1302.pdf#.171	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Avg  Table 3: Inter-annotator agreement. F 1 on sentence-level aspect identification (higher is better). sMAE M  on sentence-level sentiment agreement (only on matching aspects, lower is better).
true	D15-1302.pdf#.675	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Avg  Table 3: Inter-annotator agreement. F 1 on sentence-level aspect identification (higher is better). sMAE M  on sentence-level sentiment agreement (only on matching aspects, lower is better).
true	D15-1302.pdf#.584	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Avg  Table 4: Linear Chain CRFs experiments. F 1 on sentence-level aspect identification (higher is better).  MAE M on sentence-level sentiment assignment (only on correctly identified aspects, lower is better).
true	D15-1302.pdf#.574	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Avg  Table 4: Linear Chain CRFs experiments. F 1 on sentence-level aspect identification (higher is better).  MAE M on sentence-level sentiment assignment (only on correctly identified aspects, lower is better).
true	D15-1302.pdf#.586	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Avg  Table 4: Linear Chain CRFs experiments. F 1 on sentence-level aspect identification (higher is better).  MAE M on sentence-level sentiment assignment (only on correctly identified aspects, lower is better).
true	D15-1302.pdf#.549	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Avg  Table 4: Linear Chain CRFs experiments. F 1 on sentence-level aspect identification (higher is better).  MAE M on sentence-level sentiment assignment (only on correctly identified aspects, lower is better).
true	D15-1302.pdf#.579	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Avg  Table 4: Linear Chain CRFs experiments. F 1 on sentence-level aspect identification (higher is better).  MAE M on sentence-level sentiment assignment (only on correctly identified aspects, lower is better).
true	D15-1302.pdf#.465	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Avg  Table 4: Linear Chain CRFs experiments. F 1 on sentence-level aspect identification (higher is better).  MAE M on sentence-level sentiment assignment (only on correctly identified aspects, lower is better).
true	D12-1127.pdf#87.1	Penn Treebank, Test perplexity	Wiktionary English  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#86.1	Penn Treebank, Test perplexity	Wiktionary Swedish  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#87.9	Penn Treebank, Test perplexity	Unsupervised Portuguese  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#86.8	Penn Treebank, Test perplexity	Unsupervised Italian  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#82.5	Penn Treebank, Test perplexity	Unsupervised Greek  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#83.3	Penn Treebank, Test perplexity	Wiktionary Danish  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#86.3	Penn Treebank, Test perplexity	Wiktionary Dutch  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#84.8	Penn Treebank, Test perplexity	Wiktionary avg .  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#86.4	Penn Treebank, Test perplexity	Wiktionary Spanish  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#85.8	Penn Treebank, Test perplexity	Wiktionary German  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#87.1	Penn Treebank, Bit per Character (BPC)	Wiktionary English  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#86.1	Penn Treebank, Bit per Character (BPC)	Wiktionary Swedish  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#87.9	Penn Treebank, Bit per Character (BPC)	Unsupervised Portuguese  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#86.8	Penn Treebank, Bit per Character (BPC)	Unsupervised Italian  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#82.5	Penn Treebank, Bit per Character (BPC)	Unsupervised Greek  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#83.3	Penn Treebank, Bit per Character (BPC)	Wiktionary Danish  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#86.3	Penn Treebank, Bit per Character (BPC)	Wiktionary Dutch  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#84.8	Penn Treebank, Bit per Character (BPC)	Wiktionary avg .  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#86.4	Penn Treebank, Bit per Character (BPC)	Wiktionary Spanish  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#85.8	Penn Treebank, Bit per Character (BPC)	Wiktionary German  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#87.1	Penn Treebank, Number of params	Wiktionary English  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#86.1	Penn Treebank, Number of params	Wiktionary Swedish  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#87.9	Penn Treebank, Number of params	Unsupervised Portuguese  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#86.8	Penn Treebank, Number of params	Unsupervised Italian  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#82.5	Penn Treebank, Number of params	Unsupervised Greek  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#83.3	Penn Treebank, Number of params	Wiktionary Danish  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#86.3	Penn Treebank, Number of params	Wiktionary Dutch  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#84.8	Penn Treebank, Number of params	Wiktionary avg .  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#86.4	Penn Treebank, Number of params	Wiktionary Spanish  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#85.8	Penn Treebank, Number of params	Wiktionary German  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#87.1	Penn Treebank, Accuracy	Wiktionary English  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#86.1	Penn Treebank, Accuracy	Wiktionary Swedish  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#87.9	Penn Treebank, Accuracy	Unsupervised Portuguese  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#86.8	Penn Treebank, Accuracy	Unsupervised Italian  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#82.5	Penn Treebank, Accuracy	Unsupervised Greek  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#83.3	Penn Treebank, Accuracy	Wiktionary Danish  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#86.3	Penn Treebank, Accuracy	Wiktionary Dutch  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#84.8	Penn Treebank, Accuracy	Wiktionary avg .  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#86.4	Penn Treebank, Accuracy	Wiktionary Spanish  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	D12-1127.pdf#85.8	Penn Treebank, Accuracy	Wiktionary German  Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.
true	P10-1139.pdf#0.3978	Text8, Number of params	Evaluation metrics COAE08 MAP  Table 1: Comparison of different approaches on  COAE08 dataset, and the best is highlighted.  Most of the above models were originally de- signed for opinion retrieval in English, and  re-designed them to handle Chinese opinionated  documents. We incorporated our own Chinese  sentiment lexicon for this purpose. In our expe- riments, in addition to MAP, other metrics such  as R-precision (R-prec), binary Preference (bPref)
true	P10-1139.pdf#0.7309	Text8, Number of params	Evaluation metrics COAE08 P@10  Table 1: Comparison of different approaches on  COAE08 dataset, and the best is highlighted.  Most of the above models were originally de- signed for opinion retrieval in English, and  re-designed them to handle Chinese opinionated  documents. We incorporated our own Chinese  sentiment lexicon for this purpose. In our expe- riments, in addition to MAP, other metrics such  as R-precision (R-prec), binary Preference (bPref)
true	P10-1139.pdf#0.4835	Text8, Number of params	Evaluation metrics COAE08 R - pre  Table 1: Comparison of different approaches on  COAE08 dataset, and the best is highlighted.  Most of the above models were originally de- signed for opinion retrieval in English, and  re-designed them to handle Chinese opinionated  documents. We incorporated our own Chinese  sentiment lexicon for this purpose. In our expe- riments, in addition to MAP, other metrics such  as R-precision (R-prec), binary Preference (bPref)
true	P10-1139.pdf#0.4265	Text8, Number of params	Evaluation metrics COAE08 bPref  Table 1: Comparison of different approaches on  COAE08 dataset, and the best is highlighted.  Most of the above models were originally de- signed for opinion retrieval in English, and  re-designed them to handle Chinese opinionated  documents. We incorporated our own Chinese  sentiment lexicon for this purpose. In our expe- riments, in addition to MAP, other metrics such  as R-precision (R-prec), binary Preference (bPref)
true	P10-1139.pdf#-0.4	Text8, Number of params	0 COAE08  Table 1.
true	P10-1139.pdf#-0.2	Text8, Number of params	0 COAE08  Table 1.
true	P10-1139.pdf#-0.1	Text8, Number of params	0 COAE08  Table 1.
true	P10-1139.pdf#-0.3	Text8, Number of params	0 COAE08  Table 1.
true	P10-1139.pdf#0.3978	New York Times Corpus, P@10%	Evaluation metrics COAE08 MAP  Table 1: Comparison of different approaches on  COAE08 dataset, and the best is highlighted.  Most of the above models were originally de- signed for opinion retrieval in English, and  re-designed them to handle Chinese opinionated  documents. We incorporated our own Chinese  sentiment lexicon for this purpose. In our expe- riments, in addition to MAP, other metrics such  as R-precision (R-prec), binary Preference (bPref)
true	P10-1139.pdf#0.7309	New York Times Corpus, P@10%	Evaluation metrics COAE08 P@10  Table 1: Comparison of different approaches on  COAE08 dataset, and the best is highlighted.  Most of the above models were originally de- signed for opinion retrieval in English, and  re-designed them to handle Chinese opinionated  documents. We incorporated our own Chinese  sentiment lexicon for this purpose. In our expe- riments, in addition to MAP, other metrics such  as R-precision (R-prec), binary Preference (bPref)
true	P10-1139.pdf#0.4835	New York Times Corpus, P@10%	Evaluation metrics COAE08 R - pre  Table 1: Comparison of different approaches on  COAE08 dataset, and the best is highlighted.  Most of the above models were originally de- signed for opinion retrieval in English, and  re-designed them to handle Chinese opinionated  documents. We incorporated our own Chinese  sentiment lexicon for this purpose. In our expe- riments, in addition to MAP, other metrics such  as R-precision (R-prec), binary Preference (bPref)
true	P10-1139.pdf#0.4265	New York Times Corpus, P@10%	Evaluation metrics COAE08 bPref  Table 1: Comparison of different approaches on  COAE08 dataset, and the best is highlighted.  Most of the above models were originally de- signed for opinion retrieval in English, and  re-designed them to handle Chinese opinionated  documents. We incorporated our own Chinese  sentiment lexicon for this purpose. In our expe- riments, in addition to MAP, other metrics such  as R-precision (R-prec), binary Preference (bPref)
true	P10-1139.pdf#-0.4	New York Times Corpus, P@10%	0 COAE08  Table 1.
true	P10-1139.pdf#-0.2	New York Times Corpus, P@10%	0 COAE08  Table 1.
true	P10-1139.pdf#-0.1	New York Times Corpus, P@10%	0 COAE08  Table 1.
true	P10-1139.pdf#-0.3	New York Times Corpus, P@10%	0 COAE08  Table 1.
true	P13-1062.pdf#0.53	New York Times Corpus, P@10%	Table 3: Evaluation results of the methods.
true	P13-1062.pdf#0.80	New York Times Corpus, P@10%	Table 3: Evaluation results of the methods.
true	P13-1062.pdf#0.52	New York Times Corpus, P@10%	Table 3: Evaluation results of the methods.
true	P13-1062.pdf#0.80	New York Times Corpus, P@10%	Table 3: Evaluation results of the methods.
true	P13-1062.pdf#0.56	New York Times Corpus, P@10%	Table 3: Evaluation results of the methods.
true	P13-1062.pdf#0.81	New York Times Corpus, P@10%	Table 3: Evaluation results of the methods.
true	P13-1062.pdf#0.79	New York Times Corpus, P@10%	Table 3: Evaluation results of the methods.
true	P13-1062.pdf#0.54	New York Times Corpus, P@10%	Table 3: Evaluation results of the methods.
true	P11-1097.pdf#78.0	Text8, Bit per Character (BPC)	IEER ) . ORG PER  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#43.8	Text8, Bit per Character (BPC)	KDD - T ) . ALL  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#77.4	Text8, Bit per Character (BPC)	IEER ) . LOC  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#93.0	Text8, Bit per Character (BPC)	CoNLL ) . ALL  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#85.1	Text8, Bit per Character (BPC)	CoNLL ) . ORG PER  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#96.5	Text8, Bit per Character (BPC)	CoNLL ) . ORG PER  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#68.1	Text8, Bit per Character (BPC)	IEER ) . ALL  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#41.2	Text8, Bit per Character (BPC)	IEER ) . ORG PER  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#52.3	Text8, Bit per Character (BPC)	KDD - T ) . ORG PER  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#57.0	Text8, Bit per Character (BPC)	KDD - T ) . LOC  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#94.9	Text8, Bit per Character (BPC)	CoNLL ) . LOC  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#94.9	Text8, Bit per Character (BPC)	CoNLL ) . LOC  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#68.1	Text8, Bit per Character (BPC)	IEER ) . ALL  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#43.8	Text8, Bit per Character (BPC)	KDD - T ) . ALL  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#77.4	Text8, Bit per Character (BPC)	IEER ) . LOC  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#25.2	Text8, Bit per Character (BPC)	KDD - T ) . ORG PER  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#78.0	Text8, Number of params	IEER ) . ORG PER  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#43.8	Text8, Number of params	KDD - T ) . ALL  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#77.4	Text8, Number of params	IEER ) . LOC  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#93.0	Text8, Number of params	CoNLL ) . ALL  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#85.1	Text8, Number of params	CoNLL ) . ORG PER  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#96.5	Text8, Number of params	CoNLL ) . ORG PER  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#68.1	Text8, Number of params	IEER ) . ALL  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#41.2	Text8, Number of params	IEER ) . ORG PER  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#52.3	Text8, Number of params	KDD - T ) . ORG PER  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#57.0	Text8, Number of params	KDD - T ) . LOC  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#94.9	Text8, Number of params	CoNLL ) . LOC  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#94.9	Text8, Number of params	CoNLL ) . LOC  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#68.1	Text8, Number of params	IEER ) . ALL  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#43.8	Text8, Number of params	KDD - T ) . ALL  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#77.4	Text8, Number of params	IEER ) . LOC  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	P11-1097.pdf#25.2	Text8, Number of params	KDD - T ) . ORG PER  Table 3: Evaluation results. l = text lowercased, c = orig- inal capitalization preserved. ALL scores significantly  different from the best results for the three datasets (lines  c7, i8, k7) are marked  *  (see text).
true	N12-1029.pdf#97.5	AG News, Error	the features . ; the second ( LexSyn+Dist ) includes all of Acc .  Table 2. The first system (LexSyn)  includes only the lexical and syntactic features from
true	N12-1029.pdf#85.8	AG News, Error	the features . ; the second ( LexSyn+Dist ) includes all of P  Table 2. The first system (LexSyn)  includes only the lexical and syntactic features from
true	N12-1029.pdf#61.4	AG News, Error	the features . ; the second ( LexSyn+Dist ) includes all of S Acc .  Table 2. The first system (LexSyn)  includes only the lexical and syntactic features from
true	N12-1029.pdf#74.8	AG News, Error	the features . ; the second ( LexSyn+Dist ) includes all of F  Table 2. The first system (LexSyn)  includes only the lexical and syntactic features from
true	N12-1029.pdf#97.5	AG News, Error	the features . ; the second ( LexSyn+Dist ) includes all of Acc .  Table 2: Comma Restoration System Results (%)
true	N12-1029.pdf#74.8	AG News, Error	the features . ; the second ( LexSyn+Dist ) includes all of F  Table 2: Comma Restoration System Results (%)
true	N12-1029.pdf#61.4	AG News, Error	the features . ; the second ( LexSyn+Dist ) includes all of S Acc .  Table 2: Comma Restoration System Results (%)
true	N12-1029.pdf#85.8	AG News, Error	the features . ; the second ( LexSyn+Dist ) includes all of P  Table 2: Comma Restoration System Results (%)
true	N12-1029.pdf#31.7	AG News, Error	297 R  Table 5: Comma Error Correction Results (%)
true	N12-1029.pdf#47.4	AG News, Error	297 F  Table 5: Comma Error Correction Results (%)
true	N12-1029.pdf#94.0	AG News, Error	297 P  Table 5: Comma Error Correction Results (%)
true	N13-1023.pdf#0.3	WMT 2014 EN-FR, BLEU	7 . 9±0 . 2 Reference text Mean 
true	N13-1023.pdf#10.9±	WMT 2014 EN-FR, BLEU	7 . 9±0 . 2 Reference text Mean 
true	N13-1023.pdf#0.3	WMT 2014 EN-DE, BLEU	7 . 9±0 . 2 Reference text Mean 
true	N13-1023.pdf#10.9±	WMT 2014 EN-DE, BLEU	7 . 9±0 . 2 Reference text Mean 
true	P13-1145.pdf#62.9	New York Times Corpus, P@10%	P ( % ) R ( % )  Table 4. Performance comparison of argument  extraction on argument identification and role  determination.
true	P13-1145.pdf#60.2	New York Times Corpus, P@10%	F1 P ( % ) R ( % )  Table 4. Performance comparison of argument  extraction on argument identification and role  determination.
true	P13-1145.pdf#58.7	New York Times Corpus, P@10%	F1  Table 4. Performance comparison of argument  extraction on argument identification and role  determination.
true	P13-1145.pdf#66.1	New York Times Corpus, P@10%	P ( % ) R ( % )  Table 4. Performance comparison of argument  extraction on argument identification and role  determination.
true	P13-1145.pdf#57.2	New York Times Corpus, P@10%	F1 P ( % ) R ( % )  Table 4. Performance comparison of argument  extraction on argument identification and role  determination.
true	P13-1145.pdf#64.4	New York Times Corpus, P@10%	F1 P ( % ) R ( % )  Table 4. Performance comparison of argument  extraction on argument identification and role  determination.
true	P13-1145.pdf#+1.1	New York Times Corpus, P@10%	role determination in F1 - measure respectively . the identification Argument role determination F1  Table 5. Contributions of different event  relations on argument identification and role  determination. (Incremental)
true	P13-1145.pdf#+0.6	New York Times Corpus, P@10%	role determination in F1 - measure respectively . shows improvement Argument identification P ( % ) R ( % )  Table 5. Contributions of different event  relations on argument identification and role  determination. (Incremental)
true	P13-1145.pdf#+1.2	New York Times Corpus, P@10%	role determination in F1 - measure respectively . shows argument Argument identification F1  Table 5. Contributions of different event  relations on argument identification and role  determination. (Incremental)
true	P13-1145.pdf#+0.5	New York Times Corpus, P@10%	role determination in F1 - measure respectively . shows identification Argument role determination P ( % ) R ( % )  Table 5. Contributions of different event  relations on argument identification and role  determination. (Incremental)
true	P13-1145.pdf#+2.8	New York Times Corpus, P@10%	role determination in F1 - measure respectively . shows argument Argument identification P ( % ) R ( % )  Table 5. Contributions of different event  relations on argument identification and role  determination. (Incremental)
true	P13-1145.pdf#+2.6	New York Times Corpus, P@10%	role determination in F1 - measure respectively . shows identification Argument role determination P ( % ) R ( % )  Table 5. Contributions of different event  relations on argument identification and role  determination. (Incremental)
true	P13-1145.pdf#+0.6	New York Times Corpus, P@10%	role determination in F1 - measure respectively . shows improvement Argument identification P ( % ) R ( % )  Table 5. Contributions of different event  relations on argument identification and role  determination. (Incremental)
true	P13-1145.pdf#70.4	New York Times Corpus, P@10%	Argument role determination P ( % ) R ( % )  Table 6. Performance comparison of argument  identification and type determination. (Golden  trigger extraction)
true	P13-1145.pdf#72.0	New York Times Corpus, P@10%	Argument role determination P ( % ) R ( % )  Table 6. Performance comparison of argument  identification and type determination. (Golden  trigger extraction)
true	P13-1145.pdf#71.2	New York Times Corpus, P@10%	Argument role determination F1  Table 6. Performance comparison of argument  identification and type determination. (Golden  trigger extraction)
true	P13-1145.pdf#76.8	New York Times Corpus, P@10%	Argument identification F1  Table 6. Performance comparison of argument  identification and type determination. (Golden  trigger extraction)
true	P13-1145.pdf#76.2	New York Times Corpus, P@10%	Argument identification P ( % ) R ( % )  Table 6. Performance comparison of argument  identification and type determination. (Golden  trigger extraction)
true	P13-1145.pdf#77.4	New York Times Corpus, P@10%	Argument identification P ( % ) R ( % )  Table 6. Performance comparison of argument  identification and type determination. (Golden  trigger extraction)
true	P13-1145.pdf#62.9	Chinese Treebank 6, F1	P ( % ) R ( % )  Table 4. Performance comparison of argument  extraction on argument identification and role  determination.
true	P13-1145.pdf#60.2	Chinese Treebank 6, F1	F1 P ( % ) R ( % )  Table 4. Performance comparison of argument  extraction on argument identification and role  determination.
true	P13-1145.pdf#58.7	Chinese Treebank 6, F1	F1  Table 4. Performance comparison of argument  extraction on argument identification and role  determination.
true	P13-1145.pdf#66.1	Chinese Treebank 6, F1	P ( % ) R ( % )  Table 4. Performance comparison of argument  extraction on argument identification and role  determination.
true	P13-1145.pdf#57.2	Chinese Treebank 6, F1	F1 P ( % ) R ( % )  Table 4. Performance comparison of argument  extraction on argument identification and role  determination.
true	P13-1145.pdf#64.4	Chinese Treebank 6, F1	F1 P ( % ) R ( % )  Table 4. Performance comparison of argument  extraction on argument identification and role  determination.
true	P13-1145.pdf#+1.1	Chinese Treebank 6, F1	role determination in F1 - measure respectively . the identification Argument role determination F1  Table 5. Contributions of different event  relations on argument identification and role  determination. (Incremental)
true	P13-1145.pdf#+0.6	Chinese Treebank 6, F1	role determination in F1 - measure respectively . shows improvement Argument identification P ( % ) R ( % )  Table 5. Contributions of different event  relations on argument identification and role  determination. (Incremental)
true	P13-1145.pdf#+1.2	Chinese Treebank 6, F1	role determination in F1 - measure respectively . shows argument Argument identification F1  Table 5. Contributions of different event  relations on argument identification and role  determination. (Incremental)
true	P13-1145.pdf#+0.5	Chinese Treebank 6, F1	role determination in F1 - measure respectively . shows identification Argument role determination P ( % ) R ( % )  Table 5. Contributions of different event  relations on argument identification and role  determination. (Incremental)
true	P13-1145.pdf#+2.8	Chinese Treebank 6, F1	role determination in F1 - measure respectively . shows argument Argument identification P ( % ) R ( % )  Table 5. Contributions of different event  relations on argument identification and role  determination. (Incremental)
true	P13-1145.pdf#+2.6	Chinese Treebank 6, F1	role determination in F1 - measure respectively . shows identification Argument role determination P ( % ) R ( % )  Table 5. Contributions of different event  relations on argument identification and role  determination. (Incremental)
true	P13-1145.pdf#+0.6	Chinese Treebank 6, F1	role determination in F1 - measure respectively . shows improvement Argument identification P ( % ) R ( % )  Table 5. Contributions of different event  relations on argument identification and role  determination. (Incremental)
true	P13-1145.pdf#70.4	Chinese Treebank 6, F1	Argument role determination P ( % ) R ( % )  Table 6. Performance comparison of argument  identification and type determination. (Golden  trigger extraction)
true	P13-1145.pdf#72.0	Chinese Treebank 6, F1	Argument role determination P ( % ) R ( % )  Table 6. Performance comparison of argument  identification and type determination. (Golden  trigger extraction)
true	P13-1145.pdf#71.2	Chinese Treebank 6, F1	Argument role determination F1  Table 6. Performance comparison of argument  identification and type determination. (Golden  trigger extraction)
true	P13-1145.pdf#76.8	Chinese Treebank 6, F1	Argument identification F1  Table 6. Performance comparison of argument  identification and type determination. (Golden  trigger extraction)
true	P13-1145.pdf#76.2	Chinese Treebank 6, F1	Argument identification P ( % ) R ( % )  Table 6. Performance comparison of argument  identification and type determination. (Golden  trigger extraction)
true	P13-1145.pdf#77.4	Chinese Treebank 6, F1	Argument identification P ( % ) R ( % )  Table 6. Performance comparison of argument  identification and type determination. (Golden  trigger extraction)
true	P13-2108.pdf#.090	Penn Treebank, UAS	- Test Set BROWN UAS  Table 2: Results of statistical significance evaluations of hypotheses of the form X's accuracy is greater  than Y's on the various test sets and metrics. Bold face indicates p < .05.
true	P13-2108.pdf#.059	Penn Treebank, UAS	- Test Set BROWN F 1  Table 2: Results of statistical significance evaluations of hypotheses of the form X's accuracy is greater  than Y's on the various test sets and metrics. Bold face indicates p < .05.
true	P13-2108.pdf#.151	Penn Treebank, UAS	- Test Set WSJ LAS  Table 2: Results of statistical significance evaluations of hypotheses of the form X's accuracy is greater  than Y's on the various test sets and metrics. Bold face indicates p < .05.
true	P13-2108.pdf#<.001	Penn Treebank, UAS	- Test Set WSJ LAS  Table 2: Results of statistical significance evaluations of hypotheses of the form X's accuracy is greater  than Y's on the various test sets and metrics. Bold face indicates p < .05.
true	P13-2108.pdf#.122	Penn Treebank, UAS	- Test Set WSJ UAS  Table 2: Results of statistical significance evaluations of hypotheses of the form X's accuracy is greater  than Y's on the various test sets and metrics. Bold face indicates p < .05.
true	P13-2108.pdf#<.001	Penn Treebank, UAS	- Test Set BROWN LAS  Table 2: Results of statistical significance evaluations of hypotheses of the form X's accuracy is greater  than Y's on the various test sets and metrics. Bold face indicates p < .05.
true	P13-2108.pdf#<.001	Penn Treebank, UAS	- Test Set BROWN LAS  Table 2: Results of statistical significance evaluations of hypotheses of the form X's accuracy is greater  than Y's on the various test sets and metrics. Bold face indicates p < .05.
true	P13-2108.pdf#<.001	Penn Treebank, UAS	- Test Set WSJ LAS  Table 2: Results of statistical significance evaluations of hypotheses of the form X's accuracy is greater  than Y's on the various test sets and metrics. Bold face indicates p < .05.
true	P13-2108.pdf#.140	Penn Treebank, UAS	Test Set BROWN F 1  Table 2: Results of statistical significance evaluations of hypotheses of the form X's accuracy is greater  than Y's on the various test sets and metrics. Bold face indicates p < .05.
true	P13-2108.pdf#.090	benchmark Vietnamese dependency treebank VnDT, UAS	- Test Set BROWN UAS  Table 2: Results of statistical significance evaluations of hypotheses of the form X's accuracy is greater  than Y's on the various test sets and metrics. Bold face indicates p < .05.
true	P13-2108.pdf#.059	benchmark Vietnamese dependency treebank VnDT, UAS	- Test Set BROWN F 1  Table 2: Results of statistical significance evaluations of hypotheses of the form X's accuracy is greater  than Y's on the various test sets and metrics. Bold face indicates p < .05.
true	P13-2108.pdf#.151	benchmark Vietnamese dependency treebank VnDT, UAS	- Test Set WSJ LAS  Table 2: Results of statistical significance evaluations of hypotheses of the form X's accuracy is greater  than Y's on the various test sets and metrics. Bold face indicates p < .05.
true	P13-2108.pdf#<.001	benchmark Vietnamese dependency treebank VnDT, UAS	- Test Set WSJ LAS  Table 2: Results of statistical significance evaluations of hypotheses of the form X's accuracy is greater  than Y's on the various test sets and metrics. Bold face indicates p < .05.
true	P13-2108.pdf#.122	benchmark Vietnamese dependency treebank VnDT, UAS	- Test Set WSJ UAS  Table 2: Results of statistical significance evaluations of hypotheses of the form X's accuracy is greater  than Y's on the various test sets and metrics. Bold face indicates p < .05.
true	P13-2108.pdf#<.001	benchmark Vietnamese dependency treebank VnDT, UAS	- Test Set BROWN LAS  Table 2: Results of statistical significance evaluations of hypotheses of the form X's accuracy is greater  than Y's on the various test sets and metrics. Bold face indicates p < .05.
true	P13-2108.pdf#<.001	benchmark Vietnamese dependency treebank VnDT, UAS	- Test Set BROWN LAS  Table 2: Results of statistical significance evaluations of hypotheses of the form X's accuracy is greater  than Y's on the various test sets and metrics. Bold face indicates p < .05.
true	P13-2108.pdf#<.001	benchmark Vietnamese dependency treebank VnDT, UAS	- Test Set WSJ LAS  Table 2: Results of statistical significance evaluations of hypotheses of the form X's accuracy is greater  than Y's on the various test sets and metrics. Bold face indicates p < .05.
true	P13-2108.pdf#.140	benchmark Vietnamese dependency treebank VnDT, UAS	Test Set BROWN F 1  Table 2: Results of statistical significance evaluations of hypotheses of the form X's accuracy is greater  than Y's on the various test sets and metrics. Bold face indicates p < .05.
true	D14-1054.pdf#85.51	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Joint Macro - F1  Table 6: Macro-F1 for positive vs negative classi- fication of tweets.
true	P15-1067.pdf#88.0	FB15K-237, MRR	- FB15K  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#90.0	FB15K-237, MRR	- FB13  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#86.4	FB15K-237, MRR	- WN11  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#49.4	FB15K-237, MRR	FB15K Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#79.8	FB15K-237, MRR	WN18 Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#77.3	FB15K-237, MRR	FB15K Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#53.4	FB15K-237, MRR	FB15K Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#74.2	FB15K-237, MRR	FB15K Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#92.5	FB15K-237, MRR	WN18 Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#39.8	FB15K-237, MRR	Prediction Head ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#50.6	FB15K-237, MRR	Prediction Tail ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#94.4	FB15K-237, MRR	Prediction Tail ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#77.9	FB15K-237, MRR	Prediction Tail ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#54.5	FB15K-237, MRR	Prediction Tail ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#47.1	FB15K-237, MRR	Prediction Head ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#85.4	FB15K-237, MRR	Prediction Tail ( Hits@10 ) 1 - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#81.2	FB15K-237, MRR	Prediction Tail ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#95.5	FB15K-237, MRR	Prediction Head ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#86.1	FB15K-237, MRR	Prediction Head ( Hits@10 ) 1 - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#78.5	FB15K-237, MRR	Prediction Head ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#75.6	FB15K-237, MRR	Prediction Head ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#88.0	FB15K-237, H@1	- FB15K  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#90.0	FB15K-237, H@1	- FB13  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#86.4	FB15K-237, H@1	- WN11  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#49.4	FB15K-237, H@1	FB15K Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#79.8	FB15K-237, H@1	WN18 Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#77.3	FB15K-237, H@1	FB15K Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#53.4	FB15K-237, H@1	FB15K Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#74.2	FB15K-237, H@1	FB15K Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#92.5	FB15K-237, H@1	WN18 Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#39.8	FB15K-237, H@1	Prediction Head ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#50.6	FB15K-237, H@1	Prediction Tail ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#94.4	FB15K-237, H@1	Prediction Tail ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#77.9	FB15K-237, H@1	Prediction Tail ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#54.5	FB15K-237, H@1	Prediction Tail ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#47.1	FB15K-237, H@1	Prediction Head ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#85.4	FB15K-237, H@1	Prediction Tail ( Hits@10 ) 1 - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#81.2	FB15K-237, H@1	Prediction Tail ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#95.5	FB15K-237, H@1	Prediction Head ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#86.1	FB15K-237, H@1	Prediction Head ( Hits@10 ) 1 - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#78.5	FB15K-237, H@1	Prediction Head ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#75.6	FB15K-237, H@1	Prediction Head ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#88.0	FB15K-237, H@10	- FB15K  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#90.0	FB15K-237, H@10	- FB13  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#86.4	FB15K-237, H@10	- WN11  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#49.4	FB15K-237, H@10	FB15K Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#79.8	FB15K-237, H@10	WN18 Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#77.3	FB15K-237, H@10	FB15K Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#53.4	FB15K-237, H@10	FB15K Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#74.2	FB15K-237, H@10	FB15K Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#92.5	FB15K-237, H@10	WN18 Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#39.8	FB15K-237, H@10	Prediction Head ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#50.6	FB15K-237, H@10	Prediction Tail ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#94.4	FB15K-237, H@10	Prediction Tail ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#77.9	FB15K-237, H@10	Prediction Tail ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#54.5	FB15K-237, H@10	Prediction Tail ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#47.1	FB15K-237, H@10	Prediction Head ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#85.4	FB15K-237, H@10	Prediction Tail ( Hits@10 ) 1 - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#81.2	FB15K-237, H@10	Prediction Tail ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#95.5	FB15K-237, H@10	Prediction Head ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#86.1	FB15K-237, H@10	Prediction Head ( Hits@10 ) 1 - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#78.5	FB15K-237, H@10	Prediction Head ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#75.6	FB15K-237, H@10	Prediction Head ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#88.0	WN18RR, H@10	- FB15K  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#90.0	WN18RR, H@10	- FB13  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#86.4	WN18RR, H@10	- WN11  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#49.4	WN18RR, H@10	FB15K Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#79.8	WN18RR, H@10	WN18 Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#77.3	WN18RR, H@10	FB15K Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#53.4	WN18RR, H@10	FB15K Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#74.2	WN18RR, H@10	FB15K Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#92.5	WN18RR, H@10	WN18 Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#39.8	WN18RR, H@10	Prediction Head ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#50.6	WN18RR, H@10	Prediction Tail ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#94.4	WN18RR, H@10	Prediction Tail ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#77.9	WN18RR, H@10	Prediction Tail ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#54.5	WN18RR, H@10	Prediction Tail ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#47.1	WN18RR, H@10	Prediction Head ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#85.4	WN18RR, H@10	Prediction Tail ( Hits@10 ) 1 - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#81.2	WN18RR, H@10	Prediction Tail ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#95.5	WN18RR, H@10	Prediction Head ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#86.1	WN18RR, H@10	Prediction Head ( Hits@10 ) 1 - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#78.5	WN18RR, H@10	Prediction Head ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#75.6	WN18RR, H@10	Prediction Head ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#88.0	WN18RR, H@1	- FB15K  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#90.0	WN18RR, H@1	- FB13  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#86.4	WN18RR, H@1	- WN11  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#49.4	WN18RR, H@1	FB15K Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#79.8	WN18RR, H@1	WN18 Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#77.3	WN18RR, H@1	FB15K Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#53.4	WN18RR, H@1	FB15K Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#74.2	WN18RR, H@1	FB15K Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#92.5	WN18RR, H@1	WN18 Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#39.8	WN18RR, H@1	Prediction Head ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#50.6	WN18RR, H@1	Prediction Tail ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#94.4	WN18RR, H@1	Prediction Tail ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#77.9	WN18RR, H@1	Prediction Tail ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#54.5	WN18RR, H@1	Prediction Tail ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#47.1	WN18RR, H@1	Prediction Head ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#85.4	WN18RR, H@1	Prediction Tail ( Hits@10 ) 1 - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#81.2	WN18RR, H@1	Prediction Tail ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#95.5	WN18RR, H@1	Prediction Head ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#86.1	WN18RR, H@1	Prediction Head ( Hits@10 ) 1 - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#78.5	WN18RR, H@1	Prediction Head ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#75.6	WN18RR, H@1	Prediction Head ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P13-1161.pdf#59.79*	New York Times Corpus, P@10%	Opinion Expression F1  Table 2: Performance on opinion entity extraction using overlap and exact matching metrics (the top table uses overlap and
true	P13-1161.pdf#74.35*	New York Times Corpus, P@10%	Opinion Expression F1  Table 2: Performance on opinion entity extraction using overlap and exact matching metrics (the top table uses overlap and
true	P13-1161.pdf#64.92**	New York Times Corpus, P@10%	Opinion Target F1  Table 2: Performance on opinion entity extraction using overlap and exact matching metrics (the top table uses overlap and
true	P13-1161.pdf#62.47**	New York Times Corpus, P@10%	Opinion Holder F1  Table 2: Performance on opinion entity extraction using overlap and exact matching metrics (the top table uses overlap and
true	P13-1161.pdf#66.73**	New York Times Corpus, P@10%	Opinion Holder F1  Table 2: Performance on opinion entity extraction using overlap and exact matching metrics (the top table uses overlap and
true	P13-1161.pdf#43.07**	New York Times Corpus, P@10%	Opinion Target F1  Table 2: Performance on opinion entity extraction using overlap and exact matching metrics (the top table uses overlap and
true	P13-1161.pdf#61.63**	New York Times Corpus, P@10%	IS - FROM F1  Table 3: Performance on opinion relation extraction using the overlap metric.
true	P13-1161.pdf#57.04**	New York Times Corpus, P@10%	IS - ABOUT F1  Table 3: Performance on opinion relation extraction using the overlap metric.
true	P13-1161.pdf#57.04**	New York Times Corpus, P@10%	Method  Table 4: Comparison between our approach and ILP baselines that omit some potentials in our approach.
true	P13-1161.pdf#61.63*	New York Times Corpus, P@10%	Method  Table 4: Comparison between our approach and ILP baselines that omit some potentials in our approach.
true	D15-1041.pdf#76.16	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#80.02	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#74.91	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#92.94	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#86.88	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#87.20	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#87.00	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#86.80	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#85.38	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#85.58	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#84.34	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#91.23	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#84.97	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#74.11	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#89.14	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#80.36	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#62.87	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#88.98	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#62.91	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#81.84	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#90.94	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#87.44	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#86.21	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#75.03	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#77.96	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#80.85	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#79.78	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#78.23	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#84.81	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#78.22	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#72.09	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#81.70	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#80.16	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#89.32	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#85.98	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#82.71	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#88.68	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#83.86	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#79.52	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#70.58	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#79.92	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#91.98	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#84.36	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#75.61	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#89.04	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#89.58	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#90.63	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#77.06	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#85.69	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#81.99	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#78.37	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#66.74	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#85.19	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.83	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#88.39	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#71.26	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#80.68	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#62.28	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#64.34	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#91.47	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.27	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#87.06	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.21	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#90.31	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#92.57	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.09	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#84.04	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#84.62	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.08	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#84.40	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.15	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#83.46	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.40	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#88.94	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#84.68	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#88.30	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#80.92	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#82.03	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.34	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#80.35	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#87.33	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.80	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#80.92	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.32	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#83.44	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#72.70	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.18	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.84	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#78.15	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#85.22	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#83.43	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.21	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#85.34	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.93	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.34	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.07	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#78.61	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#75.19	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.29	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#85.36	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#71.19	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#83.41	benchmark Vietnamese dependency treebank VnDT, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.94	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#85.96	benchmark Vietnamese dependency treebank VnDT, UAS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.16	Penn Treebank, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#80.02	Penn Treebank, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#74.91	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#92.94	Penn Treebank, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#86.88	Penn Treebank, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#87.20	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#87.00	Penn Treebank, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#86.80	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#85.38	Penn Treebank, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#85.58	Penn Treebank, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#84.34	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#91.23	Penn Treebank, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#84.97	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#74.11	Penn Treebank, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#89.14	Penn Treebank, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#80.36	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#62.87	Penn Treebank, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#88.98	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#62.91	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#81.84	Penn Treebank, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#90.94	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#87.44	Penn Treebank, UAS	LAS Words  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#86.21	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#75.03	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#77.96	Penn Treebank, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#80.85	Penn Treebank, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#79.78	Penn Treebank, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#78.23	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#84.81	Penn Treebank, UAS	LAS Words  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#78.22	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#72.09	Penn Treebank, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#81.70	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#80.16	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#89.32	Penn Treebank, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#85.98	Penn Treebank, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#82.71	Penn Treebank, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#88.68	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#83.86	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#79.52	Penn Treebank, UAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#70.58	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#79.92	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#91.98	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#84.36	Penn Treebank, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#75.61	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#89.04	Penn Treebank, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#89.58	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#90.63	Penn Treebank, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#77.06	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#85.69	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#81.99	Penn Treebank, UAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#78.37	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#66.74	Penn Treebank, UAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#85.19	Penn Treebank, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.83	Penn Treebank, UAS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#88.39	Penn Treebank, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#71.26	Penn Treebank, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#80.68	Penn Treebank, UAS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#62.28	Penn Treebank, UAS	LAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#64.34	Penn Treebank, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#91.47	Penn Treebank, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.27	Penn Treebank, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#87.06	Penn Treebank, UAS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.21	Penn Treebank, UAS	LAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#90.31	Penn Treebank, UAS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#92.57	Penn Treebank, UAS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.09	Penn Treebank, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#84.04	Penn Treebank, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#84.62	Penn Treebank, UAS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.08	Penn Treebank, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#84.40	Penn Treebank, UAS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.15	Penn Treebank, UAS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#83.46	Penn Treebank, UAS	LAS Words  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.40	Penn Treebank, UAS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#88.94	Penn Treebank, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#84.68	Penn Treebank, UAS	UAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#88.30	Penn Treebank, UAS	UAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#80.92	Penn Treebank, UAS	UAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#82.03	Penn Treebank, UAS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.34	Penn Treebank, UAS	UAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#80.35	Penn Treebank, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#87.33	Penn Treebank, UAS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.80	Penn Treebank, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#80.92	Penn Treebank, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.32	Penn Treebank, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#83.44	Penn Treebank, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#72.70	Penn Treebank, UAS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.18	Penn Treebank, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.84	Penn Treebank, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#78.15	Penn Treebank, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#85.22	Penn Treebank, UAS	UAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#83.43	Penn Treebank, UAS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.21	Penn Treebank, UAS	LAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#85.34	Penn Treebank, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.93	Penn Treebank, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.34	Penn Treebank, UAS	LAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.07	Penn Treebank, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#78.61	Penn Treebank, UAS	LAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#75.19	Penn Treebank, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.29	Penn Treebank, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#85.36	Penn Treebank, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#71.19	Penn Treebank, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#83.41	Penn Treebank, UAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.94	Penn Treebank, UAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#85.96	Penn Treebank, UAS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.16	Penn Treebank, LAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#80.02	Penn Treebank, LAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#74.91	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#92.94	Penn Treebank, LAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#86.88	Penn Treebank, LAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#87.20	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#87.00	Penn Treebank, LAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#86.80	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#85.38	Penn Treebank, LAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#85.58	Penn Treebank, LAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#84.34	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#91.23	Penn Treebank, LAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#84.97	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#74.11	Penn Treebank, LAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#89.14	Penn Treebank, LAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#80.36	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#62.87	Penn Treebank, LAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#88.98	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#62.91	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#81.84	Penn Treebank, LAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#90.94	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#87.44	Penn Treebank, LAS	LAS Words  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#86.21	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#75.03	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#77.96	Penn Treebank, LAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#80.85	Penn Treebank, LAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#79.78	Penn Treebank, LAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#78.23	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#84.81	Penn Treebank, LAS	LAS Words  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#78.22	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#72.09	Penn Treebank, LAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#81.70	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#80.16	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#89.32	Penn Treebank, LAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#85.98	Penn Treebank, LAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#82.71	Penn Treebank, LAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#88.68	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#83.86	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#79.52	Penn Treebank, LAS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#70.58	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#79.92	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#91.98	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#84.36	Penn Treebank, LAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#75.61	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#89.04	Penn Treebank, LAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#89.58	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#90.63	Penn Treebank, LAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#77.06	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#85.69	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#81.99	Penn Treebank, LAS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#78.37	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#66.74	Penn Treebank, LAS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#85.19	Penn Treebank, LAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.83	Penn Treebank, LAS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#88.39	Penn Treebank, LAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#71.26	Penn Treebank, LAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#80.68	Penn Treebank, LAS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#62.28	Penn Treebank, LAS	LAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#64.34	Penn Treebank, LAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#91.47	Penn Treebank, LAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.27	Penn Treebank, LAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#87.06	Penn Treebank, LAS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.21	Penn Treebank, LAS	LAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#90.31	Penn Treebank, LAS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#92.57	Penn Treebank, LAS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.09	Penn Treebank, LAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#84.04	Penn Treebank, LAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#84.62	Penn Treebank, LAS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.08	Penn Treebank, LAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#84.40	Penn Treebank, LAS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.15	Penn Treebank, LAS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#83.46	Penn Treebank, LAS	LAS Words  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.40	Penn Treebank, LAS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#88.94	Penn Treebank, LAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#84.68	Penn Treebank, LAS	UAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#88.30	Penn Treebank, LAS	UAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#80.92	Penn Treebank, LAS	UAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#82.03	Penn Treebank, LAS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.34	Penn Treebank, LAS	UAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#80.35	Penn Treebank, LAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#87.33	Penn Treebank, LAS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.80	Penn Treebank, LAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#80.92	Penn Treebank, LAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.32	Penn Treebank, LAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#83.44	Penn Treebank, LAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#72.70	Penn Treebank, LAS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.18	Penn Treebank, LAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.84	Penn Treebank, LAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#78.15	Penn Treebank, LAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#85.22	Penn Treebank, LAS	UAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#83.43	Penn Treebank, LAS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.21	Penn Treebank, LAS	LAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#85.34	Penn Treebank, LAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.93	Penn Treebank, LAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.34	Penn Treebank, LAS	LAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.07	Penn Treebank, LAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#78.61	Penn Treebank, LAS	LAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#75.19	Penn Treebank, LAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.29	Penn Treebank, LAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#85.36	Penn Treebank, LAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#71.19	Penn Treebank, LAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#83.41	Penn Treebank, LAS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.94	Penn Treebank, LAS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#85.96	Penn Treebank, LAS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.16	Penn Treebank, POS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#80.02	Penn Treebank, POS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#74.91	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#92.94	Penn Treebank, POS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#86.88	Penn Treebank, POS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#87.20	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#87.00	Penn Treebank, POS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#86.80	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#85.38	Penn Treebank, POS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#85.58	Penn Treebank, POS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#84.34	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#91.23	Penn Treebank, POS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#84.97	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#74.11	Penn Treebank, POS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#89.14	Penn Treebank, POS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#80.36	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#62.87	Penn Treebank, POS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#88.98	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#62.91	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#81.84	Penn Treebank, POS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#90.94	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#87.44	Penn Treebank, POS	LAS Words  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#86.21	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#75.03	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#77.96	Penn Treebank, POS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#80.85	Penn Treebank, POS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#79.78	Penn Treebank, POS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#78.23	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#84.81	Penn Treebank, POS	LAS Words  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#78.22	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#72.09	Penn Treebank, POS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#81.70	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#80.16	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#89.32	Penn Treebank, POS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#85.98	Penn Treebank, POS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#82.71	Penn Treebank, POS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#88.68	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#83.86	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#79.52	Penn Treebank, POS	LAS Chars + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#70.58	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#79.92	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#91.98	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#84.36	Penn Treebank, POS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#75.61	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#89.04	Penn Treebank, POS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#89.58	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#90.63	Penn Treebank, POS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#77.06	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#85.69	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#81.99	Penn Treebank, POS	LAS Words + POS  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#78.37	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#66.74	Penn Treebank, POS	LAS Chars  Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development  sets (not a standard development set for Turkish). In each table, the first two columns show the results of  the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns  add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +  POS vs. Chars + POS.
true	D15-1041.pdf#85.19	Penn Treebank, POS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.83	Penn Treebank, POS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#88.39	Penn Treebank, POS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#71.26	Penn Treebank, POS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#80.68	Penn Treebank, POS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#62.28	Penn Treebank, POS	LAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#64.34	Penn Treebank, POS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#91.47	Penn Treebank, POS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.27	Penn Treebank, POS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#87.06	Penn Treebank, POS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.21	Penn Treebank, POS	LAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#90.31	Penn Treebank, POS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#92.57	Penn Treebank, POS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.09	Penn Treebank, POS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#84.04	Penn Treebank, POS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#84.62	Penn Treebank, POS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.08	Penn Treebank, POS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#84.40	Penn Treebank, POS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.15	Penn Treebank, POS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#83.46	Penn Treebank, POS	LAS Words  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.40	Penn Treebank, POS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#88.94	Penn Treebank, POS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#84.68	Penn Treebank, POS	UAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#88.30	Penn Treebank, POS	UAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#80.92	Penn Treebank, POS	UAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#82.03	Penn Treebank, POS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.34	Penn Treebank, POS	UAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#80.35	Penn Treebank, POS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#87.33	Penn Treebank, POS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.80	Penn Treebank, POS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#80.92	Penn Treebank, POS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.32	Penn Treebank, POS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#83.44	Penn Treebank, POS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#72.70	Penn Treebank, POS	LAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.18	Penn Treebank, POS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.84	Penn Treebank, POS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#78.15	Penn Treebank, POS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#85.22	Penn Treebank, POS	UAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#83.43	Penn Treebank, POS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.21	Penn Treebank, POS	LAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#85.34	Penn Treebank, POS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.93	Penn Treebank, POS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.34	Penn Treebank, POS	LAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#86.07	Penn Treebank, POS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#78.61	Penn Treebank, POS	LAS Chars + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#75.19	Penn Treebank, POS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#76.29	Penn Treebank, POS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#85.36	Penn Treebank, POS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#71.19	Penn Treebank, POS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#83.41	Penn Treebank, POS	LAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#79.94	Penn Treebank, POS	UAS Chars  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D15-1041.pdf#85.96	Penn Treebank, POS	UAS Words + POS  Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In  each table, the first two columns show the results of the parser with word lookup (Words) vs. character- based (Chars) representations. The last two columns add POS tags. Boldface shows the better result  comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
true	D14-1128.pdf#.94	Senseval 2, F1	F 1  Table 2: Classification results; bl: baseline
true	D14-1128.pdf#.89	Senseval 2, F1	F 1  Table 2: Classification results; bl: baseline
true	D14-1128.pdf#.94	Senseval 3, F1	F 1  Table 2: Classification results; bl: baseline
true	D14-1128.pdf#.89	Senseval 3, F1	F 1  Table 2: Classification results; bl: baseline
true	D14-1128.pdf#.94	SemEval 2013, F1	F 1  Table 2: Classification results; bl: baseline
true	D14-1128.pdf#.89	SemEval 2013, F1	F 1  Table 2: Classification results; bl: baseline
true	D14-1128.pdf#.94	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F 1  Table 2: Classification results; bl: baseline
true	D14-1128.pdf#.89	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F 1  Table 2: Classification results; bl: baseline
true	D14-1128.pdf#.94	SemEval 2007, F1	F 1  Table 2: Classification results; bl: baseline
true	D14-1128.pdf#.89	SemEval 2007, F1	F 1  Table 2: Classification results; bl: baseline
true	D14-1128.pdf#.94	SemEval 2015, F1	F 1  Table 2: Classification results; bl: baseline
true	D14-1128.pdf#.89	SemEval 2015, F1	F 1  Table 2: Classification results; bl: baseline
true	D11-1005.pdf#83.3	Penn Treebank, POS	( a ) Es Es  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#82.8	Penn Treebank, POS	( a ) Sl Sl  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#82.8	Penn Treebank, POS	( a ) Sl Sl  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#53.5	Penn Treebank, POS	Pt  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#43.3	Penn Treebank, POS	Avg  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#82.3	Penn Treebank, POS	( a ) Da Da  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#69.0	Penn Treebank, POS	( a ) Sv Sv  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#43.6	Penn Treebank, POS	Tr  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#84.7	Penn Treebank, POS	( a ) Pt Pt  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#50.4	Penn Treebank, POS	( a ) Tr Tr  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#38.9	Penn Treebank, POS	El  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#48.8	Penn Treebank, POS	Da  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#40.5	Penn Treebank, POS	Es  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#61.7	Penn Treebank, POS	Jp  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#50.4	Penn Treebank, POS	( a ) Tr Tr  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#88.0	Penn Treebank, POS	( a ) El El  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#77.3	Penn Treebank, POS	( a ) Avg Avg  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#50.2	Penn Treebank, POS	Nl  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#80.0	Penn Treebank, POS	( a ) Nl Nl  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#39.9	Penn Treebank, POS	Sv  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#82.6	Penn Treebank, POS	( a ) Bg Bg  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#38.0	Penn Treebank, POS	Bg  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#83.4	Penn Treebank, POS	( a ) Jp Jp  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	D11-1005.pdf#36.0	Penn Treebank, POS	Sl  Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con- structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient  method of
true	P15-2039.pdf#94.21	benchmark Vietnamese dependency treebank VnDT, UAS	UAS  Table 3: Parsing results.
true	P15-2039.pdf#56.01	benchmark Vietnamese dependency treebank VnDT, UAS	F1 score LAS HF2  Table 3: Parsing results.
true	P15-2039.pdf#53.56	benchmark Vietnamese dependency treebank VnDT, UAS	F1 score LAS HF2  Table 3: Parsing results.
true	P15-2039.pdf#83.07	benchmark Vietnamese dependency treebank VnDT, UAS	F1 score LAS HF2  Table 3: Parsing results.
true	P15-2039.pdf#82.12	benchmark Vietnamese dependency treebank VnDT, UAS	F1 score LAS HF2  Table 3: Parsing results.
true	P15-2039.pdf#89.66	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 3: Parsing results.
true	P15-2039.pdf#39.46	benchmark Vietnamese dependency treebank VnDT, UAS	F1 score LAS PCH  Table 3: Parsing results.
true	P15-2039.pdf#92.29	benchmark Vietnamese dependency treebank VnDT, UAS	F1 score LAS PCH  Table 3: Parsing results.
true	P15-2039.pdf#66.70	benchmark Vietnamese dependency treebank VnDT, UAS	F1 score LAS HF2  Table 3: Parsing results.
true	P15-2039.pdf#62.95	benchmark Vietnamese dependency treebank VnDT, UAS	F1 score LAS PCH  Table 3: Parsing results.
true	P15-2039.pdf#57.45	benchmark Vietnamese dependency treebank VnDT, UAS	F1 score LAS HF2  Table 3: Parsing results.
true	P15-2039.pdf#36.36	benchmark Vietnamese dependency treebank VnDT, UAS	F1 score LAS PCH  Table 3: Parsing results.
true	P15-2039.pdf#70.78	benchmark Vietnamese dependency treebank VnDT, UAS	F1 score UAS HF1  Table 3: Parsing results.
true	P15-2039.pdf#82.05	benchmark Vietnamese dependency treebank VnDT, UAS	F1 score UAS HF1  Table 3: Parsing results.
true	P15-2039.pdf#82.5	benchmark Vietnamese dependency treebank VnDT, UAS	Precision  Table 4: Predicate-argument structure analysis.
true	P15-2039.pdf#77.1	benchmark Vietnamese dependency treebank VnDT, UAS	F1 score  Table 4: Predicate-argument structure analysis.
true	P15-2039.pdf#72.4	benchmark Vietnamese dependency treebank VnDT, UAS	Recall  Table 4: Predicate-argument structure analysis.
true	P15-2039.pdf#94.21	Penn Treebank, UAS	UAS  Table 3: Parsing results.
true	P15-2039.pdf#56.01	Penn Treebank, UAS	F1 score LAS HF2  Table 3: Parsing results.
true	P15-2039.pdf#53.56	Penn Treebank, UAS	F1 score LAS HF2  Table 3: Parsing results.
true	P15-2039.pdf#83.07	Penn Treebank, UAS	F1 score LAS HF2  Table 3: Parsing results.
true	P15-2039.pdf#82.12	Penn Treebank, UAS	F1 score LAS HF2  Table 3: Parsing results.
true	P15-2039.pdf#89.66	Penn Treebank, UAS	LAS  Table 3: Parsing results.
true	P15-2039.pdf#39.46	Penn Treebank, UAS	F1 score LAS PCH  Table 3: Parsing results.
true	P15-2039.pdf#92.29	Penn Treebank, UAS	F1 score LAS PCH  Table 3: Parsing results.
true	P15-2039.pdf#66.70	Penn Treebank, UAS	F1 score LAS HF2  Table 3: Parsing results.
true	P15-2039.pdf#62.95	Penn Treebank, UAS	F1 score LAS PCH  Table 3: Parsing results.
true	P15-2039.pdf#57.45	Penn Treebank, UAS	F1 score LAS HF2  Table 3: Parsing results.
true	P15-2039.pdf#36.36	Penn Treebank, UAS	F1 score LAS PCH  Table 3: Parsing results.
true	P15-2039.pdf#70.78	Penn Treebank, UAS	F1 score UAS HF1  Table 3: Parsing results.
true	P15-2039.pdf#82.05	Penn Treebank, UAS	F1 score UAS HF1  Table 3: Parsing results.
true	P15-2039.pdf#82.5	Penn Treebank, UAS	Precision  Table 4: Predicate-argument structure analysis.
true	P15-2039.pdf#77.1	Penn Treebank, UAS	F1 score  Table 4: Predicate-argument structure analysis.
true	P15-2039.pdf#72.4	Penn Treebank, UAS	Recall  Table 4: Predicate-argument structure analysis.
true	P11-2121.pdf#89.15*	Penn Treebank, UAS	English LAS  Table 3: Accuracy comparisons between different pars- ing approaches (LAS/UAS: labeled/unlabeled attachment  score).  *  indicates a statistically significant improvement.  (
true	P11-2121.pdf#91.18*	Penn Treebank, UAS	English UAS  Table 3: Accuracy comparisons between different pars- ing approaches (LAS/UAS: labeled/unlabeled attachment  score).  *  indicates a statistically significant improvement.  (
true	P11-2121.pdf#80.24*	Penn Treebank, UAS	Czech LAS  Table 3: Accuracy comparisons between different pars- ing approaches (LAS/UAS: labeled/unlabeled attachment  score).  *  indicates a statistically significant improvement.  (
true	P11-2121.pdf#85.24*	Penn Treebank, UAS	Czech UAS  Table 3: Accuracy comparisons between different pars- ing approaches (LAS/UAS: labeled/unlabeled attachment  score).  *  indicates a statistically significant improvement.  (
true	P11-2121.pdf#89.15*	benchmark Vietnamese dependency treebank VnDT, UAS	English LAS  Table 3: Accuracy comparisons between different pars- ing approaches (LAS/UAS: labeled/unlabeled attachment  score).  *  indicates a statistically significant improvement.  (
true	P11-2121.pdf#91.18*	benchmark Vietnamese dependency treebank VnDT, UAS	English UAS  Table 3: Accuracy comparisons between different pars- ing approaches (LAS/UAS: labeled/unlabeled attachment  score).  *  indicates a statistically significant improvement.  (
true	P11-2121.pdf#80.24*	benchmark Vietnamese dependency treebank VnDT, UAS	Czech LAS  Table 3: Accuracy comparisons between different pars- ing approaches (LAS/UAS: labeled/unlabeled attachment  score).  *  indicates a statistically significant improvement.  (
true	P11-2121.pdf#85.24*	benchmark Vietnamese dependency treebank VnDT, UAS	Czech UAS  Table 3: Accuracy comparisons between different pars- ing approaches (LAS/UAS: labeled/unlabeled attachment  score).  *  indicates a statistically significant improvement.  (
true	P15-1166.pdf#+1.38	WMT 2014 EN-FR, BLEU	En - Es  Table 3: Multi-task neural translation v.s. single  model given large-scale corpus in all language  pairs
true	P15-1166.pdf#+1.13	WMT 2014 EN-FR, BLEU	En - Nl  Table 3: Multi-task neural translation v.s. single  model given large-scale corpus in all language  pairs
true	P15-1166.pdf#+0.48	WMT 2014 EN-FR, BLEU	En - Pt  Table 3: Multi-task neural translation v.s. single  model given large-scale corpus in all language  pairs
true	P15-1166.pdf#+1.25	WMT 2014 EN-FR, BLEU	En - Fr  Table 3: Multi-task neural translation v.s. single  model given large-scale corpus in all language  pairs
true	P15-1166.pdf#+1.64	WMT 2014 EN-FR, BLEU	En - Es  Table 4: Multi-task neural translation v.s. single  model with a small-scale training corpus on some  language pairs. * means that the language pair is  sub-sampled.
true	P15-1166.pdf#+0.67	WMT 2014 EN-FR, BLEU	En - Fr  Table 4: Multi-task neural translation v.s. single  model with a small-scale training corpus on some  language pairs. * means that the language pair is  sub-sampled.
true	P15-1166.pdf#+1.26	WMT 2014 EN-FR, BLEU	En - Nl *  Table 4: Multi-task neural translation v.s. single  model with a small-scale training corpus on some  language pairs. * means that the language pair is  sub-sampled.
true	P15-1166.pdf#+1.06	WMT 2014 EN-FR, BLEU	En - Pt *  Table 4: Multi-task neural translation v.s. single  model with a small-scale training corpus on some  language pairs. * means that the language pair is  sub-sampled.
true	P15-1166.pdf#+1.38	WMT 2014 EN-DE, BLEU	En - Es  Table 3: Multi-task neural translation v.s. single  model given large-scale corpus in all language  pairs
true	P15-1166.pdf#+1.13	WMT 2014 EN-DE, BLEU	En - Nl  Table 3: Multi-task neural translation v.s. single  model given large-scale corpus in all language  pairs
true	P15-1166.pdf#+0.48	WMT 2014 EN-DE, BLEU	En - Pt  Table 3: Multi-task neural translation v.s. single  model given large-scale corpus in all language  pairs
true	P15-1166.pdf#+1.25	WMT 2014 EN-DE, BLEU	En - Fr  Table 3: Multi-task neural translation v.s. single  model given large-scale corpus in all language  pairs
true	P15-1166.pdf#+1.64	WMT 2014 EN-DE, BLEU	En - Es  Table 4: Multi-task neural translation v.s. single  model with a small-scale training corpus on some  language pairs. * means that the language pair is  sub-sampled.
true	P15-1166.pdf#+0.67	WMT 2014 EN-DE, BLEU	En - Fr  Table 4: Multi-task neural translation v.s. single  model with a small-scale training corpus on some  language pairs. * means that the language pair is  sub-sampled.
true	P15-1166.pdf#+1.26	WMT 2014 EN-DE, BLEU	En - Nl *  Table 4: Multi-task neural translation v.s. single  model with a small-scale training corpus on some  language pairs. * means that the language pair is  sub-sampled.
true	P15-1166.pdf#+1.06	WMT 2014 EN-DE, BLEU	En - Pt *  Table 4: Multi-task neural translation v.s. single  model with a small-scale training corpus on some  language pairs. * means that the language pair is  sub-sampled.
true	D13-1058.pdf#64.8	SemEval 2007, F1	F1 score  Table 1: WSD results: Macro-averaged F1 score (points)
true	D13-1058.pdf#79.4	SemEval 2007, F1	( a ) Reuters Transcribed F1 score F1 score  Table 3: Document classification results: F1 score (%)  (larger is better) and skewness of the N 10 distribution for  each similarity measure (smaller is better).
true	D13-1058.pdf#−0.10	SemEval 2007, F1	Skewness  Table 3: Document classification results: F1 score (%)  (larger is better) and skewness of the N 10 distribution for  each similarity measure (smaller is better).
true	D13-1058.pdf#61.2	SemEval 2007, F1	F1 score  Table 3: Document classification results: F1 score (%)  (larger is better) and skewness of the N 10 distribution for  each similarity measure (smaller is better).
true	D13-1058.pdf#64.8	SemEval 2013, F1	F1 score  Table 1: WSD results: Macro-averaged F1 score (points)
true	D13-1058.pdf#79.4	SemEval 2013, F1	( a ) Reuters Transcribed F1 score F1 score  Table 3: Document classification results: F1 score (%)  (larger is better) and skewness of the N 10 distribution for  each similarity measure (smaller is better).
true	D13-1058.pdf#−0.10	SemEval 2013, F1	Skewness  Table 3: Document classification results: F1 score (%)  (larger is better) and skewness of the N 10 distribution for  each similarity measure (smaller is better).
true	D13-1058.pdf#61.2	SemEval 2013, F1	F1 score  Table 3: Document classification results: F1 score (%)  (larger is better) and skewness of the N 10 distribution for  each similarity measure (smaller is better).
true	D13-1058.pdf#64.8	Senseval 2, F1	F1 score  Table 1: WSD results: Macro-averaged F1 score (points)
true	D13-1058.pdf#79.4	Senseval 2, F1	( a ) Reuters Transcribed F1 score F1 score  Table 3: Document classification results: F1 score (%)  (larger is better) and skewness of the N 10 distribution for  each similarity measure (smaller is better).
true	D13-1058.pdf#−0.10	Senseval 2, F1	Skewness  Table 3: Document classification results: F1 score (%)  (larger is better) and skewness of the N 10 distribution for  each similarity measure (smaller is better).
true	D13-1058.pdf#61.2	Senseval 2, F1	F1 score  Table 3: Document classification results: F1 score (%)  (larger is better) and skewness of the N 10 distribution for  each similarity measure (smaller is better).
true	D13-1058.pdf#64.8	Senseval 3, F1	F1 score  Table 1: WSD results: Macro-averaged F1 score (points)
true	D13-1058.pdf#79.4	Senseval 3, F1	( a ) Reuters Transcribed F1 score F1 score  Table 3: Document classification results: F1 score (%)  (larger is better) and skewness of the N 10 distribution for  each similarity measure (smaller is better).
true	D13-1058.pdf#−0.10	Senseval 3, F1	Skewness  Table 3: Document classification results: F1 score (%)  (larger is better) and skewness of the N 10 distribution for  each similarity measure (smaller is better).
true	D13-1058.pdf#61.2	Senseval 3, F1	F1 score  Table 3: Document classification results: F1 score (%)  (larger is better) and skewness of the N 10 distribution for  each similarity measure (smaller is better).
true	D13-1058.pdf#64.8	SemEval 2015, F1	F1 score  Table 1: WSD results: Macro-averaged F1 score (points)
true	D13-1058.pdf#79.4	SemEval 2015, F1	( a ) Reuters Transcribed F1 score F1 score  Table 3: Document classification results: F1 score (%)  (larger is better) and skewness of the N 10 distribution for  each similarity measure (smaller is better).
true	D13-1058.pdf#−0.10	SemEval 2015, F1	Skewness  Table 3: Document classification results: F1 score (%)  (larger is better) and skewness of the N 10 distribution for  each similarity measure (smaller is better).
true	D13-1058.pdf#61.2	SemEval 2015, F1	F1 score  Table 3: Document classification results: F1 score (%)  (larger is better) and skewness of the N 10 distribution for  each similarity measure (smaller is better).
true	P15-1070.pdf#21.21	SemEval 2015, F1	P  Table 1: Coverage, precision, recall, and F 1 for  various pun diasmbiguation algorithms.
true	P15-1070.pdf#16.77	SemEval 2015, F1	F 1  Table 1: Coverage, precision, recall, and F 1 for  various pun diasmbiguation algorithms.
true	P15-1070.pdf#98.69	SemEval 2015, F1	C  Table 1: Coverage, precision, recall, and F 1 for  various pun diasmbiguation algorithms.
true	P15-1070.pdf#14.10	SemEval 2015, F1	R  Table 1: Coverage, precision, recall, and F 1 for  various pun diasmbiguation algorithms.
true	P15-1070.pdf#21.21	Senseval 2, F1	P  Table 1: Coverage, precision, recall, and F 1 for  various pun diasmbiguation algorithms.
true	P15-1070.pdf#16.77	Senseval 2, F1	F 1  Table 1: Coverage, precision, recall, and F 1 for  various pun diasmbiguation algorithms.
true	P15-1070.pdf#98.69	Senseval 2, F1	C  Table 1: Coverage, precision, recall, and F 1 for  various pun diasmbiguation algorithms.
true	P15-1070.pdf#14.10	Senseval 2, F1	R  Table 1: Coverage, precision, recall, and F 1 for  various pun diasmbiguation algorithms.
true	P15-1070.pdf#21.21	Senseval 3, F1	P  Table 1: Coverage, precision, recall, and F 1 for  various pun diasmbiguation algorithms.
true	P15-1070.pdf#16.77	Senseval 3, F1	F 1  Table 1: Coverage, precision, recall, and F 1 for  various pun diasmbiguation algorithms.
true	P15-1070.pdf#98.69	Senseval 3, F1	C  Table 1: Coverage, precision, recall, and F 1 for  various pun diasmbiguation algorithms.
true	P15-1070.pdf#14.10	Senseval 3, F1	R  Table 1: Coverage, precision, recall, and F 1 for  various pun diasmbiguation algorithms.
true	P15-1070.pdf#21.21	SemEval 2007, F1	P  Table 1: Coverage, precision, recall, and F 1 for  various pun diasmbiguation algorithms.
true	P15-1070.pdf#16.77	SemEval 2007, F1	F 1  Table 1: Coverage, precision, recall, and F 1 for  various pun diasmbiguation algorithms.
true	P15-1070.pdf#98.69	SemEval 2007, F1	C  Table 1: Coverage, precision, recall, and F 1 for  various pun diasmbiguation algorithms.
true	P15-1070.pdf#14.10	SemEval 2007, F1	R  Table 1: Coverage, precision, recall, and F 1 for  various pun diasmbiguation algorithms.
true	P15-1070.pdf#21.21	SemEval 2013, F1	P  Table 1: Coverage, precision, recall, and F 1 for  various pun diasmbiguation algorithms.
true	P15-1070.pdf#16.77	SemEval 2013, F1	F 1  Table 1: Coverage, precision, recall, and F 1 for  various pun diasmbiguation algorithms.
true	P15-1070.pdf#98.69	SemEval 2013, F1	C  Table 1: Coverage, precision, recall, and F 1 for  various pun diasmbiguation algorithms.
true	P15-1070.pdf#14.10	SemEval 2013, F1	R  Table 1: Coverage, precision, recall, and F 1 for  various pun diasmbiguation algorithms.
true	N15-1008.pdf#.014	Penn Treebank, F1	Removal precision  Table 5: Analysis of each feature used in Logit- Expanded. Results based on the first 50 sources returned,  averaged over all reports. Our Starting Point* system  listed within the "Addage" columns used LDA-Bayes as  the only feature. Our Starting Point* system within the  "Removal" columns used every feature.
true	N15-1008.pdf#.564	Penn Treebank, F1	Addage recall  Table 5: Analysis of each feature used in Logit- Expanded. Results based on the first 50 sources returned,  averaged over all reports. Our Starting Point* system  listed within the "Addage" columns used LDA-Bayes as  the only feature. Our Starting Point* system within the  "Removal" columns used every feature.
true	N15-1008.pdf#.015	Penn Treebank, F1	Removal precision  Table 5: Analysis of each feature used in Logit- Expanded. Results based on the first 50 sources returned,  averaged over all reports. Our Starting Point* system  listed within the "Addage" columns used LDA-Bayes as  the only feature. Our Starting Point* system within the  "Removal" columns used every feature.
true	N15-1008.pdf#.028	Penn Treebank, F1	Removal fscore  Table 5: Analysis of each feature used in Logit- Expanded. Results based on the first 50 sources returned,  averaged over all reports. Our Starting Point* system  listed within the "Addage" columns used LDA-Bayes as  the only feature. Our Starting Point* system within the  "Removal" columns used every feature.
true	N15-1008.pdf#.014	Penn Treebank, F1	Removal precision  Table 5: Analysis of each feature used in Logit- Expanded. Results based on the first 50 sources returned,  averaged over all reports. Our Starting Point* system  listed within the "Addage" columns used LDA-Bayes as  the only feature. Our Starting Point* system within the  "Removal" columns used every feature.
true	N15-1008.pdf#.028	Penn Treebank, F1	Addage fscore  Table 5: Analysis of each feature used in Logit- Expanded. Results based on the first 50 sources returned,  averaged over all reports. Our Starting Point* system  listed within the "Addage" columns used LDA-Bayes as  the only feature. Our Starting Point* system within the  "Removal" columns used every feature.
true	N15-1008.pdf#.606	Penn Treebank, F1	Removal recall  Table 5: Analysis of each feature used in Logit- Expanded. Results based on the first 50 sources returned,  averaged over all reports. Our Starting Point* system  listed within the "Addage" columns used LDA-Bayes as  the only feature. Our Starting Point* system within the  "Removal" columns used every feature.
true	N15-1008.pdf#.583	Penn Treebank, F1	Removal recall  Table 5: Analysis of each feature used in Logit- Expanded. Results based on the first 50 sources returned,  averaged over all reports. Our Starting Point* system  listed within the "Addage" columns used LDA-Bayes as  the only feature. Our Starting Point* system within the  "Removal" columns used every feature.
true	N15-1008.pdf#.014	Penn Treebank, F1	Addage precision  Table 5: Analysis of each feature used in Logit- Expanded. Results based on the first 50 sources returned,  averaged over all reports. Our Starting Point* system  listed within the "Addage" columns used LDA-Bayes as  the only feature. Our Starting Point* system within the  "Removal" columns used every feature.
true	N15-1008.pdf#.014	Penn Treebank, F1	Addage precision  Table 5: Analysis of each feature used in Logit- Expanded. Results based on the first 50 sources returned,  averaged over all reports. Our Starting Point* system  listed within the "Addage" columns used LDA-Bayes as  the only feature. Our Starting Point* system within the  "Removal" columns used every feature.
true	N15-1008.pdf#.027	Penn Treebank, F1	Addage fscore  Table 5: Analysis of each feature used in Logit- Expanded. Results based on the first 50 sources returned,  averaged over all reports. Our Starting Point* system  listed within the "Addage" columns used LDA-Bayes as  the only feature. Our Starting Point* system within the  "Removal" columns used every feature.
true	N15-1008.pdf#.599	Penn Treebank, F1	Removal recall  Table 5: Analysis of each feature used in Logit- Expanded. Results based on the first 50 sources returned,  averaged over all reports. Our Starting Point* system  listed within the "Addage" columns used LDA-Bayes as  the only feature. Our Starting Point* system within the  "Removal" columns used every feature.
true	N15-1008.pdf#.028	Penn Treebank, F1	Removal fscore  Table 5: Analysis of each feature used in Logit- Expanded. Results based on the first 50 sources returned,  averaged over all reports. Our Starting Point* system  listed within the "Addage" columns used LDA-Bayes as  the only feature. Our Starting Point* system within the  "Removal" columns used every feature.
true	N15-1008.pdf#.581	Penn Treebank, F1	Addage recall  Table 5: Analysis of each feature used in Logit- Expanded. Results based on the first 50 sources returned,  averaged over all reports. Our Starting Point* system  listed within the "Addage" columns used LDA-Bayes as  the only feature. Our Starting Point* system within the  "Removal" columns used every feature.
true	N15-1008.pdf#.028	Penn Treebank, F1	Removal fscore  Table 5: Analysis of each feature used in Logit- Expanded. Results based on the first 50 sources returned,  averaged over all reports. Our Starting Point* system  listed within the "Addage" columns used LDA-Bayes as  the only feature. Our Starting Point* system within the  "Removal" columns used every feature.
true	D15-1119.pdf#=0.1	WMT 2014 EN-DE, BLEU	ja - en  Table 1: Test set BLEU scores for Japanese- English and German-English translations. (  ‡ , sta- tistical significance at p < 0.01;  † , at p < 0.05; bold- face, no significance; all compared with GIZA++)
true	D15-1119.pdf#=4.0	WMT 2014 EN-DE, BLEU	ja - en  Table 1: Test set BLEU scores for Japanese- English and German-English translations. (  ‡ , sta- tistical significance at p < 0.01;  † , at p < 0.05; bold- face, no significance; all compared with GIZA++)
true	D15-1119.pdf#=0.1	WMT 2014 EN-FR, BLEU	ja - en  Table 1: Test set BLEU scores for Japanese- English and German-English translations. (  ‡ , sta- tistical significance at p < 0.01;  † , at p < 0.05; bold- face, no significance; all compared with GIZA++)
true	D15-1119.pdf#=4.0	WMT 2014 EN-FR, BLEU	ja - en  Table 1: Test set BLEU scores for Japanese- English and German-English translations. (  ‡ , sta- tistical significance at p < 0.01;  † , at p < 0.05; bold- face, no significance; all compared with GIZA++)
true	D15-1232.pdf#1.89	PKU, F1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	PKU, F1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	PKU, F1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	PKU, F1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Penn Treebank, LAS	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Penn Treebank, LAS	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Penn Treebank, LAS	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Penn Treebank, LAS	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	FB15K-237, MRR	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	FB15K-237, MRR	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	FB15K-237, MRR	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	FB15K-237, MRR	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Chinese Treebank 6, F1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Chinese Treebank 6, F1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Chinese Treebank 6, F1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Chinese Treebank 6, F1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	LDC2015E86, Smatch	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	LDC2015E86, Smatch	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	LDC2015E86, Smatch	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	LDC2015E86, Smatch	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	CNN / Daily Mail (Anonymized version), ROUGE-1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	CNN / Daily Mail (Anonymized version), ROUGE-1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	CNN / Daily Mail (Anonymized version), ROUGE-1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	CNN / Daily Mail (Anonymized version), ROUGE-1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	IMDb, Accuracy	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	IMDb, Accuracy	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	IMDb, Accuracy	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	IMDb, Accuracy	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	FB15K-237, H@10	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	FB15K-237, H@10	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	FB15K-237, H@10	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	FB15K-237, H@10	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	SemEval 2013, F1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	SemEval 2013, F1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	SemEval 2013, F1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	SemEval 2013, F1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	WikiText-2, Test perplexity	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	WikiText-2, Test perplexity	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	WikiText-2, Test perplexity	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	WikiText-2, Test perplexity	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Quasar, EM (Quasar-T)	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Quasar, EM (Quasar-T)	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Quasar, EM (Quasar-T)	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Quasar, EM (Quasar-T)	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Text8, Number of params	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Text8, Number of params	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Text8, Number of params	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Text8, Number of params	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	benchmark Vietnamese dependency treebank VnDT, UAS	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	benchmark Vietnamese dependency treebank VnDT, UAS	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	benchmark Vietnamese dependency treebank VnDT, UAS	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	benchmark Vietnamese dependency treebank VnDT, UAS	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Penn Treebank, F1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Penn Treebank, F1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Penn Treebank, F1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Penn Treebank, F1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	LDC2014T12, F1 on Newswire	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	LDC2014T12, F1 on Newswire	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	LDC2014T12, F1 on Newswire	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	LDC2014T12, F1 on Newswire	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	FB15K-237, H@1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	FB15K-237, H@1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	FB15K-237, H@1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	FB15K-237, H@1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	LDC2014T12, F1 on Full	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	LDC2014T12, F1 on Full	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	LDC2014T12, F1 on Full	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	LDC2014T12, F1 on Full	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Gigaword, ROUGE-1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Gigaword, ROUGE-1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Gigaword, ROUGE-1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Gigaword, ROUGE-1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	SearchQA, Unigram Acc	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	SearchQA, Unigram Acc	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	SearchQA, Unigram Acc	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	SearchQA, Unigram Acc	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	SearchQA, N-gram F1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	SearchQA, N-gram F1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	SearchQA, N-gram F1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	SearchQA, N-gram F1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	WN18RR, H@10	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	WN18RR, H@10	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	WN18RR, H@10	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	WN18RR, H@10	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	SUBJ, Accuracy	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	SUBJ, Accuracy	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	SUBJ, Accuracy	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	SUBJ, Accuracy	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Penn Treebank, UAS	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Penn Treebank, UAS	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Penn Treebank, UAS	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Penn Treebank, UAS	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	WMT 2014 EN-DE, BLEU	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	WMT 2014 EN-DE, BLEU	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	WMT 2014 EN-DE, BLEU	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	WMT 2014 EN-DE, BLEU	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	SQuAD, EM	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	SQuAD, EM	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	SQuAD, EM	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	SQuAD, EM	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	CNN / Daily Mail (Non-anonymized version), ROUGE-2	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	CNN / Daily Mail (Non-anonymized version), ROUGE-2	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	CNN / Daily Mail (Non-anonymized version), ROUGE-2	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	CNN / Daily Mail (Non-anonymized version), ROUGE-2	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	SemEval 2018, MRR	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	SemEval 2018, MRR	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	SemEval 2018, MRR	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	SemEval 2018, MRR	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Senseval 3, F1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Senseval 3, F1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Senseval 3, F1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Senseval 3, F1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	CNN / Daily Mail (Anonymized version), ROUGE-2	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	CNN / Daily Mail (Anonymized version), ROUGE-2	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	CNN / Daily Mail (Anonymized version), ROUGE-2	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	CNN / Daily Mail (Anonymized version), ROUGE-2	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	WN18RR, MRR	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	WN18RR, MRR	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	WN18RR, MRR	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	WN18RR, MRR	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	SemEval 2015, F1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	SemEval 2015, F1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	SemEval 2015, F1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	SemEval 2015, F1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Hutter Prize, Number of params	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Hutter Prize, Number of params	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Hutter Prize, Number of params	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Hutter Prize, Number of params	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Ontonotes v5 (English), F1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Ontonotes v5 (English), F1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Ontonotes v5 (English), F1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Ontonotes v5 (English), F1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	New York Times Corpus, P@10%	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	New York Times Corpus, P@10%	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	New York Times Corpus, P@10%	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	New York Times Corpus, P@10%	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Penn Treebank, Validation perplexity	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Penn Treebank, Validation perplexity	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Penn Treebank, Validation perplexity	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Penn Treebank, Validation perplexity	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Penn Treebank, Test perplexity	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Penn Treebank, Test perplexity	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Penn Treebank, Test perplexity	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Penn Treebank, Test perplexity	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	VLSP 2013 word segmentation shared task, F1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	VLSP 2013 word segmentation shared task, F1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	VLSP 2013 word segmentation shared task, F1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	VLSP 2013 word segmentation shared task, F1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	VLSP 2016 NER shared task, F1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	VLSP 2016 NER shared task, F1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	VLSP 2016 NER shared task, F1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	VLSP 2016 NER shared task, F1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Text8, Bit per Character (BPC)	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Text8, Bit per Character (BPC)	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Text8, Bit per Character (BPC)	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Text8, Bit per Character (BPC)	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	SemEval 2018, MAP	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	SemEval 2018, MAP	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	SemEval 2018, MAP	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	SemEval 2018, MAP	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Penn Treebank, F1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Penn Treebank, F1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Penn Treebank, F1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Penn Treebank, F1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	CNN / Daily Mail (Non-anonymized version), METEOR	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	CNN / Daily Mail (Non-anonymized version), METEOR	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	CNN / Daily Mail (Non-anonymized version), METEOR	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	CNN / Daily Mail (Non-anonymized version), METEOR	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	TREC, Error	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	TREC, Error	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	TREC, Error	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	TREC, Error	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Penn Treebank, POS	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Penn Treebank, POS	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Penn Treebank, POS	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Penn Treebank, POS	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	WikiText-2, Number of params	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	WikiText-2, Number of params	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	WikiText-2, Number of params	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	WikiText-2, Number of params	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	AG News, Error	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	AG News, Error	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	AG News, Error	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	AG News, Error	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	DUC 2004 Task 1, ROUGE-1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	DUC 2004 Task 1, ROUGE-1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	DUC 2004 Task 1, ROUGE-1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	DUC 2004 Task 1, ROUGE-1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	CNN / Daily Mail (Non-anonymized version), ROUGE-1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	CNN / Daily Mail (Non-anonymized version), ROUGE-1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	CNN / Daily Mail (Non-anonymized version), ROUGE-1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	CNN / Daily Mail (Non-anonymized version), ROUGE-1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Gigaword, ROUGE-2	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Gigaword, ROUGE-2	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Gigaword, ROUGE-2	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Gigaword, ROUGE-2	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	SST-2, Accuracy	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	SST-2, Accuracy	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	SST-2, Accuracy	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	SST-2, Accuracy	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	WN18RR, H@1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	WN18RR, H@1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	WN18RR, H@1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	WN18RR, H@1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Gigaword, ROUGE-L	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Gigaword, ROUGE-L	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Gigaword, ROUGE-L	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Gigaword, ROUGE-L	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	WikiText-2, Validation perplexity	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	WikiText-2, Validation perplexity	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	WikiText-2, Validation perplexity	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	WikiText-2, Validation perplexity	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	SQuAD, F1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	SQuAD, F1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	SQuAD, F1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	SQuAD, F1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Penn Treebank, Accuracy	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Penn Treebank, Accuracy	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Penn Treebank, Accuracy	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Penn Treebank, Accuracy	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	1B Words / Google Billion Word benchmark, Test perplexity	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	1B Words / Google Billion Word benchmark, Test perplexity	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	1B Words / Google Billion Word benchmark, Test perplexity	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	1B Words / Google Billion Word benchmark, Test perplexity	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Senseval 2, F1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Senseval 2, F1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Senseval 2, F1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Senseval 2, F1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Penn Treebank, Bit per Character (BPC)	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Penn Treebank, Bit per Character (BPC)	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Penn Treebank, Bit per Character (BPC)	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Penn Treebank, Bit per Character (BPC)	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	CNN / Daily Mail (Non-anonymized version), ROUGE-L	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	CNN / Daily Mail (Non-anonymized version), ROUGE-L	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	CNN / Daily Mail (Non-anonymized version), ROUGE-L	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	CNN / Daily Mail (Non-anonymized version), ROUGE-L	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	CCGBank, Accuracy	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	CCGBank, Accuracy	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	CCGBank, Accuracy	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	CCGBank, Accuracy	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	WikiText-103, Test perplexity	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	WikiText-103, Test perplexity	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	WikiText-103, Test perplexity	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	WikiText-103, Test perplexity	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	CoNLL 2003 (English), F1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	CoNLL 2003 (English), F1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	CoNLL 2003 (English), F1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	CoNLL 2003 (English), F1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Quasar, F1 (Quasar-T)	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Quasar, F1 (Quasar-T)	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Quasar, F1 (Quasar-T)	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Quasar, F1 (Quasar-T)	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	DUC 2004 Task 1, ROUGE-L	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	DUC 2004 Task 1, ROUGE-L	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	DUC 2004 Task 1, ROUGE-L	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	DUC 2004 Task 1, ROUGE-L	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Penn Treebank, Number of params	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Penn Treebank, Number of params	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Penn Treebank, Number of params	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Penn Treebank, Number of params	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	VLSP 2013 POS tagging shared task, Accuracy	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	VLSP 2013 POS tagging shared task, Accuracy	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	VLSP 2013 POS tagging shared task, Accuracy	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	VLSP 2013 POS tagging shared task, Accuracy	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	DUC 2004 Task 1, ROUGE-2	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	DUC 2004 Task 1, ROUGE-2	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	DUC 2004 Task 1, ROUGE-2	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	DUC 2004 Task 1, ROUGE-2	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	SemEval 2007, F1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	SemEval 2007, F1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	SemEval 2007, F1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	SemEval 2007, F1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	MSR, F1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	MSR, F1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	MSR, F1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	MSR, F1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	SemEval 2018, P@5	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	SemEval 2018, P@5	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	SemEval 2018, P@5	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	SemEval 2018, P@5	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	Hutter Prize, Bit per Character (BPC)	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	Hutter Prize, Bit per Character (BPC)	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	Hutter Prize, Bit per Character (BPC)	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	Hutter Prize, Bit per Character (BPC)	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	DBpedia, Error	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	DBpedia, Error	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	DBpedia, Error	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	DBpedia, Error	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	CNN / Daily Mail (Anonymized version), ROUGE-L	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	CNN / Daily Mail (Anonymized version), ROUGE-L	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	CNN / Daily Mail (Anonymized version), ROUGE-L	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	CNN / Daily Mail (Anonymized version), ROUGE-L	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	SemEval-2010 Task 8, F1	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	SemEval-2010 Task 8, F1	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	SemEval-2010 Task 8, F1	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	SemEval-2010 Task 8, F1	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	benchmark Vietnamese dependency treebank VnDT, LAS	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	benchmark Vietnamese dependency treebank VnDT, LAS	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	benchmark Vietnamese dependency treebank VnDT, LAS	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	benchmark Vietnamese dependency treebank VnDT, LAS	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#1.89	WMT 2014 EN-FR, BLEU	R - 4  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#17.24	WMT 2014 EN-FR, BLEU	R - 2  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#5.40	WMT 2014 EN-FR, BLEU	R - 3  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	D15-1232.pdf#56.29	WMT 2014 EN-FR, BLEU	R - 1  Table 1: ROUGE-N (R-N) metrics of DocEmb,  EmbDist, SenEmb, and TfIdf.
true	N15-1121.pdf#75.88	New York Times Corpus, P@10%	Including predicate senses Brown - test  Table 3: SRL labeled F-score of our model variants, and state-of-the-art systems on the CoNLL shared  task. We consider a tensor-free variant of our model, and tensor-based variants that include first-order SRL  features. For the latter, we consider implementations with 3-way and 4-way tensors. Winning systems (with  and without a reranker) are marked in bold. Statistical significance with p < 0.05 is marked with  * .
true	N15-1121.pdf#86.86	New York Times Corpus, P@10%	Including predicate senses WSJ - test WSJ - test  Table 3: SRL labeled F-score of our model variants, and state-of-the-art systems on the CoNLL shared  task. We consider a tensor-free variant of our model, and tensor-based variants that include first-order SRL  features. For the latter, we consider implementations with 3-way and 4-way tensors. Winning systems (with  and without a reranker) are marked in bold. Statistical significance with p < 0.05 is marked with  * .
true	N15-1121.pdf#86.58*	New York Times Corpus, P@10%	Including predicate senses WSJ - test  Table 3: SRL labeled F-score of our model variants, and state-of-the-art systems on the CoNLL shared  task. We consider a tensor-free variant of our model, and tensor-based variants that include first-order SRL  features. For the latter, we consider implementations with 3-way and 4-way tensors. Winning systems (with  and without a reranker) are marked in bold. Statistical significance with p < 0.05 is marked with  * .
true	N15-1121.pdf#82.87	New York Times Corpus, P@10%	Excluding predicate senses WSJ - test WSJ - test  Table 3: SRL labeled F-score of our model variants, and state-of-the-art systems on the CoNLL shared  task. We consider a tensor-free variant of our model, and tensor-based variants that include first-order SRL  features. For the latter, we consider implementations with 3-way and 4-way tensors. Winning systems (with  and without a reranker) are marked in bold. Statistical significance with p < 0.05 is marked with  * .
true	N15-1121.pdf#81.03	New York Times Corpus, P@10%	Excluding predicate senses WSJ - dev  Table 3: SRL labeled F-score of our model variants, and state-of-the-art systems on the CoNLL shared  task. We consider a tensor-free variant of our model, and tensor-based variants that include first-order SRL  features. For the latter, we consider implementations with 3-way and 4-way tensors. Winning systems (with  and without a reranker) are marked in bold. Statistical significance with p < 0.05 is marked with  * .
true	N15-1121.pdf#71.12	New York Times Corpus, P@10%	Excluding predicate senses Brown - test  Table 3: SRL labeled F-score of our model variants, and state-of-the-art systems on the CoNLL shared  task. We consider a tensor-free variant of our model, and tensor-based variants that include first-order SRL  features. For the latter, we consider implementations with 3-way and 4-way tensors. Winning systems (with  and without a reranker) are marked in bold. Statistical significance with p < 0.05 is marked with  * .
true	N15-1121.pdf#82.51*	New York Times Corpus, P@10%	Excluding predicate senses WSJ - test  Table 3: SRL labeled F-score of our model variants, and state-of-the-art systems on the CoNLL shared  task. We consider a tensor-free variant of our model, and tensor-based variants that include first-order SRL  features. For the latter, we consider implementations with 3-way and 4-way tensors. Winning systems (with  and without a reranker) are marked in bold. Statistical significance with p < 0.05 is marked with  * .
true	N15-1121.pdf#75.57	New York Times Corpus, P@10%	Including predicate senses Brown - test  Table 3: SRL labeled F-score of our model variants, and state-of-the-art systems on the CoNLL shared  task. We consider a tensor-free variant of our model, and tensor-based variants that include first-order SRL  features. For the latter, we consider implementations with 3-way and 4-way tensors. Winning systems (with  and without a reranker) are marked in bold. Statistical significance with p < 0.05 is marked with  * .
true	N15-1121.pdf#70.77	New York Times Corpus, P@10%	Excluding predicate senses Brown - test  Table 3: SRL labeled F-score of our model variants, and state-of-the-art systems on the CoNLL shared  task. We consider a tensor-free variant of our model, and tensor-based variants that include first-order SRL  features. For the latter, we consider implementations with 3-way and 4-way tensors. Winning systems (with  and without a reranker) are marked in bold. Statistical significance with p < 0.05 is marked with  * .
true	N15-1121.pdf#76.94	New York Times Corpus, P@10%	Test set Ours ( 4 - way tensor )  Table 4: Semantic labeled F-score excluding predicate senses on 5 languages in the CoNLL-2009 shared  task. Statistical significance with p < 0.05 is marked with  * . Adding the tensor leads to more than 2%  absolute gain on average F-score. Our method with the same feature configuration (a standard set + 4- way tensor) rivals the best CoNLL-2009 system which explores much richer feature sets, language-specific  feature engineering, and n-best parse combination (
true	N15-1121.pdf#77.33*	New York Times Corpus, P@10%	Test set CoNLL 1st  Table 4: Semantic labeled F-score excluding predicate senses on 5 languages in the CoNLL-2009 shared  task. Statistical significance with p < 0.05 is marked with  * . Adding the tensor leads to more than 2%  absolute gain on average F-score. Our method with the same feature configuration (a standard set + 4- way tensor) rivals the best CoNLL-2009 system which explores much richer feature sets, language-specific  feature engineering, and n-best parse combination (
true	N15-1121.pdf#82.51*	New York Times Corpus, P@10%	Test set Ours CoNLL 1st  Table 4: Semantic labeled F-score excluding predicate senses on 5 languages in the CoNLL-2009 shared  task. Statistical significance with p < 0.05 is marked with  * . Adding the tensor leads to more than 2%  absolute gain on average F-score. Our method with the same feature configuration (a standard set + 4- way tensor) rivals the best CoNLL-2009 system which explores much richer feature sets, language-specific  feature engineering, and n-best parse combination (
true	N15-1121.pdf#76.78*	New York Times Corpus, P@10%	Test set CoNLL 1st  Table 4: Semantic labeled F-score excluding predicate senses on 5 languages in the CoNLL-2009 shared  task. Statistical significance with p < 0.05 is marked with  * . Adding the tensor leads to more than 2%  absolute gain on average F-score. Our method with the same feature configuration (a standard set + 4- way tensor) rivals the best CoNLL-2009 system which explores much richer feature sets, language-specific  feature engineering, and n-best parse combination (
true	N15-1121.pdf#69.16*	New York Times Corpus, P@10%	Test set Ours ( 4 - way tensor )  Table 4: Semantic labeled F-score excluding predicate senses on 5 languages in the CoNLL-2009 shared  task. Statistical significance with p < 0.05 is marked with  * . Adding the tensor leads to more than 2%  absolute gain on average F-score. Our method with the same feature configuration (a standard set + 4- way tensor) rivals the best CoNLL-2009 system which explores much richer feature sets, language-specific  feature engineering, and n-best parse combination (
true	D15-1201.pdf#85.82	SemEval 2007, F1	48 Spearman ρ NDCG  Table 1: Results for each model's ability to predict  non-compositionality.
true	D15-1201.pdf#48.71	SemEval 2007, F1	48 F 1  Table 1: Results for each model's ability to predict  non-compositionality.
true	D15-1201.pdf#41.03	SemEval 2007, F1	48 Spearman ρ NDCG  Table 1: Results for each model's ability to predict  non-compositionality.
true	D15-1201.pdf#49.12	SemEval 2007, F1	56 49  Table 2:  Results comparing the auto- reconstructive models' ability to predict non- compositionality.
true	D15-1201.pdf#85.71	SemEval 2007, F1	49 49  Table 2:  Results comparing the auto- reconstructive models' ability to predict non- compositionality.
true	D15-1201.pdf#40.25	SemEval 2007, F1	49  Table 2:  Results comparing the auto- reconstructive models' ability to predict non- compositionality.
true	D15-1201.pdf#40.56	SemEval 2007, F1	48 Spearman ρ  Table 3: Results comparing the latent annotation  models' ability to predict non-compositionality.
true	D15-1201.pdf#85.36	SemEval 2007, F1	51 NDCG  Table 3: Results comparing the latent annotation  models' ability to predict non-compositionality.
true	D15-1201.pdf#48.64	SemEval 2007, F1	49 F 1  Table 3: Results comparing the latent annotation  models' ability to predict non-compositionality.
true	D15-1201.pdf#85.82	SemEval 2013, F1	48 Spearman ρ NDCG  Table 1: Results for each model's ability to predict  non-compositionality.
true	D15-1201.pdf#48.71	SemEval 2013, F1	48 F 1  Table 1: Results for each model's ability to predict  non-compositionality.
true	D15-1201.pdf#41.03	SemEval 2013, F1	48 Spearman ρ NDCG  Table 1: Results for each model's ability to predict  non-compositionality.
true	D15-1201.pdf#49.12	SemEval 2013, F1	56 49  Table 2:  Results comparing the auto- reconstructive models' ability to predict non- compositionality.
true	D15-1201.pdf#85.71	SemEval 2013, F1	49 49  Table 2:  Results comparing the auto- reconstructive models' ability to predict non- compositionality.
true	D15-1201.pdf#40.25	SemEval 2013, F1	49  Table 2:  Results comparing the auto- reconstructive models' ability to predict non- compositionality.
true	D15-1201.pdf#40.56	SemEval 2013, F1	48 Spearman ρ  Table 3: Results comparing the latent annotation  models' ability to predict non-compositionality.
true	D15-1201.pdf#85.36	SemEval 2013, F1	51 NDCG  Table 3: Results comparing the latent annotation  models' ability to predict non-compositionality.
true	D15-1201.pdf#48.64	SemEval 2013, F1	49 F 1  Table 3: Results comparing the latent annotation  models' ability to predict non-compositionality.
true	D15-1201.pdf#85.82	SemEval 2015, F1	48 Spearman ρ NDCG  Table 1: Results for each model's ability to predict  non-compositionality.
true	D15-1201.pdf#48.71	SemEval 2015, F1	48 F 1  Table 1: Results for each model's ability to predict  non-compositionality.
true	D15-1201.pdf#41.03	SemEval 2015, F1	48 Spearman ρ NDCG  Table 1: Results for each model's ability to predict  non-compositionality.
true	D15-1201.pdf#49.12	SemEval 2015, F1	56 49  Table 2:  Results comparing the auto- reconstructive models' ability to predict non- compositionality.
true	D15-1201.pdf#85.71	SemEval 2015, F1	49 49  Table 2:  Results comparing the auto- reconstructive models' ability to predict non- compositionality.
true	D15-1201.pdf#40.25	SemEval 2015, F1	49  Table 2:  Results comparing the auto- reconstructive models' ability to predict non- compositionality.
true	D15-1201.pdf#40.56	SemEval 2015, F1	48 Spearman ρ  Table 3: Results comparing the latent annotation  models' ability to predict non-compositionality.
true	D15-1201.pdf#85.36	SemEval 2015, F1	51 NDCG  Table 3: Results comparing the latent annotation  models' ability to predict non-compositionality.
true	D15-1201.pdf#48.64	SemEval 2015, F1	49 F 1  Table 3: Results comparing the latent annotation  models' ability to predict non-compositionality.
true	D15-1201.pdf#85.82	Senseval 2, F1	48 Spearman ρ NDCG  Table 1: Results for each model's ability to predict  non-compositionality.
true	D15-1201.pdf#48.71	Senseval 2, F1	48 F 1  Table 1: Results for each model's ability to predict  non-compositionality.
true	D15-1201.pdf#41.03	Senseval 2, F1	48 Spearman ρ NDCG  Table 1: Results for each model's ability to predict  non-compositionality.
true	D15-1201.pdf#49.12	Senseval 2, F1	56 49  Table 2:  Results comparing the auto- reconstructive models' ability to predict non- compositionality.
true	D15-1201.pdf#85.71	Senseval 2, F1	49 49  Table 2:  Results comparing the auto- reconstructive models' ability to predict non- compositionality.
true	D15-1201.pdf#40.25	Senseval 2, F1	49  Table 2:  Results comparing the auto- reconstructive models' ability to predict non- compositionality.
true	D15-1201.pdf#40.56	Senseval 2, F1	48 Spearman ρ  Table 3: Results comparing the latent annotation  models' ability to predict non-compositionality.
true	D15-1201.pdf#85.36	Senseval 2, F1	51 NDCG  Table 3: Results comparing the latent annotation  models' ability to predict non-compositionality.
true	D15-1201.pdf#48.64	Senseval 2, F1	49 F 1  Table 3: Results comparing the latent annotation  models' ability to predict non-compositionality.
true	D15-1201.pdf#85.82	Senseval 3, F1	48 Spearman ρ NDCG  Table 1: Results for each model's ability to predict  non-compositionality.
true	D15-1201.pdf#48.71	Senseval 3, F1	48 F 1  Table 1: Results for each model's ability to predict  non-compositionality.
true	D15-1201.pdf#41.03	Senseval 3, F1	48 Spearman ρ NDCG  Table 1: Results for each model's ability to predict  non-compositionality.
true	D15-1201.pdf#49.12	Senseval 3, F1	56 49  Table 2:  Results comparing the auto- reconstructive models' ability to predict non- compositionality.
true	D15-1201.pdf#85.71	Senseval 3, F1	49 49  Table 2:  Results comparing the auto- reconstructive models' ability to predict non- compositionality.
true	D15-1201.pdf#40.25	Senseval 3, F1	49  Table 2:  Results comparing the auto- reconstructive models' ability to predict non- compositionality.
true	D15-1201.pdf#40.56	Senseval 3, F1	48 Spearman ρ  Table 3: Results comparing the latent annotation  models' ability to predict non-compositionality.
true	D15-1201.pdf#85.36	Senseval 3, F1	51 NDCG  Table 3: Results comparing the latent annotation  models' ability to predict non-compositionality.
true	D15-1201.pdf#48.64	Senseval 3, F1	49 F 1  Table 3: Results comparing the latent annotation  models' ability to predict non-compositionality.
true	D14-1092.pdf#79.3	MSR, F1	76 . 3 ( 0 . 013 ) P  Table 2: Experiment results and comparison to state-of-the-art systems. The figures in parentheses denote  the variance the of F-Scores.
true	D14-1092.pdf#84.2	MSR, F1	76 . 3 ( 0 . 013 ) R  Table 2: Experiment results and comparison to state-of-the-art systems. The figures in parentheses denote  the variance the of F-Scores.
true	D14-1092.pdf#79.2	MSR, F1	76 . 3 ( 0 . 013 ) P  Table 2: Experiment results and comparison to state-of-the-art systems. The figures in parentheses denote  the variance the of F-Scores.
true	D14-1092.pdf#83.1	MSR, F1	76 . 3 ( 0 . 013 ) R  Table 2: Experiment results and comparison to state-of-the-art systems. The figures in parentheses denote  the variance the of F-Scores.
true	D14-1092.pdf#6305	MSR, F1	MSRA ( 38506 )  Table 3: Statistics of combinational ambiguity.  This table shows the total number of mistakes  made by different systems at combinational am- biguous strings. The numbers in parentheses de- note the total number of combinational ambiguous  strings.
true	D14-1092.pdf#7916	MSR, F1	MSRA ( 38506 )  Table 3: Statistics of combinational ambiguity.  This table shows the total number of mistakes  made by different systems at combinational am- biguous strings. The numbers in parentheses de- note the total number of combinational ambiguous  strings.
true	D14-1092.pdf#216	MSR, F1	PKU ( 603 )  Table 4: Statistics of overlapping ambiguity. This  table shows the total number of mistakes made  by different systems at overlapping ambiguous  strings. The numbers in parentheses denote the to- tal number of overlapping ambiguous strings.
true	D14-1092.pdf#157	MSR, F1	MSRA ( 467 )  Table 4: Statistics of overlapping ambiguity. This  table shows the total number of mistakes made  by different systems at overlapping ambiguous  strings. The numbers in parentheses denote the to- tal number of overlapping ambiguous strings.
true	D14-1092.pdf#79.3	Chinese Treebank 6, F1	76 . 3 ( 0 . 013 ) P  Table 2: Experiment results and comparison to state-of-the-art systems. The figures in parentheses denote  the variance the of F-Scores.
true	D14-1092.pdf#84.2	Chinese Treebank 6, F1	76 . 3 ( 0 . 013 ) R  Table 2: Experiment results and comparison to state-of-the-art systems. The figures in parentheses denote  the variance the of F-Scores.
true	D14-1092.pdf#79.2	Chinese Treebank 6, F1	76 . 3 ( 0 . 013 ) P  Table 2: Experiment results and comparison to state-of-the-art systems. The figures in parentheses denote  the variance the of F-Scores.
true	D14-1092.pdf#83.1	Chinese Treebank 6, F1	76 . 3 ( 0 . 013 ) R  Table 2: Experiment results and comparison to state-of-the-art systems. The figures in parentheses denote  the variance the of F-Scores.
true	D14-1092.pdf#6305	Chinese Treebank 6, F1	MSRA ( 38506 )  Table 3: Statistics of combinational ambiguity.  This table shows the total number of mistakes  made by different systems at combinational am- biguous strings. The numbers in parentheses de- note the total number of combinational ambiguous  strings.
true	D14-1092.pdf#7916	Chinese Treebank 6, F1	MSRA ( 38506 )  Table 3: Statistics of combinational ambiguity.  This table shows the total number of mistakes  made by different systems at combinational am- biguous strings. The numbers in parentheses de- note the total number of combinational ambiguous  strings.
true	D14-1092.pdf#216	Chinese Treebank 6, F1	PKU ( 603 )  Table 4: Statistics of overlapping ambiguity. This  table shows the total number of mistakes made  by different systems at overlapping ambiguous  strings. The numbers in parentheses denote the to- tal number of overlapping ambiguous strings.
true	D14-1092.pdf#157	Chinese Treebank 6, F1	MSRA ( 467 )  Table 4: Statistics of overlapping ambiguity. This  table shows the total number of mistakes made  by different systems at overlapping ambiguous  strings. The numbers in parentheses denote the to- tal number of overlapping ambiguous strings.
true	D14-1092.pdf#79.3	PKU, F1	76 . 3 ( 0 . 013 ) P  Table 2: Experiment results and comparison to state-of-the-art systems. The figures in parentheses denote  the variance the of F-Scores.
true	D14-1092.pdf#84.2	PKU, F1	76 . 3 ( 0 . 013 ) R  Table 2: Experiment results and comparison to state-of-the-art systems. The figures in parentheses denote  the variance the of F-Scores.
true	D14-1092.pdf#79.2	PKU, F1	76 . 3 ( 0 . 013 ) P  Table 2: Experiment results and comparison to state-of-the-art systems. The figures in parentheses denote  the variance the of F-Scores.
true	D14-1092.pdf#83.1	PKU, F1	76 . 3 ( 0 . 013 ) R  Table 2: Experiment results and comparison to state-of-the-art systems. The figures in parentheses denote  the variance the of F-Scores.
true	D14-1092.pdf#6305	PKU, F1	MSRA ( 38506 )  Table 3: Statistics of combinational ambiguity.  This table shows the total number of mistakes  made by different systems at combinational am- biguous strings. The numbers in parentheses de- note the total number of combinational ambiguous  strings.
true	D14-1092.pdf#7916	PKU, F1	MSRA ( 38506 )  Table 3: Statistics of combinational ambiguity.  This table shows the total number of mistakes  made by different systems at combinational am- biguous strings. The numbers in parentheses de- note the total number of combinational ambiguous  strings.
true	D14-1092.pdf#216	PKU, F1	PKU ( 603 )  Table 4: Statistics of overlapping ambiguity. This  table shows the total number of mistakes made  by different systems at overlapping ambiguous  strings. The numbers in parentheses denote the to- tal number of overlapping ambiguous strings.
true	D14-1092.pdf#157	PKU, F1	MSRA ( 467 )  Table 4: Statistics of overlapping ambiguity. This  table shows the total number of mistakes made  by different systems at overlapping ambiguous  strings. The numbers in parentheses denote the to- tal number of overlapping ambiguous strings.
true	P10-2040.pdf#0.499	VLSP 2013 POS tagging shared task, Accuracy	PTB45  Table 1. Tagging accuracy under the best M-to-1 map, the greedy 1-to-1 map, and  VI, for the full PTB45 tagset and the reduced PTB17 tagset. HMM-EM, HMM-VB  and HMM-GS show the best results from
true	P10-2040.pdf#3.84	VLSP 2013 POS tagging shared task, Accuracy	PTB45  Table 1. Tagging accuracy under the best M-to-1 map, the greedy 1-to-1 map, and  VI, for the full PTB45 tagset and the reduced PTB17 tagset. HMM-EM, HMM-VB  and HMM-GS show the best results from
true	P10-2040.pdf#0.660	VLSP 2013 POS tagging shared task, Accuracy	PTB45  Table 1. Tagging accuracy under the best M-to-1 map, the greedy 1-to-1 map, and  VI, for the full PTB45 tagset and the reduced PTB17 tagset. HMM-EM, HMM-VB  and HMM-GS show the best results from
true	P10-2040.pdf#3.02	VLSP 2013 POS tagging shared task, Accuracy	PTB17  Table 1. Tagging accuracy under the best M-to-1 map, the greedy 1-to-1 map, and  VI, for the full PTB45 tagset and the reduced PTB17 tagset. HMM-EM, HMM-VB  and HMM-GS show the best results from
true	P10-2040.pdf#0.730	VLSP 2013 POS tagging shared task, Accuracy	PTB17  Table 1. Tagging accuracy under the best M-to-1 map, the greedy 1-to-1 map, and  VI, for the full PTB45 tagset and the reduced PTB17 tagset. HMM-EM, HMM-VB  and HMM-GS show the best results from
true	P10-2040.pdf#0.660	VLSP 2013 POS tagging shared task, Accuracy	PTB45  Table 1. Tagging accuracy under the best M-to-1 map, the greedy 1-to-1 map, and  VI, for the full PTB45 tagset and the reduced PTB17 tagset. HMM-EM, HMM-VB  and HMM-GS show the best results from
true	P10-2040.pdf#0.528	VLSP 2013 POS tagging shared task, Accuracy	PTB17  Table 1. Tagging accuracy under the best M-to-1 map, the greedy 1-to-1 map, and  VI, for the full PTB45 tagset and the reduced PTB17 tagset. HMM-EM, HMM-VB  and HMM-GS show the best results from
true	P11-2051.pdf#+1.01	Text8, Number of params	n / a BLEU scores avg  Table 2: Experiment results on Chinese-English transla- tion tasks, the abbreviations for systems are as follows:  BL: Baseline system, GS: System trained with only gen- erated sentence pairs, IT: Interpolated phrase table with  GS and BL,. GA and IA are GS and IT systems trained  with baseline word alignment models accordingly. LS is  the GALE system with 8.7M sentence pairs.
true	P11-2051.pdf#31.58	Text8, Number of params	n / a BLEU scores mt05  Table 2: Experiment results on Chinese-English transla- tion tasks, the abbreviations for systems are as follows:  BL: Baseline system, GS: System trained with only gen- erated sentence pairs, IT: Interpolated phrase table with  GS and BL,. GA and IA are GS and IT systems trained  with baseline word alignment models accordingly. LS is  the GALE system with 8.7M sentence pairs.
true	P11-2051.pdf#31.67	Text8, Number of params	n / a BLEU scores mt03  Table 2: Experiment results on Chinese-English transla- tion tasks, the abbreviations for systems are as follows:  BL: Baseline system, GS: System trained with only gen- erated sentence pairs, IT: Interpolated phrase table with  GS and BL,. GA and IA are GS and IT systems trained  with baseline word alignment models accordingly. LS is  the GALE system with 8.7M sentence pairs.
true	P11-2051.pdf#24.81	Text8, Number of params	n / a BLEU scores mt08  Table 2: Experiment results on Chinese-English transla- tion tasks, the abbreviations for systems are as follows:  BL: Baseline system, GS: System trained with only gen- erated sentence pairs, IT: Interpolated phrase table with  GS and BL,. GA and IA are GS and IT systems trained  with baseline word alignment models accordingly. LS is  the GALE system with 8.7M sentence pairs.
true	P11-1070.pdf#92.0	benchmark Vietnamese dependency treebank VnDT, UAS	+ Web features  Table 1: UAS results for English WSJ dependency parsing. Dev  is WSJ section 22 (all sentences) and Test is WSJ section 23  (all sentences). The order 2 baseline represents McDonald and  Pereira (2006).
true	P11-1070.pdf#92.7	benchmark Vietnamese dependency treebank VnDT, UAS	+ Web features  Table 1: UAS results for English WSJ dependency parsing. Dev  is WSJ section 22 (all sentences) and Test is WSJ section 23  (all sentences). The order 2 baseline represents McDonald and  Pereira (2006).
true	P11-1070.pdf#91.4	benchmark Vietnamese dependency treebank VnDT, UAS	Test ( sec 23 ) F1  Table 3: Parsing results for reranking 50-best lists of Berkeley  parser (Dev is WSJ section 22 and Test is WSJ section 23, all  lengths).
true	P11-1070.pdf#41.4	benchmark Vietnamese dependency treebank VnDT, UAS	Test ( sec 23 ) EX  Table 3: Parsing results for reranking 50-best lists of Berkeley  parser (Dev is WSJ section 22 and Test is WSJ section 23, all  lengths).
true	P11-1070.pdf#44.0	benchmark Vietnamese dependency treebank VnDT, UAS	Dev ( sec 22 ) EX  Table 3: Parsing results for reranking 50-best lists of Berkeley  parser (Dev is WSJ section 22 and Test is WSJ section 23, all  lengths).
true	P11-1070.pdf#92.1	benchmark Vietnamese dependency treebank VnDT, UAS	Dev ( sec 22 ) F1  Table 3: Parsing results for reranking 50-best lists of Berkeley  parser (Dev is WSJ section 22 and Test is WSJ section 23, all  lengths).
true	P12-1025.pdf#94.93%	Penn Treebank, Accuracy	6 ) . R  Table 6: Performance of different systems on the test  data.
true	P12-1025.pdf#94.42%	Penn Treebank, Accuracy	6 ) . P  Table 6: Performance of different systems on the test  data.
true	P12-1025.pdf#94.68	Penn Treebank, Accuracy	6 ) . F  Table 6: Performance of different systems on the test  data.
true	P13-2104.pdf#89.5	Penn Treebank, UAS	WSJ - to - B UAS  Table 2: Results with the first-order parsing model  in the first and second experiments. The super- script indicates the source of labeled data used in  training.
true	P13-2104.pdf#45.5	Penn Treebank, UAS	WSJ - to - B CM  Table 2: Results with the first-order parsing model  in the first and second experiments. The super- script indicates the source of labeled data used in  training.
true	P13-2104.pdf#90.2	Penn Treebank, UAS	B - to - WSJ UAS  Table 2: Results with the first-order parsing model  in the first and second experiments. The super- script indicates the source of labeled data used in  training.
true	P13-2104.pdf#33.4	Penn Treebank, UAS	B - to - WSJ CM  Table 2: Results with the first-order parsing model  in the first and second experiments. The super- script indicates the source of labeled data used in  training.
true	P13-2104.pdf#90.6	Penn Treebank, UAS	WSJ - to - B UAS  Table 3: Results with the second-order sibling  parsing model in the first and second experiments.
true	P13-2104.pdf#91.5	Penn Treebank, UAS	B - to - WSJ UAS  Table 3: Results with the second-order sibling  parsing model in the first and second experiments.
true	P13-2104.pdf#49.6	Penn Treebank, UAS	WSJ - to - B CM  Table 3: Results with the second-order sibling  parsing model in the first and second experiments.
true	P13-2104.pdf#38.8	Penn Treebank, UAS	B - to - WSJ CM  Table 3: Results with the second-order sibling  parsing model in the first and second experiments.
true	P13-2104.pdf#88.4	Penn Treebank, UAS	WSJ - to - G UAS  Table 4: Results with first-order parsing model in  the third experiment. "Plank (2011)" refers to the  approach in Plank and van Noord (2011).
true	P13-2104.pdf#87.1	Penn Treebank, UAS	WSJ - to - G LAS  Table 4: Results with first-order parsing model in  the third experiment. "Plank (2011)" refers to the  approach in Plank and van Noord (2011).
true	P13-2104.pdf#89.5	benchmark Vietnamese dependency treebank VnDT, UAS	WSJ - to - B UAS  Table 2: Results with the first-order parsing model  in the first and second experiments. The super- script indicates the source of labeled data used in  training.
true	P13-2104.pdf#45.5	benchmark Vietnamese dependency treebank VnDT, UAS	WSJ - to - B CM  Table 2: Results with the first-order parsing model  in the first and second experiments. The super- script indicates the source of labeled data used in  training.
true	P13-2104.pdf#90.2	benchmark Vietnamese dependency treebank VnDT, UAS	B - to - WSJ UAS  Table 2: Results with the first-order parsing model  in the first and second experiments. The super- script indicates the source of labeled data used in  training.
true	P13-2104.pdf#33.4	benchmark Vietnamese dependency treebank VnDT, UAS	B - to - WSJ CM  Table 2: Results with the first-order parsing model  in the first and second experiments. The super- script indicates the source of labeled data used in  training.
true	P13-2104.pdf#90.6	benchmark Vietnamese dependency treebank VnDT, UAS	WSJ - to - B UAS  Table 3: Results with the second-order sibling  parsing model in the first and second experiments.
true	P13-2104.pdf#91.5	benchmark Vietnamese dependency treebank VnDT, UAS	B - to - WSJ UAS  Table 3: Results with the second-order sibling  parsing model in the first and second experiments.
true	P13-2104.pdf#49.6	benchmark Vietnamese dependency treebank VnDT, UAS	WSJ - to - B CM  Table 3: Results with the second-order sibling  parsing model in the first and second experiments.
true	P13-2104.pdf#38.8	benchmark Vietnamese dependency treebank VnDT, UAS	B - to - WSJ CM  Table 3: Results with the second-order sibling  parsing model in the first and second experiments.
true	P13-2104.pdf#88.4	benchmark Vietnamese dependency treebank VnDT, UAS	WSJ - to - G UAS  Table 4: Results with first-order parsing model in  the third experiment. "Plank (2011)" refers to the  approach in Plank and van Noord (2011).
true	P13-2104.pdf#87.1	benchmark Vietnamese dependency treebank VnDT, UAS	WSJ - to - G LAS  Table 4: Results with first-order parsing model in  the third experiment. "Plank (2011)" refers to the  approach in Plank and van Noord (2011).
true	P13-2104.pdf#89.5	SUBJ, Accuracy	WSJ - to - B UAS  Table 2: Results with the first-order parsing model  in the first and second experiments. The super- script indicates the source of labeled data used in  training.
true	P13-2104.pdf#45.5	SUBJ, Accuracy	WSJ - to - B CM  Table 2: Results with the first-order parsing model  in the first and second experiments. The super- script indicates the source of labeled data used in  training.
true	P13-2104.pdf#90.2	SUBJ, Accuracy	B - to - WSJ UAS  Table 2: Results with the first-order parsing model  in the first and second experiments. The super- script indicates the source of labeled data used in  training.
true	P13-2104.pdf#33.4	SUBJ, Accuracy	B - to - WSJ CM  Table 2: Results with the first-order parsing model  in the first and second experiments. The super- script indicates the source of labeled data used in  training.
true	P13-2104.pdf#90.6	SUBJ, Accuracy	WSJ - to - B UAS  Table 3: Results with the second-order sibling  parsing model in the first and second experiments.
true	P13-2104.pdf#91.5	SUBJ, Accuracy	B - to - WSJ UAS  Table 3: Results with the second-order sibling  parsing model in the first and second experiments.
true	P13-2104.pdf#49.6	SUBJ, Accuracy	WSJ - to - B CM  Table 3: Results with the second-order sibling  parsing model in the first and second experiments.
true	P13-2104.pdf#38.8	SUBJ, Accuracy	B - to - WSJ CM  Table 3: Results with the second-order sibling  parsing model in the first and second experiments.
true	P13-2104.pdf#88.4	SUBJ, Accuracy	WSJ - to - G UAS  Table 4: Results with first-order parsing model in  the third experiment. "Plank (2011)" refers to the  approach in Plank and van Noord (2011).
true	P13-2104.pdf#87.1	SUBJ, Accuracy	WSJ - to - G LAS  Table 4: Results with first-order parsing model in  the third experiment. "Plank (2011)" refers to the  approach in Plank and van Noord (2011).
true	P11-1160.pdf#80.11*†	Penn Treebank, POS	Parser Bulgarian NoPunct  Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained  on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates  without punctuation. * next to an accuracy denotes statistically significant (McNemar's and p < 0.05)  improvement over the baseline.  † denotes significance over GNPPA
true	P11-1160.pdf#78.93*†	Penn Treebank, POS	Parser Bulgarian Punct  Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained  on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates  without punctuation. * next to an accuracy denotes statistically significant (McNemar's and p < 0.05)  improvement over the baseline.  † denotes significance over GNPPA
true	P11-1160.pdf#79.94*†	Penn Treebank, POS	Parser Hindi NoPunct  Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained  on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates  without punctuation. * next to an accuracy denotes statistically significant (McNemar's and p < 0.05)  improvement over the baseline.  † denotes significance over GNPPA
true	P11-1160.pdf#81.10*†	Penn Treebank, POS	Parser Hindi Punct  Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained  on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates  without punctuation. * next to an accuracy denotes statistically significant (McNemar's and p < 0.05)  improvement over the baseline.  † denotes significance over GNPPA
true	P11-1160.pdf#70.90*†	Penn Treebank, POS	Parser Spanish NoPunct  Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained  on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates  without punctuation. * next to an accuracy denotes statistically significant (McNemar's and p < 0.05)  improvement over the baseline.  † denotes significance over GNPPA
true	P11-1160.pdf#67.69*†	Penn Treebank, POS	Parser Spanish Punct  Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained  on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates  without punctuation. * next to an accuracy denotes statistically significant (McNemar's and p < 0.05)  improvement over the baseline.  † denotes significance over GNPPA
true	P11-1160.pdf#72.3	Penn Treebank, POS	Spanish  Table 4: Comparison of baseline, GNPPA and E- GNPPA with baseline and discriminative model  from (
true	P11-1160.pdf#80.11	Penn Treebank, POS	Spanish  Table 4: Comparison of baseline, GNPPA and E- GNPPA with baseline and discriminative model  from (
true	P11-1160.pdf#80.11*†	benchmark Vietnamese dependency treebank VnDT, LAS	Parser Bulgarian NoPunct  Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained  on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates  without punctuation. * next to an accuracy denotes statistically significant (McNemar's and p < 0.05)  improvement over the baseline.  † denotes significance over GNPPA
true	P11-1160.pdf#78.93*†	benchmark Vietnamese dependency treebank VnDT, LAS	Parser Bulgarian Punct  Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained  on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates  without punctuation. * next to an accuracy denotes statistically significant (McNemar's and p < 0.05)  improvement over the baseline.  † denotes significance over GNPPA
true	P11-1160.pdf#79.94*†	benchmark Vietnamese dependency treebank VnDT, LAS	Parser Hindi NoPunct  Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained  on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates  without punctuation. * next to an accuracy denotes statistically significant (McNemar's and p < 0.05)  improvement over the baseline.  † denotes significance over GNPPA
true	P11-1160.pdf#81.10*†	benchmark Vietnamese dependency treebank VnDT, LAS	Parser Hindi Punct  Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained  on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates  without punctuation. * next to an accuracy denotes statistically significant (McNemar's and p < 0.05)  improvement over the baseline.  † denotes significance over GNPPA
true	P11-1160.pdf#70.90*†	benchmark Vietnamese dependency treebank VnDT, LAS	Parser Spanish NoPunct  Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained  on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates  without punctuation. * next to an accuracy denotes statistically significant (McNemar's and p < 0.05)  improvement over the baseline.  † denotes significance over GNPPA
true	P11-1160.pdf#67.69*†	benchmark Vietnamese dependency treebank VnDT, LAS	Parser Spanish Punct  Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained  on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates  without punctuation. * next to an accuracy denotes statistically significant (McNemar's and p < 0.05)  improvement over the baseline.  † denotes significance over GNPPA
true	P11-1160.pdf#72.3	benchmark Vietnamese dependency treebank VnDT, LAS	Spanish  Table 4: Comparison of baseline, GNPPA and E- GNPPA with baseline and discriminative model  from (
true	P11-1160.pdf#80.11	benchmark Vietnamese dependency treebank VnDT, LAS	Spanish  Table 4: Comparison of baseline, GNPPA and E- GNPPA with baseline and discriminative model  from (
true	P11-1160.pdf#80.11*†	benchmark Vietnamese dependency treebank VnDT, UAS	Parser Bulgarian NoPunct  Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained  on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates  without punctuation. * next to an accuracy denotes statistically significant (McNemar's and p < 0.05)  improvement over the baseline.  † denotes significance over GNPPA
true	P11-1160.pdf#78.93*†	benchmark Vietnamese dependency treebank VnDT, UAS	Parser Bulgarian Punct  Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained  on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates  without punctuation. * next to an accuracy denotes statistically significant (McNemar's and p < 0.05)  improvement over the baseline.  † denotes significance over GNPPA
true	P11-1160.pdf#79.94*†	benchmark Vietnamese dependency treebank VnDT, UAS	Parser Hindi NoPunct  Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained  on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates  without punctuation. * next to an accuracy denotes statistically significant (McNemar's and p < 0.05)  improvement over the baseline.  † denotes significance over GNPPA
true	P11-1160.pdf#81.10*†	benchmark Vietnamese dependency treebank VnDT, UAS	Parser Hindi Punct  Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained  on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates  without punctuation. * next to an accuracy denotes statistically significant (McNemar's and p < 0.05)  improvement over the baseline.  † denotes significance over GNPPA
true	P11-1160.pdf#70.90*†	benchmark Vietnamese dependency treebank VnDT, UAS	Parser Spanish NoPunct  Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained  on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates  without punctuation. * next to an accuracy denotes statistically significant (McNemar's and p < 0.05)  improvement over the baseline.  † denotes significance over GNPPA
true	P11-1160.pdf#67.69*†	benchmark Vietnamese dependency treebank VnDT, UAS	Parser Spanish Punct  Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained  on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates  without punctuation. * next to an accuracy denotes statistically significant (McNemar's and p < 0.05)  improvement over the baseline.  † denotes significance over GNPPA
true	P11-1160.pdf#72.3	benchmark Vietnamese dependency treebank VnDT, UAS	Spanish  Table 4: Comparison of baseline, GNPPA and E- GNPPA with baseline and discriminative model  from (
true	P11-1160.pdf#80.11	benchmark Vietnamese dependency treebank VnDT, UAS	Spanish  Table 4: Comparison of baseline, GNPPA and E- GNPPA with baseline and discriminative model  from (
true	D10-1113.pdf#53.39	Senseval 3, F1	Spearman ρ  Table 2: Results on out of context word similarity using  a simple co-occurrence based vector space model (SVS),  latent semantic analysis, non-negative matrix factoriza- tion and latent Dirichlet allocation as individual models  with the best parameter setting (LSA, NMF, LDA) and as  mixtures (LSA MIX , NMF MIX , LDA MIX ).
true	D10-1113.pdf#52.99	Senseval 3, F1	Spearman ρ  Table 2: Results on out of context word similarity using  a simple co-occurrence based vector space model (SVS),  latent semantic analysis, non-negative matrix factoriza- tion and latent Dirichlet allocation as individual models  with the best parameter setting (LSA, NMF, LDA) and as  mixtures (LSA MIX , NMF MIX , LDA MIX ).
true	D10-1113.pdf#16.31	Senseval 3, F1	Noun  Table 4: Results on lexical substitution for different parts  of speech with a simple semantic space model (SVS), two  compositional models (Add-SVS, Mult-SVS), and con- textualized mixture models with NMF and LDA (Cont- NMF MIX , Cont-LDA MIX ), using Kendall's τ b correlation  coefficient.
true	D10-1113.pdf#17.10	Senseval 3, F1	Adj  Table 4: Results on lexical substitution for different parts  of speech with a simple semantic space model (SVS), two  compositional models (Add-SVS, Mult-SVS), and con- textualized mixture models with NMF and LDA (Cont- NMF MIX , Cont-LDA MIX ), using Kendall's τ b correlation  coefficient.
true	D10-1113.pdf#14.18	Senseval 3, F1	Verb  Table 4: Results on lexical substitution for different parts  of speech with a simple semantic space model (SVS), two  compositional models (Add-SVS, Mult-SVS), and con- textualized mixture models with NMF and LDA (Cont- NMF MIX , Cont-LDA MIX ), using Kendall's τ b correlation  coefficient.
true	D10-1113.pdf#26.13	Senseval 3, F1	Adv  Table 4: Results on lexical substitution for different parts  of speech with a simple semantic space model (SVS), two  compositional models (Add-SVS, Mult-SVS), and con- textualized mixture models with NMF and LDA (Cont- NMF MIX , Cont-LDA MIX ), using Kendall's τ b correlation  coefficient.
true	D10-1113.pdf#53.39	SemEval 2015, F1	Spearman ρ  Table 2: Results on out of context word similarity using  a simple co-occurrence based vector space model (SVS),  latent semantic analysis, non-negative matrix factoriza- tion and latent Dirichlet allocation as individual models  with the best parameter setting (LSA, NMF, LDA) and as  mixtures (LSA MIX , NMF MIX , LDA MIX ).
true	D10-1113.pdf#52.99	SemEval 2015, F1	Spearman ρ  Table 2: Results on out of context word similarity using  a simple co-occurrence based vector space model (SVS),  latent semantic analysis, non-negative matrix factoriza- tion and latent Dirichlet allocation as individual models  with the best parameter setting (LSA, NMF, LDA) and as  mixtures (LSA MIX , NMF MIX , LDA MIX ).
true	D10-1113.pdf#16.31	SemEval 2015, F1	Noun  Table 4: Results on lexical substitution for different parts  of speech with a simple semantic space model (SVS), two  compositional models (Add-SVS, Mult-SVS), and con- textualized mixture models with NMF and LDA (Cont- NMF MIX , Cont-LDA MIX ), using Kendall's τ b correlation  coefficient.
true	D10-1113.pdf#17.10	SemEval 2015, F1	Adj  Table 4: Results on lexical substitution for different parts  of speech with a simple semantic space model (SVS), two  compositional models (Add-SVS, Mult-SVS), and con- textualized mixture models with NMF and LDA (Cont- NMF MIX , Cont-LDA MIX ), using Kendall's τ b correlation  coefficient.
true	D10-1113.pdf#14.18	SemEval 2015, F1	Verb  Table 4: Results on lexical substitution for different parts  of speech with a simple semantic space model (SVS), two  compositional models (Add-SVS, Mult-SVS), and con- textualized mixture models with NMF and LDA (Cont- NMF MIX , Cont-LDA MIX ), using Kendall's τ b correlation  coefficient.
true	D10-1113.pdf#26.13	SemEval 2015, F1	Adv  Table 4: Results on lexical substitution for different parts  of speech with a simple semantic space model (SVS), two  compositional models (Add-SVS, Mult-SVS), and con- textualized mixture models with NMF and LDA (Cont- NMF MIX , Cont-LDA MIX ), using Kendall's τ b correlation  coefficient.
true	D10-1113.pdf#53.39	SemEval 2013, F1	Spearman ρ  Table 2: Results on out of context word similarity using  a simple co-occurrence based vector space model (SVS),  latent semantic analysis, non-negative matrix factoriza- tion and latent Dirichlet allocation as individual models  with the best parameter setting (LSA, NMF, LDA) and as  mixtures (LSA MIX , NMF MIX , LDA MIX ).
true	D10-1113.pdf#52.99	SemEval 2013, F1	Spearman ρ  Table 2: Results on out of context word similarity using  a simple co-occurrence based vector space model (SVS),  latent semantic analysis, non-negative matrix factoriza- tion and latent Dirichlet allocation as individual models  with the best parameter setting (LSA, NMF, LDA) and as  mixtures (LSA MIX , NMF MIX , LDA MIX ).
true	D10-1113.pdf#16.31	SemEval 2013, F1	Noun  Table 4: Results on lexical substitution for different parts  of speech with a simple semantic space model (SVS), two  compositional models (Add-SVS, Mult-SVS), and con- textualized mixture models with NMF and LDA (Cont- NMF MIX , Cont-LDA MIX ), using Kendall's τ b correlation  coefficient.
true	D10-1113.pdf#17.10	SemEval 2013, F1	Adj  Table 4: Results on lexical substitution for different parts  of speech with a simple semantic space model (SVS), two  compositional models (Add-SVS, Mult-SVS), and con- textualized mixture models with NMF and LDA (Cont- NMF MIX , Cont-LDA MIX ), using Kendall's τ b correlation  coefficient.
true	D10-1113.pdf#14.18	SemEval 2013, F1	Verb  Table 4: Results on lexical substitution for different parts  of speech with a simple semantic space model (SVS), two  compositional models (Add-SVS, Mult-SVS), and con- textualized mixture models with NMF and LDA (Cont- NMF MIX , Cont-LDA MIX ), using Kendall's τ b correlation  coefficient.
true	D10-1113.pdf#26.13	SemEval 2013, F1	Adv  Table 4: Results on lexical substitution for different parts  of speech with a simple semantic space model (SVS), two  compositional models (Add-SVS, Mult-SVS), and con- textualized mixture models with NMF and LDA (Cont- NMF MIX , Cont-LDA MIX ), using Kendall's τ b correlation  coefficient.
true	D10-1113.pdf#53.39	SemEval 2007, F1	Spearman ρ  Table 2: Results on out of context word similarity using  a simple co-occurrence based vector space model (SVS),  latent semantic analysis, non-negative matrix factoriza- tion and latent Dirichlet allocation as individual models  with the best parameter setting (LSA, NMF, LDA) and as  mixtures (LSA MIX , NMF MIX , LDA MIX ).
true	D10-1113.pdf#52.99	SemEval 2007, F1	Spearman ρ  Table 2: Results on out of context word similarity using  a simple co-occurrence based vector space model (SVS),  latent semantic analysis, non-negative matrix factoriza- tion and latent Dirichlet allocation as individual models  with the best parameter setting (LSA, NMF, LDA) and as  mixtures (LSA MIX , NMF MIX , LDA MIX ).
true	D10-1113.pdf#16.31	SemEval 2007, F1	Noun  Table 4: Results on lexical substitution for different parts  of speech with a simple semantic space model (SVS), two  compositional models (Add-SVS, Mult-SVS), and con- textualized mixture models with NMF and LDA (Cont- NMF MIX , Cont-LDA MIX ), using Kendall's τ b correlation  coefficient.
true	D10-1113.pdf#17.10	SemEval 2007, F1	Adj  Table 4: Results on lexical substitution for different parts  of speech with a simple semantic space model (SVS), two  compositional models (Add-SVS, Mult-SVS), and con- textualized mixture models with NMF and LDA (Cont- NMF MIX , Cont-LDA MIX ), using Kendall's τ b correlation  coefficient.
true	D10-1113.pdf#14.18	SemEval 2007, F1	Verb  Table 4: Results on lexical substitution for different parts  of speech with a simple semantic space model (SVS), two  compositional models (Add-SVS, Mult-SVS), and con- textualized mixture models with NMF and LDA (Cont- NMF MIX , Cont-LDA MIX ), using Kendall's τ b correlation  coefficient.
true	D10-1113.pdf#26.13	SemEval 2007, F1	Adv  Table 4: Results on lexical substitution for different parts  of speech with a simple semantic space model (SVS), two  compositional models (Add-SVS, Mult-SVS), and con- textualized mixture models with NMF and LDA (Cont- NMF MIX , Cont-LDA MIX ), using Kendall's τ b correlation  coefficient.
true	D10-1113.pdf#53.39	Senseval 2, F1	Spearman ρ  Table 2: Results on out of context word similarity using  a simple co-occurrence based vector space model (SVS),  latent semantic analysis, non-negative matrix factoriza- tion and latent Dirichlet allocation as individual models  with the best parameter setting (LSA, NMF, LDA) and as  mixtures (LSA MIX , NMF MIX , LDA MIX ).
true	D10-1113.pdf#52.99	Senseval 2, F1	Spearman ρ  Table 2: Results on out of context word similarity using  a simple co-occurrence based vector space model (SVS),  latent semantic analysis, non-negative matrix factoriza- tion and latent Dirichlet allocation as individual models  with the best parameter setting (LSA, NMF, LDA) and as  mixtures (LSA MIX , NMF MIX , LDA MIX ).
true	D10-1113.pdf#16.31	Senseval 2, F1	Noun  Table 4: Results on lexical substitution for different parts  of speech with a simple semantic space model (SVS), two  compositional models (Add-SVS, Mult-SVS), and con- textualized mixture models with NMF and LDA (Cont- NMF MIX , Cont-LDA MIX ), using Kendall's τ b correlation  coefficient.
true	D10-1113.pdf#17.10	Senseval 2, F1	Adj  Table 4: Results on lexical substitution for different parts  of speech with a simple semantic space model (SVS), two  compositional models (Add-SVS, Mult-SVS), and con- textualized mixture models with NMF and LDA (Cont- NMF MIX , Cont-LDA MIX ), using Kendall's τ b correlation  coefficient.
true	D10-1113.pdf#14.18	Senseval 2, F1	Verb  Table 4: Results on lexical substitution for different parts  of speech with a simple semantic space model (SVS), two  compositional models (Add-SVS, Mult-SVS), and con- textualized mixture models with NMF and LDA (Cont- NMF MIX , Cont-LDA MIX ), using Kendall's τ b correlation  coefficient.
true	D10-1113.pdf#26.13	Senseval 2, F1	Adv  Table 4: Results on lexical substitution for different parts  of speech with a simple semantic space model (SVS), two  compositional models (Add-SVS, Mult-SVS), and con- textualized mixture models with NMF and LDA (Cont- NMF MIX , Cont-LDA MIX ), using Kendall's τ b correlation  coefficient.
true	N13-1003.pdf#35.4	Text8, Number of params	- Chinese - English mt06  Table 5: Comparing reordering methods according to BLEU score. n indicates the number of tuning replications,  while standard deviations (std) indicate optimizer stability. Test scores that are significantly higher (p < 0.01) than  the HRM baseline are highlighted in bold.
true	N13-1003.pdf#29.0	Text8, Number of params	- Chinese - English mt08  Table 5: Comparing reordering methods according to BLEU score. n indicates the number of tuning replications,  while standard deviations (std) indicate optimizer stability. Test scores that are significantly higher (p < 0.01) than  the HRM baseline are highlighted in bold.
true	N13-1003.pdf#47.3	Text8, Number of params	- Arabic - English mt09  Table 5: Comparing reordering methods according to BLEU score. n indicates the number of tuning replications,  while standard deviations (std) indicate optimizer stability. Test scores that are significantly higher (p < 0.01) than  the HRM baseline are highlighted in bold.
true	N13-1003.pdf#44.6	Text8, Number of params	- Arabic - English mt08  Table 5: Comparing reordering methods according to BLEU score. n indicates the number of tuning replications,  while standard deviations (std) indicate optimizer stability. Test scores that are significantly higher (p < 0.01) than  the HRM baseline are highlighted in bold.
true	P14-1031.pdf#70.6	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	MD  Table 2: Accuracy results (%) for supervised sen- timent classification (two-way)
true	P14-1031.pdf#82.7	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	CR  Table 2: Accuracy results (%) for supervised sen- timent classification (two-way)
true	P14-1031.pdf#61.0	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	timent classification ( two - way ) Table 2 : Accuracy results ( % ) for supervised sen - Electronics  Table 3: Accuracy results (%) for semi-supervised  sentiment classification (three-way) on the MD  dataset
true	P14-1031.pdf#64.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	timent classification ( two - way ) Table 2 : Accuracy results ( % ) for supervised sen - Music  Table 3: Accuracy results (%) for semi-supervised  sentiment classification (three-way) on the MD  dataset
true	P14-1031.pdf#62.3	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	timent classification ( two - way ) Table 2 : Accuracy results ( % ) for supervised sen - Avg  Table 3: Accuracy results (%) for semi-supervised  sentiment classification (three-way) on the MD  dataset
true	P14-1031.pdf#61.6	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	timent classification ( two - way ) Table 2 : Accuracy results ( % ) for supervised sen - Books  Table 3: Accuracy results (%) for semi-supervised  sentiment classification (three-way) on the MD  dataset
true	D15-1176.pdf#34.92	Penn Treebank, Accuracy	Fusional CA  Table 1: Language Modeling Results
true	D15-1176.pdf#40.92	Penn Treebank, Accuracy	Fusional PT  Table 1: Language Modeling Results
true	D15-1176.pdf#57.39	Penn Treebank, Accuracy	Fusional EN  Table 1: Language Modeling Results
true	D15-1176.pdf#41.94	Penn Treebank, Accuracy	Agglutinative DE  Table 1: Language Modeling Results
true	D15-1176.pdf#32.88	Penn Treebank, Accuracy	Agglutinative TR  Table 1: Language Modeling Results
true	D15-1176.pdf#97.36	Penn Treebank, Accuracy	4K acc 2000k 40k 80k 80k  Table 3: POS accuracy results for the English PTB  using word representation models.
true	D15-1176.pdf#91.59	Penn Treebank, Accuracy	Agglutinative TR  Table 4: POS accuracies on different languages
true	D15-1176.pdf#98.92	Penn Treebank, Accuracy	Fusional CA  Table 4: POS accuracies on different languages
true	D15-1176.pdf#97.54	Penn Treebank, Accuracy	Fusional PT  Table 4: POS accuracies on different languages
true	D15-1176.pdf#97.36	Penn Treebank, Accuracy	Fusional EN  Table 4: POS accuracies on different languages
true	D15-1176.pdf#98.08	Penn Treebank, Accuracy	Agglutinative DE  Table 4: POS accuracies on different languages
true	P10-1036.pdf#73.3	CCGBank, Accuracy	Speed ( sents / sec ) 4m  Table 2: Speed improvements on newswire, using various amounts of parser-annotated NANC data.
true	P10-1036.pdf#59.3	CCGBank, Accuracy	- Speed ( sents / sec ) 4m Speed ( sents / sec ) Bio  Table 4: Accuracy optimisation on newswire, using various amounts of parser-annotated NANC data.
true	P10-1036.pdf#85.98	CCGBank, Accuracy	Speed ( sents / sec ) 4m  Table 4: Accuracy optimisation on newswire, using various amounts of parser-annotated NANC data.
true	P10-1036.pdf#83.9	CCGBank, Accuracy	- Speed ( sents / sec ) 400k Speed ( sents / sec ) Wiki  Table 4: Accuracy optimisation on newswire, using various amounts of parser-annotated NANC data.
true	P10-1036.pdf#60.3	CCGBank, Accuracy	- Speed ( sents / sec ) 4m Speed ( sents / sec ) Bio  Table 4: Accuracy optimisation on newswire, using various amounts of parser-annotated NANC data.
true	P10-1036.pdf#90.4	CCGBank, Accuracy	- Speed ( sents / sec ) 400k Speed ( sents / sec ) Wiki  Table 4: Accuracy optimisation on newswire, using various amounts of parser-annotated NANC data.
true	P10-1036.pdf#96.66	CCGBank, Accuracy	Tagging Accuracy ( % ) 4m  Table 4: Accuracy optimisation on newswire, using various amounts of parser-annotated NANC data.
true	P10-1036.pdf#73.3	CCGBank, Accuracy	- Speed ( sents / sec ) 40k Speed ( sents / sec ) News  Table 4: Accuracy optimisation on newswire, using various amounts of parser-annotated NANC data.
true	P10-1036.pdf#73.9	CCGBank, Accuracy	- Speed ( sents / sec ) 400k Speed ( sents / sec ) Wiki  Table 4: Accuracy optimisation on newswire, using various amounts of parser-annotated NANC data.
true	P10-1036.pdf#90.4	CCGBank, Accuracy	Speed ( sents / sec ) Wiki  Table 5: Cross-corpus speed improvement, models trained with MIRA and 4,000,000 sentences. The  highlighted values are the top speed for each evaluation set and results that are statistically indistinguish- able from it.
true	P10-1036.pdf#83.9	CCGBank, Accuracy	Speed ( sents / sec ) Wiki  Table 5: Cross-corpus speed improvement, models trained with MIRA and 4,000,000 sentences. The  highlighted values are the top speed for each evaluation set and results that are statistically indistinguish- able from it.
true	P10-1036.pdf#73.9	CCGBank, Accuracy	Speed ( sents / sec ) Wiki  Table 5: Cross-corpus speed improvement, models trained with MIRA and 4,000,000 sentences. The  highlighted values are the top speed for each evaluation set and results that are statistically indistinguish- able from it.
true	P10-1036.pdf#59.3	CCGBank, Accuracy	Speed ( sents / sec ) Bio  Table 5: Cross-corpus speed improvement, models trained with MIRA and 4,000,000 sentences. The  highlighted values are the top speed for each evaluation set and results that are statistically indistinguish- able from it.
true	P10-1036.pdf#60.3	CCGBank, Accuracy	Speed ( sents / sec ) Bio  Table 5: Cross-corpus speed improvement, models trained with MIRA and 4,000,000 sentences. The  highlighted values are the top speed for each evaluation set and results that are statistically indistinguish- able from it.
true	P10-1036.pdf#73.3	CCGBank, Accuracy	Speed ( sents / sec ) News  Table 5: Cross-corpus speed improvement, models trained with MIRA and 4,000,000 sentences. The  highlighted values are the top speed for each evaluation set and results that are statistically indistinguish- able from it.
true	P10-1036.pdf#85.95	CCGBank, Accuracy	F - score ( % )  Table 6: Evaluation of top models on Section 23 of  CCGbank. All changes in F-score are statistically  significant.
true	P10-1036.pdf#94.86	CCGBank, Accuracy	Tag . Acc . Wiki  Table 7: Cross-corpus accuracy optimisation, models trained with GIS and 400,000 sentences.
true	P10-1036.pdf#85.84	CCGBank, Accuracy	F - score News  Table 7: Cross-corpus accuracy optimisation, models trained with GIS and 400,000 sentences.
true	P10-1036.pdf#96.53	CCGBank, Accuracy	Tag . Acc . News  Table 7: Cross-corpus accuracy optimisation, models trained with GIS and 400,000 sentences.
true	P10-1036.pdf#91.32	CCGBank, Accuracy	Tag . Acc . Bio  Table 7: Cross-corpus accuracy optimisation, models trained with GIS and 400,000 sentences.
true	P10-1036.pdf#81.7	CCGBank, Accuracy	F - score Wiki  Table 7: Cross-corpus accuracy optimisation, models trained with GIS and 400,000 sentences.
true	P10-1036.pdf#76.1	CCGBank, Accuracy	F - score Bio  Table 7: Cross-corpus accuracy optimisation, models trained with GIS and 400,000 sentences.
true	D12-1075.pdf#88.52	Penn Treebank, Number of params	Total  Table 1: English tagging accuracy using PTB sections 00-15 to build the tag dictionary. Known word types  are those appearing in the tag dictionary.
true	D12-1075.pdf#89.92	Penn Treebank, Number of params	Known  Table 1: English tagging accuracy using PTB sections 00-15 to build the tag dictionary. Known word types  are those appearing in the tag dictionary.
true	D12-1075.pdf#88.51	Penn Treebank, Number of params	Total  Table 1: English tagging accuracy using PTB sections 00-15 to build the tag dictionary. Known word types  are those appearing in the tag dictionary.
true	D12-1075.pdf#44.80	Penn Treebank, Number of params	Unk .  Table 1: English tagging accuracy using PTB sections 00-15 to build the tag dictionary. Known word types  are those appearing in the tag dictionary.
true	D12-1075.pdf#44.80	Penn Treebank, Number of params	Unk .  Table 1: English tagging accuracy using PTB sections 00-15 to build the tag dictionary. Known word types  are those appearing in the tag dictionary.
true	D12-1075.pdf#89.92	Penn Treebank, Number of params	Known  Table 1: English tagging accuracy using PTB sections 00-15 to build the tag dictionary. Known word types  are those appearing in the tag dictionary.
true	D12-1075.pdf#44.18	Penn Treebank, Number of params	07 ) Unk .  Table 2: Tagging accuracy using PTB sections 00-07 and TUT to build the tag dictionary. Known word  types are those appearing in the tag dictionary. Scenario numbers correspond to
true	D12-1075.pdf#72.86	Penn Treebank, Number of params	TUT Total  Table 2: Tagging accuracy using PTB sections 00-07 and TUT to build the tag dictionary. Known word  types are those appearing in the tag dictionary. Scenario numbers correspond to
true	D12-1075.pdf#87.95	Penn Treebank, Number of params	07 ) Total  Table 2: Tagging accuracy using PTB sections 00-07 and TUT to build the tag dictionary. Known word  types are those appearing in the tag dictionary. Scenario numbers correspond to
true	D12-1075.pdf#83.45	Penn Treebank, Number of params	TUT Known  Table 2: Tagging accuracy using PTB sections 00-07 and TUT to build the tag dictionary. Known word  types are those appearing in the tag dictionary. Scenario numbers correspond to
true	D12-1075.pdf#90.12	Penn Treebank, Number of params	07 ) Known  Table 2: Tagging accuracy using PTB sections 00-07 and TUT to build the tag dictionary. Known word  types are those appearing in the tag dictionary. Scenario numbers correspond to
true	D12-1075.pdf#25.28	Penn Treebank, Number of params	TUT Unk .  Table 2: Tagging accuracy using PTB sections 00-07 and TUT to build the tag dictionary. Known word  types are those appearing in the tag dictionary. Scenario numbers correspond to
true	D12-1075.pdf#88.52	Penn Treebank, Accuracy	Total  Table 1: English tagging accuracy using PTB sections 00-15 to build the tag dictionary. Known word types  are those appearing in the tag dictionary.
true	D12-1075.pdf#89.92	Penn Treebank, Accuracy	Known  Table 1: English tagging accuracy using PTB sections 00-15 to build the tag dictionary. Known word types  are those appearing in the tag dictionary.
true	D12-1075.pdf#88.51	Penn Treebank, Accuracy	Total  Table 1: English tagging accuracy using PTB sections 00-15 to build the tag dictionary. Known word types  are those appearing in the tag dictionary.
true	D12-1075.pdf#44.80	Penn Treebank, Accuracy	Unk .  Table 1: English tagging accuracy using PTB sections 00-15 to build the tag dictionary. Known word types  are those appearing in the tag dictionary.
true	D12-1075.pdf#44.80	Penn Treebank, Accuracy	Unk .  Table 1: English tagging accuracy using PTB sections 00-15 to build the tag dictionary. Known word types  are those appearing in the tag dictionary.
true	D12-1075.pdf#89.92	Penn Treebank, Accuracy	Known  Table 1: English tagging accuracy using PTB sections 00-15 to build the tag dictionary. Known word types  are those appearing in the tag dictionary.
true	D12-1075.pdf#44.18	Penn Treebank, Accuracy	07 ) Unk .  Table 2: Tagging accuracy using PTB sections 00-07 and TUT to build the tag dictionary. Known word  types are those appearing in the tag dictionary. Scenario numbers correspond to
true	D12-1075.pdf#72.86	Penn Treebank, Accuracy	TUT Total  Table 2: Tagging accuracy using PTB sections 00-07 and TUT to build the tag dictionary. Known word  types are those appearing in the tag dictionary. Scenario numbers correspond to
true	D12-1075.pdf#87.95	Penn Treebank, Accuracy	07 ) Total  Table 2: Tagging accuracy using PTB sections 00-07 and TUT to build the tag dictionary. Known word  types are those appearing in the tag dictionary. Scenario numbers correspond to
true	D12-1075.pdf#83.45	Penn Treebank, Accuracy	TUT Known  Table 2: Tagging accuracy using PTB sections 00-07 and TUT to build the tag dictionary. Known word  types are those appearing in the tag dictionary. Scenario numbers correspond to
true	D12-1075.pdf#90.12	Penn Treebank, Accuracy	07 ) Known  Table 2: Tagging accuracy using PTB sections 00-07 and TUT to build the tag dictionary. Known word  types are those appearing in the tag dictionary. Scenario numbers correspond to
true	D12-1075.pdf#25.28	Penn Treebank, Accuracy	TUT Unk .  Table 2: Tagging accuracy using PTB sections 00-07 and TUT to build the tag dictionary. Known word  types are those appearing in the tag dictionary. Scenario numbers correspond to
true	N13-1073.pdf#1.7	WMT 2014 EN-DE, BLEU	Log - linear  Table 1: CPU time (hours) required to train alignment  models in one direction.
true	N13-1073.pdf#6.0	WMT 2014 EN-DE, BLEU	Log - linear  Table 1: CPU time (hours) required to train alignment  models in one direction.
true	N13-1073.pdf#0.2	WMT 2014 EN-DE, BLEU	Log - linear  Table 1: CPU time (hours) required to train alignment  models in one direction.
true	N13-1073.pdf#44.1	WMT 2014 EN-DE, BLEU	ZH - EN  Table 2: Alignment quality (AER) on the WMT 2012  French-English and FBIS Chinese-English. Rows with  EM use expectation maximization to estimate the θ f , and  ∼Dir use variational Bayes.
true	N13-1073.pdf#10.4	WMT 2014 EN-DE, BLEU	FR - EN  Table 2: Alignment quality (AER) on the WMT 2012  French-English and FBIS Chinese-English. Rows with  EM use expectation maximization to estimate the θ f , and  ∼Dir use variational Bayes.
true	N13-1073.pdf#44.1	WMT 2014 EN-DE, BLEU	ZH - EN  Table 3: Translation quality (BLEU) as a function of  alignment type.
true	N13-1073.pdf#27.7	WMT 2014 EN-DE, BLEU	ZH - EN Log - linear  Table 3: Translation quality (BLEU) as a function of  alignment type.
true	N13-1073.pdf#55.7	WMT 2014 EN-DE, BLEU	ZH - EN Log - linear  Table 3: Translation quality (BLEU) as a function of  alignment type.
true	N13-1073.pdf#34.7	WMT 2014 EN-DE, BLEU	ZH - EN Log - linear  Table 3: Translation quality (BLEU) as a function of  alignment type.
true	N13-1073.pdf#10.4	WMT 2014 EN-DE, BLEU	FR - EN  Table 3: Translation quality (BLEU) as a function of  alignment type.
true	N13-1073.pdf#1.7	WMT 2014 EN-FR, BLEU	Log - linear  Table 1: CPU time (hours) required to train alignment  models in one direction.
true	N13-1073.pdf#6.0	WMT 2014 EN-FR, BLEU	Log - linear  Table 1: CPU time (hours) required to train alignment  models in one direction.
true	N13-1073.pdf#0.2	WMT 2014 EN-FR, BLEU	Log - linear  Table 1: CPU time (hours) required to train alignment  models in one direction.
true	N13-1073.pdf#44.1	WMT 2014 EN-FR, BLEU	ZH - EN  Table 2: Alignment quality (AER) on the WMT 2012  French-English and FBIS Chinese-English. Rows with  EM use expectation maximization to estimate the θ f , and  ∼Dir use variational Bayes.
true	N13-1073.pdf#10.4	WMT 2014 EN-FR, BLEU	FR - EN  Table 2: Alignment quality (AER) on the WMT 2012  French-English and FBIS Chinese-English. Rows with  EM use expectation maximization to estimate the θ f , and  ∼Dir use variational Bayes.
true	N13-1073.pdf#44.1	WMT 2014 EN-FR, BLEU	ZH - EN  Table 3: Translation quality (BLEU) as a function of  alignment type.
true	N13-1073.pdf#27.7	WMT 2014 EN-FR, BLEU	ZH - EN Log - linear  Table 3: Translation quality (BLEU) as a function of  alignment type.
true	N13-1073.pdf#55.7	WMT 2014 EN-FR, BLEU	ZH - EN Log - linear  Table 3: Translation quality (BLEU) as a function of  alignment type.
true	N13-1073.pdf#34.7	WMT 2014 EN-FR, BLEU	ZH - EN Log - linear  Table 3: Translation quality (BLEU) as a function of  alignment type.
true	N13-1073.pdf#10.4	WMT 2014 EN-FR, BLEU	FR - EN  Table 3: Translation quality (BLEU) as a function of  alignment type.
true	D12-1105.pdf#84.9	Penn Treebank, F1	Parsing Exact  Table 1: The effect of algorithm choice for training and  parsing on a product of two 2-state parsers on F1. Petrov  is the product parser of Petrov (2010), and Indep. refers  to independently trained models. For comparison, a four- state parser achieves a score of 83.2.
true	D15-1205.pdf#47.59	New York Times Corpus, P@10%	wl R  Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our  reimplementation of the features of Nguyen and Grishman (2014).
true	D15-1205.pdf#55.17	New York Times Corpus, P@10%	Avg . F1  Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our  reimplementation of the features of Nguyen and Grishman (2014).
true	D15-1205.pdf#63.48	New York Times Corpus, P@10%	bc F1  Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our  reimplementation of the features of Nguyen and Grishman (2014).
true	D15-1205.pdf#58.26	New York Times Corpus, P@10%	Avg . F1  Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our  reimplementation of the features of Nguyen and Grishman (2014).
true	D15-1205.pdf#74.53	New York Times Corpus, P@10%	cts P  Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our  reimplementation of the features of Nguyen and Grishman (2014).
true	D15-1205.pdf#57.86	New York Times Corpus, P@10%	bc  Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our  reimplementation of the features of Nguyen and Grishman (2014).
true	D15-1205.pdf#65.63	New York Times Corpus, P@10%	wl P  Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our  reimplementation of the features of Nguyen and Grishman (2014).
true	D15-1205.pdf#45.01	New York Times Corpus, P@10%	cts R  Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our  reimplementation of the features of Nguyen and Grishman (2014).
true	D15-1205.pdf#56.12	New York Times Corpus, P@10%	cts F1  Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our  reimplementation of the features of Nguyen and Grishman (2014).
true	D15-1205.pdf#74.89	New York Times Corpus, P@10%	bc P  Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our  reimplementation of the features of Nguyen and Grishman (2014).
true	D15-1205.pdf#84.1	New York Times Corpus, P@10%	Google n - gram , paraphrases , TextRunner F1 POS , prefixes , morphological , WordNet , dependency parse ,  Table 4: Comparison of FCM with previously published results for SemEval 2010 Task 8.
true	D15-1205.pdf#83.4	New York Times Corpus, P@10%	Google n - gram , paraphrases , TextRunner F1 POS , prefixes , morphological , WordNet , dependency parse ,  Table 4: Comparison of FCM with previously published results for SemEval 2010 Task 8.
true	D15-1205.pdf#56.73	New York Times Corpus, P@10%	Rec  Table 5: Ablation test of FCM on development set.
true	D15-1205.pdf#71.33	New York Times Corpus, P@10%	Prec  Table 5: Ablation test of FCM on development set.
true	D15-1205.pdf#62.33	New York Times Corpus, P@10%	F1  Table 5: Ablation test of FCM on development set.
true	D15-1205.pdf#47.59	SemEval-2010 Task 8, F1	wl R  Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our  reimplementation of the features of Nguyen and Grishman (2014).
true	D15-1205.pdf#55.17	SemEval-2010 Task 8, F1	Avg . F1  Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our  reimplementation of the features of Nguyen and Grishman (2014).
true	D15-1205.pdf#63.48	SemEval-2010 Task 8, F1	bc F1  Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our  reimplementation of the features of Nguyen and Grishman (2014).
true	D15-1205.pdf#58.26	SemEval-2010 Task 8, F1	Avg . F1  Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our  reimplementation of the features of Nguyen and Grishman (2014).
true	D15-1205.pdf#74.53	SemEval-2010 Task 8, F1	cts P  Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our  reimplementation of the features of Nguyen and Grishman (2014).
true	D15-1205.pdf#57.86	SemEval-2010 Task 8, F1	bc  Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our  reimplementation of the features of Nguyen and Grishman (2014).
true	D15-1205.pdf#65.63	SemEval-2010 Task 8, F1	wl P  Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our  reimplementation of the features of Nguyen and Grishman (2014).
true	D15-1205.pdf#45.01	SemEval-2010 Task 8, F1	cts R  Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our  reimplementation of the features of Nguyen and Grishman (2014).
true	D15-1205.pdf#56.12	SemEval-2010 Task 8, F1	cts F1  Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our  reimplementation of the features of Nguyen and Grishman (2014).
true	D15-1205.pdf#74.89	SemEval-2010 Task 8, F1	bc P  Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our  reimplementation of the features of Nguyen and Grishman (2014).
true	D15-1205.pdf#84.1	SemEval-2010 Task 8, F1	Google n - gram , paraphrases , TextRunner F1 POS , prefixes , morphological , WordNet , dependency parse ,  Table 4: Comparison of FCM with previously published results for SemEval 2010 Task 8.
true	D15-1205.pdf#83.4	SemEval-2010 Task 8, F1	Google n - gram , paraphrases , TextRunner F1 POS , prefixes , morphological , WordNet , dependency parse ,  Table 4: Comparison of FCM with previously published results for SemEval 2010 Task 8.
true	D15-1205.pdf#56.73	SemEval-2010 Task 8, F1	Rec  Table 5: Ablation test of FCM on development set.
true	D15-1205.pdf#71.33	SemEval-2010 Task 8, F1	Prec  Table 5: Ablation test of FCM on development set.
true	D15-1205.pdf#62.33	SemEval-2010 Task 8, F1	F1  Table 5: Ablation test of FCM on development set.
true	P15-2136.pdf#38.55	DUC 2004 Task 1, ROUGE-2	ROUGE - 1  Table 2: Comparison results (%) on DUC datasets.
true	P15-2136.pdf#35.98	DUC 2004 Task 1, ROUGE-2	ROUGE - 1  Table 2: Comparison results (%) on DUC datasets.
true	P15-2136.pdf#8.97	DUC 2004 Task 1, ROUGE-2	ROUGE - 2  Table 2: Comparison results (%) on DUC datasets.
true	P15-2136.pdf#7.89	DUC 2004 Task 1, ROUGE-2	ROUGE - 2  Table 2: Comparison results (%) on DUC datasets.
true	P15-2136.pdf#10.07	DUC 2004 Task 1, ROUGE-2	ROUGE - 2  Table 2: Comparison results (%) on DUC datasets.
true	P15-2136.pdf#38.91	DUC 2004 Task 1, ROUGE-2	ROUGE - 1  Table 2: Comparison results (%) on DUC datasets.
true	P15-2136.pdf#38.55	DUC 2004 Task 1, ROUGE-L	ROUGE - 1  Table 2: Comparison results (%) on DUC datasets.
true	P15-2136.pdf#35.98	DUC 2004 Task 1, ROUGE-L	ROUGE - 1  Table 2: Comparison results (%) on DUC datasets.
true	P15-2136.pdf#8.97	DUC 2004 Task 1, ROUGE-L	ROUGE - 2  Table 2: Comparison results (%) on DUC datasets.
true	P15-2136.pdf#7.89	DUC 2004 Task 1, ROUGE-L	ROUGE - 2  Table 2: Comparison results (%) on DUC datasets.
true	P15-2136.pdf#10.07	DUC 2004 Task 1, ROUGE-L	ROUGE - 2  Table 2: Comparison results (%) on DUC datasets.
true	P15-2136.pdf#38.91	DUC 2004 Task 1, ROUGE-L	ROUGE - 1  Table 2: Comparison results (%) on DUC datasets.
true	P15-2136.pdf#38.55	DUC 2004 Task 1, ROUGE-1	ROUGE - 1  Table 2: Comparison results (%) on DUC datasets.
true	P15-2136.pdf#35.98	DUC 2004 Task 1, ROUGE-1	ROUGE - 1  Table 2: Comparison results (%) on DUC datasets.
true	P15-2136.pdf#8.97	DUC 2004 Task 1, ROUGE-1	ROUGE - 2  Table 2: Comparison results (%) on DUC datasets.
true	P15-2136.pdf#7.89	DUC 2004 Task 1, ROUGE-1	ROUGE - 2  Table 2: Comparison results (%) on DUC datasets.
true	P15-2136.pdf#10.07	DUC 2004 Task 1, ROUGE-1	ROUGE - 2  Table 2: Comparison results (%) on DUC datasets.
true	P15-2136.pdf#38.91	DUC 2004 Task 1, ROUGE-1	ROUGE - 1  Table 2: Comparison results (%) on DUC datasets.
true	P15-1112.pdf#95	Penn Treebank, POS	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#80	Penn Treebank, POS	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#95	Penn Treebank, POS	Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#93	Penn Treebank, POS	RCNN Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#94	Penn Treebank, POS	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#97	Penn Treebank, POS	Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#90	Penn Treebank, POS	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#92	Penn Treebank, POS	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#95	Penn Treebank, POS	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#85	Penn Treebank, POS	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#90	Penn Treebank, POS	Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#80	Penn Treebank, POS	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#75	Penn Treebank, POS	RCNN Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#96	Penn Treebank, POS	Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#85	Penn Treebank, POS	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#75	Penn Treebank, POS	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#95	Penn Treebank, F1	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#80	Penn Treebank, F1	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#95	Penn Treebank, F1	Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#93	Penn Treebank, F1	RCNN Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#94	Penn Treebank, F1	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#97	Penn Treebank, F1	Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#90	Penn Treebank, F1	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#92	Penn Treebank, F1	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#95	Penn Treebank, F1	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#85	Penn Treebank, F1	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#90	Penn Treebank, F1	Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#80	Penn Treebank, F1	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#75	Penn Treebank, F1	RCNN Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#96	Penn Treebank, F1	Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#85	Penn Treebank, F1	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#75	Penn Treebank, F1	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#95	Penn Treebank, LAS	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#80	Penn Treebank, LAS	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#95	Penn Treebank, LAS	Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#93	Penn Treebank, LAS	RCNN Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#94	Penn Treebank, LAS	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#97	Penn Treebank, LAS	Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#90	Penn Treebank, LAS	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#92	Penn Treebank, LAS	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#95	Penn Treebank, LAS	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#85	Penn Treebank, LAS	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#90	Penn Treebank, LAS	Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#80	Penn Treebank, LAS	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#75	Penn Treebank, LAS	RCNN Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#96	Penn Treebank, LAS	Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#85	Penn Treebank, LAS	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#75	Penn Treebank, LAS	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#95	benchmark Vietnamese dependency treebank VnDT, UAS	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#80	benchmark Vietnamese dependency treebank VnDT, UAS	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#95	benchmark Vietnamese dependency treebank VnDT, UAS	Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#93	benchmark Vietnamese dependency treebank VnDT, UAS	RCNN Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#94	benchmark Vietnamese dependency treebank VnDT, UAS	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#97	benchmark Vietnamese dependency treebank VnDT, UAS	Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#90	benchmark Vietnamese dependency treebank VnDT, UAS	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#92	benchmark Vietnamese dependency treebank VnDT, UAS	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#95	benchmark Vietnamese dependency treebank VnDT, UAS	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#85	benchmark Vietnamese dependency treebank VnDT, UAS	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#90	benchmark Vietnamese dependency treebank VnDT, UAS	Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#80	benchmark Vietnamese dependency treebank VnDT, UAS	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#75	benchmark Vietnamese dependency treebank VnDT, UAS	RCNN Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#96	benchmark Vietnamese dependency treebank VnDT, UAS	Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#85	benchmark Vietnamese dependency treebank VnDT, UAS	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#75	benchmark Vietnamese dependency treebank VnDT, UAS	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#95	Penn Treebank, UAS	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#80	Penn Treebank, UAS	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#95	Penn Treebank, UAS	Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#93	Penn Treebank, UAS	RCNN Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#94	Penn Treebank, UAS	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#97	Penn Treebank, UAS	Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#90	Penn Treebank, UAS	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#92	Penn Treebank, UAS	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#95	Penn Treebank, UAS	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#85	Penn Treebank, UAS	Re - ranker Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#90	Penn Treebank, UAS	Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#80	Penn Treebank, UAS	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#75	Penn Treebank, UAS	RCNN Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#96	Penn Treebank, UAS	Oracle Best  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#85	Penn Treebank, UAS	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	P15-1112.pdf#75	Penn Treebank, UAS	RCNN and base parser . Oracle Best k ( a ) without the oracle worst result  Table 1: Accuracy on English test set. Our base- line is the result of base parser; our re-ranker uses  the mixture strategy on the 64-best outputs of base  parser; our re-ranker(with oracle) is to add the or- acle to k-best outputs of base parser.
true	D13-1184.pdf#85.30	New York Times Corpus, P@10%	ACE  Table 1: Performance on Wikification datasets, BOT F1  Performance. Our system, Relational Inference (RI) ex- hibits significant improvements over M&W (Milne and  Witten, 2008) and R&R (Ratinov et al., 2011).
true	D13-1184.pdf#88.88	New York Times Corpus, P@10%	AQUAINT  Table 1: Performance on Wikification datasets, BOT F1  Performance. Our system, Relational Inference (RI) ex- hibits significant improvements over M&W (Milne and  Witten, 2008) and R&R (Ratinov et al., 2011).
true	D13-1184.pdf#81.20	New York Times Corpus, P@10%	MSNBC  Table 1: Performance on Wikification datasets, BOT F1  Performance. Our system, Relational Inference (RI) ex- hibits significant improvements over M&W (Milne and  Witten, 2008) and R&R (Ratinov et al., 2011).
true	D13-1184.pdf#93.09	New York Times Corpus, P@10%	Wiki  Table 1: Performance on Wikification datasets, BOT F1  Performance. Our system, Relational Inference (RI) ex- hibits significant improvements over M&W (Milne and  Witten, 2008) and R&R (Ratinov et al., 2011).
true	P14-1146.pdf#86.58	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Macro - F1  Table 2: Macro-F1 on positive/negative classifica- tion of tweets.
true	P14-1146.pdf#84.73	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Macro - F1  Table 2: Macro-F1 on positive/negative classifica- tion of tweets.
true	P14-1146.pdf#86.48	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Macro - F1  Table 2: Macro-F1 on positive/negative classifica- tion of tweets.
true	P14-1146.pdf#84.98	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Macro - F1  Table 2: Macro-F1 on positive/negative classifica- tion of tweets.
true	P14-1146.pdf#84.98	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	bigram and trigram embedding . represents a word Each column s - uni+bi+tri - - -  Table 3: Macro-F1 on positive/negative classifica- tion of tweets with different word embeddings.
true	P14-1146.pdf#84.70	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	bigram and trigram embedding . represents a word embedding learning algorithm . uni+bi - - -  Table 3: Macro-F1 on positive/negative classifica- tion of tweets with different word embeddings.
true	P14-1146.pdf#83.70	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	bigram and trigram embedding . represents a word embedding learning algorithm . unigram - - -  Table 3: Macro-F1 on positive/negative classifica- tion of tweets with different word embeddings.
true	P14-1146.pdf#77.30	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	HL
true	P14-1146.pdf#71.74	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	MPQA
true	P14-1146.pdf#77.33	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Joint
true	P14-1146.pdf#77.33	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Joint  Table 4: Statistics of the sentiment lexicons. Join- t stands for the words that occur in both HL and  MPQA with the same sentiment polarity.
true	P14-1146.pdf#77.30	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	HL  Table 4: Statistics of the sentiment lexicons. Join- t stands for the words that occur in both HL and  MPQA with the same sentiment polarity.
true	P14-1146.pdf#71.74	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	MPQA  Table 4: Statistics of the sentiment lexicons. Join- t stands for the words that occur in both HL and  MPQA with the same sentiment polarity.
true	P14-1146.pdf#86.58	SST-2, Accuracy	Macro - F1  Table 2: Macro-F1 on positive/negative classifica- tion of tweets.
true	P14-1146.pdf#84.73	SST-2, Accuracy	Macro - F1  Table 2: Macro-F1 on positive/negative classifica- tion of tweets.
true	P14-1146.pdf#86.48	SST-2, Accuracy	Macro - F1  Table 2: Macro-F1 on positive/negative classifica- tion of tweets.
true	P14-1146.pdf#84.98	SST-2, Accuracy	Macro - F1  Table 2: Macro-F1 on positive/negative classifica- tion of tweets.
true	P14-1146.pdf#84.98	SST-2, Accuracy	bigram and trigram embedding . represents a word Each column s - uni+bi+tri - - -  Table 3: Macro-F1 on positive/negative classifica- tion of tweets with different word embeddings.
true	P14-1146.pdf#84.70	SST-2, Accuracy	bigram and trigram embedding . represents a word embedding learning algorithm . uni+bi - - -  Table 3: Macro-F1 on positive/negative classifica- tion of tweets with different word embeddings.
true	P14-1146.pdf#83.70	SST-2, Accuracy	bigram and trigram embedding . represents a word embedding learning algorithm . unigram - - -  Table 3: Macro-F1 on positive/negative classifica- tion of tweets with different word embeddings.
true	P14-1146.pdf#77.30	SST-2, Accuracy	HL
true	P14-1146.pdf#71.74	SST-2, Accuracy	MPQA
true	P14-1146.pdf#77.33	SST-2, Accuracy	Joint
true	P14-1146.pdf#77.33	SST-2, Accuracy	Joint  Table 4: Statistics of the sentiment lexicons. Join- t stands for the words that occur in both HL and  MPQA with the same sentiment polarity.
true	P14-1146.pdf#77.30	SST-2, Accuracy	HL  Table 4: Statistics of the sentiment lexicons. Join- t stands for the words that occur in both HL and  MPQA with the same sentiment polarity.
true	P14-1146.pdf#71.74	SST-2, Accuracy	MPQA  Table 4: Statistics of the sentiment lexicons. Join- t stands for the words that occur in both HL and  MPQA with the same sentiment polarity.
true	D14-1017.pdf#0.8851	New York Times Corpus, P@10%	Table 2 : Results of word alignment evaluation with the heuristics - based method ( GDF ) Hansard ( French - English ) recall  Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)
true	D14-1017.pdf#0.4633	New York Times Corpus, P@10%	Table 2 : Results of word alignment evaluation with the heuristics - based method ( GDF ) KFTT precision  Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)
true	D14-1017.pdf#47.73	New York Times Corpus, P@10%	Table 2 : Results of word alignment evaluation with the heuristics - based method ( GDF ) KFTT AER  Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)
true	D14-1017.pdf#0.5998	New York Times Corpus, P@10%	Table 2 : Results of word alignment evaluation with the heuristics - based method ( GDF ) KFTT recall  Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)
true	D14-1017.pdf#0.5997	New York Times Corpus, P@10%	Table 2 : Results of word alignment evaluation with the heuristics - based method ( GDF ) KFTT recall  Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)
true	D14-1017.pdf#0.5226	New York Times Corpus, P@10%	Table 2 : Results of word alignment evaluation with the heuristics - based method ( GDF ) KFTT F  Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)
true	D14-1017.pdf#47.74	New York Times Corpus, P@10%	Table 2 : Results of word alignment evaluation with the heuristics - based method ( GDF ) KFTT AER  Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)
true	D14-1017.pdf#7.10	New York Times Corpus, P@10%	Table 2 : Results of word alignment evaluation with the heuristics - based method ( GDF ) Hansard ( French - English ) AER  Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)
true	D14-1017.pdf#7.29	New York Times Corpus, P@10%	Table 2 : Results of word alignment evaluation with the heuristics - based method ( GDF ) Hansard ( French - English )  Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)
true	D14-1017.pdf#0.7848	New York Times Corpus, P@10%	Table 2 : Results of word alignment evaluation with the heuristics - based method ( GDF ) Hansard ( French - English ) F  Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)
true	D14-1017.pdf#0.8871	New York Times Corpus, P@10%	Table 2 : Results of word alignment evaluation with the heuristics - based method ( GDF ) Hansard ( French - English ) recall  Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)
true	D14-1017.pdf#0.7042	New York Times Corpus, P@10%	Table 2 : Results of word alignment evaluation with the heuristics - based method ( GDF ) Hansard ( French - English ) precision  Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)
true	D14-1017.pdf#7.29	New York Times Corpus, P@10%	Table 2 : Results of word alignment evaluation with the heuristics - based method ( GDF ) Hansard ( French - English ) AER  Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)
true	D14-1017.pdf#0.7844	New York Times Corpus, P@10%	Table 2 : Results of word alignment evaluation with the heuristics - based method ( GDF ) Hansard ( French - English ) F  Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)
true	D14-1017.pdf#0.5227	New York Times Corpus, P@10%	Table 2 : Results of word alignment evaluation with the heuristics - based method ( GDF ) Hansard ( French - English ) F  Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)
true	D14-1017.pdf#0.4630	New York Times Corpus, P@10%	Table 2 : Results of word alignment evaluation with the heuristics - based method ( GDF ) KFTT precision  Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)
true	D14-1017.pdf#0.7037	New York Times Corpus, P@10%	Table 2 : Results of word alignment evaluation with the heuristics - based method ( GDF ) Hansard ( French - English ) precision  Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)
true	D14-1017.pdf#29.92	New York Times Corpus, P@10%	Table 3 : Results of translation evaluation NTCIR10 Filtered  Table 3: Results of translation evaluation
true	D14-1017.pdf#28.36	New York Times Corpus, P@10%	Table 3 : Results of translation evaluation NTCIR10 GDF  Table 3: Results of translation evaluation
true	D14-1017.pdf#28.36	New York Times Corpus, P@10%	Table 3 : Results of translation evaluation NTCIR10 GDF  Table 3: Results of translation evaluation
true	D14-1017.pdf#28.3	New York Times Corpus, P@10%	Table 3 : Results of translation evaluation NTCIR10  Table 3: Results of translation evaluation
true	D14-1017.pdf#19.15	New York Times Corpus, P@10%	Table 3 : Results of translation evaluation KFTT GDF  Table 3: Results of translation evaluation
true	D14-1017.pdf#19.26	New York Times Corpus, P@10%	Table 3 : Results of translation evaluation KFTT GDF  Table 3: Results of translation evaluation
true	D14-1017.pdf#28.36	New York Times Corpus, P@10%	Table 3 : Results of translation evaluation NTCIR10 GDF  Table 3: Results of translation evaluation
true	D14-1017.pdf#29.74	New York Times Corpus, P@10%	Table 3 : Results of translation evaluation NTCIR10 Filtered  Table 3: Results of translation evaluation
true	D14-1017.pdf#19.28	New York Times Corpus, P@10%	Table 3 : Results of translation evaluation KFTT  Table 3: Results of translation evaluation
true	D14-1017.pdf#19.20	New York Times Corpus, P@10%	Table 3 : Results of translation evaluation KFTT Filtered  Table 3: Results of translation evaluation
true	D14-1017.pdf#28.36	New York Times Corpus, P@10%	Table 3 : Results of translation evaluation NTCIR10 GDF  Table 3. Our proposed methods achieved large  gains in NTCIR10 task with the filtered method,  but observed no gain in the KFTT with the filtered  method. In NTCIR10 task with GDF, the gain in  BLEU was smaller than that of KFTT. We cal- culate p-values and the difference between sym- metric and c2c (the most effective proposed con- straint) are lower than 0.05 in kftt with GDF and  NTCIR10 with filtered method. There seems to  be no clear tendency in the improved alignment  qualities and the translation qualities, as shown in  numerous previous studies (
true	D14-1017.pdf#28.3	New York Times Corpus, P@10%	Table 3 : Results of translation evaluation NTCIR10  Table 3. Our proposed methods achieved large  gains in NTCIR10 task with the filtered method,  but observed no gain in the KFTT with the filtered  method. In NTCIR10 task with GDF, the gain in  BLEU was smaller than that of KFTT. We cal- culate p-values and the difference between sym- metric and c2c (the most effective proposed con- straint) are lower than 0.05 in kftt with GDF and  NTCIR10 with filtered method. There seems to  be no clear tendency in the improved alignment  qualities and the translation qualities, as shown in  numerous previous studies (
true	D14-1017.pdf#28.36	New York Times Corpus, P@10%	Table 3 : Results of translation evaluation NTCIR10 GDF  Table 3. Our proposed methods achieved large  gains in NTCIR10 task with the filtered method,  but observed no gain in the KFTT with the filtered  method. In NTCIR10 task with GDF, the gain in  BLEU was smaller than that of KFTT. We cal- culate p-values and the difference between sym- metric and c2c (the most effective proposed con- straint) are lower than 0.05 in kftt with GDF and  NTCIR10 with filtered method. There seems to  be no clear tendency in the improved alignment  qualities and the translation qualities, as shown in  numerous previous studies (
true	D14-1017.pdf#19.26	New York Times Corpus, P@10%	Table 3 : Results of translation evaluation KFTT GDF  Table 3. Our proposed methods achieved large  gains in NTCIR10 task with the filtered method,  but observed no gain in the KFTT with the filtered  method. In NTCIR10 task with GDF, the gain in  BLEU was smaller than that of KFTT. We cal- culate p-values and the difference between sym- metric and c2c (the most effective proposed con- straint) are lower than 0.05 in kftt with GDF and  NTCIR10 with filtered method. There seems to  be no clear tendency in the improved alignment  qualities and the translation qualities, as shown in  numerous previous studies (
true	D14-1017.pdf#28.36	New York Times Corpus, P@10%	Table 3 : Results of translation evaluation NTCIR10 GDF  Table 3. Our proposed methods achieved large  gains in NTCIR10 task with the filtered method,  but observed no gain in the KFTT with the filtered  method. In NTCIR10 task with GDF, the gain in  BLEU was smaller than that of KFTT. We cal- culate p-values and the difference between sym- metric and c2c (the most effective proposed con- straint) are lower than 0.05 in kftt with GDF and  NTCIR10 with filtered method. There seems to  be no clear tendency in the improved alignment  qualities and the translation qualities, as shown in  numerous previous studies (
true	D14-1017.pdf#29.74	New York Times Corpus, P@10%	Table 3 : Results of translation evaluation NTCIR10 Filtered  Table 3. Our proposed methods achieved large  gains in NTCIR10 task with the filtered method,  but observed no gain in the KFTT with the filtered  method. In NTCIR10 task with GDF, the gain in  BLEU was smaller than that of KFTT. We cal- culate p-values and the difference between sym- metric and c2c (the most effective proposed con- straint) are lower than 0.05 in kftt with GDF and  NTCIR10 with filtered method. There seems to  be no clear tendency in the improved alignment  qualities and the translation qualities, as shown in  numerous previous studies (
true	D14-1017.pdf#29.92	New York Times Corpus, P@10%	Table 3 : Results of translation evaluation NTCIR10 Filtered  Table 3. Our proposed methods achieved large  gains in NTCIR10 task with the filtered method,  but observed no gain in the KFTT with the filtered  method. In NTCIR10 task with GDF, the gain in  BLEU was smaller than that of KFTT. We cal- culate p-values and the difference between sym- metric and c2c (the most effective proposed con- straint) are lower than 0.05 in kftt with GDF and  NTCIR10 with filtered method. There seems to  be no clear tendency in the improved alignment  qualities and the translation qualities, as shown in  numerous previous studies (
true	D14-1017.pdf#19.28	New York Times Corpus, P@10%	Table 3 : Results of translation evaluation KFTT  Table 3. Our proposed methods achieved large  gains in NTCIR10 task with the filtered method,  but observed no gain in the KFTT with the filtered  method. In NTCIR10 task with GDF, the gain in  BLEU was smaller than that of KFTT. We cal- culate p-values and the difference between sym- metric and c2c (the most effective proposed con- straint) are lower than 0.05 in kftt with GDF and  NTCIR10 with filtered method. There seems to  be no clear tendency in the improved alignment  qualities and the translation qualities, as shown in  numerous previous studies (
true	D14-1017.pdf#19.15	New York Times Corpus, P@10%	Table 3 : Results of translation evaluation KFTT GDF  Table 3. Our proposed methods achieved large  gains in NTCIR10 task with the filtered method,  but observed no gain in the KFTT with the filtered  method. In NTCIR10 task with GDF, the gain in  BLEU was smaller than that of KFTT. We cal- culate p-values and the difference between sym- metric and c2c (the most effective proposed con- straint) are lower than 0.05 in kftt with GDF and  NTCIR10 with filtered method. There seems to  be no clear tendency in the improved alignment  qualities and the translation qualities, as shown in  numerous previous studies (
true	D14-1017.pdf#19.20	New York Times Corpus, P@10%	Table 3 : Results of translation evaluation KFTT Filtered  Table 3. Our proposed methods achieved large  gains in NTCIR10 task with the filtered method,  but observed no gain in the KFTT with the filtered  method. In NTCIR10 task with GDF, the gain in  BLEU was smaller than that of KFTT. We cal- culate p-values and the difference between sym- metric and c2c (the most effective proposed con- straint) are lower than 0.05 in kftt with GDF and  NTCIR10 with filtered method. There seems to  be no clear tendency in the improved alignment  qualities and the translation qualities, as shown in  numerous previous studies (
true	D15-1236.pdf#83.51	SearchQA, Unigram Acc	Dev  Table 2: SceneQA outperforms two competitive  systems on two of the three test sets. The high- lighted improvements are statistically significant.
true	D15-1236.pdf#65.38	SearchQA, Unigram Acc	Test2  Table 2: SceneQA outperforms two competitive  systems on two of the three test sets. The high- lighted improvements are statistically significant.
true	D15-1236.pdf#55.20	SearchQA, Unigram Acc	Test3  Table 2: SceneQA outperforms two competitive  systems on two of the three test sets. The high- lighted improvements are statistically significant.
true	D15-1236.pdf#55.20	SearchQA, Unigram Acc	• - Both : No new nodes , no pruning  Table 3: SceneQA outperforms all the ablations  on two of the three test sets. The highlighted im- provements are statistically significant.
true	D15-1236.pdf#65.38	SearchQA, Unigram Acc	• - Both : No new nodes , no pruning  Table 3: SceneQA outperforms all the ablations  on two of the three test sets. The highlighted im- provements are statistically significant.
true	D15-1236.pdf#83.51	SearchQA, Unigram Acc	• - Both : No new nodes , no pruning  Table 3: SceneQA outperforms all the ablations  on two of the three test sets. The highlighted im- provements are statistically significant.
true	D14-1130.pdf#59.71	WMT 2014 EN-DE, BLEU	BLEU↑ TER↓  Table 6: Main re-tuning results for interactive  data. baseline is the BLEU-tuned system used  in the translation user study. re-tune is the base- line feature set re-tuned to HTER on int-tune. re- tune+feat adds the human feature templates de- scribed in section 3.3. bold indicates statistical  significance relative to the baseline at p < 0.001;  italic at p < 0.05 by the permutation test of
true	D14-1130.pdf#42.35	WMT 2014 EN-DE, BLEU	HTER  Table 6: Main re-tuning results for interactive  data. baseline is the BLEU-tuned system used  in the translation user study. re-tune is the base- line feature set re-tuned to HTER on int-tune. re- tune+feat adds the human feature templates de- scribed in section 3.3. bold indicates statistical  significance relative to the baseline at p < 0.001;  italic at p < 0.05 by the permutation test of
true	D14-1130.pdf#40.30	WMT 2014 EN-DE, BLEU	( a ) En - De int - test results . BLEU↑ TER↓ BLEU↑ TER↓  Table 6: Main re-tuning results for interactive  data. baseline is the BLEU-tuned system used  in the translation user study. re-tune is the base- line feature set re-tuned to HTER on int-tune. re- tune+feat adds the human feature templates de- scribed in section 3.3. bold indicates statistical  significance relative to the baseline at p < 0.001;  italic at p < 0.05 by the permutation test of
true	D14-1130.pdf#26.40	WMT 2014 EN-DE, BLEU	( a ) En - De int - test results . HTER HTER  Table 6: Main re-tuning results for interactive  data. baseline is the BLEU-tuned system used  in the translation user study. re-tune is the base- line feature set re-tuned to HTER on int-tune. re- tune+feat adds the human feature templates de- scribed in section 3.3. bold indicates statistical  significance relative to the baseline at p < 0.001;  italic at p < 0.05 by the permutation test of
true	D14-1130.pdf#59.71	WMT 2014 EN-FR, BLEU	BLEU↑ TER↓  Table 6: Main re-tuning results for interactive  data. baseline is the BLEU-tuned system used  in the translation user study. re-tune is the base- line feature set re-tuned to HTER on int-tune. re- tune+feat adds the human feature templates de- scribed in section 3.3. bold indicates statistical  significance relative to the baseline at p < 0.001;  italic at p < 0.05 by the permutation test of
true	D14-1130.pdf#42.35	WMT 2014 EN-FR, BLEU	HTER  Table 6: Main re-tuning results for interactive  data. baseline is the BLEU-tuned system used  in the translation user study. re-tune is the base- line feature set re-tuned to HTER on int-tune. re- tune+feat adds the human feature templates de- scribed in section 3.3. bold indicates statistical  significance relative to the baseline at p < 0.001;  italic at p < 0.05 by the permutation test of
true	D14-1130.pdf#40.30	WMT 2014 EN-FR, BLEU	( a ) En - De int - test results . BLEU↑ TER↓ BLEU↑ TER↓  Table 6: Main re-tuning results for interactive  data. baseline is the BLEU-tuned system used  in the translation user study. re-tune is the base- line feature set re-tuned to HTER on int-tune. re- tune+feat adds the human feature templates de- scribed in section 3.3. bold indicates statistical  significance relative to the baseline at p < 0.001;  italic at p < 0.05 by the permutation test of
true	D14-1130.pdf#26.40	WMT 2014 EN-FR, BLEU	( a ) En - De int - test results . HTER HTER  Table 6: Main re-tuning results for interactive  data. baseline is the BLEU-tuned system used  in the translation user study. re-tune is the base- line feature set re-tuned to HTER on int-tune. re- tune+feat adds the human feature templates de- scribed in section 3.3. bold indicates statistical  significance relative to the baseline at p < 0.001;  italic at p < 0.05 by the permutation test of
true	P14-2108.pdf#89.5	Penn Treebank, F1	Gold Tags <=40 F  Table 4: Parsing results with Berkeley Parser. The corpus versions used are Release (Rl), Reduced (Rd),  Reduced+NPs (RdNPs), and Reduced+NPs+VPs (RdNPsVPs). Results are shown for the parser forced  to use the gold POS tags from the corpus, and with the parser supplying its own tags. For the latter case,  the tagging accuracy is shown in the last column.
true	P14-2108.pdf#88.4	Penn Treebank, F1	Parser Tags <=40 F  Table 4: Parsing results with Berkeley Parser. The corpus versions used are Release (Rl), Reduced (Rd),  Reduced+NPs (RdNPs), and Reduced+NPs+VPs (RdNPsVPs). Results are shown for the parser forced  to use the gold POS tags from the corpus, and with the parser supplying its own tags. For the latter case,  the tagging accuracy is shown in the last column.
true	P14-2108.pdf#89.5	Penn Treebank, Accuracy	Gold Tags <=40 F  Table 4: Parsing results with Berkeley Parser. The corpus versions used are Release (Rl), Reduced (Rd),  Reduced+NPs (RdNPs), and Reduced+NPs+VPs (RdNPsVPs). Results are shown for the parser forced  to use the gold POS tags from the corpus, and with the parser supplying its own tags. For the latter case,  the tagging accuracy is shown in the last column.
true	P14-2108.pdf#88.4	Penn Treebank, Accuracy	Parser Tags <=40 F  Table 4: Parsing results with Berkeley Parser. The corpus versions used are Release (Rl), Reduced (Rd),  Reduced+NPs (RdNPs), and Reduced+NPs+VPs (RdNPsVPs). Results are shown for the parser forced  to use the gold POS tags from the corpus, and with the parser supplying its own tags. For the latter case,  the tagging accuracy is shown in the last column.
true	D14-1169.pdf#55.52%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Laptop P  Table 3: Comparison to unsupervised baselines. (P is short for purity, E for entropy, and RI for random  index.)
true	D14-1169.pdf#90.62%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Cellphone RI  Table 3: Comparison to unsupervised baselines. (P is short for purity, E for entropy, and RI for random  index.)
true	D14-1169.pdf#67.95%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Cellphone P  Table 3: Comparison to unsupervised baselines. (P is short for purity, E for entropy, and RI for random  index.)
true	D14-1169.pdf#1.266	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Cellphone E  Table 3: Comparison to unsupervised baselines. (P is short for purity, E for entropy, and RI for random  index.)
true	D14-1169.pdf#1.690	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Camera E  Table 3: Comparison to unsupervised baselines. (P is short for purity, E for entropy, and RI for random  index.)
true	D14-1169.pdf#83.57%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	MP3 RI  Table 3: Comparison to unsupervised baselines. (P is short for purity, E for entropy, and RI for random  index.)
true	D14-1169.pdf#56.42%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Camera P  Table 3: Comparison to unsupervised baselines. (P is short for purity, E for entropy, and RI for random  index.)
true	D14-1169.pdf#1.780	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Laptop E  Table 3: Comparison to unsupervised baselines. (P is short for purity, E for entropy, and RI for random  index.)
true	D14-1169.pdf#90.72%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Laptop RI  Table 3: Comparison to unsupervised baselines. (P is short for purity, E for entropy, and RI for random  index.)
true	D14-1169.pdf#88.16%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Camera RI  Table 3: Comparison to unsupervised baselines. (P is short for purity, E for entropy, and RI for random  index.)
true	D14-1169.pdf#1.578	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	MP3 E  Table 3: Comparison to unsupervised baselines. (P is short for purity, E for entropy, and RI for random  index.)
true	D14-1169.pdf#58.06%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	MP3 P  Table 3: Comparison to unsupervised baselines. (P is short for purity, E for entropy, and RI for random  index.)
true	D14-1169.pdf#1.580	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Entropy  Table 4: Comparison to supervised baselines.  MNB-5% means MNB with 5% labeled data.
true	D14-1169.pdf#88.26%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	RI  Table 4: Comparison to supervised baselines.  MNB-5% means MNB with 5% labeled data.
true	D14-1169.pdf#1.643	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Entropy  Table 4: Comparison to supervised baselines.  MNB-5% means MNB with 5% labeled data.
true	D14-1169.pdf#1.656	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Entropy  Table 4: Comparison to supervised baselines.  MNB-5% means MNB with 5% labeled data.
true	D14-1169.pdf#59.49%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	RI  Table 4: Comparison to supervised baselines.  MNB-5% means MNB with 5% labeled data.
true	D14-1169.pdf#57.00%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	RI  Table 4: Comparison to supervised baselines.  MNB-5% means MNB with 5% labeled data.
true	D14-1169.pdf#86.70%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	RI  Table 4: Comparison to supervised baselines.  MNB-5% means MNB with 5% labeled data.
true	D14-1169.pdf#86.89%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	RI  Table 4: Comparison to supervised baselines.  MNB-5% means MNB with 5% labeled data.
true	D14-1169.pdf#59.55%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	RI  Table 4: Comparison to supervised baselines.  MNB-5% means MNB with 5% labeled data.
true	P10-1051.pdf#64.3	CCGBank, Accuracy	all - punc  Table 2: Supertagging accuracy for CCGbank sec- tions 22-24. Accuracies are reported for four  settings-(1) ambiguous word tokens in the test  corpus, (2) ambiguous word tokens, ignoring  punctuation, (3) all word tokens, and (4) all word  tokens except punctuation.
true	P10-1051.pdf#62.3	CCGBank, Accuracy	ambig - punc  Table 2: Supertagging accuracy for CCGbank sec- tions 22-24. Accuracies are reported for four  settings-(1) ambiguous word tokens in the test  corpus, (2) ambiguous word tokens, ignoring  punctuation, (3) all word tokens, and (4) all word  tokens except punctuation.
true	P10-1051.pdf#63.8	CCGBank, Accuracy	all - punc  Table 2: Supertagging accuracy for CCGbank sec- tions 22-24. Accuracies are reported for four  settings-(1) ambiguous word tokens in the test  corpus, (2) ambiguous word tokens, ignoring  punctuation, (3) all word tokens, and (4) all word  tokens except punctuation.
true	P10-1051.pdf#59.6	CCGBank, Accuracy	ambig - punc  Table 2: Supertagging accuracy for CCGbank sec- tions 22-24. Accuracies are reported for four  settings-(1) ambiguous word tokens in the test  corpus, (2) ambiguous word tokens, ignoring  punctuation, (3) all word tokens, and (4) all word  tokens except punctuation.
true	P10-1051.pdf#47.5	CCGBank, Accuracy	TEST 2 ( using lexicon from : ) NPAPER  Table 5: Comparison of supertagging results for  CCG-TUT. Accuracies are for ambiguous word  tokens in the test corpus, ignoring punctuation.
true	P10-1051.pdf#45.8	CCGBank, Accuracy	TEST 1 NPAPER+CIVIL  Table 5: Comparison of supertagging results for  CCG-TUT. Accuracies are for ambiguous word  tokens in the test corpus, ignoring punctuation.
true	P10-1051.pdf#43.9	CCGBank, Accuracy	TEST 2 ( using lexicon from : ) NPAPER+CIVIL  Table 5: Comparison of supertagging results for  CCG-TUT. Accuracies are for ambiguous word  tokens in the test corpus, ignoring punctuation.
true	P10-1051.pdf#40.9	CCGBank, Accuracy	TEST 2 ( using lexicon from : ) CIVIL  Table 5: Comparison of supertagging results for  CCG-TUT. Accuracies are for ambiguous word  tokens in the test corpus, ignoring punctuation.
true	P10-1116.pdf#78.14	Senseval 2, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#70.21	Senseval 2, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#78.99*	Senseval 2, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#78.63*	Senseval 2, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#79.99*	Senseval 2, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#78.14	Senseval 2, F1	Rec .  Table 3: Model II performance on different con- text size. attempted rate (Ate.), precision (Pre.),  recall (Rec.), F-score (F1).
true	P10-1116.pdf#78.14	Senseval 2, F1	F1  Table 3: Model II performance on different con- text size. attempted rate (Ate.), precision (Pre.),  recall (Rec.), F-score (F1).
true	P10-1116.pdf#78.14	Senseval 2, F1	Pre .  Table 3: Model II performance on different con- text size. attempted rate (Ate.), precision (Pre.),  recall (Rec.), F-score (F1).
true	P10-1116.pdf#78.14	SemEval 2013, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#70.21	SemEval 2013, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#78.99*	SemEval 2013, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#78.63*	SemEval 2013, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#79.99*	SemEval 2013, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#78.14	SemEval 2013, F1	Rec .  Table 3: Model II performance on different con- text size. attempted rate (Ate.), precision (Pre.),  recall (Rec.), F-score (F1).
true	P10-1116.pdf#78.14	SemEval 2013, F1	F1  Table 3: Model II performance on different con- text size. attempted rate (Ate.), precision (Pre.),  recall (Rec.), F-score (F1).
true	P10-1116.pdf#78.14	SemEval 2013, F1	Pre .  Table 3: Model II performance on different con- text size. attempted rate (Ate.), precision (Pre.),  recall (Rec.), F-score (F1).
true	P10-1116.pdf#78.14	SemEval 2007, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#70.21	SemEval 2007, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#78.99*	SemEval 2007, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#78.63*	SemEval 2007, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#79.99*	SemEval 2007, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#78.14	SemEval 2007, F1	Rec .  Table 3: Model II performance on different con- text size. attempted rate (Ate.), precision (Pre.),  recall (Rec.), F-score (F1).
true	P10-1116.pdf#78.14	SemEval 2007, F1	F1  Table 3: Model II performance on different con- text size. attempted rate (Ate.), precision (Pre.),  recall (Rec.), F-score (F1).
true	P10-1116.pdf#78.14	SemEval 2007, F1	Pre .  Table 3: Model II performance on different con- text size. attempted rate (Ate.), precision (Pre.),  recall (Rec.), F-score (F1).
true	P10-1116.pdf#78.14	SemEval 2015, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#70.21	SemEval 2015, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#78.99*	SemEval 2015, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#78.63*	SemEval 2015, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#79.99*	SemEval 2015, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#78.14	SemEval 2015, F1	Rec .  Table 3: Model II performance on different con- text size. attempted rate (Ate.), precision (Pre.),  recall (Rec.), F-score (F1).
true	P10-1116.pdf#78.14	SemEval 2015, F1	F1  Table 3: Model II performance on different con- text size. attempted rate (Ate.), precision (Pre.),  recall (Rec.), F-score (F1).
true	P10-1116.pdf#78.14	SemEval 2015, F1	Pre .  Table 3: Model II performance on different con- text size. attempted rate (Ate.), precision (Pre.),  recall (Rec.), F-score (F1).
true	P10-1116.pdf#78.14	Senseval 3, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#70.21	Senseval 3, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#78.99*	Senseval 3, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#78.63*	Senseval 3, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#79.99*	Senseval 3, F1	All  Table 2: Model performance (F-score) on the  coarse-grained dataset (context=sentence). Para- phrases with/without reference synsets (+ref/-ref).
true	P10-1116.pdf#78.14	Senseval 3, F1	Rec .  Table 3: Model II performance on different con- text size. attempted rate (Ate.), precision (Pre.),  recall (Rec.), F-score (F1).
true	P10-1116.pdf#78.14	Senseval 3, F1	F1  Table 3: Model II performance on different con- text size. attempted rate (Ate.), precision (Pre.),  recall (Rec.), F-score (F1).
true	P10-1116.pdf#78.14	Senseval 3, F1	Pre .  Table 3: Model II performance on different con- text size. attempted rate (Ate.), precision (Pre.),  recall (Rec.), F-score (F1).
true	D15-1251.pdf#2	IMDb, Accuracy	Reg .  Table 5: Classification accuracies and the best hyperparameters for each of the datasets in our experiments. "Acc" shows  accuracies for our logistic regression model. "Min" and "Max" correspond to the min n-grams and max n-grams respectively.  "Reg." is the regularization type, "Strength" is the regularization strength, and "Conv." is the convergence tolerance. For  regularization strength, we round it to the nearest integer for readability.
true	D15-1251.pdf#2	IMDb, Accuracy	Reg .  Table 5: Classification accuracies and the best hyperparameters for each of the datasets in our experiments. "Acc" shows  accuracies for our logistic regression model. "Min" and "Max" correspond to the min n-grams and max n-grams respectively.  "Reg." is the regularization type, "Strength" is the regularization strength, and "Conv." is the convergence tolerance. For  regularization strength, we round it to the nearest integer for readability.
true	D15-1251.pdf#5	IMDb, Accuracy	40 Stopword removal ? Stanford sentiment accuracy 70 60  Table 5: Classification accuracies and the best hyperparameters for each of the datasets in our experiments. "Acc" shows  accuracies for our logistic regression model. "Min" and "Max" correspond to the min n-grams and max n-grams respectively.  "Reg." is the regularization type, "Strength" is the regularization strength, and "Conv." is the convergence tolerance. For  regularization strength, we round it to the nearest integer for readability.
true	D15-1251.pdf#1	IMDb, Accuracy	Reg .  Table 5: Classification accuracies and the best hyperparameters for each of the datasets in our experiments. "Acc" shows  accuracies for our logistic regression model. "Min" and "Max" correspond to the min n-grams and max n-grams respectively.  "Reg." is the regularization type, "Strength" is the regularization strength, and "Conv." is the convergence tolerance. For  regularization strength, we round it to the nearest integer for readability.
true	D15-1251.pdf#2	IMDb, Accuracy	Reg .  Table 5: Classification accuracies and the best hyperparameters for each of the datasets in our experiments. "Acc" shows  accuracies for our logistic regression model. "Min" and "Max" correspond to the min n-grams and max n-grams respectively.  "Reg." is the regularization type, "Strength" is the regularization strength, and "Conv." is the convergence tolerance. For  regularization strength, we round it to the nearest integer for readability.
true	D15-1251.pdf#2	IMDb, Accuracy	Reg .  Table 5: Classification accuracies and the best hyperparameters for each of the datasets in our experiments. "Acc" shows  accuracies for our logistic regression model. "Min" and "Max" correspond to the min n-grams and max n-grams respectively.  "Reg." is the regularization type, "Strength" is the regularization strength, and "Conv." is the convergence tolerance. For  regularization strength, we round it to the nearest integer for readability.
true	D15-1251.pdf#2	IMDb, Accuracy	Reg .  Table 5: Classification accuracies and the best hyperparameters for each of the datasets in our experiments. "Acc" shows  accuracies for our logistic regression model. "Min" and "Max" correspond to the min n-grams and max n-grams respectively.  "Reg." is the regularization type, "Strength" is the regularization strength, and "Conv." is the convergence tolerance. For  regularization strength, we round it to the nearest integer for readability.
true	D15-1251.pdf#2	IMDb, Accuracy	Table 5: Classification accuracies and the best hyperparameters for each of the datasets in our experiments. "Acc" shows  accuracies for our logistic regression model. "Min" and "Max" correspond to the min n-grams and max n-grams respectively.  "Reg." is the regularization type, "Strength" is the regularization strength, and "Conv." is the convergence tolerance. For  regularization strength, we round it to the nearest integer for readability.
true	D15-1251.pdf#5	IMDb, Accuracy	40 Strength Congress vote accuracy  Table 5: Classification accuracies and the best hyperparameters for each of the datasets in our experiments. "Acc" shows  accuracies for our logistic regression model. "Min" and "Max" correspond to the min n-grams and max n-grams respectively.  "Reg." is the regularization type, "Strength" is the regularization strength, and "Conv." is the convergence tolerance. For  regularization strength, we round it to the nearest integer for readability.
true	D15-1251.pdf#0	IMDb, Accuracy	40 Strength Congress vote accuracy  Table 5: Classification accuracies and the best hyperparameters for each of the datasets in our experiments. "Acc" shows  accuracies for our logistic regression model. "Min" and "Max" correspond to the min n-grams and max n-grams respectively.  "Reg." is the regularization type, "Strength" is the regularization strength, and "Conv." is the convergence tolerance. For  regularization strength, we round it to the nearest integer for readability.
true	D15-1251.pdf#0	IMDb, Accuracy	40 Stopword removal ? Stanford sentiment accuracy 70 60  Table 5: Classification accuracies and the best hyperparameters for each of the datasets in our experiments. "Acc" shows  accuracies for our logistic regression model. "Min" and "Max" correspond to the min n-grams and max n-grams respectively.  "Reg." is the regularization type, "Strength" is the regularization strength, and "Conv." is the convergence tolerance. For  regularization strength, we round it to the nearest integer for readability.
true	D15-1251.pdf#2	IMDb, Accuracy	Reg .  Table 5: Classification accuracies and the best hyperparameters for each of the datasets in our experiments. "Acc" shows  accuracies for our logistic regression model. "Min" and "Max" correspond to the min n-grams and max n-grams respectively.  "Reg." is the regularization type, "Strength" is the regularization strength, and "Conv." is the convergence tolerance. For  regularization strength, we round it to the nearest integer for readability.
true	P11-1100.pdf#89.38	New York Times Corpus, P@10%	Accidents  Table 4: Test set ranking accuracy. The first row shows  the baseline performance, the next four show our model  with different settings, and the last row is a combined  model. Double (**) and single (*) asterisks indicate that  the respective model significantly outperforms the base- line at p < 0.01 and p < 0.05, respectively. We follow
true	D10-1059.pdf#27.62	WMT 2014 EN-FR, BLEU	RS - 200  Table 1: Test set BLEU Scores for six different "Source- Target" Pairs
true	D10-1059.pdf#32.91	WMT 2014 EN-FR, BLEU	RS - 200  Table 1: Test set BLEU Scores for six different "Source- Target" Pairs
true	D10-1059.pdf#30.19	WMT 2014 EN-FR, BLEU	RS - 200  Table 1: Test set BLEU Scores for six different "Source- Target" Pairs
true	D10-1059.pdf#20.93	WMT 2014 EN-FR, BLEU	RS - 200  Table 1: Test set BLEU Scores for six different "Source- Target" Pairs
true	D10-1059.pdf#31.66	WMT 2014 EN-FR, BLEU	RS - 200  Table 1: Test set BLEU Scores for six different "Source- Target" Pairs
true	D10-1059.pdf#32.76	WMT 2014 EN-FR, BLEU	RS - 200  Table 1: Test set BLEU Scores for six different "Source- Target" Pairs
true	D10-1059.pdf#27.62	WMT 2014 EN-DE, BLEU	RS - 200  Table 1: Test set BLEU Scores for six different "Source- Target" Pairs
true	D10-1059.pdf#32.91	WMT 2014 EN-DE, BLEU	RS - 200  Table 1: Test set BLEU Scores for six different "Source- Target" Pairs
true	D10-1059.pdf#30.19	WMT 2014 EN-DE, BLEU	RS - 200  Table 1: Test set BLEU Scores for six different "Source- Target" Pairs
true	D10-1059.pdf#20.93	WMT 2014 EN-DE, BLEU	RS - 200  Table 1: Test set BLEU Scores for six different "Source- Target" Pairs
true	D10-1059.pdf#31.66	WMT 2014 EN-DE, BLEU	RS - 200  Table 1: Test set BLEU Scores for six different "Source- Target" Pairs
true	D10-1059.pdf#32.76	WMT 2014 EN-DE, BLEU	RS - 200  Table 1: Test set BLEU Scores for six different "Source- Target" Pairs
true	P12-1071.pdf#0.06)	Penn Treebank, Number of params	with QG  Table 2: QG features used to enhance the baseline parsing models. dir(h, m) denotes the direction of the dependency  (h, m), whereas dist(h, m) is the distance |h − m|. ⊕dir(h, m) • dist(h, m) indicates that the features listed in the  corresponding column are also conjoined with dir(h, m) • dist(h, m) to form new features.
true	P12-1071.pdf#0.003)	Penn Treebank, Number of params	with QG  Table 2: QG features used to enhance the baseline parsing models. dir(h, m) denotes the direction of the dependency  (h, m), whereas dist(h, m) is the distance |h − m|. ⊕dir(h, m) • dist(h, m) indicates that the features listed in the  corresponding column are also conjoined with dir(h, m) • dist(h, m) to form new features.
true	P12-1071.pdf#0.06)	Penn Treebank, LAS	with QG  Table 2: QG features used to enhance the baseline parsing models. dir(h, m) denotes the direction of the dependency  (h, m), whereas dist(h, m) is the distance |h − m|. ⊕dir(h, m) • dist(h, m) indicates that the features listed in the  corresponding column are also conjoined with dir(h, m) • dist(h, m) to form new features.
true	P12-1071.pdf#0.003)	Penn Treebank, LAS	with QG  Table 2: QG features used to enhance the baseline parsing models. dir(h, m) denotes the direction of the dependency  (h, m), whereas dist(h, m) is the distance |h − m|. ⊕dir(h, m) • dist(h, m) indicates that the features listed in the  corresponding column are also conjoined with dir(h, m) • dist(h, m) to form new features.
true	P12-1071.pdf#0.06)	benchmark Vietnamese dependency treebank VnDT, UAS	with QG  Table 2: QG features used to enhance the baseline parsing models. dir(h, m) denotes the direction of the dependency  (h, m), whereas dist(h, m) is the distance |h − m|. ⊕dir(h, m) • dist(h, m) indicates that the features listed in the  corresponding column are also conjoined with dir(h, m) • dist(h, m) to form new features.
true	P12-1071.pdf#0.003)	benchmark Vietnamese dependency treebank VnDT, UAS	with QG  Table 2: QG features used to enhance the baseline parsing models. dir(h, m) denotes the direction of the dependency  (h, m), whereas dist(h, m) is the distance |h − m|. ⊕dir(h, m) • dist(h, m) indicates that the features listed in the  corresponding column are also conjoined with dir(h, m) • dist(h, m) to form new features.
true	P12-1071.pdf#0.06)	Chinese Treebank 6, F1	with QG  Table 2: QG features used to enhance the baseline parsing models. dir(h, m) denotes the direction of the dependency  (h, m), whereas dist(h, m) is the distance |h − m|. ⊕dir(h, m) • dist(h, m) indicates that the features listed in the  corresponding column are also conjoined with dir(h, m) • dist(h, m) to form new features.
true	P12-1071.pdf#0.003)	Chinese Treebank 6, F1	with QG  Table 2: QG features used to enhance the baseline parsing models. dir(h, m) denotes the direction of the dependency  (h, m), whereas dist(h, m) is the distance |h − m|. ⊕dir(h, m) • dist(h, m) indicates that the features listed in the  corresponding column are also conjoined with dir(h, m) • dist(h, m) to form new features.
true	P12-1071.pdf#0.06)	Penn Treebank, F1	with QG  Table 2: QG features used to enhance the baseline parsing models. dir(h, m) denotes the direction of the dependency  (h, m), whereas dist(h, m) is the distance |h − m|. ⊕dir(h, m) • dist(h, m) indicates that the features listed in the  corresponding column are also conjoined with dir(h, m) • dist(h, m) to form new features.
true	P12-1071.pdf#0.003)	Penn Treebank, F1	with QG  Table 2: QG features used to enhance the baseline parsing models. dir(h, m) denotes the direction of the dependency  (h, m), whereas dist(h, m) is the distance |h − m|. ⊕dir(h, m) • dist(h, m) indicates that the features listed in the  corresponding column are also conjoined with dir(h, m) • dist(h, m) to form new features.
true	P12-1071.pdf#0.06)	Penn Treebank, POS	with QG  Table 2: QG features used to enhance the baseline parsing models. dir(h, m) denotes the direction of the dependency  (h, m), whereas dist(h, m) is the distance |h − m|. ⊕dir(h, m) • dist(h, m) indicates that the features listed in the  corresponding column are also conjoined with dir(h, m) • dist(h, m) to form new features.
true	P12-1071.pdf#0.003)	Penn Treebank, POS	with QG  Table 2: QG features used to enhance the baseline parsing models. dir(h, m) denotes the direction of the dependency  (h, m), whereas dist(h, m) is the distance |h − m|. ⊕dir(h, m) • dist(h, m) indicates that the features listed in the  corresponding column are also conjoined with dir(h, m) • dist(h, m) to form new features.
true	P12-1071.pdf#0.06)	Penn Treebank, UAS	with QG  Table 2: QG features used to enhance the baseline parsing models. dir(h, m) denotes the direction of the dependency  (h, m), whereas dist(h, m) is the distance |h − m|. ⊕dir(h, m) • dist(h, m) indicates that the features listed in the  corresponding column are also conjoined with dir(h, m) • dist(h, m) to form new features.
true	P12-1071.pdf#0.003)	Penn Treebank, UAS	with QG  Table 2: QG features used to enhance the baseline parsing models. dir(h, m) denotes the direction of the dependency  (h, m), whereas dist(h, m) is the distance |h − m|. ⊕dir(h, m) • dist(h, m) indicates that the features listed in the  corresponding column are also conjoined with dir(h, m) • dist(h, m) to form new features.
true	P10-1125.pdf#62.03	SearchQA, Unigram Acc	effective and meaningful . MRR ( % )  Table 1: Results on Forum Dataset
true	P10-1125.pdf#45.00	SearchQA, Unigram Acc	effective and meaningful . P@1 ( % )  Table 1: Results on Forum Dataset
true	P10-1125.pdf#59.64	SearchQA, Unigram Acc	effective and meaningful . MRR ( % )  Table 1: Results on Forum Dataset
true	P10-1125.pdf#41.45	SearchQA, Unigram Acc	effective and meaningful . P@1 ( % )  Table 1: Results on Forum Dataset
true	P10-1125.pdf#72.74	SearchQA, Unigram Acc	MRR ( % )  Table 2: Results on cQA Dataset
true	P10-1125.pdf#58.15	SearchQA, Unigram Acc	P@1 ( % )  Table 2: Results on cQA Dataset
true	P10-1125.pdf#70.56	SearchQA, Unigram Acc	MRR ( % )  Table 2: Results on cQA Dataset
true	P10-1125.pdf#56.20	SearchQA, Unigram Acc	P@1 ( % )  Table 2: Results on cQA Dataset
true	N12-1076.pdf#±0.0	Text8, Bit per Character (BPC)	∆ Literature  Table 1: GAP scores LST data.
true	N12-1076.pdf#±0.0	Text8, Number of params	∆ Literature  Table 1: GAP scores LST data.
true	P13-1076.pdf#58.36	MSR, F1	POS Tagging OOV - R  Table 4: The performance of segmentation and  POS tagging on testing data.
true	P13-1076.pdf#92.89	MSR, F1	POS Tagging F1  Table 4: The performance of segmentation and  POS tagging on testing data.
true	P13-1076.pdf#68.09	MSR, F1	Segmentation OOV - R  Table 4: The performance of segmentation and  POS tagging on testing data.
true	P13-1076.pdf#96.85	MSR, F1	Segmentation F1  Table 4: The performance of segmentation and  POS tagging on testing data.
true	P13-1076.pdf#58.36	Chinese Treebank 6, F1	POS Tagging OOV - R  Table 4: The performance of segmentation and  POS tagging on testing data.
true	P13-1076.pdf#92.89	Chinese Treebank 6, F1	POS Tagging F1  Table 4: The performance of segmentation and  POS tagging on testing data.
true	P13-1076.pdf#68.09	Chinese Treebank 6, F1	Segmentation OOV - R  Table 4: The performance of segmentation and  POS tagging on testing data.
true	P13-1076.pdf#96.85	Chinese Treebank 6, F1	Segmentation F1  Table 4: The performance of segmentation and  POS tagging on testing data.
true	P13-1076.pdf#58.36	VLSP 2013 POS tagging shared task, Accuracy	POS Tagging OOV - R  Table 4: The performance of segmentation and  POS tagging on testing data.
true	P13-1076.pdf#92.89	VLSP 2013 POS tagging shared task, Accuracy	POS Tagging F1  Table 4: The performance of segmentation and  POS tagging on testing data.
true	P13-1076.pdf#68.09	VLSP 2013 POS tagging shared task, Accuracy	Segmentation OOV - R  Table 4: The performance of segmentation and  POS tagging on testing data.
true	P13-1076.pdf#96.85	VLSP 2013 POS tagging shared task, Accuracy	Segmentation F1  Table 4: The performance of segmentation and  POS tagging on testing data.
true	P11-1158.pdf#90.1	CCGBank, Accuracy	UR  Table 4: Results on CCGbank section 00 when applying adaptive supertagging (AST) to two models of a generative  CCG parser. Performance is measured in terms of parse failures, labelled and unlabelled precision (LP/UP), recall  (LR/UR) and F-score (LF/UF). Evaluation is based only on sentences for which each parser returned an analysis.
true	P11-1158.pdf#83.1	CCGBank, Accuracy	LF  Table 4: Results on CCGbank section 00 when applying adaptive supertagging (AST) to two models of a generative  CCG parser. Performance is measured in terms of parse failures, labelled and unlabelled precision (LP/UP), recall  (LR/UR) and F-score (LF/UF). Evaluation is based only on sentences for which each parser returned an analysis.
true	P11-1158.pdf#83.0	CCGBank, Accuracy	LR  Table 4: Results on CCGbank section 00 when applying adaptive supertagging (AST) to two models of a generative  CCG parser. Performance is measured in terms of parse failures, labelled and unlabelled precision (LP/UP), recall  (LR/UR) and F-score (LF/UF). Evaluation is based only on sentences for which each parser returned an analysis.
true	P11-1158.pdf#83.2	CCGBank, Accuracy	LP  Table 4: Results on CCGbank section 00 when applying adaptive supertagging (AST) to two models of a generative  CCG parser. Performance is measured in terms of parse failures, labelled and unlabelled precision (LP/UP), recall  (LR/UR) and F-score (LF/UF). Evaluation is based only on sentences for which each parser returned an analysis.
true	P11-1158.pdf#90.2	CCGBank, Accuracy	UF  Table 4: Results on CCGbank section 00 when applying adaptive supertagging (AST) to two models of a generative  CCG parser. Performance is measured in terms of parse failures, labelled and unlabelled precision (LP/UP), recall  (LR/UR) and F-score (LF/UF). Evaluation is based only on sentences for which each parser returned an analysis.
true	P11-1158.pdf#79.5	CCGBank, Accuracy	LP  Table 4: Results on CCGbank section 00 when applying adaptive supertagging (AST) to two models of a generative  CCG parser. Performance is measured in terms of parse failures, labelled and unlabelled precision (LP/UP), recall  (LR/UR) and F-score (LF/UF). Evaluation is based only on sentences for which each parser returned an analysis.
true	P11-1158.pdf#14.4	CCGBank, Accuracy	Sent / sec  Table 4: Results on CCGbank section 00 when applying adaptive supertagging (AST) to two models of a generative  CCG parser. Performance is measured in terms of parse failures, labelled and unlabelled precision (LP/UP), recall  (LR/UR) and F-score (LF/UF). Evaluation is based only on sentences for which each parser returned an analysis.
true	P11-1158.pdf#29.5	CCGBank, Accuracy	Sent / sec  Table 4: Results on CCGbank section 00 when applying adaptive supertagging (AST) to two models of a generative  CCG parser. Performance is measured in terms of parse failures, labelled and unlabelled precision (LP/UP), recall  (LR/UR) and F-score (LF/UF). Evaluation is based only on sentences for which each parser returned an analysis.
true	P11-1158.pdf#87.1	CCGBank, Accuracy	UF  Table 4: Results on CCGbank section 00 when applying adaptive supertagging (AST) to two models of a generative  CCG parser. Performance is measured in terms of parse failures, labelled and unlabelled precision (LP/UP), recall  (LR/UR) and F-score (LF/UF). Evaluation is based only on sentences for which each parser returned an analysis.
true	P11-1158.pdf#78.9	CCGBank, Accuracy	LF  Table 4: Results on CCGbank section 00 when applying adaptive supertagging (AST) to two models of a generative  CCG parser. Performance is measured in terms of parse failures, labelled and unlabelled precision (LP/UP), recall  (LR/UR) and F-score (LF/UF). Evaluation is based only on sentences for which each parser returned an analysis.
true	P11-1158.pdf#87.4	CCGBank, Accuracy	UP  Table 4: Results on CCGbank section 00 when applying adaptive supertagging (AST) to two models of a generative  CCG parser. Performance is measured in terms of parse failures, labelled and unlabelled precision (LP/UP), recall  (LR/UR) and F-score (LF/UF). Evaluation is based only on sentences for which each parser returned an analysis.
true	P11-1158.pdf#86.9	CCGBank, Accuracy	UR  Table 4: Results on CCGbank section 00 when applying adaptive supertagging (AST) to two models of a generative  CCG parser. Performance is measured in terms of parse failures, labelled and unlabelled precision (LP/UP), recall  (LR/UR) and F-score (LF/UF). Evaluation is based only on sentences for which each parser returned an analysis.
true	P11-1158.pdf#78.8	CCGBank, Accuracy	LR  Table 4: Results on CCGbank section 00 when applying adaptive supertagging (AST) to two models of a generative  CCG parser. Performance is measured in terms of parse failures, labelled and unlabelled precision (LP/UP), recall  (LR/UR) and F-score (LF/UF). Evaluation is based only on sentences for which each parser returned an analysis.
true	P11-1158.pdf#90.2	CCGBank, Accuracy	UP  Table 4: Results on CCGbank section 00 when applying adaptive supertagging (AST) to two models of a generative  CCG parser. Performance is measured in terms of parse failures, labelled and unlabelled precision (LP/UP), recall  (LR/UR) and F-score (LF/UF). Evaluation is based only on sentences for which each parser returned an analysis.
true	P11-1158.pdf#0.03	CCGBank, Accuracy	β  Table 6: Breakdown of the number of sentences parsed  for the HWDep (AST) model (see
true	P11-1158.pdf#0.001k=150	CCGBank, Accuracy	β  Table 6: Breakdown of the number of sentences parsed  for the HWDep (AST) model (see
true	P11-1158.pdf#0.005	CCGBank, Accuracy	β  Table 6: Breakdown of the number of sentences parsed  for the HWDep (AST) model (see
true	P11-1158.pdf#0.01	CCGBank, Accuracy	β  Table 6: Breakdown of the number of sentences parsed  for the HWDep (AST) model (see
true	P11-1158.pdf#0.075	CCGBank, Accuracy	β  Table 6: Breakdown of the number of sentences parsed  for the HWDep (AST) model (see
true	P11-1158.pdf#10.0	CCGBank, Accuracy	100 Edges popped Std  Table 7: Exhaustive search (EXH), A* with no heuristic (NULL) and with the SX heuristic in terms of millions of edges  pushed, popped and traversals computed using the PCFG grammar with and without adaptive supertagging.
true	P11-1158.pdf#20.9	CCGBank, Accuracy	100 Edges pushed Std  Table 7: Exhaustive search (EXH), A* with no heuristic (NULL) and with the SX heuristic in terms of millions of edges  pushed, popped and traversals computed using the PCFG grammar with and without adaptive supertagging.
true	P11-1158.pdf#73	CCGBank, Accuracy	100 Traversals %  Table 7: Exhaustive search (EXH), A* with no heuristic (NULL) and with the SX heuristic in terms of millions of edges  pushed, popped and traversals computed using the PCFG grammar with and without adaptive supertagging.
true	P11-1158.pdf#62	CCGBank, Accuracy	100 Edges popped %  Table 7: Exhaustive search (EXH), A* with no heuristic (NULL) and with the SX heuristic in terms of millions of edges  pushed, popped and traversals computed using the PCFG grammar with and without adaptive supertagging.
true	P11-1158.pdf#9.7	CCGBank, Accuracy	100 Traversals AST  Table 7: Exhaustive search (EXH), A* with no heuristic (NULL) and with the SX heuristic in terms of millions of edges  pushed, popped and traversals computed using the PCFG grammar with and without adaptive supertagging.
true	P11-1158.pdf#96.5	CCGBank, Accuracy	100 Traversals Std  Table 7: Exhaustive search (EXH), A* with no heuristic (NULL) and with the SX heuristic in terms of millions of edges  pushed, popped and traversals computed using the PCFG grammar with and without adaptive supertagging.
true	P11-1158.pdf#72	CCGBank, Accuracy	100 Traversals %  Table 7: Exhaustive search (EXH), A* with no heuristic (NULL) and with the SX heuristic in terms of millions of edges  pushed, popped and traversals computed using the PCFG grammar with and without adaptive supertagging.
true	P11-1158.pdf#4.3	CCGBank, Accuracy	100 Edges pushed AST  Table 7: Exhaustive search (EXH), A* with no heuristic (NULL) and with the SX heuristic in terms of millions of edges  pushed, popped and traversals computed using the PCFG grammar with and without adaptive supertagging.
true	P11-1158.pdf#68	CCGBank, Accuracy	100 Edges pushed %  Table 7: Exhaustive search (EXH), A* with no heuristic (NULL) and with the SX heuristic in terms of millions of edges  pushed, popped and traversals computed using the PCFG grammar with and without adaptive supertagging.
true	P11-1158.pdf#61	CCGBank, Accuracy	100 Edges pushed %  Table 7: Exhaustive search (EXH), A* with no heuristic (NULL) and with the SX heuristic in terms of millions of edges  pushed, popped and traversals computed using the PCFG grammar with and without adaptive supertagging.
true	P11-1158.pdf#64	CCGBank, Accuracy	100 Edges popped %  Table 7: Exhaustive search (EXH), A* with no heuristic (NULL) and with the SX heuristic in terms of millions of edges  pushed, popped and traversals computed using the PCFG grammar with and without adaptive supertagging.
true	P11-1158.pdf#2.6	CCGBank, Accuracy	100 Edges popped AST  Table 7: Exhaustive search (EXH), A* with no heuristic (NULL) and with the SX heuristic in terms of millions of edges  pushed, popped and traversals computed using the PCFG grammar with and without adaptive supertagging.
true	P12-1043.pdf#0.64	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	product vs . movie F 1  Table 2: Results on sentiment lexicon extraction. Num- bers in boldface denote significant improvement.  product vs. movie  movie vs. product  Prec. Rec. F 1 Prec. Rec.
true	P12-1043.pdf#0.62	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	movie vs . product F 1  Table 2: Results on sentiment lexicon extraction. Num- bers in boldface denote significant improvement.  product vs. movie  movie vs. product  Prec. Rec. F 1 Prec. Rec.
true	P12-1043.pdf#0.65	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F 1  Table 3: Results on topic lexicon extraction. Numbers in  boldface denote significant improvement.
true	P12-1043.pdf#0.66	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F 1  Table 3: Results on topic lexicon extraction. Numbers in  boldface denote significant improvement.
true	P12-1043.pdf#84.05	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Ours  Table 5: Sentiment classification results (accuracy in %).  Numbers in boldface denotes significant improvement.
true	P12-1043.pdf#88.83	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Ours  Table 5: Sentiment classification results (accuracy in %).  Numbers in boldface denotes significant improvement.
true	P12-1043.pdf#81.65	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Ours  Table 5: Sentiment classification results (accuracy in %).  Numbers in boldface denotes significant improvement.
true	P12-1043.pdf#86.71	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Ours  Table 5: Sentiment classification results (accuracy in %).  Numbers in boldface denotes significant improvement.
true	P12-1043.pdf#0.64	SUBJ, Accuracy	product vs . movie F 1  Table 2: Results on sentiment lexicon extraction. Num- bers in boldface denote significant improvement.  product vs. movie  movie vs. product  Prec. Rec. F 1 Prec. Rec.
true	P12-1043.pdf#0.62	SUBJ, Accuracy	movie vs . product F 1  Table 2: Results on sentiment lexicon extraction. Num- bers in boldface denote significant improvement.  product vs. movie  movie vs. product  Prec. Rec. F 1 Prec. Rec.
true	P12-1043.pdf#0.65	SUBJ, Accuracy	F 1  Table 3: Results on topic lexicon extraction. Numbers in  boldface denote significant improvement.
true	P12-1043.pdf#0.66	SUBJ, Accuracy	F 1  Table 3: Results on topic lexicon extraction. Numbers in  boldface denote significant improvement.
true	P12-1043.pdf#84.05	SUBJ, Accuracy	Ours  Table 5: Sentiment classification results (accuracy in %).  Numbers in boldface denotes significant improvement.
true	P12-1043.pdf#88.83	SUBJ, Accuracy	Ours  Table 5: Sentiment classification results (accuracy in %).  Numbers in boldface denotes significant improvement.
true	P12-1043.pdf#81.65	SUBJ, Accuracy	Ours  Table 5: Sentiment classification results (accuracy in %).  Numbers in boldface denotes significant improvement.
true	P12-1043.pdf#86.71	SUBJ, Accuracy	Ours  Table 5: Sentiment classification results (accuracy in %).  Numbers in boldface denotes significant improvement.
true	P14-1058.pdf#75.85†	SUBJ, Accuracy	Proposed  Table 2: POS tagging accuracies on SANCL.
true	P14-1058.pdf#67.09	SUBJ, Accuracy	Proposed  Table 2: POS tagging accuracies on SANCL.
true	P14-1058.pdf#76.97†	SUBJ, Accuracy	Proposed  Table 2: POS tagging accuracies on SANCL.
true	P14-1058.pdf#69.28†	SUBJ, Accuracy	Proposed  Table 2: POS tagging accuracies on SANCL.
true	P14-1058.pdf#56.93†	SUBJ, Accuracy	Proposed  Table 2: POS tagging accuracies on SANCL.
true	P15-1130.pdf#87.2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Accuracy ( % )  Table 1: Accuracies of different classifiers.
true	P15-1130.pdf#87.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Accuracy ( % )  Table 1: Accuracies of different classifiers.
true	P15-1130.pdf#88.3	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Accuracy ( % ) hashtag  Table 2: Comparison with different feature engi- neering methods.
true	P15-1130.pdf#87.2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Accuracy ( % ) hashtag  Table 2: Comparison with different feature engi- neering methods.
true	P15-1130.pdf#84.0	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Accuracy ( % )  Table 3: Accuracies of different methods on Se- mEval 2013
true	P15-1130.pdf#64.85	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	% )  Table 4: Accuracy on generated negation phrases  test set.
true	P10-4014.pdf#72.9%	SemEval 2015, F1	SensEval - 3  Table 2: WSD accuracies on SensEval lexical- sample tasks
true	P10-4014.pdf#65.3%	SemEval 2015, F1	SensEval - 2  Table 2: WSD accuracies on SensEval lexical- sample tasks
true	P10-4014.pdf#72.9%	SemEval 2013, F1	SensEval - 3  Table 2: WSD accuracies on SensEval lexical- sample tasks
true	P10-4014.pdf#65.3%	SemEval 2013, F1	SensEval - 2  Table 2: WSD accuracies on SensEval lexical- sample tasks
true	P10-4014.pdf#72.9%	Senseval 3, F1	SensEval - 3  Table 2: WSD accuracies on SensEval lexical- sample tasks
true	P10-4014.pdf#65.3%	Senseval 3, F1	SensEval - 2  Table 2: WSD accuracies on SensEval lexical- sample tasks
true	P10-4014.pdf#72.9%	SemEval 2007, F1	SensEval - 3  Table 2: WSD accuracies on SensEval lexical- sample tasks
true	P10-4014.pdf#65.3%	SemEval 2007, F1	SensEval - 2  Table 2: WSD accuracies on SensEval lexical- sample tasks
true	P10-4014.pdf#72.9%	Senseval 2, F1	SensEval - 3  Table 2: WSD accuracies on SensEval lexical- sample tasks
true	P10-4014.pdf#65.3%	Senseval 2, F1	SensEval - 2  Table 2: WSD accuracies on SensEval lexical- sample tasks
true	D14-1213.pdf#64.5	New York Times Corpus, P@10%	1  Table 6: SD level classification accuracies and F- measures using annotated data. Acc is accuracy,  and G F 1 is F-measure for classifying the G level.  Avg F 1 is the macroaveraged value of G F 1 , M F 1  and H F 1 . SDTM outperforms all other methods  compared. The difference between SDTM and  FirstP is statistically significant (p-value < 0.05  for accuracy, < 0.0001 for Avg F 1 ).
true	D14-1213.pdf#0.43	New York Times Corpus, P@10%	1  Table 6: SD level classification accuracies and F- measures using annotated data. Acc is accuracy,  and G F 1 is F-measure for classifying the G level.  Avg F 1 is the macroaveraged value of G F 1 , M F 1  and H F 1 . SDTM outperforms all other methods  compared. The difference between SDTM and  FirstP is statistically significant (p-value < 0.05  for accuracy, < 0.0001 for Avg F 1 ).
true	D14-1213.pdf#0.64	New York Times Corpus, P@10%	1  Table 6: SD level classification accuracies and F- measures using annotated data. Acc is accuracy,  and G F 1 is F-measure for classifying the G level.  Avg F 1 is the macroaveraged value of G F 1 , M F 1  and H F 1 . SDTM outperforms all other methods  compared. The difference between SDTM and  FirstP is statistically significant (p-value < 0.05  for accuracy, < 0.0001 for Avg F 1 ).
true	D14-1213.pdf#0.58	New York Times Corpus, P@10%	1  Table 6: SD level classification accuracies and F- measures using annotated data. Acc is accuracy,  and G F 1 is F-measure for classifying the G level.  Avg F 1 is the macroaveraged value of G F 1 , M F 1  and H F 1 . SDTM outperforms all other methods  compared. The difference between SDTM and  FirstP is statistically significant (p-value < 0.05  for accuracy, < 0.0001 for Avg F 1 ).
true	D14-1213.pdf#0.71	New York Times Corpus, P@10%	1  Table 6: SD level classification accuracies and F- measures using annotated data. Acc is accuracy,  and G F 1 is F-measure for classifying the G level.  Avg F 1 is the macroaveraged value of G F 1 , M F 1  and H F 1 . SDTM outperforms all other methods  compared. The difference between SDTM and  FirstP is statistically significant (p-value < 0.05  for accuracy, < 0.0001 for Avg F 1 ).
true	D14-1213.pdf#36	New York Times Corpus, P@10%	Table 7: High ranked topics in each level by comparing KL-divergence with other level's topics
true	D14-1213.pdf#82	New York Times Corpus, P@10%	Table 7: High ranked topics in each level by comparing KL-divergence with other level's topics
true	D14-1213.pdf#33	New York Times Corpus, P@10%	Table 7: High ranked topics in each level by comparing KL-divergence with other level's topics
true	D14-1213.pdf#19	New York Times Corpus, P@10%	Table 7: High ranked topics in each level by comparing KL-divergence with other level's topics
true	P14-1128.pdf#63.72	Chinese Treebank 6, F1	NIST  Table 1: Translation performances (%) on MT-05  testing data by using ten different CWS models.
true	P14-1128.pdf#57.64	Chinese Treebank 6, F1	METEOR  Table 1: Translation performances (%) on MT-05  testing data by using ten different CWS models.
true	P14-1128.pdf#32.75	Chinese Treebank 6, F1	BLEU  Table 1: Translation performances (%) on MT-05  testing data by using ten different CWS models.
true	P15-2081.pdf#107	Penn Treebank, Number of params	put context window and projection layer . Table 1 : Perplexities on PTB for various LMs . Test PPL Perplexities on LTCB for various lan - Test PPL  Table 1: Perplexities on PTB for various LMs.  Model  Test PPL  KN 5-gram (Mikolov, 2011)  141  FNNLM (Mikolov, 2012)  140  RNNLM (Mikolov, 2011)  123  LSTM (Graves, 2013)  117  bigram FNNLM  176  trigram FNNLM  131  4-gram FNNLM  118  5-gram FNNLM  114  6-gram FNNLM  113  1st-order FOFE-FNNLM  116  2nd-order FOFE-FNNLM  108
true	P15-2081.pdf#108	Penn Treebank, Number of params	Table 1 : Perplexities on PTB for various LMs . Test PPL  Table 1: Perplexities on PTB for various LMs.  Model  Test PPL  KN 5-gram (Mikolov, 2011)  141  FNNLM (Mikolov, 2012)  140  RNNLM (Mikolov, 2011)  123  LSTM (Graves, 2013)  117  bigram FNNLM  176  trigram FNNLM  131  4-gram FNNLM  118  5-gram FNNLM  114  6-gram FNNLM  113  1st-order FOFE-FNNLM  116  2nd-order FOFE-FNNLM  108
true	P15-2081.pdf#107	Penn Treebank, Number of params	put context window and projection layer . Table 1 : Perplexities on PTB for various LMs . Test PPL Perplexities on LTCB for various lan - Test PPL  Table 2: Perplexities on LTCB for various lan- guage models. [M*N] denotes the sizes of the in- put context window and projection layer.  Model  Architecture  Test PPL  KN 3-gram  - 156  KN 5-gram  - 132  [1*200]-400-400-80k  241  [2*200]-400-400-80k  155  FNN-LM [2*200]-600-600-80k  150  [3*200]-400-400-80k  131  [4*200]-400-400-80k  125  RNN-LM  [1*600]-80k  112  [1*200]-400-400-80k  120  FOFE  [1*200]-600-600-80k  115  FNN-LM [2*200]-400-400-80k  112  [2*200]-600-600-80k  107
true	P15-2081.pdf#108	Penn Treebank, Number of params	Table 1 : Perplexities on PTB for various LMs . Test PPL  Table 2: Perplexities on LTCB for various lan- guage models. [M*N] denotes the sizes of the in- put context window and projection layer.  Model  Architecture  Test PPL  KN 3-gram  - 156  KN 5-gram  - 132  [1*200]-400-400-80k  241  [2*200]-400-400-80k  155  FNN-LM [2*200]-600-600-80k  150  [3*200]-400-400-80k  131  [4*200]-400-400-80k  125  RNN-LM  [1*600]-80k  112  [1*200]-400-400-80k  120  FOFE  [1*200]-600-600-80k  115  FNN-LM [2*200]-400-400-80k  112  [2*200]-600-600-80k  107
true	P15-2081.pdf#107	Penn Treebank, Test perplexity	put context window and projection layer . Table 1 : Perplexities on PTB for various LMs . Test PPL Perplexities on LTCB for various lan - Test PPL  Table 1: Perplexities on PTB for various LMs.  Model  Test PPL  KN 5-gram (Mikolov, 2011)  141  FNNLM (Mikolov, 2012)  140  RNNLM (Mikolov, 2011)  123  LSTM (Graves, 2013)  117  bigram FNNLM  176  trigram FNNLM  131  4-gram FNNLM  118  5-gram FNNLM  114  6-gram FNNLM  113  1st-order FOFE-FNNLM  116  2nd-order FOFE-FNNLM  108
true	P15-2081.pdf#108	Penn Treebank, Test perplexity	Table 1 : Perplexities on PTB for various LMs . Test PPL  Table 1: Perplexities on PTB for various LMs.  Model  Test PPL  KN 5-gram (Mikolov, 2011)  141  FNNLM (Mikolov, 2012)  140  RNNLM (Mikolov, 2011)  123  LSTM (Graves, 2013)  117  bigram FNNLM  176  trigram FNNLM  131  4-gram FNNLM  118  5-gram FNNLM  114  6-gram FNNLM  113  1st-order FOFE-FNNLM  116  2nd-order FOFE-FNNLM  108
true	P15-2081.pdf#107	Penn Treebank, Test perplexity	put context window and projection layer . Table 1 : Perplexities on PTB for various LMs . Test PPL Perplexities on LTCB for various lan - Test PPL  Table 2: Perplexities on LTCB for various lan- guage models. [M*N] denotes the sizes of the in- put context window and projection layer.  Model  Architecture  Test PPL  KN 3-gram  - 156  KN 5-gram  - 132  [1*200]-400-400-80k  241  [2*200]-400-400-80k  155  FNN-LM [2*200]-600-600-80k  150  [3*200]-400-400-80k  131  [4*200]-400-400-80k  125  RNN-LM  [1*600]-80k  112  [1*200]-400-400-80k  120  FOFE  [1*200]-600-600-80k  115  FNN-LM [2*200]-400-400-80k  112  [2*200]-600-600-80k  107
true	P15-2081.pdf#108	Penn Treebank, Test perplexity	Table 1 : Perplexities on PTB for various LMs . Test PPL  Table 2: Perplexities on LTCB for various lan- guage models. [M*N] denotes the sizes of the in- put context window and projection layer.  Model  Architecture  Test PPL  KN 3-gram  - 156  KN 5-gram  - 132  [1*200]-400-400-80k  241  [2*200]-400-400-80k  155  FNN-LM [2*200]-600-600-80k  150  [3*200]-400-400-80k  131  [4*200]-400-400-80k  125  RNN-LM  [1*600]-80k  112  [1*200]-400-400-80k  120  FOFE  [1*200]-600-600-80k  115  FNN-LM [2*200]-400-400-80k  112  [2*200]-600-600-80k  107
true	D12-1090.pdf#55.6	WMT 2014 EN-FR, BLEU	pFSM vs . pPDA pFSM j1  Table 1: pFSM vs. pPDA results for the round-robin  approach on OpenMT08 data set over three languages  (A=Arabic, C=Chinese, U=Urdu). Numbers in this table  are Spearman's ρ for correlation between human assess- ment scores and model predictions; tr stands for training  set, and te stands for test set. nx means the model has  x-gram block edits. jy means the model has jump distance  limit y. The Best result for each test set row is highlighted  in bold.
true	D12-1090.pdf#64.0	WMT 2014 EN-FR, BLEU	pFSM vs . pPDA pPDA j10  Table 1: pFSM vs. pPDA results for the round-robin  approach on OpenMT08 data set over three languages  (A=Arabic, C=Chinese, U=Urdu). Numbers in this table  are Spearman's ρ for correlation between human assess- ment scores and model predictions; tr stands for training  set, and te stands for test set. nx means the model has  x-gram block edits. jy means the model has jump distance  limit y. The Best result for each test set row is highlighted  in bold.
true	D12-1090.pdf#61.2	WMT 2014 EN-FR, BLEU	pFSM vs . pPDA pFSM n1  Table 1: pFSM vs. pPDA results for the round-robin  approach on OpenMT08 data set over three languages  (A=Arabic, C=Chinese, U=Urdu). Numbers in this table  are Spearman's ρ for correlation between human assess- ment scores and model predictions; tr stands for training  set, and te stands for test set. nx means the model has  x-gram block edits. jy means the model has jump distance  limit y. The Best result for each test set row is highlighted  in bold.
true	D12-1090.pdf#61.2	WMT 2014 EN-FR, BLEU	pFSM vs . pPDA pFSM n2  Table 1: pFSM vs. pPDA results for the round-robin  approach on OpenMT08 data set over three languages  (A=Arabic, C=Chinese, U=Urdu). Numbers in this table  are Spearman's ρ for correlation between human assess- ment scores and model predictions; tr stands for training  set, and te stands for test set. nx means the model has  x-gram block edits. jy means the model has jump distance  limit y. The Best result for each test set row is highlighted  in bold.
true	D12-1090.pdf#61.2	WMT 2014 EN-FR, BLEU	Our Metrics pFSM  Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR)  refers to the regression model trained for each baseline metric, same as Pado et al. (2009). The first three rows are  round-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu). The last row are  results trained on entire OpenMT08 (A+C+U) and tested on OpenMT06. Numbers in this table are Spearman's rank  correlation ρ between human assessment scores and model predictions. The pPDA column describes our pPDA model  with jump distance limit 5. METR is shorthand for METEORR. +f means the model includes synonyms, paraphrase  and parsing features (cf. Section 3). Best results and scores that are not statistically significantly worse are highlighted  in bold in each row.
true	D12-1090.pdf#65.2	WMT 2014 EN-FR, BLEU	Combined Metrics MT+RTER  Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR)  refers to the regression model trained for each baseline metric, same as Pado et al. (2009). The first three rows are  round-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu). The last row are  results trained on entire OpenMT08 (A+C+U) and tested on OpenMT06. Numbers in this table are Spearman's rank  correlation ρ between human assessment scores and model predictions. The pPDA column describes our pPDA model  with jump distance limit 5. METR is shorthand for METEORR. +f means the model includes synonyms, paraphrase  and parsing features (cf. Section 3). Best results and scores that are not statistically significantly worse are highlighted  in bold in each row.
true	D12-1090.pdf#57.2	WMT 2014 EN-FR, BLEU	Our Metrics pPDA+f  Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR)  refers to the regression model trained for each baseline metric, same as Pado et al. (2009). The first three rows are  round-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu). The last row are  results trained on entire OpenMT08 (A+C+U) and tested on OpenMT06. Numbers in this table are Spearman's rank  correlation ρ between human assessment scores and model predictions. The pPDA column describes our pPDA model  with jump distance limit 5. METR is shorthand for METEORR. +f means the model includes synonyms, paraphrase  and parsing features (cf. Section 3). Best results and scores that are not statistically significantly worse are highlighted  in bold in each row.
true	D12-1090.pdf#61.1	WMT 2014 EN-FR, BLEU	Combined Metrics MT+RTER  Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR)  refers to the regression model trained for each baseline metric, same as Pado et al. (2009). The first three rows are  round-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu). The last row are  results trained on entire OpenMT08 (A+C+U) and tested on OpenMT06. Numbers in this table are Spearman's rank  correlation ρ between human assessment scores and model predictions. The pPDA column describes our pPDA model  with jump distance limit 5. METR is shorthand for METEORR. +f means the model includes synonyms, paraphrase  and parsing features (cf. Section 3). Best results and scores that are not statistically significantly worse are highlighted  in bold in each row.
true	D12-1090.pdf#65.2	WMT 2014 EN-FR, BLEU	Our Metrics pFSM  Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR)  refers to the regression model trained for each baseline metric, same as Pado et al. (2009). The first three rows are  round-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu). The last row are  results trained on entire OpenMT08 (A+C+U) and tested on OpenMT06. Numbers in this table are Spearman's rank  correlation ρ between human assessment scores and model predictions. The pPDA column describes our pPDA model  with jump distance limit 5. METR is shorthand for METEORR. +f means the model includes synonyms, paraphrase  and parsing features (cf. Section 3). Best results and scores that are not statistically significantly worse are highlighted  in bold in each row.
true	D12-1090.pdf#65.8	WMT 2014 EN-FR, BLEU	Our Metrics pPDA+f  Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR)  refers to the regression model trained for each baseline metric, same as Pado et al. (2009). The first three rows are  round-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu). The last row are  results trained on entire OpenMT08 (A+C+U) and tested on OpenMT06. Numbers in this table are Spearman's rank  correlation ρ between human assessment scores and model predictions. The pPDA column describes our pPDA model  with jump distance limit 5. METR is shorthand for METEORR. +f means the model includes synonyms, paraphrase  and parsing features (cf. Section 3). Best results and scores that are not statistically significantly worse are highlighted  in bold in each row.
true	D12-1090.pdf#57.2	WMT 2014 EN-FR, BLEU	Urdu  Table 3: Results for OpenMT08 with linguistic features,  using the same round robin scheme as in Table 1. Numbers  in this table are Spearman's rank correlation ρ between  human assessment scores and model predictions. Best  results on each test set are highlighted in bold.
true	D12-1090.pdf#65.8	WMT 2014 EN-FR, BLEU	Chinese  Table 3: Results for OpenMT08 with linguistic features,  using the same round robin scheme as in Table 1. Numbers  in this table are Spearman's rank correlation ρ between  human assessment scores and model predictions. Best  results on each test set are highlighted in bold.
true	D12-1090.pdf#60.7	WMT 2014 EN-FR, BLEU	Arabic  Table 3: Results for OpenMT08 with linguistic features,  using the same round robin scheme as in Table 1. Numbers  in this table are Spearman's rank correlation ρ between  human assessment scores and model predictions. Best  results on each test set are highlighted in bold.
true	D12-1090.pdf#52.4	WMT 2014 EN-FR, BLEU	. 8% . 11 WMT07  Table 4: Pairwise preference prediction results on WMT08  test set. Each column shows a different training data set.  Numbers in this table are model's consistency with human  pairwise preference judgments. Best result on each test  set is highlighted in bold.
true	D12-1090.pdf#52.5	WMT 2014 EN-FR, BLEU	. 8% . 11 WMT06+07  Table 4: Pairwise preference prediction results on WMT08  test set. Each column shows a different training data set.  Numbers in this table are model's consistency with human  pairwise preference judgments. Best result on each test  set is highlighted in bold.
true	D12-1090.pdf#52.3	WMT 2014 EN-FR, BLEU	. 8% . 11 WMT06  Table 4: Pairwise preference prediction results on WMT08  test set. Each column shows a different training data set.  Numbers in this table are model's consistency with human  pairwise preference judgments. Best result on each test  set is highlighted in bold.
true	D12-1090.pdf#55.6	Text8, Number of params	pFSM vs . pPDA pFSM j1  Table 1: pFSM vs. pPDA results for the round-robin  approach on OpenMT08 data set over three languages  (A=Arabic, C=Chinese, U=Urdu). Numbers in this table  are Spearman's ρ for correlation between human assess- ment scores and model predictions; tr stands for training  set, and te stands for test set. nx means the model has  x-gram block edits. jy means the model has jump distance  limit y. The Best result for each test set row is highlighted  in bold.
true	D12-1090.pdf#64.0	Text8, Number of params	pFSM vs . pPDA pPDA j10  Table 1: pFSM vs. pPDA results for the round-robin  approach on OpenMT08 data set over three languages  (A=Arabic, C=Chinese, U=Urdu). Numbers in this table  are Spearman's ρ for correlation between human assess- ment scores and model predictions; tr stands for training  set, and te stands for test set. nx means the model has  x-gram block edits. jy means the model has jump distance  limit y. The Best result for each test set row is highlighted  in bold.
true	D12-1090.pdf#61.2	Text8, Number of params	pFSM vs . pPDA pFSM n1  Table 1: pFSM vs. pPDA results for the round-robin  approach on OpenMT08 data set over three languages  (A=Arabic, C=Chinese, U=Urdu). Numbers in this table  are Spearman's ρ for correlation between human assess- ment scores and model predictions; tr stands for training  set, and te stands for test set. nx means the model has  x-gram block edits. jy means the model has jump distance  limit y. The Best result for each test set row is highlighted  in bold.
true	D12-1090.pdf#61.2	Text8, Number of params	pFSM vs . pPDA pFSM n2  Table 1: pFSM vs. pPDA results for the round-robin  approach on OpenMT08 data set over three languages  (A=Arabic, C=Chinese, U=Urdu). Numbers in this table  are Spearman's ρ for correlation between human assess- ment scores and model predictions; tr stands for training  set, and te stands for test set. nx means the model has  x-gram block edits. jy means the model has jump distance  limit y. The Best result for each test set row is highlighted  in bold.
true	D12-1090.pdf#61.2	Text8, Number of params	Our Metrics pFSM  Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR)  refers to the regression model trained for each baseline metric, same as Pado et al. (2009). The first three rows are  round-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu). The last row are  results trained on entire OpenMT08 (A+C+U) and tested on OpenMT06. Numbers in this table are Spearman's rank  correlation ρ between human assessment scores and model predictions. The pPDA column describes our pPDA model  with jump distance limit 5. METR is shorthand for METEORR. +f means the model includes synonyms, paraphrase  and parsing features (cf. Section 3). Best results and scores that are not statistically significantly worse are highlighted  in bold in each row.
true	D12-1090.pdf#65.2	Text8, Number of params	Combined Metrics MT+RTER  Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR)  refers to the regression model trained for each baseline metric, same as Pado et al. (2009). The first three rows are  round-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu). The last row are  results trained on entire OpenMT08 (A+C+U) and tested on OpenMT06. Numbers in this table are Spearman's rank  correlation ρ between human assessment scores and model predictions. The pPDA column describes our pPDA model  with jump distance limit 5. METR is shorthand for METEORR. +f means the model includes synonyms, paraphrase  and parsing features (cf. Section 3). Best results and scores that are not statistically significantly worse are highlighted  in bold in each row.
true	D12-1090.pdf#57.2	Text8, Number of params	Our Metrics pPDA+f  Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR)  refers to the regression model trained for each baseline metric, same as Pado et al. (2009). The first three rows are  round-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu). The last row are  results trained on entire OpenMT08 (A+C+U) and tested on OpenMT06. Numbers in this table are Spearman's rank  correlation ρ between human assessment scores and model predictions. The pPDA column describes our pPDA model  with jump distance limit 5. METR is shorthand for METEORR. +f means the model includes synonyms, paraphrase  and parsing features (cf. Section 3). Best results and scores that are not statistically significantly worse are highlighted  in bold in each row.
true	D12-1090.pdf#61.1	Text8, Number of params	Combined Metrics MT+RTER  Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR)  refers to the regression model trained for each baseline metric, same as Pado et al. (2009). The first three rows are  round-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu). The last row are  results trained on entire OpenMT08 (A+C+U) and tested on OpenMT06. Numbers in this table are Spearman's rank  correlation ρ between human assessment scores and model predictions. The pPDA column describes our pPDA model  with jump distance limit 5. METR is shorthand for METEORR. +f means the model includes synonyms, paraphrase  and parsing features (cf. Section 3). Best results and scores that are not statistically significantly worse are highlighted  in bold in each row.
true	D12-1090.pdf#65.2	Text8, Number of params	Our Metrics pFSM  Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR)  refers to the regression model trained for each baseline metric, same as Pado et al. (2009). The first three rows are  round-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu). The last row are  results trained on entire OpenMT08 (A+C+U) and tested on OpenMT06. Numbers in this table are Spearman's rank  correlation ρ between human assessment scores and model predictions. The pPDA column describes our pPDA model  with jump distance limit 5. METR is shorthand for METEORR. +f means the model includes synonyms, paraphrase  and parsing features (cf. Section 3). Best results and scores that are not statistically significantly worse are highlighted  in bold in each row.
true	D12-1090.pdf#65.8	Text8, Number of params	Our Metrics pPDA+f  Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR)  refers to the regression model trained for each baseline metric, same as Pado et al. (2009). The first three rows are  round-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu). The last row are  results trained on entire OpenMT08 (A+C+U) and tested on OpenMT06. Numbers in this table are Spearman's rank  correlation ρ between human assessment scores and model predictions. The pPDA column describes our pPDA model  with jump distance limit 5. METR is shorthand for METEORR. +f means the model includes synonyms, paraphrase  and parsing features (cf. Section 3). Best results and scores that are not statistically significantly worse are highlighted  in bold in each row.
true	D12-1090.pdf#57.2	Text8, Number of params	Urdu  Table 3: Results for OpenMT08 with linguistic features,  using the same round robin scheme as in Table 1. Numbers  in this table are Spearman's rank correlation ρ between  human assessment scores and model predictions. Best  results on each test set are highlighted in bold.
true	D12-1090.pdf#65.8	Text8, Number of params	Chinese  Table 3: Results for OpenMT08 with linguistic features,  using the same round robin scheme as in Table 1. Numbers  in this table are Spearman's rank correlation ρ between  human assessment scores and model predictions. Best  results on each test set are highlighted in bold.
true	D12-1090.pdf#60.7	Text8, Number of params	Arabic  Table 3: Results for OpenMT08 with linguistic features,  using the same round robin scheme as in Table 1. Numbers  in this table are Spearman's rank correlation ρ between  human assessment scores and model predictions. Best  results on each test set are highlighted in bold.
true	D12-1090.pdf#52.4	Text8, Number of params	. 8% . 11 WMT07  Table 4: Pairwise preference prediction results on WMT08  test set. Each column shows a different training data set.  Numbers in this table are model's consistency with human  pairwise preference judgments. Best result on each test  set is highlighted in bold.
true	D12-1090.pdf#52.5	Text8, Number of params	. 8% . 11 WMT06+07  Table 4: Pairwise preference prediction results on WMT08  test set. Each column shows a different training data set.  Numbers in this table are model's consistency with human  pairwise preference judgments. Best result on each test  set is highlighted in bold.
true	D12-1090.pdf#52.3	Text8, Number of params	. 8% . 11 WMT06  Table 4: Pairwise preference prediction results on WMT08  test set. Each column shows a different training data set.  Numbers in this table are model's consistency with human  pairwise preference judgments. Best result on each test  set is highlighted in bold.
true	D12-1090.pdf#55.6	WMT 2014 EN-DE, BLEU	pFSM vs . pPDA pFSM j1  Table 1: pFSM vs. pPDA results for the round-robin  approach on OpenMT08 data set over three languages  (A=Arabic, C=Chinese, U=Urdu). Numbers in this table  are Spearman's ρ for correlation between human assess- ment scores and model predictions; tr stands for training  set, and te stands for test set. nx means the model has  x-gram block edits. jy means the model has jump distance  limit y. The Best result for each test set row is highlighted  in bold.
true	D12-1090.pdf#64.0	WMT 2014 EN-DE, BLEU	pFSM vs . pPDA pPDA j10  Table 1: pFSM vs. pPDA results for the round-robin  approach on OpenMT08 data set over three languages  (A=Arabic, C=Chinese, U=Urdu). Numbers in this table  are Spearman's ρ for correlation between human assess- ment scores and model predictions; tr stands for training  set, and te stands for test set. nx means the model has  x-gram block edits. jy means the model has jump distance  limit y. The Best result for each test set row is highlighted  in bold.
true	D12-1090.pdf#61.2	WMT 2014 EN-DE, BLEU	pFSM vs . pPDA pFSM n1  Table 1: pFSM vs. pPDA results for the round-robin  approach on OpenMT08 data set over three languages  (A=Arabic, C=Chinese, U=Urdu). Numbers in this table  are Spearman's ρ for correlation between human assess- ment scores and model predictions; tr stands for training  set, and te stands for test set. nx means the model has  x-gram block edits. jy means the model has jump distance  limit y. The Best result for each test set row is highlighted  in bold.
true	D12-1090.pdf#61.2	WMT 2014 EN-DE, BLEU	pFSM vs . pPDA pFSM n2  Table 1: pFSM vs. pPDA results for the round-robin  approach on OpenMT08 data set over three languages  (A=Arabic, C=Chinese, U=Urdu). Numbers in this table  are Spearman's ρ for correlation between human assess- ment scores and model predictions; tr stands for training  set, and te stands for test set. nx means the model has  x-gram block edits. jy means the model has jump distance  limit y. The Best result for each test set row is highlighted  in bold.
true	D12-1090.pdf#61.2	WMT 2014 EN-DE, BLEU	Our Metrics pFSM  Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR)  refers to the regression model trained for each baseline metric, same as Pado et al. (2009). The first three rows are  round-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu). The last row are  results trained on entire OpenMT08 (A+C+U) and tested on OpenMT06. Numbers in this table are Spearman's rank  correlation ρ between human assessment scores and model predictions. The pPDA column describes our pPDA model  with jump distance limit 5. METR is shorthand for METEORR. +f means the model includes synonyms, paraphrase  and parsing features (cf. Section 3). Best results and scores that are not statistically significantly worse are highlighted  in bold in each row.
true	D12-1090.pdf#65.2	WMT 2014 EN-DE, BLEU	Combined Metrics MT+RTER  Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR)  refers to the regression model trained for each baseline metric, same as Pado et al. (2009). The first three rows are  round-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu). The last row are  results trained on entire OpenMT08 (A+C+U) and tested on OpenMT06. Numbers in this table are Spearman's rank  correlation ρ between human assessment scores and model predictions. The pPDA column describes our pPDA model  with jump distance limit 5. METR is shorthand for METEORR. +f means the model includes synonyms, paraphrase  and parsing features (cf. Section 3). Best results and scores that are not statistically significantly worse are highlighted  in bold in each row.
true	D12-1090.pdf#57.2	WMT 2014 EN-DE, BLEU	Our Metrics pPDA+f  Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR)  refers to the regression model trained for each baseline metric, same as Pado et al. (2009). The first three rows are  round-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu). The last row are  results trained on entire OpenMT08 (A+C+U) and tested on OpenMT06. Numbers in this table are Spearman's rank  correlation ρ between human assessment scores and model predictions. The pPDA column describes our pPDA model  with jump distance limit 5. METR is shorthand for METEORR. +f means the model includes synonyms, paraphrase  and parsing features (cf. Section 3). Best results and scores that are not statistically significantly worse are highlighted  in bold in each row.
true	D12-1090.pdf#61.1	WMT 2014 EN-DE, BLEU	Combined Metrics MT+RTER  Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR)  refers to the regression model trained for each baseline metric, same as Pado et al. (2009). The first three rows are  round-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu). The last row are  results trained on entire OpenMT08 (A+C+U) and tested on OpenMT06. Numbers in this table are Spearman's rank  correlation ρ between human assessment scores and model predictions. The pPDA column describes our pPDA model  with jump distance limit 5. METR is shorthand for METEORR. +f means the model includes synonyms, paraphrase  and parsing features (cf. Section 3). Best results and scores that are not statistically significantly worse are highlighted  in bold in each row.
true	D12-1090.pdf#65.2	WMT 2014 EN-DE, BLEU	Our Metrics pFSM  Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR)  refers to the regression model trained for each baseline metric, same as Pado et al. (2009). The first three rows are  round-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu). The last row are  results trained on entire OpenMT08 (A+C+U) and tested on OpenMT06. Numbers in this table are Spearman's rank  correlation ρ between human assessment scores and model predictions. The pPDA column describes our pPDA model  with jump distance limit 5. METR is shorthand for METEORR. +f means the model includes synonyms, paraphrase  and parsing features (cf. Section 3). Best results and scores that are not statistically significantly worse are highlighted  in bold in each row.
true	D12-1090.pdf#65.8	WMT 2014 EN-DE, BLEU	Our Metrics pPDA+f  Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR)  refers to the regression model trained for each baseline metric, same as Pado et al. (2009). The first three rows are  round-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu). The last row are  results trained on entire OpenMT08 (A+C+U) and tested on OpenMT06. Numbers in this table are Spearman's rank  correlation ρ between human assessment scores and model predictions. The pPDA column describes our pPDA model  with jump distance limit 5. METR is shorthand for METEORR. +f means the model includes synonyms, paraphrase  and parsing features (cf. Section 3). Best results and scores that are not statistically significantly worse are highlighted  in bold in each row.
true	D12-1090.pdf#57.2	WMT 2014 EN-DE, BLEU	Urdu  Table 3: Results for OpenMT08 with linguistic features,  using the same round robin scheme as in Table 1. Numbers  in this table are Spearman's rank correlation ρ between  human assessment scores and model predictions. Best  results on each test set are highlighted in bold.
true	D12-1090.pdf#65.8	WMT 2014 EN-DE, BLEU	Chinese  Table 3: Results for OpenMT08 with linguistic features,  using the same round robin scheme as in Table 1. Numbers  in this table are Spearman's rank correlation ρ between  human assessment scores and model predictions. Best  results on each test set are highlighted in bold.
true	D12-1090.pdf#60.7	WMT 2014 EN-DE, BLEU	Arabic  Table 3: Results for OpenMT08 with linguistic features,  using the same round robin scheme as in Table 1. Numbers  in this table are Spearman's rank correlation ρ between  human assessment scores and model predictions. Best  results on each test set are highlighted in bold.
true	D12-1090.pdf#52.4	WMT 2014 EN-DE, BLEU	. 8% . 11 WMT07  Table 4: Pairwise preference prediction results on WMT08  test set. Each column shows a different training data set.  Numbers in this table are model's consistency with human  pairwise preference judgments. Best result on each test  set is highlighted in bold.
true	D12-1090.pdf#52.5	WMT 2014 EN-DE, BLEU	. 8% . 11 WMT06+07  Table 4: Pairwise preference prediction results on WMT08  test set. Each column shows a different training data set.  Numbers in this table are model's consistency with human  pairwise preference judgments. Best result on each test  set is highlighted in bold.
true	D12-1090.pdf#52.3	WMT 2014 EN-DE, BLEU	. 8% . 11 WMT06  Table 4: Pairwise preference prediction results on WMT08  test set. Each column shows a different training data set.  Numbers in this table are model's consistency with human  pairwise preference judgments. Best result on each test  set is highlighted in bold.
true	P15-1051.pdf#0.6494	CNN / Daily Mail (Non-anonymized version), ROUGE-L	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	CNN / Daily Mail (Non-anonymized version), ROUGE-L	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	CNN / Daily Mail (Non-anonymized version), ROUGE-L	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	CNN / Daily Mail (Non-anonymized version), ROUGE-L	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	CNN / Daily Mail (Non-anonymized version), ROUGE-L	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	CNN / Daily Mail (Non-anonymized version), ROUGE-L	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	CNN / Daily Mail (Non-anonymized version), ROUGE-L	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	CNN / Daily Mail (Non-anonymized version), ROUGE-L	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	CNN / Daily Mail (Non-anonymized version), ROUGE-L	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	CNN / Daily Mail (Non-anonymized version), ROUGE-L	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	CNN / Daily Mail (Non-anonymized version), ROUGE-L	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	CNN / Daily Mail (Non-anonymized version), ROUGE-L	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	CNN / Daily Mail (Non-anonymized version), ROUGE-L	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	WikiText-2, Test perplexity	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	WikiText-2, Test perplexity	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	WikiText-2, Test perplexity	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	WikiText-2, Test perplexity	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	WikiText-2, Test perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	WikiText-2, Test perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	WikiText-2, Test perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	WikiText-2, Test perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	WikiText-2, Test perplexity	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	WikiText-2, Test perplexity	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	WikiText-2, Test perplexity	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	WikiText-2, Test perplexity	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	WikiText-2, Test perplexity	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	WikiText-2, Test perplexity	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	WikiText-2, Test perplexity	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	WikiText-2, Test perplexity	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	WikiText-2, Test perplexity	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	CNN / Daily Mail (Non-anonymized version), METEOR	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	CNN / Daily Mail (Non-anonymized version), METEOR	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	CNN / Daily Mail (Non-anonymized version), METEOR	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	CNN / Daily Mail (Non-anonymized version), METEOR	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	CNN / Daily Mail (Non-anonymized version), METEOR	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	CNN / Daily Mail (Non-anonymized version), METEOR	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	CNN / Daily Mail (Non-anonymized version), METEOR	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	CNN / Daily Mail (Non-anonymized version), METEOR	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	CNN / Daily Mail (Non-anonymized version), METEOR	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	CNN / Daily Mail (Non-anonymized version), METEOR	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	CNN / Daily Mail (Non-anonymized version), METEOR	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	CNN / Daily Mail (Non-anonymized version), METEOR	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	CNN / Daily Mail (Non-anonymized version), METEOR	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	CNN / Daily Mail (Non-anonymized version), METEOR	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	CNN / Daily Mail (Non-anonymized version), METEOR	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	CNN / Daily Mail (Non-anonymized version), METEOR	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	CNN / Daily Mail (Non-anonymized version), METEOR	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Penn Treebank, F1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Penn Treebank, F1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Penn Treebank, F1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Penn Treebank, F1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Penn Treebank, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Penn Treebank, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Penn Treebank, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Penn Treebank, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Penn Treebank, F1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Penn Treebank, F1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Penn Treebank, F1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Penn Treebank, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Penn Treebank, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Penn Treebank, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Penn Treebank, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Penn Treebank, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Penn Treebank, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Text8, Bit per Character (BPC)	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Text8, Bit per Character (BPC)	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Text8, Bit per Character (BPC)	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Text8, Bit per Character (BPC)	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Text8, Bit per Character (BPC)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Text8, Bit per Character (BPC)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Text8, Bit per Character (BPC)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Text8, Bit per Character (BPC)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Text8, Bit per Character (BPC)	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Text8, Bit per Character (BPC)	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Text8, Bit per Character (BPC)	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Text8, Bit per Character (BPC)	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Text8, Bit per Character (BPC)	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Text8, Bit per Character (BPC)	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Text8, Bit per Character (BPC)	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Text8, Bit per Character (BPC)	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Text8, Bit per Character (BPC)	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	WMT 2014 EN-DE, BLEU	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	WMT 2014 EN-DE, BLEU	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	WMT 2014 EN-DE, BLEU	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	WMT 2014 EN-DE, BLEU	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	WMT 2014 EN-DE, BLEU	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	WMT 2014 EN-DE, BLEU	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	WMT 2014 EN-DE, BLEU	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	WMT 2014 EN-DE, BLEU	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	WMT 2014 EN-DE, BLEU	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	WMT 2014 EN-DE, BLEU	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	WMT 2014 EN-DE, BLEU	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	WMT 2014 EN-DE, BLEU	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	WMT 2014 EN-DE, BLEU	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	WMT 2014 EN-DE, BLEU	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	WMT 2014 EN-DE, BLEU	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	WMT 2014 EN-DE, BLEU	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	WMT 2014 EN-DE, BLEU	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Penn Treebank, LAS	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Penn Treebank, LAS	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Penn Treebank, LAS	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Penn Treebank, LAS	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Penn Treebank, LAS	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Penn Treebank, LAS	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Penn Treebank, LAS	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Penn Treebank, LAS	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Penn Treebank, LAS	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Penn Treebank, LAS	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Penn Treebank, LAS	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Penn Treebank, LAS	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Penn Treebank, LAS	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Penn Treebank, LAS	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Penn Treebank, LAS	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Penn Treebank, LAS	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Penn Treebank, LAS	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	SST-2, Accuracy	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SST-2, Accuracy	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	SST-2, Accuracy	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	SST-2, Accuracy	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SST-2, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	SST-2, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	SST-2, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	SST-2, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	SST-2, Accuracy	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	SST-2, Accuracy	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SST-2, Accuracy	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SST-2, Accuracy	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	SST-2, Accuracy	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	SST-2, Accuracy	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	SST-2, Accuracy	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	SST-2, Accuracy	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	SST-2, Accuracy	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	SQuAD, F1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SQuAD, F1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	SQuAD, F1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	SQuAD, F1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SQuAD, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	SQuAD, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	SQuAD, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	SQuAD, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	SQuAD, F1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	SQuAD, F1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SQuAD, F1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SQuAD, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	SQuAD, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	SQuAD, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	SQuAD, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	SQuAD, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	SQuAD, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	AG News, Error	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	AG News, Error	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	AG News, Error	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	AG News, Error	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	AG News, Error	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	AG News, Error	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	AG News, Error	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	AG News, Error	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	AG News, Error	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	AG News, Error	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	AG News, Error	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	AG News, Error	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	AG News, Error	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	AG News, Error	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	AG News, Error	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	AG News, Error	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	AG News, Error	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Senseval 3, F1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Senseval 3, F1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Senseval 3, F1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Senseval 3, F1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Senseval 3, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Senseval 3, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Senseval 3, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Senseval 3, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Senseval 3, F1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Senseval 3, F1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Senseval 3, F1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Senseval 3, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Senseval 3, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Senseval 3, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Senseval 3, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Senseval 3, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Senseval 3, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Penn Treebank, Test perplexity	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Penn Treebank, Test perplexity	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Penn Treebank, Test perplexity	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Penn Treebank, Test perplexity	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Penn Treebank, Test perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Penn Treebank, Test perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Penn Treebank, Test perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Penn Treebank, Test perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Penn Treebank, Test perplexity	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Penn Treebank, Test perplexity	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Penn Treebank, Test perplexity	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Penn Treebank, Test perplexity	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Penn Treebank, Test perplexity	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Penn Treebank, Test perplexity	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Penn Treebank, Test perplexity	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Penn Treebank, Test perplexity	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Penn Treebank, Test perplexity	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	DUC 2004 Task 1, ROUGE-L	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	DUC 2004 Task 1, ROUGE-L	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	DUC 2004 Task 1, ROUGE-L	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	DUC 2004 Task 1, ROUGE-L	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	DUC 2004 Task 1, ROUGE-L	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	DUC 2004 Task 1, ROUGE-L	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	DUC 2004 Task 1, ROUGE-L	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	DUC 2004 Task 1, ROUGE-L	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	DUC 2004 Task 1, ROUGE-L	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	DUC 2004 Task 1, ROUGE-L	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	DUC 2004 Task 1, ROUGE-L	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	DUC 2004 Task 1, ROUGE-L	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	DUC 2004 Task 1, ROUGE-L	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	DUC 2004 Task 1, ROUGE-L	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	DUC 2004 Task 1, ROUGE-L	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	DUC 2004 Task 1, ROUGE-L	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	DUC 2004 Task 1, ROUGE-L	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	SemEval 2018, P@5	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SemEval 2018, P@5	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	SemEval 2018, P@5	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	SemEval 2018, P@5	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SemEval 2018, P@5	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	SemEval 2018, P@5	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	SemEval 2018, P@5	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	SemEval 2018, P@5	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	SemEval 2018, P@5	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	SemEval 2018, P@5	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SemEval 2018, P@5	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SemEval 2018, P@5	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	SemEval 2018, P@5	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	SemEval 2018, P@5	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	SemEval 2018, P@5	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	SemEval 2018, P@5	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	SemEval 2018, P@5	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Penn Treebank, Number of params	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Penn Treebank, Number of params	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Penn Treebank, Number of params	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Penn Treebank, Number of params	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Penn Treebank, Number of params	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Penn Treebank, Number of params	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Penn Treebank, Number of params	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Penn Treebank, Number of params	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Penn Treebank, Number of params	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Penn Treebank, Number of params	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Penn Treebank, Number of params	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Penn Treebank, Number of params	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Penn Treebank, Number of params	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Penn Treebank, Number of params	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Penn Treebank, Number of params	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Penn Treebank, Number of params	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Penn Treebank, Number of params	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Penn Treebank, Bit per Character (BPC)	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Penn Treebank, Bit per Character (BPC)	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Penn Treebank, Bit per Character (BPC)	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Penn Treebank, Bit per Character (BPC)	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Penn Treebank, Bit per Character (BPC)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Penn Treebank, Bit per Character (BPC)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Penn Treebank, Bit per Character (BPC)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Penn Treebank, Bit per Character (BPC)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Penn Treebank, Bit per Character (BPC)	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Penn Treebank, Bit per Character (BPC)	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Penn Treebank, Bit per Character (BPC)	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Penn Treebank, Bit per Character (BPC)	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Penn Treebank, Bit per Character (BPC)	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Penn Treebank, Bit per Character (BPC)	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Penn Treebank, Bit per Character (BPC)	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Penn Treebank, Bit per Character (BPC)	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Penn Treebank, Bit per Character (BPC)	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Quasar, F1 (Quasar-T)	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Quasar, F1 (Quasar-T)	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Quasar, F1 (Quasar-T)	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Quasar, F1 (Quasar-T)	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Quasar, F1 (Quasar-T)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Quasar, F1 (Quasar-T)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Quasar, F1 (Quasar-T)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Quasar, F1 (Quasar-T)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Quasar, F1 (Quasar-T)	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Quasar, F1 (Quasar-T)	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Quasar, F1 (Quasar-T)	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Quasar, F1 (Quasar-T)	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Quasar, F1 (Quasar-T)	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Quasar, F1 (Quasar-T)	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Quasar, F1 (Quasar-T)	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Quasar, F1 (Quasar-T)	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Quasar, F1 (Quasar-T)	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	CNN / Daily Mail (Anonymized version), ROUGE-2	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	CNN / Daily Mail (Anonymized version), ROUGE-2	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	CNN / Daily Mail (Anonymized version), ROUGE-2	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	CNN / Daily Mail (Anonymized version), ROUGE-2	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	CNN / Daily Mail (Anonymized version), ROUGE-2	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	CNN / Daily Mail (Anonymized version), ROUGE-2	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	CNN / Daily Mail (Anonymized version), ROUGE-2	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	CNN / Daily Mail (Anonymized version), ROUGE-2	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	CNN / Daily Mail (Anonymized version), ROUGE-2	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	CNN / Daily Mail (Anonymized version), ROUGE-2	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	CNN / Daily Mail (Anonymized version), ROUGE-2	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	CNN / Daily Mail (Anonymized version), ROUGE-2	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	CNN / Daily Mail (Anonymized version), ROUGE-2	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	CNN / Daily Mail (Anonymized version), ROUGE-2	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	CNN / Daily Mail (Anonymized version), ROUGE-2	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	CNN / Daily Mail (Anonymized version), ROUGE-2	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	CNN / Daily Mail (Anonymized version), ROUGE-2	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	SemEval 2015, F1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SemEval 2015, F1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	SemEval 2015, F1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	SemEval 2015, F1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SemEval 2015, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	SemEval 2015, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	SemEval 2015, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	SemEval 2015, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	SemEval 2015, F1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	SemEval 2015, F1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SemEval 2015, F1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SemEval 2015, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	SemEval 2015, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	SemEval 2015, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	SemEval 2015, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	SemEval 2015, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	SemEval 2015, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	CNN / Daily Mail (Non-anonymized version), ROUGE-2	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	CNN / Daily Mail (Non-anonymized version), ROUGE-2	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	CNN / Daily Mail (Non-anonymized version), ROUGE-2	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	CNN / Daily Mail (Non-anonymized version), ROUGE-2	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	CNN / Daily Mail (Non-anonymized version), ROUGE-2	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	CNN / Daily Mail (Non-anonymized version), ROUGE-2	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	CNN / Daily Mail (Non-anonymized version), ROUGE-2	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	CNN / Daily Mail (Non-anonymized version), ROUGE-2	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	CNN / Daily Mail (Non-anonymized version), ROUGE-2	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	CNN / Daily Mail (Non-anonymized version), ROUGE-2	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	CNN / Daily Mail (Non-anonymized version), ROUGE-2	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	CNN / Daily Mail (Non-anonymized version), ROUGE-2	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	CNN / Daily Mail (Non-anonymized version), ROUGE-2	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	MSR, F1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	MSR, F1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	MSR, F1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	MSR, F1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	MSR, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	MSR, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	MSR, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	MSR, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	MSR, F1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	MSR, F1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	MSR, F1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	MSR, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	MSR, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	MSR, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	MSR, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	MSR, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	MSR, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Penn Treebank, F1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Penn Treebank, F1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Penn Treebank, F1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Penn Treebank, F1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Penn Treebank, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Penn Treebank, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Penn Treebank, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Penn Treebank, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Penn Treebank, F1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Penn Treebank, F1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Penn Treebank, F1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Penn Treebank, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Penn Treebank, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Penn Treebank, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Penn Treebank, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Penn Treebank, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Penn Treebank, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	CoNLL 2003 (English), F1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	CoNLL 2003 (English), F1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	CoNLL 2003 (English), F1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	CoNLL 2003 (English), F1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	CoNLL 2003 (English), F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	CoNLL 2003 (English), F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	CoNLL 2003 (English), F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	CoNLL 2003 (English), F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	CoNLL 2003 (English), F1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	CoNLL 2003 (English), F1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	CoNLL 2003 (English), F1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	CoNLL 2003 (English), F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	CoNLL 2003 (English), F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	CoNLL 2003 (English), F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	CoNLL 2003 (English), F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	CoNLL 2003 (English), F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	CoNLL 2003 (English), F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Chinese Treebank 6, F1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Chinese Treebank 6, F1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Chinese Treebank 6, F1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Chinese Treebank 6, F1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Chinese Treebank 6, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Chinese Treebank 6, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Chinese Treebank 6, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Chinese Treebank 6, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Chinese Treebank 6, F1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Chinese Treebank 6, F1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Chinese Treebank 6, F1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Chinese Treebank 6, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Chinese Treebank 6, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Chinese Treebank 6, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Chinese Treebank 6, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Chinese Treebank 6, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Chinese Treebank 6, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	1B Words / Google Billion Word benchmark, Test perplexity	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	1B Words / Google Billion Word benchmark, Test perplexity	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	1B Words / Google Billion Word benchmark, Test perplexity	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	1B Words / Google Billion Word benchmark, Test perplexity	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	1B Words / Google Billion Word benchmark, Test perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	1B Words / Google Billion Word benchmark, Test perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	1B Words / Google Billion Word benchmark, Test perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	1B Words / Google Billion Word benchmark, Test perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	1B Words / Google Billion Word benchmark, Test perplexity	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	1B Words / Google Billion Word benchmark, Test perplexity	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	1B Words / Google Billion Word benchmark, Test perplexity	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	1B Words / Google Billion Word benchmark, Test perplexity	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	1B Words / Google Billion Word benchmark, Test perplexity	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	1B Words / Google Billion Word benchmark, Test perplexity	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	1B Words / Google Billion Word benchmark, Test perplexity	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	1B Words / Google Billion Word benchmark, Test perplexity	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	1B Words / Google Billion Word benchmark, Test perplexity	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	VLSP 2016 NER shared task, F1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	VLSP 2016 NER shared task, F1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	VLSP 2016 NER shared task, F1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	VLSP 2016 NER shared task, F1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	VLSP 2016 NER shared task, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	VLSP 2016 NER shared task, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	VLSP 2016 NER shared task, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	VLSP 2016 NER shared task, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	VLSP 2016 NER shared task, F1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	VLSP 2016 NER shared task, F1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	VLSP 2016 NER shared task, F1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	VLSP 2016 NER shared task, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	VLSP 2016 NER shared task, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	VLSP 2016 NER shared task, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	VLSP 2016 NER shared task, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	VLSP 2016 NER shared task, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	VLSP 2016 NER shared task, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	WN18RR, H@1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	WN18RR, H@1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	WN18RR, H@1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	WN18RR, H@1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	WN18RR, H@1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	WN18RR, H@1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	WN18RR, H@1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	WN18RR, H@1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	WN18RR, H@1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	WN18RR, H@1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	WN18RR, H@1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	WN18RR, H@1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	WN18RR, H@1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	WN18RR, H@1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	WN18RR, H@1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	WN18RR, H@1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	WN18RR, H@1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	CNN / Daily Mail (Anonymized version), ROUGE-1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	CNN / Daily Mail (Anonymized version), ROUGE-1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	CNN / Daily Mail (Anonymized version), ROUGE-1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	CNN / Daily Mail (Anonymized version), ROUGE-1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	CNN / Daily Mail (Anonymized version), ROUGE-1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	CNN / Daily Mail (Anonymized version), ROUGE-1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	CNN / Daily Mail (Anonymized version), ROUGE-1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	CNN / Daily Mail (Anonymized version), ROUGE-1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	CNN / Daily Mail (Anonymized version), ROUGE-1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	CNN / Daily Mail (Anonymized version), ROUGE-1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	CNN / Daily Mail (Anonymized version), ROUGE-1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	CNN / Daily Mail (Anonymized version), ROUGE-1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	CNN / Daily Mail (Anonymized version), ROUGE-1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	CNN / Daily Mail (Anonymized version), ROUGE-1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	CNN / Daily Mail (Anonymized version), ROUGE-1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	CNN / Daily Mail (Anonymized version), ROUGE-1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	CNN / Daily Mail (Anonymized version), ROUGE-1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	SemEval 2007, F1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SemEval 2007, F1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	SemEval 2007, F1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	SemEval 2007, F1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SemEval 2007, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	SemEval 2007, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	SemEval 2007, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	SemEval 2007, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	SemEval 2007, F1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	SemEval 2007, F1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SemEval 2007, F1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SemEval 2007, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	SemEval 2007, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	SemEval 2007, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	SemEval 2007, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	SemEval 2007, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	SemEval 2007, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	SemEval 2018, MAP	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SemEval 2018, MAP	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	SemEval 2018, MAP	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	SemEval 2018, MAP	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SemEval 2018, MAP	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	SemEval 2018, MAP	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	SemEval 2018, MAP	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	SemEval 2018, MAP	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	SemEval 2018, MAP	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	SemEval 2018, MAP	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SemEval 2018, MAP	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SemEval 2018, MAP	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	SemEval 2018, MAP	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	SemEval 2018, MAP	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	SemEval 2018, MAP	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	SemEval 2018, MAP	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	SemEval 2018, MAP	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	FB15K-237, H@10	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	FB15K-237, H@10	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	FB15K-237, H@10	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	FB15K-237, H@10	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	FB15K-237, H@10	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	FB15K-237, H@10	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	FB15K-237, H@10	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	FB15K-237, H@10	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	FB15K-237, H@10	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	FB15K-237, H@10	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	FB15K-237, H@10	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	FB15K-237, H@10	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	FB15K-237, H@10	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	FB15K-237, H@10	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	FB15K-237, H@10	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	FB15K-237, H@10	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	FB15K-237, H@10	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	FB15K-237, H@1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	FB15K-237, H@1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	FB15K-237, H@1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	FB15K-237, H@1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	FB15K-237, H@1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	FB15K-237, H@1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	FB15K-237, H@1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	FB15K-237, H@1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	FB15K-237, H@1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	FB15K-237, H@1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	FB15K-237, H@1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	FB15K-237, H@1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	FB15K-237, H@1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	FB15K-237, H@1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	FB15K-237, H@1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	FB15K-237, H@1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	FB15K-237, H@1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Penn Treebank, Accuracy	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Penn Treebank, Accuracy	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Penn Treebank, Accuracy	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Penn Treebank, Accuracy	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Penn Treebank, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Penn Treebank, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Penn Treebank, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Penn Treebank, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Penn Treebank, Accuracy	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Penn Treebank, Accuracy	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Penn Treebank, Accuracy	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Penn Treebank, Accuracy	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Penn Treebank, Accuracy	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Penn Treebank, Accuracy	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Penn Treebank, Accuracy	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Penn Treebank, Accuracy	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Penn Treebank, Accuracy	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	SearchQA, N-gram F1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SearchQA, N-gram F1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	SearchQA, N-gram F1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	SearchQA, N-gram F1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SearchQA, N-gram F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	SearchQA, N-gram F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	SearchQA, N-gram F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	SearchQA, N-gram F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	SearchQA, N-gram F1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	SearchQA, N-gram F1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SearchQA, N-gram F1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SearchQA, N-gram F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	SearchQA, N-gram F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	SearchQA, N-gram F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	SearchQA, N-gram F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	SearchQA, N-gram F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	SearchQA, N-gram F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	SearchQA, Unigram Acc	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SearchQA, Unigram Acc	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	SearchQA, Unigram Acc	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	SearchQA, Unigram Acc	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SearchQA, Unigram Acc	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	SearchQA, Unigram Acc	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	SearchQA, Unigram Acc	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	SearchQA, Unigram Acc	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	SearchQA, Unigram Acc	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	SearchQA, Unigram Acc	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SearchQA, Unigram Acc	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SearchQA, Unigram Acc	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	SearchQA, Unigram Acc	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	SearchQA, Unigram Acc	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	SearchQA, Unigram Acc	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	SearchQA, Unigram Acc	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	SearchQA, Unigram Acc	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	FB15K-237, MRR	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	FB15K-237, MRR	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	FB15K-237, MRR	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	FB15K-237, MRR	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	FB15K-237, MRR	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	FB15K-237, MRR	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	FB15K-237, MRR	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	FB15K-237, MRR	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	FB15K-237, MRR	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	FB15K-237, MRR	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	FB15K-237, MRR	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	FB15K-237, MRR	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	FB15K-237, MRR	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	FB15K-237, MRR	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	FB15K-237, MRR	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	FB15K-237, MRR	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	FB15K-237, MRR	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Text8, Number of params	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Text8, Number of params	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Text8, Number of params	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Text8, Number of params	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Text8, Number of params	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Text8, Number of params	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Text8, Number of params	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Text8, Number of params	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Text8, Number of params	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Text8, Number of params	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Text8, Number of params	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Text8, Number of params	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Text8, Number of params	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Text8, Number of params	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Text8, Number of params	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Text8, Number of params	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Text8, Number of params	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	WikiText-2, Validation perplexity	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	WikiText-2, Validation perplexity	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	WikiText-2, Validation perplexity	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	WikiText-2, Validation perplexity	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	WikiText-2, Validation perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	WikiText-2, Validation perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	WikiText-2, Validation perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	WikiText-2, Validation perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	WikiText-2, Validation perplexity	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	WikiText-2, Validation perplexity	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	WikiText-2, Validation perplexity	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	WikiText-2, Validation perplexity	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	WikiText-2, Validation perplexity	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	WikiText-2, Validation perplexity	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	WikiText-2, Validation perplexity	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	WikiText-2, Validation perplexity	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	WikiText-2, Validation perplexity	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	SemEval-2010 Task 8, F1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SemEval-2010 Task 8, F1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	SemEval-2010 Task 8, F1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	SemEval-2010 Task 8, F1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SemEval-2010 Task 8, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	SemEval-2010 Task 8, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	SemEval-2010 Task 8, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	SemEval-2010 Task 8, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	SemEval-2010 Task 8, F1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	SemEval-2010 Task 8, F1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SemEval-2010 Task 8, F1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SemEval-2010 Task 8, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	SemEval-2010 Task 8, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	SemEval-2010 Task 8, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	SemEval-2010 Task 8, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	SemEval-2010 Task 8, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	SemEval-2010 Task 8, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	CCGBank, Accuracy	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	CCGBank, Accuracy	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	CCGBank, Accuracy	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	CCGBank, Accuracy	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	CCGBank, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	CCGBank, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	CCGBank, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	CCGBank, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	CCGBank, Accuracy	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	CCGBank, Accuracy	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	CCGBank, Accuracy	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	CCGBank, Accuracy	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	CCGBank, Accuracy	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	CCGBank, Accuracy	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	CCGBank, Accuracy	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	CCGBank, Accuracy	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	CCGBank, Accuracy	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	WikiText-2, Number of params	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	WikiText-2, Number of params	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	WikiText-2, Number of params	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	WikiText-2, Number of params	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	WikiText-2, Number of params	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	WikiText-2, Number of params	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	WikiText-2, Number of params	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	WikiText-2, Number of params	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	WikiText-2, Number of params	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	WikiText-2, Number of params	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	WikiText-2, Number of params	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	WikiText-2, Number of params	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	WikiText-2, Number of params	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	WikiText-2, Number of params	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	WikiText-2, Number of params	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	WikiText-2, Number of params	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	WikiText-2, Number of params	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	TREC, Error	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	TREC, Error	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	TREC, Error	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	TREC, Error	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	TREC, Error	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	TREC, Error	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	TREC, Error	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	TREC, Error	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	TREC, Error	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	TREC, Error	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	TREC, Error	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	TREC, Error	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	TREC, Error	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	TREC, Error	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	TREC, Error	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	TREC, Error	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	TREC, Error	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	WikiText-103, Test perplexity	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	WikiText-103, Test perplexity	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	WikiText-103, Test perplexity	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	WikiText-103, Test perplexity	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	WikiText-103, Test perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	WikiText-103, Test perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	WikiText-103, Test perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	WikiText-103, Test perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	WikiText-103, Test perplexity	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	WikiText-103, Test perplexity	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	WikiText-103, Test perplexity	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	WikiText-103, Test perplexity	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	WikiText-103, Test perplexity	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	WikiText-103, Test perplexity	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	WikiText-103, Test perplexity	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	WikiText-103, Test perplexity	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	WikiText-103, Test perplexity	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	SUBJ, Accuracy	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SUBJ, Accuracy	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	SUBJ, Accuracy	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	SUBJ, Accuracy	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SUBJ, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	SUBJ, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	SUBJ, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	SUBJ, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	SUBJ, Accuracy	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	SUBJ, Accuracy	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SUBJ, Accuracy	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SUBJ, Accuracy	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	SUBJ, Accuracy	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	SUBJ, Accuracy	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	SUBJ, Accuracy	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	SUBJ, Accuracy	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	SUBJ, Accuracy	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	WN18RR, H@10	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	WN18RR, H@10	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	WN18RR, H@10	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	WN18RR, H@10	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	WN18RR, H@10	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	WN18RR, H@10	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	WN18RR, H@10	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	WN18RR, H@10	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	WN18RR, H@10	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	WN18RR, H@10	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	WN18RR, H@10	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	WN18RR, H@10	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	WN18RR, H@10	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	WN18RR, H@10	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	WN18RR, H@10	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	WN18RR, H@10	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	WN18RR, H@10	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	DUC 2004 Task 1, ROUGE-2	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	DUC 2004 Task 1, ROUGE-2	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	DUC 2004 Task 1, ROUGE-2	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	DUC 2004 Task 1, ROUGE-2	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	DUC 2004 Task 1, ROUGE-2	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	DUC 2004 Task 1, ROUGE-2	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	DUC 2004 Task 1, ROUGE-2	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	DUC 2004 Task 1, ROUGE-2	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	DUC 2004 Task 1, ROUGE-2	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	DUC 2004 Task 1, ROUGE-2	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	DUC 2004 Task 1, ROUGE-2	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	DUC 2004 Task 1, ROUGE-2	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	DUC 2004 Task 1, ROUGE-2	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	DUC 2004 Task 1, ROUGE-2	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	DUC 2004 Task 1, ROUGE-2	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	DUC 2004 Task 1, ROUGE-2	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	DUC 2004 Task 1, ROUGE-2	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Gigaword, ROUGE-1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Gigaword, ROUGE-1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Gigaword, ROUGE-1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Gigaword, ROUGE-1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Gigaword, ROUGE-1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Gigaword, ROUGE-1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Gigaword, ROUGE-1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Gigaword, ROUGE-1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Gigaword, ROUGE-1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Gigaword, ROUGE-1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Gigaword, ROUGE-1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Gigaword, ROUGE-1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Gigaword, ROUGE-1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Gigaword, ROUGE-1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Gigaword, ROUGE-1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Gigaword, ROUGE-1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Gigaword, ROUGE-1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	SemEval 2013, F1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SemEval 2013, F1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	SemEval 2013, F1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	SemEval 2013, F1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SemEval 2013, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	SemEval 2013, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	SemEval 2013, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	SemEval 2013, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	SemEval 2013, F1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	SemEval 2013, F1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SemEval 2013, F1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SemEval 2013, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	SemEval 2013, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	SemEval 2013, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	SemEval 2013, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	SemEval 2013, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	SemEval 2013, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	benchmark Vietnamese dependency treebank VnDT, UAS	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	benchmark Vietnamese dependency treebank VnDT, UAS	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	benchmark Vietnamese dependency treebank VnDT, UAS	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	benchmark Vietnamese dependency treebank VnDT, UAS	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	benchmark Vietnamese dependency treebank VnDT, UAS	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	benchmark Vietnamese dependency treebank VnDT, UAS	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	benchmark Vietnamese dependency treebank VnDT, UAS	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	benchmark Vietnamese dependency treebank VnDT, UAS	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	benchmark Vietnamese dependency treebank VnDT, UAS	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	benchmark Vietnamese dependency treebank VnDT, UAS	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	benchmark Vietnamese dependency treebank VnDT, UAS	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	benchmark Vietnamese dependency treebank VnDT, UAS	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	benchmark Vietnamese dependency treebank VnDT, UAS	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	benchmark Vietnamese dependency treebank VnDT, UAS	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	benchmark Vietnamese dependency treebank VnDT, UAS	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	benchmark Vietnamese dependency treebank VnDT, UAS	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	benchmark Vietnamese dependency treebank VnDT, UAS	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Gigaword, ROUGE-2	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Gigaword, ROUGE-2	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Gigaword, ROUGE-2	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Gigaword, ROUGE-2	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Gigaword, ROUGE-2	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Gigaword, ROUGE-2	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Gigaword, ROUGE-2	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Gigaword, ROUGE-2	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Gigaword, ROUGE-2	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Gigaword, ROUGE-2	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Gigaword, ROUGE-2	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Gigaword, ROUGE-2	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Gigaword, ROUGE-2	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Gigaword, ROUGE-2	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Gigaword, ROUGE-2	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Gigaword, ROUGE-2	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Gigaword, ROUGE-2	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	LDC2014T12, F1 on Newswire	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	LDC2014T12, F1 on Newswire	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	LDC2014T12, F1 on Newswire	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	LDC2014T12, F1 on Newswire	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	LDC2014T12, F1 on Newswire	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	LDC2014T12, F1 on Newswire	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	LDC2014T12, F1 on Newswire	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	LDC2014T12, F1 on Newswire	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	LDC2014T12, F1 on Newswire	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	LDC2014T12, F1 on Newswire	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	LDC2014T12, F1 on Newswire	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	LDC2014T12, F1 on Newswire	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	LDC2014T12, F1 on Newswire	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	LDC2014T12, F1 on Newswire	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	LDC2014T12, F1 on Newswire	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	LDC2014T12, F1 on Newswire	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	LDC2014T12, F1 on Newswire	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	VLSP 2013 word segmentation shared task, F1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	VLSP 2013 word segmentation shared task, F1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	VLSP 2013 word segmentation shared task, F1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	VLSP 2013 word segmentation shared task, F1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	VLSP 2013 word segmentation shared task, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	VLSP 2013 word segmentation shared task, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	VLSP 2013 word segmentation shared task, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	VLSP 2013 word segmentation shared task, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	VLSP 2013 word segmentation shared task, F1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	VLSP 2013 word segmentation shared task, F1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	VLSP 2013 word segmentation shared task, F1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	VLSP 2013 word segmentation shared task, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	VLSP 2013 word segmentation shared task, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	VLSP 2013 word segmentation shared task, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	VLSP 2013 word segmentation shared task, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	VLSP 2013 word segmentation shared task, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	VLSP 2013 word segmentation shared task, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Penn Treebank, POS	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Penn Treebank, POS	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Penn Treebank, POS	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Penn Treebank, POS	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Penn Treebank, POS	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Penn Treebank, POS	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Penn Treebank, POS	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Penn Treebank, POS	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Penn Treebank, POS	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Penn Treebank, POS	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Penn Treebank, POS	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Penn Treebank, POS	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Penn Treebank, POS	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Penn Treebank, POS	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Penn Treebank, POS	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Penn Treebank, POS	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Penn Treebank, POS	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	SQuAD, EM	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SQuAD, EM	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	SQuAD, EM	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	SQuAD, EM	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SQuAD, EM	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	SQuAD, EM	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	SQuAD, EM	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	SQuAD, EM	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	SQuAD, EM	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	SQuAD, EM	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SQuAD, EM	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SQuAD, EM	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	SQuAD, EM	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	SQuAD, EM	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	SQuAD, EM	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	SQuAD, EM	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	SQuAD, EM	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Gigaword, ROUGE-L	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Gigaword, ROUGE-L	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Gigaword, ROUGE-L	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Gigaword, ROUGE-L	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Gigaword, ROUGE-L	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Gigaword, ROUGE-L	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Gigaword, ROUGE-L	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Gigaword, ROUGE-L	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Gigaword, ROUGE-L	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Gigaword, ROUGE-L	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Gigaword, ROUGE-L	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Gigaword, ROUGE-L	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Gigaword, ROUGE-L	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Gigaword, ROUGE-L	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Gigaword, ROUGE-L	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Gigaword, ROUGE-L	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Gigaword, ROUGE-L	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	VLSP 2013 POS tagging shared task, Accuracy	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	VLSP 2013 POS tagging shared task, Accuracy	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	VLSP 2013 POS tagging shared task, Accuracy	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	VLSP 2013 POS tagging shared task, Accuracy	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	VLSP 2013 POS tagging shared task, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	VLSP 2013 POS tagging shared task, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	VLSP 2013 POS tagging shared task, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	VLSP 2013 POS tagging shared task, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	VLSP 2013 POS tagging shared task, Accuracy	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	VLSP 2013 POS tagging shared task, Accuracy	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	VLSP 2013 POS tagging shared task, Accuracy	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	VLSP 2013 POS tagging shared task, Accuracy	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	VLSP 2013 POS tagging shared task, Accuracy	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	VLSP 2013 POS tagging shared task, Accuracy	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	VLSP 2013 POS tagging shared task, Accuracy	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	VLSP 2013 POS tagging shared task, Accuracy	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	VLSP 2013 POS tagging shared task, Accuracy	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	SemEval 2018, MRR	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SemEval 2018, MRR	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	SemEval 2018, MRR	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	SemEval 2018, MRR	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SemEval 2018, MRR	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	SemEval 2018, MRR	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	SemEval 2018, MRR	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	SemEval 2018, MRR	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	SemEval 2018, MRR	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	SemEval 2018, MRR	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SemEval 2018, MRR	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SemEval 2018, MRR	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	SemEval 2018, MRR	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	SemEval 2018, MRR	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	SemEval 2018, MRR	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	SemEval 2018, MRR	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	SemEval 2018, MRR	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	DUC 2004 Task 1, ROUGE-1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	DUC 2004 Task 1, ROUGE-1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	DUC 2004 Task 1, ROUGE-1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	DUC 2004 Task 1, ROUGE-1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	DUC 2004 Task 1, ROUGE-1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	DUC 2004 Task 1, ROUGE-1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	DUC 2004 Task 1, ROUGE-1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	DUC 2004 Task 1, ROUGE-1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	DUC 2004 Task 1, ROUGE-1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	DUC 2004 Task 1, ROUGE-1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	DUC 2004 Task 1, ROUGE-1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	DUC 2004 Task 1, ROUGE-1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	DUC 2004 Task 1, ROUGE-1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	DUC 2004 Task 1, ROUGE-1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	DUC 2004 Task 1, ROUGE-1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	DUC 2004 Task 1, ROUGE-1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	DUC 2004 Task 1, ROUGE-1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	New York Times Corpus, P@10%	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	New York Times Corpus, P@10%	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	New York Times Corpus, P@10%	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	New York Times Corpus, P@10%	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	New York Times Corpus, P@10%	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	New York Times Corpus, P@10%	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	New York Times Corpus, P@10%	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	New York Times Corpus, P@10%	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	New York Times Corpus, P@10%	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	New York Times Corpus, P@10%	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	New York Times Corpus, P@10%	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	New York Times Corpus, P@10%	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	New York Times Corpus, P@10%	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	New York Times Corpus, P@10%	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	New York Times Corpus, P@10%	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	New York Times Corpus, P@10%	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	New York Times Corpus, P@10%	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	benchmark Vietnamese dependency treebank VnDT, LAS	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	benchmark Vietnamese dependency treebank VnDT, LAS	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	benchmark Vietnamese dependency treebank VnDT, LAS	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	benchmark Vietnamese dependency treebank VnDT, LAS	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	benchmark Vietnamese dependency treebank VnDT, LAS	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	benchmark Vietnamese dependency treebank VnDT, LAS	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	benchmark Vietnamese dependency treebank VnDT, LAS	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	benchmark Vietnamese dependency treebank VnDT, LAS	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	benchmark Vietnamese dependency treebank VnDT, LAS	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	benchmark Vietnamese dependency treebank VnDT, LAS	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	benchmark Vietnamese dependency treebank VnDT, LAS	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	benchmark Vietnamese dependency treebank VnDT, LAS	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	benchmark Vietnamese dependency treebank VnDT, LAS	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	benchmark Vietnamese dependency treebank VnDT, LAS	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	benchmark Vietnamese dependency treebank VnDT, LAS	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	benchmark Vietnamese dependency treebank VnDT, LAS	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	benchmark Vietnamese dependency treebank VnDT, LAS	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Quasar, EM (Quasar-T)	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Quasar, EM (Quasar-T)	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Quasar, EM (Quasar-T)	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Quasar, EM (Quasar-T)	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Quasar, EM (Quasar-T)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Quasar, EM (Quasar-T)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Quasar, EM (Quasar-T)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Quasar, EM (Quasar-T)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Quasar, EM (Quasar-T)	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Quasar, EM (Quasar-T)	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Quasar, EM (Quasar-T)	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Quasar, EM (Quasar-T)	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Quasar, EM (Quasar-T)	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Quasar, EM (Quasar-T)	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Quasar, EM (Quasar-T)	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Quasar, EM (Quasar-T)	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Quasar, EM (Quasar-T)	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	LDC2014T12, F1 on Full	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	LDC2014T12, F1 on Full	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	LDC2014T12, F1 on Full	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	LDC2014T12, F1 on Full	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	LDC2014T12, F1 on Full	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	LDC2014T12, F1 on Full	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	LDC2014T12, F1 on Full	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	LDC2014T12, F1 on Full	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	LDC2014T12, F1 on Full	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	LDC2014T12, F1 on Full	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	LDC2014T12, F1 on Full	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	LDC2014T12, F1 on Full	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	LDC2014T12, F1 on Full	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	LDC2014T12, F1 on Full	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	LDC2014T12, F1 on Full	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	LDC2014T12, F1 on Full	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	LDC2014T12, F1 on Full	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Senseval 2, F1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Senseval 2, F1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Senseval 2, F1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Senseval 2, F1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Senseval 2, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Senseval 2, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Senseval 2, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Senseval 2, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Senseval 2, F1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Senseval 2, F1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Senseval 2, F1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Senseval 2, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Senseval 2, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Senseval 2, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Senseval 2, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Senseval 2, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Senseval 2, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	CNN / Daily Mail (Non-anonymized version), ROUGE-1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	CNN / Daily Mail (Non-anonymized version), ROUGE-1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	CNN / Daily Mail (Non-anonymized version), ROUGE-1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	CNN / Daily Mail (Non-anonymized version), ROUGE-1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	CNN / Daily Mail (Non-anonymized version), ROUGE-1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	CNN / Daily Mail (Non-anonymized version), ROUGE-1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	CNN / Daily Mail (Non-anonymized version), ROUGE-1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	CNN / Daily Mail (Non-anonymized version), ROUGE-1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	CNN / Daily Mail (Non-anonymized version), ROUGE-1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	CNN / Daily Mail (Non-anonymized version), ROUGE-1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	CNN / Daily Mail (Non-anonymized version), ROUGE-1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	CNN / Daily Mail (Non-anonymized version), ROUGE-1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	CNN / Daily Mail (Non-anonymized version), ROUGE-1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Penn Treebank, UAS	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Penn Treebank, UAS	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Penn Treebank, UAS	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Penn Treebank, UAS	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Penn Treebank, UAS	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Penn Treebank, UAS	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Penn Treebank, UAS	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Penn Treebank, UAS	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Penn Treebank, UAS	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Penn Treebank, UAS	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Penn Treebank, UAS	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Penn Treebank, UAS	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Penn Treebank, UAS	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Penn Treebank, UAS	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Penn Treebank, UAS	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Penn Treebank, UAS	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Penn Treebank, UAS	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Penn Treebank, Validation perplexity	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Penn Treebank, Validation perplexity	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Penn Treebank, Validation perplexity	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Penn Treebank, Validation perplexity	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Penn Treebank, Validation perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Penn Treebank, Validation perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Penn Treebank, Validation perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Penn Treebank, Validation perplexity	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Penn Treebank, Validation perplexity	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Penn Treebank, Validation perplexity	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Penn Treebank, Validation perplexity	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Penn Treebank, Validation perplexity	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Penn Treebank, Validation perplexity	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Penn Treebank, Validation perplexity	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Penn Treebank, Validation perplexity	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Penn Treebank, Validation perplexity	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Penn Treebank, Validation perplexity	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	IMDb, Accuracy	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	IMDb, Accuracy	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	IMDb, Accuracy	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	IMDb, Accuracy	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	IMDb, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	IMDb, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	IMDb, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	IMDb, Accuracy	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	IMDb, Accuracy	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	IMDb, Accuracy	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	IMDb, Accuracy	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	IMDb, Accuracy	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	IMDb, Accuracy	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	IMDb, Accuracy	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	IMDb, Accuracy	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	IMDb, Accuracy	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	IMDb, Accuracy	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	WMT 2014 EN-FR, BLEU	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	WMT 2014 EN-FR, BLEU	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	WMT 2014 EN-FR, BLEU	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	WMT 2014 EN-FR, BLEU	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	WMT 2014 EN-FR, BLEU	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	WMT 2014 EN-FR, BLEU	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	WMT 2014 EN-FR, BLEU	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	WMT 2014 EN-FR, BLEU	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	WMT 2014 EN-FR, BLEU	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	WMT 2014 EN-FR, BLEU	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	WMT 2014 EN-FR, BLEU	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	WMT 2014 EN-FR, BLEU	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	WMT 2014 EN-FR, BLEU	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	WMT 2014 EN-FR, BLEU	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	WMT 2014 EN-FR, BLEU	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	WMT 2014 EN-FR, BLEU	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	WMT 2014 EN-FR, BLEU	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	PKU, F1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	PKU, F1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	PKU, F1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	PKU, F1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	PKU, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	PKU, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	PKU, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	PKU, F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	PKU, F1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	PKU, F1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	PKU, F1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	PKU, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	PKU, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	PKU, F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	PKU, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	PKU, F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	PKU, F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	WN18RR, MRR	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	WN18RR, MRR	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	WN18RR, MRR	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	WN18RR, MRR	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	WN18RR, MRR	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	WN18RR, MRR	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	WN18RR, MRR	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	WN18RR, MRR	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	WN18RR, MRR	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	WN18RR, MRR	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	WN18RR, MRR	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	WN18RR, MRR	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	WN18RR, MRR	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	WN18RR, MRR	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	WN18RR, MRR	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	WN18RR, MRR	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	WN18RR, MRR	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Ontonotes v5 (English), F1	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Ontonotes v5 (English), F1	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Ontonotes v5 (English), F1	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Ontonotes v5 (English), F1	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Ontonotes v5 (English), F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Ontonotes v5 (English), F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Ontonotes v5 (English), F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Ontonotes v5 (English), F1	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Ontonotes v5 (English), F1	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Ontonotes v5 (English), F1	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Ontonotes v5 (English), F1	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Ontonotes v5 (English), F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Ontonotes v5 (English), F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Ontonotes v5 (English), F1	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Ontonotes v5 (English), F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Ontonotes v5 (English), F1	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Ontonotes v5 (English), F1	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Hutter Prize, Bit per Character (BPC)	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Hutter Prize, Bit per Character (BPC)	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Hutter Prize, Bit per Character (BPC)	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Hutter Prize, Bit per Character (BPC)	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Hutter Prize, Bit per Character (BPC)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Hutter Prize, Bit per Character (BPC)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Hutter Prize, Bit per Character (BPC)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Hutter Prize, Bit per Character (BPC)	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Hutter Prize, Bit per Character (BPC)	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Hutter Prize, Bit per Character (BPC)	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Hutter Prize, Bit per Character (BPC)	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Hutter Prize, Bit per Character (BPC)	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Hutter Prize, Bit per Character (BPC)	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Hutter Prize, Bit per Character (BPC)	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Hutter Prize, Bit per Character (BPC)	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Hutter Prize, Bit per Character (BPC)	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Hutter Prize, Bit per Character (BPC)	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	Hutter Prize, Number of params	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Hutter Prize, Number of params	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	Hutter Prize, Number of params	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	Hutter Prize, Number of params	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	Hutter Prize, Number of params	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	Hutter Prize, Number of params	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	Hutter Prize, Number of params	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	Hutter Prize, Number of params	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	Hutter Prize, Number of params	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	Hutter Prize, Number of params	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Hutter Prize, Number of params	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	Hutter Prize, Number of params	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	Hutter Prize, Number of params	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	Hutter Prize, Number of params	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	Hutter Prize, Number of params	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	Hutter Prize, Number of params	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	Hutter Prize, Number of params	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	CNN / Daily Mail (Anonymized version), ROUGE-L	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	CNN / Daily Mail (Anonymized version), ROUGE-L	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	CNN / Daily Mail (Anonymized version), ROUGE-L	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	CNN / Daily Mail (Anonymized version), ROUGE-L	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	CNN / Daily Mail (Anonymized version), ROUGE-L	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	CNN / Daily Mail (Anonymized version), ROUGE-L	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	CNN / Daily Mail (Anonymized version), ROUGE-L	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	CNN / Daily Mail (Anonymized version), ROUGE-L	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	CNN / Daily Mail (Anonymized version), ROUGE-L	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	CNN / Daily Mail (Anonymized version), ROUGE-L	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	CNN / Daily Mail (Anonymized version), ROUGE-L	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	CNN / Daily Mail (Anonymized version), ROUGE-L	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	CNN / Daily Mail (Anonymized version), ROUGE-L	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	CNN / Daily Mail (Anonymized version), ROUGE-L	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	CNN / Daily Mail (Anonymized version), ROUGE-L	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	CNN / Daily Mail (Anonymized version), ROUGE-L	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	CNN / Daily Mail (Anonymized version), ROUGE-L	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	DBpedia, Error	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	DBpedia, Error	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	DBpedia, Error	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	DBpedia, Error	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	DBpedia, Error	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	DBpedia, Error	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	DBpedia, Error	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	DBpedia, Error	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	DBpedia, Error	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	DBpedia, Error	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	DBpedia, Error	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	DBpedia, Error	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	DBpedia, Error	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	DBpedia, Error	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	DBpedia, Error	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	DBpedia, Error	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	DBpedia, Error	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.6494	LDC2015E86, Smatch	MAP 5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	LDC2015E86, Smatch	MAP 10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5597	LDC2015E86, Smatch	P@5  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.5502	LDC2015E86, Smatch	P@10  Table 1: The performance evaluated as a ranking  task. Here Ours corresponds to our full model,  while Ours-S is a simplified version of our model  without iterative propagation (see Section 3.2).
true	P15-1051.pdf#0.6338	LDC2015E86, Smatch	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5502	LDC2015E86, Smatch	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.5597	LDC2015E86, Smatch	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.6494	LDC2015E86, Smatch	Table 2: The performance of our full model (Full)  and two simplified models without modifications:
true	P15-1051.pdf#0.8748	LDC2015E86, Smatch	P  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8689	LDC2015E86, Smatch	R  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	LDC2015E86, Smatch	F  Table 3: The performance of document classifica- tion with (LDA+SVM+Ours-S, LDA+SVM+Ours)  and without (others) background knowledge.
true	P15-1051.pdf#0.8691	LDC2015E86, Smatch	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8489	LDC2015E86, Smatch	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8689	LDC2015E86, Smatch	R  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8584	LDC2015E86, Smatch	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8501	LDC2015E86, Smatch	F  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	P15-1051.pdf#0.8748	LDC2015E86, Smatch	P  Table 4: The performance of document classifica- tion with the full model (Ours) and the simplified  model (Ours-S) to introduce knowledge.
true	D15-1092.pdf#88.1	TREC, Error	Table 2: Performances of the different models. The result of PV is from our own implementation based  on Gensim.
true	D15-1092.pdf#93.8	TREC, Error	- -  Table 2: Performances of the different models. The result of PV is from our own implementation based  on Gensim.
true	D15-1092.pdf#48.5	TREC, Error	Table 2: Performances of the different models. The result of PV is from our own implementation based  on Gensim.
true	D15-1092.pdf#88.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2: Performances of the different models. The result of PV is from our own implementation based  on Gensim.
true	D15-1092.pdf#93.8	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- -  Table 2: Performances of the different models. The result of PV is from our own implementation based  on Gensim.
true	D15-1092.pdf#48.5	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2: Performances of the different models. The result of PV is from our own implementation based  on Gensim.
true	D15-1092.pdf#88.1	SST-2, Accuracy	Table 2: Performances of the different models. The result of PV is from our own implementation based  on Gensim.
true	D15-1092.pdf#93.8	SST-2, Accuracy	- -  Table 2: Performances of the different models. The result of PV is from our own implementation based  on Gensim.
true	D15-1092.pdf#48.5	SST-2, Accuracy	Table 2: Performances of the different models. The result of PV is from our own implementation based  on Gensim.
true	D14-1106.pdf#0.397	AG News, Error	using original and revised annotations as gold standard Table 5 : System performance using ERROR level evaluation on 200 utterances selected from ENNI - dev UTTERANCE level F1 - - -  Table 6: Error detection performance on NSR-development, mazes included
true	D14-1106.pdf#0.277	AG News, Error	using original and revised annotations as gold standard Table 5 : System performance using ERROR level evaluation on 200 utterances selected from ENNI - dev UTTERANCE level F1  Table 6: Error detection performance on NSR-development, mazes included
true	P12-2018.pdf#0.05	IMDb, Accuracy	vocabulary size . ∆ : upper - bounds of the differences number of average num - CV : number of cross - the  Table 1: Dataset statistics. (N + , N − ): number of  positive and negative examples. l: average num- ber of words per example. CV: number of cross- validation splits, or N for train/test split. |V |: the  vocabulary size. ∆: upper-bounds of the differences  required to be statistically significant at the p < 0.05  level.
true	P12-2018.pdf#78.1	IMDb, Accuracy	- s  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#86.7	IMDb, Accuracy	MPQA  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#86.3	IMDb, Accuracy	MPQA  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#92.6	IMDb, Accuracy	Subj .  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#86.3	IMDb, Accuracy	MPQA  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#79.4	IMDb, Accuracy	- s  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#81.4	IMDb, Accuracy	- CR - -  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#93.6	IMDb, Accuracy	Subj .  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#93.2	IMDb, Accuracy	Subj .  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#86.4	IMDb, Accuracy	MPQA -  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#79.0	IMDb, Accuracy	- s  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#81.4	IMDb, Accuracy	- CR - -  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#81.8	IMDb, Accuracy	CR  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#89.23	IMDb, Accuracy	- IMDB - - -  Table 3: Results for long reviews (RT-2k and  IMDB). The snippet dataset Subj. is also included  for comparison. Results in rows 7-11 are from  (Maas et al., 2011). BoW: linear SVM on bag of  words features. bnc: binary, no idf, cosine nor- malization. ∆t : smoothed delta idf. Full: the  full model. Unlab'd: additional unlabeled data.  BoWSVM: bag of words SVM used in (Pang and  Lee, 2004). Valence Shifter: (Kennedy and Inkpen,  2006). tf.∆idf: (Martineau and Finin, 2009). Ap- praisal Taxonomy: (Whitelaw et al., 2005). WR- RBM: Word Representation Restricted Boltzmann  Machine (Dahl et al., 2012).
true	P12-2018.pdf#89.45	IMDb, Accuracy	RT - 2k  Table 3: Results for long reviews (RT-2k and  IMDB). The snippet dataset Subj. is also included  for comparison. Results in rows 7-11 are from  (Maas et al., 2011). BoW: linear SVM on bag of  words features. bnc: binary, no idf, cosine nor- malization. ∆t : smoothed delta idf. Full: the  full model. Unlab'd: additional unlabeled data.  BoWSVM: bag of words SVM used in (Pang and  Lee, 2004). Valence Shifter: (Kennedy and Inkpen,  2006). tf.∆idf: (Martineau and Finin, 2009). Ap- praisal Taxonomy: (Whitelaw et al., 2005). WR- RBM: Word Representation Restricted Boltzmann  Machine (Dahl et al., 2012).
true	P12-2018.pdf#90.20	IMDb, Accuracy	RT - 2k - -  Table 3: Results for long reviews (RT-2k and  IMDB). The snippet dataset Subj. is also included  for comparison. Results in rows 7-11 are from  (Maas et al., 2011). BoW: linear SVM on bag of  words features. bnc: binary, no idf, cosine nor- malization. ∆t : smoothed delta idf. Full: the  full model. Unlab'd: additional unlabeled data.  BoWSVM: bag of words SVM used in (Pang and  Lee, 2004). Valence Shifter: (Kennedy and Inkpen,  2006). tf.∆idf: (Martineau and Finin, 2009). Ap- praisal Taxonomy: (Whitelaw et al., 2005). WR- RBM: Word Representation Restricted Boltzmann  Machine (Dahl et al., 2012).
true	P12-2018.pdf#89.16	IMDb, Accuracy	IMDB  Table 3: Results for long reviews (RT-2k and  IMDB). The snippet dataset Subj. is also included  for comparison. Results in rows 7-11 are from  (Maas et al., 2011). BoW: linear SVM on bag of  words features. bnc: binary, no idf, cosine nor- malization. ∆t : smoothed delta idf. Full: the  full model. Unlab'd: additional unlabeled data.  BoWSVM: bag of words SVM used in (Pang and  Lee, 2004). Valence Shifter: (Kennedy and Inkpen,  2006). tf.∆idf: (Martineau and Finin, 2009). Ap- praisal Taxonomy: (Whitelaw et al., 2005). WR- RBM: Word Representation Restricted Boltzmann  Machine (Dahl et al., 2012).
true	P12-2018.pdf#91.22	IMDb, Accuracy	IMDB  Table 3: Results for long reviews (RT-2k and  IMDB). The snippet dataset Subj. is also included  for comparison. Results in rows 7-11 are from  (Maas et al., 2011). BoW: linear SVM on bag of  words features. bnc: binary, no idf, cosine nor- malization. ∆t : smoothed delta idf. Full: the  full model. Unlab'd: additional unlabeled data.  BoWSVM: bag of words SVM used in (Pang and  Lee, 2004). Valence Shifter: (Kennedy and Inkpen,  2006). tf.∆idf: (Martineau and Finin, 2009). Ap- praisal Taxonomy: (Whitelaw et al., 2005). WR- RBM: Word Representation Restricted Boltzmann  Machine (Dahl et al., 2012).
true	P12-2018.pdf#93.18	IMDb, Accuracy	Subj .  Table 3: Results for long reviews (RT-2k and  IMDB). The snippet dataset Subj. is also included  for comparison. Results in rows 7-11 are from  (Maas et al., 2011). BoW: linear SVM on bag of  words features. bnc: binary, no idf, cosine nor- malization. ∆t : smoothed delta idf. Full: the  full model. Unlab'd: additional unlabeled data.  BoWSVM: bag of words SVM used in (Pang and  Lee, 2004). Valence Shifter: (Kennedy and Inkpen,  2006). tf.∆idf: (Martineau and Finin, 2009). Ap- praisal Taxonomy: (Whitelaw et al., 2005). WR- RBM: Word Representation Restricted Boltzmann  Machine (Dahl et al., 2012).
true	P12-2018.pdf#93.56	IMDb, Accuracy	Subj .  Table 3: Results for long reviews (RT-2k and  IMDB). The snippet dataset Subj. is also included  for comparison. Results in rows 7-11 are from  (Maas et al., 2011). BoW: linear SVM on bag of  words features. bnc: binary, no idf, cosine nor- malization. ∆t : smoothed delta idf. Full: the  full model. Unlab'd: additional unlabeled data.  BoWSVM: bag of words SVM used in (Pang and  Lee, 2004). Valence Shifter: (Kennedy and Inkpen,  2006). tf.∆idf: (Martineau and Finin, 2009). Ap- praisal Taxonomy: (Whitelaw et al., 2005). WR- RBM: Word Representation Restricted Boltzmann  Machine (Dahl et al., 2012).
true	P12-2018.pdf#88.9	IMDb, Accuracy	RT - 2k  Table 3: Results for long reviews (RT-2k and  IMDB). The snippet dataset Subj. is also included  for comparison. Results in rows 7-11 are from  (Maas et al., 2011). BoW: linear SVM on bag of  words features. bnc: binary, no idf, cosine nor- malization. ∆t : smoothed delta idf. Full: the  full model. Unlab'd: additional unlabeled data.  BoWSVM: bag of words SVM used in (Pang and  Lee, 2004). Valence Shifter: (Kennedy and Inkpen,  2006). tf.∆idf: (Martineau and Finin, 2009). Ap- praisal Taxonomy: (Whitelaw et al., 2005). WR- RBM: Word Representation Restricted Boltzmann  Machine (Dahl et al., 2012).
true	P12-2018.pdf#92.58	IMDb, Accuracy	Subj .  Table 3: Results for long reviews (RT-2k and  IMDB). The snippet dataset Subj. is also included  for comparison. Results in rows 7-11 are from  (Maas et al., 2011). BoW: linear SVM on bag of  words features. bnc: binary, no idf, cosine nor- malization. ∆t : smoothed delta idf. Full: the  full model. Unlab'd: additional unlabeled data.  BoWSVM: bag of words SVM used in (Pang and  Lee, 2004). Valence Shifter: (Kennedy and Inkpen,  2006). tf.∆idf: (Martineau and Finin, 2009). Ap- praisal Taxonomy: (Whitelaw et al., 2005). WR- RBM: Word Representation Restricted Boltzmann  Machine (Dahl et al., 2012).
true	P12-2018.pdf#87.9	IMDb, Accuracy	AthR  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P12-2018.pdf#87.7	IMDb, Accuracy	AthR  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P12-2018.pdf#91.2	IMDb, Accuracy	XGraph  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P12-2018.pdf#−0.5	IMDb, Accuracy	XGraph  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P12-2018.pdf#−0.5	IMDb, Accuracy	BbCrypt  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P12-2018.pdf#−0.2	IMDb, Accuracy	BbCrypt  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P12-2018.pdf#85.1	IMDb, Accuracy	AthR  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P12-2018.pdf#−0.2	IMDb, Accuracy	AthR  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P12-2018.pdf#99.3	IMDb, Accuracy	BbCrypt  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P12-2018.pdf#91.2	IMDb, Accuracy	XGraph  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P12-2018.pdf#90.7	IMDb, Accuracy	XGraph  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P12-2018.pdf#0.05	SUBJ, Accuracy	vocabulary size . ∆ : upper - bounds of the differences number of average num - CV : number of cross - the  Table 1: Dataset statistics. (N + , N − ): number of  positive and negative examples. l: average num- ber of words per example. CV: number of cross- validation splits, or N for train/test split. |V |: the  vocabulary size. ∆: upper-bounds of the differences  required to be statistically significant at the p < 0.05  level.
true	P12-2018.pdf#78.1	SUBJ, Accuracy	- s  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#86.7	SUBJ, Accuracy	MPQA  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#86.3	SUBJ, Accuracy	MPQA  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#92.6	SUBJ, Accuracy	Subj .  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#86.3	SUBJ, Accuracy	MPQA  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#79.4	SUBJ, Accuracy	- s  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#81.4	SUBJ, Accuracy	- CR - -  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#93.6	SUBJ, Accuracy	Subj .  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#93.2	SUBJ, Accuracy	Subj .  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#86.4	SUBJ, Accuracy	MPQA -  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#79.0	SUBJ, Accuracy	- s  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#81.4	SUBJ, Accuracy	- CR - -  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#81.8	SUBJ, Accuracy	CR  Table 2: Results for snippets datasets. Tree-CRF:  (Nakagawa et al., 2010) RAE: Recursive Autoen- coders (Socher et al., 2011). RAE-pretrain: train on  Wikipedia (Collobert
true	P12-2018.pdf#89.23	SUBJ, Accuracy	- IMDB - - -  Table 3: Results for long reviews (RT-2k and  IMDB). The snippet dataset Subj. is also included  for comparison. Results in rows 7-11 are from  (Maas et al., 2011). BoW: linear SVM on bag of  words features. bnc: binary, no idf, cosine nor- malization. ∆t : smoothed delta idf. Full: the  full model. Unlab'd: additional unlabeled data.  BoWSVM: bag of words SVM used in (Pang and  Lee, 2004). Valence Shifter: (Kennedy and Inkpen,  2006). tf.∆idf: (Martineau and Finin, 2009). Ap- praisal Taxonomy: (Whitelaw et al., 2005). WR- RBM: Word Representation Restricted Boltzmann  Machine (Dahl et al., 2012).
true	P12-2018.pdf#89.45	SUBJ, Accuracy	RT - 2k  Table 3: Results for long reviews (RT-2k and  IMDB). The snippet dataset Subj. is also included  for comparison. Results in rows 7-11 are from  (Maas et al., 2011). BoW: linear SVM on bag of  words features. bnc: binary, no idf, cosine nor- malization. ∆t : smoothed delta idf. Full: the  full model. Unlab'd: additional unlabeled data.  BoWSVM: bag of words SVM used in (Pang and  Lee, 2004). Valence Shifter: (Kennedy and Inkpen,  2006). tf.∆idf: (Martineau and Finin, 2009). Ap- praisal Taxonomy: (Whitelaw et al., 2005). WR- RBM: Word Representation Restricted Boltzmann  Machine (Dahl et al., 2012).
true	P12-2018.pdf#90.20	SUBJ, Accuracy	RT - 2k - -  Table 3: Results for long reviews (RT-2k and  IMDB). The snippet dataset Subj. is also included  for comparison. Results in rows 7-11 are from  (Maas et al., 2011). BoW: linear SVM on bag of  words features. bnc: binary, no idf, cosine nor- malization. ∆t : smoothed delta idf. Full: the  full model. Unlab'd: additional unlabeled data.  BoWSVM: bag of words SVM used in (Pang and  Lee, 2004). Valence Shifter: (Kennedy and Inkpen,  2006). tf.∆idf: (Martineau and Finin, 2009). Ap- praisal Taxonomy: (Whitelaw et al., 2005). WR- RBM: Word Representation Restricted Boltzmann  Machine (Dahl et al., 2012).
true	P12-2018.pdf#89.16	SUBJ, Accuracy	IMDB  Table 3: Results for long reviews (RT-2k and  IMDB). The snippet dataset Subj. is also included  for comparison. Results in rows 7-11 are from  (Maas et al., 2011). BoW: linear SVM on bag of  words features. bnc: binary, no idf, cosine nor- malization. ∆t : smoothed delta idf. Full: the  full model. Unlab'd: additional unlabeled data.  BoWSVM: bag of words SVM used in (Pang and  Lee, 2004). Valence Shifter: (Kennedy and Inkpen,  2006). tf.∆idf: (Martineau and Finin, 2009). Ap- praisal Taxonomy: (Whitelaw et al., 2005). WR- RBM: Word Representation Restricted Boltzmann  Machine (Dahl et al., 2012).
true	P12-2018.pdf#91.22	SUBJ, Accuracy	IMDB  Table 3: Results for long reviews (RT-2k and  IMDB). The snippet dataset Subj. is also included  for comparison. Results in rows 7-11 are from  (Maas et al., 2011). BoW: linear SVM on bag of  words features. bnc: binary, no idf, cosine nor- malization. ∆t : smoothed delta idf. Full: the  full model. Unlab'd: additional unlabeled data.  BoWSVM: bag of words SVM used in (Pang and  Lee, 2004). Valence Shifter: (Kennedy and Inkpen,  2006). tf.∆idf: (Martineau and Finin, 2009). Ap- praisal Taxonomy: (Whitelaw et al., 2005). WR- RBM: Word Representation Restricted Boltzmann  Machine (Dahl et al., 2012).
true	P12-2018.pdf#93.18	SUBJ, Accuracy	Subj .  Table 3: Results for long reviews (RT-2k and  IMDB). The snippet dataset Subj. is also included  for comparison. Results in rows 7-11 are from  (Maas et al., 2011). BoW: linear SVM on bag of  words features. bnc: binary, no idf, cosine nor- malization. ∆t : smoothed delta idf. Full: the  full model. Unlab'd: additional unlabeled data.  BoWSVM: bag of words SVM used in (Pang and  Lee, 2004). Valence Shifter: (Kennedy and Inkpen,  2006). tf.∆idf: (Martineau and Finin, 2009). Ap- praisal Taxonomy: (Whitelaw et al., 2005). WR- RBM: Word Representation Restricted Boltzmann  Machine (Dahl et al., 2012).
true	P12-2018.pdf#93.56	SUBJ, Accuracy	Subj .  Table 3: Results for long reviews (RT-2k and  IMDB). The snippet dataset Subj. is also included  for comparison. Results in rows 7-11 are from  (Maas et al., 2011). BoW: linear SVM on bag of  words features. bnc: binary, no idf, cosine nor- malization. ∆t : smoothed delta idf. Full: the  full model. Unlab'd: additional unlabeled data.  BoWSVM: bag of words SVM used in (Pang and  Lee, 2004). Valence Shifter: (Kennedy and Inkpen,  2006). tf.∆idf: (Martineau and Finin, 2009). Ap- praisal Taxonomy: (Whitelaw et al., 2005). WR- RBM: Word Representation Restricted Boltzmann  Machine (Dahl et al., 2012).
true	P12-2018.pdf#88.9	SUBJ, Accuracy	RT - 2k  Table 3: Results for long reviews (RT-2k and  IMDB). The snippet dataset Subj. is also included  for comparison. Results in rows 7-11 are from  (Maas et al., 2011). BoW: linear SVM on bag of  words features. bnc: binary, no idf, cosine nor- malization. ∆t : smoothed delta idf. Full: the  full model. Unlab'd: additional unlabeled data.  BoWSVM: bag of words SVM used in (Pang and  Lee, 2004). Valence Shifter: (Kennedy and Inkpen,  2006). tf.∆idf: (Martineau and Finin, 2009). Ap- praisal Taxonomy: (Whitelaw et al., 2005). WR- RBM: Word Representation Restricted Boltzmann  Machine (Dahl et al., 2012).
true	P12-2018.pdf#92.58	SUBJ, Accuracy	Subj .  Table 3: Results for long reviews (RT-2k and  IMDB). The snippet dataset Subj. is also included  for comparison. Results in rows 7-11 are from  (Maas et al., 2011). BoW: linear SVM on bag of  words features. bnc: binary, no idf, cosine nor- malization. ∆t : smoothed delta idf. Full: the  full model. Unlab'd: additional unlabeled data.  BoWSVM: bag of words SVM used in (Pang and  Lee, 2004). Valence Shifter: (Kennedy and Inkpen,  2006). tf.∆idf: (Martineau and Finin, 2009). Ap- praisal Taxonomy: (Whitelaw et al., 2005). WR- RBM: Word Representation Restricted Boltzmann  Machine (Dahl et al., 2012).
true	P12-2018.pdf#87.9	SUBJ, Accuracy	AthR  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P12-2018.pdf#87.7	SUBJ, Accuracy	AthR  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P12-2018.pdf#91.2	SUBJ, Accuracy	XGraph  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P12-2018.pdf#−0.5	SUBJ, Accuracy	XGraph  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P12-2018.pdf#−0.5	SUBJ, Accuracy	BbCrypt  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P12-2018.pdf#−0.2	SUBJ, Accuracy	BbCrypt  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P12-2018.pdf#85.1	SUBJ, Accuracy	AthR  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P12-2018.pdf#−0.2	SUBJ, Accuracy	AthR  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P12-2018.pdf#99.3	SUBJ, Accuracy	BbCrypt  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P12-2018.pdf#91.2	SUBJ, Accuracy	XGraph  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P12-2018.pdf#90.7	SUBJ, Accuracy	XGraph  Table 4: On 3 20-newsgroup subtasks, we compare  to DiscLDA (Lacoste-Julien et al., 2008) and Ac- tiveSVM (
true	P11-2036.pdf#85.3	Penn Treebank, F1	Table 2 : Small dataset experiments F1  Table 2: Small dataset experiments
true	P11-2036.pdf#85.3	Penn Treebank, F1	Table 2 : Small dataset experiments F1  Table 3: Full Penn Treebank dataset experiments
true	D14-1099.pdf#87.26	Penn Treebank, LAS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#81.00	Penn Treebank, LAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#85.20	Penn Treebank, LAS	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#93.56	Penn Treebank, LAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#85.34	Penn Treebank, LAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#92.18	Penn Treebank, LAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#68.90	Penn Treebank, LAS	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#84.43	Penn Treebank, LAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#80.13	Penn Treebank, LAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#82.08	Penn Treebank, LAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#85.36	Penn Treebank, LAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#90.47	Penn Treebank, LAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#82.10	Penn Treebank, LAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#70.32	Penn Treebank, LAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#79.57	Penn Treebank, LAS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#66.74	Penn Treebank, LAS	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#75.96	Penn Treebank, LAS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#74.14	Penn Treebank, LAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#70.22	Penn Treebank, LAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#77.22	Penn Treebank, LAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#74.44	Penn Treebank, LAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#81.55	Penn Treebank, LAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#72.63	Penn Treebank, LAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#89.46	Penn Treebank, LAS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#78.20	Penn Treebank, LAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#78.13	Penn Treebank, LAS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#85.99	Penn Treebank, LAS	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#88.86	Penn Treebank, LAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#82.23	Penn Treebank, LAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#90.55	Penn Treebank, LAS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#86.94	Penn Treebank, LAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#82.77	Penn Treebank, LAS	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#77.45	Penn Treebank, LAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#86.03	Penn Treebank, LAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#88.18	Penn Treebank, LAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#81.59	Penn Treebank, LAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#80.25	Penn Treebank, LAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#80.45	Penn Treebank, LAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#86.40	Penn Treebank, LAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#87.38	Penn Treebank, LAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#87.26	benchmark Vietnamese dependency treebank VnDT, UAS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#81.00	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#85.20	benchmark Vietnamese dependency treebank VnDT, UAS	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#93.56	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#85.34	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#92.18	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#68.90	benchmark Vietnamese dependency treebank VnDT, UAS	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#84.43	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#80.13	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#82.08	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#85.36	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#90.47	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#82.10	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#70.32	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#79.57	benchmark Vietnamese dependency treebank VnDT, UAS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#66.74	benchmark Vietnamese dependency treebank VnDT, UAS	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#75.96	benchmark Vietnamese dependency treebank VnDT, UAS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#74.14	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#70.22	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#77.22	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#74.44	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#81.55	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#72.63	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#89.46	benchmark Vietnamese dependency treebank VnDT, UAS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#78.20	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#78.13	benchmark Vietnamese dependency treebank VnDT, UAS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#85.99	benchmark Vietnamese dependency treebank VnDT, UAS	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#88.86	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#82.23	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#90.55	benchmark Vietnamese dependency treebank VnDT, UAS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#86.94	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#82.77	benchmark Vietnamese dependency treebank VnDT, UAS	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#77.45	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#86.03	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#88.18	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#81.59	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#80.25	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#80.45	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#86.40	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#87.38	benchmark Vietnamese dependency treebank VnDT, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#87.26	Penn Treebank, UAS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#81.00	Penn Treebank, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#85.20	Penn Treebank, UAS	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#93.56	Penn Treebank, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#85.34	Penn Treebank, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#92.18	Penn Treebank, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#68.90	Penn Treebank, UAS	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#84.43	Penn Treebank, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#80.13	Penn Treebank, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#82.08	Penn Treebank, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#85.36	Penn Treebank, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#90.47	Penn Treebank, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#82.10	Penn Treebank, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#70.32	Penn Treebank, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#79.57	Penn Treebank, UAS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#66.74	Penn Treebank, UAS	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#75.96	Penn Treebank, UAS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#74.14	Penn Treebank, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#70.22	Penn Treebank, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#77.22	Penn Treebank, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#74.44	Penn Treebank, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#81.55	Penn Treebank, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#72.63	Penn Treebank, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#89.46	Penn Treebank, UAS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#78.20	Penn Treebank, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#78.13	Penn Treebank, UAS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#85.99	Penn Treebank, UAS	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#88.86	Penn Treebank, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#82.23	Penn Treebank, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#90.55	Penn Treebank, UAS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#86.94	Penn Treebank, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#82.77	Penn Treebank, UAS	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#77.45	Penn Treebank, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#86.03	Penn Treebank, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#88.18	Penn Treebank, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#81.59	Penn Treebank, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#80.25	Penn Treebank, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#80.45	Penn Treebank, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#86.40	Penn Treebank, UAS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#87.38	Penn Treebank, UAS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#87.26	Penn Treebank, F1	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#81.00	Penn Treebank, F1	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#85.20	Penn Treebank, F1	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#93.56	Penn Treebank, F1	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#85.34	Penn Treebank, F1	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#92.18	Penn Treebank, F1	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#68.90	Penn Treebank, F1	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#84.43	Penn Treebank, F1	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#80.13	Penn Treebank, F1	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#82.08	Penn Treebank, F1	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#85.36	Penn Treebank, F1	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#90.47	Penn Treebank, F1	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#82.10	Penn Treebank, F1	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#70.32	Penn Treebank, F1	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#79.57	Penn Treebank, F1	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#66.74	Penn Treebank, F1	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#75.96	Penn Treebank, F1	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#74.14	Penn Treebank, F1	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#70.22	Penn Treebank, F1	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#77.22	Penn Treebank, F1	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#74.44	Penn Treebank, F1	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#81.55	Penn Treebank, F1	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#72.63	Penn Treebank, F1	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#89.46	Penn Treebank, F1	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#78.20	Penn Treebank, F1	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#78.13	Penn Treebank, F1	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#85.99	Penn Treebank, F1	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#88.86	Penn Treebank, F1	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#82.23	Penn Treebank, F1	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#90.55	Penn Treebank, F1	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#86.94	Penn Treebank, F1	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#82.77	Penn Treebank, F1	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#77.45	Penn Treebank, F1	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#86.03	Penn Treebank, F1	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#88.18	Penn Treebank, F1	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#81.59	Penn Treebank, F1	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#80.25	Penn Treebank, F1	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#80.45	Penn Treebank, F1	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#86.40	Penn Treebank, F1	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#87.38	Penn Treebank, F1	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#87.26	Penn Treebank, POS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#81.00	Penn Treebank, POS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#85.20	Penn Treebank, POS	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#93.56	Penn Treebank, POS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#85.34	Penn Treebank, POS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#92.18	Penn Treebank, POS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#68.90	Penn Treebank, POS	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#84.43	Penn Treebank, POS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#80.13	Penn Treebank, POS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#82.08	Penn Treebank, POS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#85.36	Penn Treebank, POS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#90.47	Penn Treebank, POS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#82.10	Penn Treebank, POS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#70.32	Penn Treebank, POS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#79.57	Penn Treebank, POS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#66.74	Penn Treebank, POS	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#75.96	Penn Treebank, POS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#74.14	Penn Treebank, POS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#70.22	Penn Treebank, POS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#77.22	Penn Treebank, POS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#74.44	Penn Treebank, POS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#81.55	Penn Treebank, POS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#72.63	Penn Treebank, POS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#89.46	Penn Treebank, POS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#78.20	Penn Treebank, POS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#78.13	Penn Treebank, POS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#85.99	Penn Treebank, POS	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#88.86	Penn Treebank, POS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#82.23	Penn Treebank, POS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#90.55	Penn Treebank, POS	static UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#86.94	Penn Treebank, POS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#82.77	Penn Treebank, POS	static LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#77.45	Penn Treebank, POS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#86.03	Penn Treebank, POS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#88.18	Penn Treebank, POS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#81.59	Penn Treebank, POS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#80.25	Penn Treebank, POS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#80.45	Penn Treebank, POS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#86.40	Penn Treebank, POS	dynamic LAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	D14-1099.pdf#87.38	Penn Treebank, POS	dynamic UAS  Table 1: Unlabelled Attachment Score (UAS) and  Labelled Attachment Score (LAS) using a static  and a dynamic oracle. Evaluation on CoNLL 2007  (first block) and CoNLL 2006 (second block) data- sets is carried out including punctuation, evalu- ation on the Penn Treebank excludes it.
true	P14-2104.pdf#81.07%	AG News, Error	All  Table 3: Experimental F1-score results on every experiment. Abbreviation on features are Gen: general  features, Kor: Korean specific features, LMR: latent morpheme representation features.
true	P15-1119.pdf#72.93	Penn Treebank, UAS	Unlabeled Attachment Score ( UAS ) FR  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#51.54	Penn Treebank, UAS	Labeled Attachment Score ( LAS ) DE  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#60.66	Penn Treebank, UAS	Unlabeled Attachment Score ( UAS ) DE  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#63.12	Penn Treebank, UAS	Labeled Attachment Score ( LAS ) FR  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#62.28	Penn Treebank, UAS	Labeled Attachment Score ( LAS ) ES  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#68.39	Penn Treebank, UAS	Unlabeled Attachment Score ( UAS ) AVG  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#71.90	Penn Treebank, UAS	Unlabeled Attachment Score ( UAS ) ES  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#58.98	Penn Treebank, UAS	Labeled Attachment Score ( LAS ) AVG  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#71.36	Penn Treebank, UAS	FR UAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#71.42	Penn Treebank, UAS	ES UAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#61.55	Penn Treebank, UAS	FR LAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#61.76	Penn Treebank, UAS	ES LAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#60.07	Penn Treebank, UAS	DE UAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#49.94	Penn Treebank, UAS	DE LAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#72.93	Penn Treebank, POS	Unlabeled Attachment Score ( UAS ) FR  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#51.54	Penn Treebank, POS	Labeled Attachment Score ( LAS ) DE  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#60.66	Penn Treebank, POS	Unlabeled Attachment Score ( UAS ) DE  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#63.12	Penn Treebank, POS	Labeled Attachment Score ( LAS ) FR  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#62.28	Penn Treebank, POS	Labeled Attachment Score ( LAS ) ES  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#68.39	Penn Treebank, POS	Unlabeled Attachment Score ( UAS ) AVG  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#71.90	Penn Treebank, POS	Unlabeled Attachment Score ( UAS ) ES  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#58.98	Penn Treebank, POS	Labeled Attachment Score ( LAS ) AVG  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#71.36	Penn Treebank, POS	FR UAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#71.42	Penn Treebank, POS	ES UAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#61.55	Penn Treebank, POS	FR LAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#61.76	Penn Treebank, POS	ES LAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#60.07	Penn Treebank, POS	DE UAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#49.94	Penn Treebank, POS	DE LAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#72.93	Penn Treebank, LAS	Unlabeled Attachment Score ( UAS ) FR  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#51.54	Penn Treebank, LAS	Labeled Attachment Score ( LAS ) DE  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#60.66	Penn Treebank, LAS	Unlabeled Attachment Score ( UAS ) DE  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#63.12	Penn Treebank, LAS	Labeled Attachment Score ( LAS ) FR  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#62.28	Penn Treebank, LAS	Labeled Attachment Score ( LAS ) ES  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#68.39	Penn Treebank, LAS	Unlabeled Attachment Score ( UAS ) AVG  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#71.90	Penn Treebank, LAS	Unlabeled Attachment Score ( UAS ) ES  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#58.98	Penn Treebank, LAS	Labeled Attachment Score ( LAS ) AVG  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#71.36	Penn Treebank, LAS	FR UAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#71.42	Penn Treebank, LAS	ES UAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#61.55	Penn Treebank, LAS	FR LAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#61.76	Penn Treebank, LAS	ES LAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#60.07	Penn Treebank, LAS	DE UAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#49.94	Penn Treebank, LAS	DE LAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#72.93	benchmark Vietnamese dependency treebank VnDT, UAS	Unlabeled Attachment Score ( UAS ) FR  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#51.54	benchmark Vietnamese dependency treebank VnDT, UAS	Labeled Attachment Score ( LAS ) DE  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#60.66	benchmark Vietnamese dependency treebank VnDT, UAS	Unlabeled Attachment Score ( UAS ) DE  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#63.12	benchmark Vietnamese dependency treebank VnDT, UAS	Labeled Attachment Score ( LAS ) FR  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#62.28	benchmark Vietnamese dependency treebank VnDT, UAS	Labeled Attachment Score ( LAS ) ES  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#68.39	benchmark Vietnamese dependency treebank VnDT, UAS	Unlabeled Attachment Score ( UAS ) AVG  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#71.90	benchmark Vietnamese dependency treebank VnDT, UAS	Unlabeled Attachment Score ( UAS ) ES  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#58.98	benchmark Vietnamese dependency treebank VnDT, UAS	Labeled Attachment Score ( LAS ) AVG  Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi- lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score  (LAS).  *  denotes our re-implementation of MCD13. Since the model varies for different target languages  in the CCA-based approach,  † indicates the averaged UAS/LAS.
true	P15-1119.pdf#71.36	benchmark Vietnamese dependency treebank VnDT, UAS	FR UAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#71.42	benchmark Vietnamese dependency treebank VnDT, UAS	ES UAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#61.55	benchmark Vietnamese dependency treebank VnDT, UAS	FR LAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#61.76	benchmark Vietnamese dependency treebank VnDT, UAS	ES LAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#60.07	benchmark Vietnamese dependency treebank VnDT, UAS	DE UAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P15-1119.pdf#49.94	benchmark Vietnamese dependency treebank VnDT, UAS	DE LAS  Table 8: Comparison with existing bilingual word embeddings.  ‡ For MTL and BIAE, we use their  released bilingual word embeddings.
true	P11-3012.pdf#38.3	New York Times Corpus, P@10%	Relation Blogspot F  Table 1: Relation extraction results on the JDPA Corpus test set, broken down by document source.
true	P11-3012.pdf#40.5	New York Times Corpus, P@10%	Relation All Documents F  Table 1: Relation extraction results on the JDPA Corpus test set, broken down by document source.
true	P11-3012.pdf#49.9	New York Times Corpus, P@10%	Relation JDPA F  Table 1: Relation extraction results on the JDPA Corpus test set, broken down by document source.
true	P11-3012.pdf#24.6	New York Times Corpus, P@10%	Relation LiveJournal F  Table 1: Relation extraction results on the JDPA Corpus test set, broken down by document source.
true	P11-3012.pdf#19.2	New York Times Corpus, P@10%	LiveJournal  Table 2: Selected document statistics for three JDPA Corpus document sources.
true	P11-3012.pdf#4.32	New York Times Corpus, P@10%	Blogspot 30% 54 , 480  Table 2: Selected document statistics for three JDPA Corpus document sources.
true	P11-3012.pdf#4.26	New York Times Corpus, P@10%	LiveJournal 33% 58 , 452  Table 2: Selected document statistics for three JDPA Corpus document sources.
true	P11-3012.pdf#2.56	New York Times Corpus, P@10%	JDPA  Table 2: Selected document statistics for three JDPA Corpus document sources.
true	P11-3012.pdf#77.3%	New York Times Corpus, P@10%	LiveJournal 33% 58 , 452  Table 2: Selected document statistics for three JDPA Corpus document sources.
true	P11-3012.pdf#1.73	New York Times Corpus, P@10%	LiveJournal 33% 58 , 452  Table 2: Selected document statistics for three JDPA Corpus document sources.
true	P11-3012.pdf#13.6%	New York Times Corpus, P@10%	Sport JDPA Mention % Phrase  Table 3: Top 10 phrases in mention pairs whose relation  was incorrectly classified, and the total percentage of er- rors from the top ten.
true	P11-3012.pdf#23.9%	New York Times Corpus, P@10%	Sport Blogspot Mention % Phrase  Table 3: Top 10 phrases in mention pairs whose relation  was incorrectly classified, and the total percentage of er- rors from the top ten.
true	P11-3012.pdf#22.9%	New York Times Corpus, P@10%	Sport Blogspot Mention % Phrase  Table 3: Top 10 phrases in mention pairs whose relation  was incorrectly classified, and the total percentage of er- rors from the top ten.
true	P11-3012.pdf#0.86	New York Times Corpus, P@10%	Word Percent of All Tokens in Documents LiveJournal  Table 4: Frequency of some common words per token.
true	P11-3012.pdf#4.60	New York Times Corpus, P@10%	Word Percent of All Tokens in Documents Blogspot  Table 4: Frequency of some common words per token.
true	P11-3012.pdf#0.35	New York Times Corpus, P@10%	Word Percent of All Tokens in Documents JDPA  Table 4: Frequency of some common words per token.
true	P11-3012.pdf#1.91	New York Times Corpus, P@10%	Word Percent of All Tokens in Documents LiveJournal  Table 4: Frequency of some common words per token.
true	P11-3012.pdf#0.25	New York Times Corpus, P@10%	Word Percent of All Tokens in Documents LiveJournal  Table 4: Frequency of some common words per token.
true	P11-3012.pdf#4.43	New York Times Corpus, P@10%	Word Percent of All Tokens in Documents LiveJournal  Table 4: Frequency of some common words per token.
true	P11-3012.pdf#1.42	New York Times Corpus, P@10%	Word Percent of All Tokens in Documents LiveJournal  Table 4: Frequency of some common words per token.
true	P11-3012.pdf#0.85	New York Times Corpus, P@10%	POS POS Occurrence Per Sentence JDPA  Table 5: Frequency of select part-of-speech tags.
true	P11-3012.pdf#0.21	New York Times Corpus, P@10%	POS POS Occurrence Per Sentence LiveJournal  Table 5: Frequency of select part-of-speech tags.
true	P11-3012.pdf#0.98	New York Times Corpus, P@10%	POS POS Occurrence Per Sentence LiveJournal  Table 5: Frequency of select part-of-speech tags.
true	P11-3012.pdf#3.21	New York Times Corpus, P@10%	POS POS Occurrence Per Sentence JDPA  Table 5: Frequency of select part-of-speech tags.
true	P11-3012.pdf#1.89	New York Times Corpus, P@10%	POS POS Occurrence Per Sentence JDPA  Table 5: Frequency of select part-of-speech tags.
true	P10-1070.pdf#2031	Penn Treebank, Validation perplexity	available , semantically annotated datasets . below for size comparison with other publicly Size  Table 2: Size of various available noun compound  datasets labeled with relation annotations. Ital- ics indicate that the dataset contains n-prep-n con- structions and/or non-nouns.
true	P10-1070.pdf#2003	Penn Treebank, Validation perplexity	available , semantically annotated datasets . below for size comparison with other publicly Work  Table 2: Size of various available noun compound  datasets labeled with relation annotations. Ital- ics indicate that the dataset contains n-prep-n con- structions and/or non-nouns.
true	P10-1070.pdf#1998	Penn Treebank, Validation perplexity	available , semantically annotated datasets . below for size comparison with other publicly Work  Table 2: Size of various available noun compound  datasets labeled with relation annotations. Ital- ics indicate that the dataset contains n-prep-n con- structions and/or non-nouns.
true	P10-1070.pdf#2007	Penn Treebank, Validation perplexity	available , semantically annotated datasets . below for size comparison with other publicly Work  Table 2: Size of various available noun compound  datasets labeled with relation annotations. Ital- ics indicate that the dataset contains n-prep-n con- structions and/or non-nouns.
true	P10-1070.pdf#2031	Penn Treebank, Accuracy	available , semantically annotated datasets . below for size comparison with other publicly Size  Table 2: Size of various available noun compound  datasets labeled with relation annotations. Ital- ics indicate that the dataset contains n-prep-n con- structions and/or non-nouns.
true	P10-1070.pdf#2003	Penn Treebank, Accuracy	available , semantically annotated datasets . below for size comparison with other publicly Work  Table 2: Size of various available noun compound  datasets labeled with relation annotations. Ital- ics indicate that the dataset contains n-prep-n con- structions and/or non-nouns.
true	P10-1070.pdf#1998	Penn Treebank, Accuracy	available , semantically annotated datasets . below for size comparison with other publicly Work  Table 2: Size of various available noun compound  datasets labeled with relation annotations. Ital- ics indicate that the dataset contains n-prep-n con- structions and/or non-nouns.
true	P10-1070.pdf#2007	Penn Treebank, Accuracy	available , semantically annotated datasets . below for size comparison with other publicly Work  Table 2: Size of various available noun compound  datasets labeled with relation annotations. Ital- ics indicate that the dataset contains n-prep-n con- structions and/or non-nouns.
true	P13-2150.pdf#86.3	TREC, Error	- whole  Table 3: Fine-grained classification accuracy  (the Wong and Dras (2010) score is the highest  score from the last column of their Table 3).
true	P13-2150.pdf#49.2	TREC, Error	- sent .  Table 3: Fine-grained classification accuracy  (the Wong and Dras (2010) score is the highest  score from the last column of their Table 3).
true	P13-2150.pdf#86.8	TREC, Error	whole  Table 3: Fine-grained classification accuracy  (the Wong and Dras (2010) score is the highest  score from the last column of their Table 3).
true	P13-2150.pdf#42.9	TREC, Error	sent .  Table 3: Fine-grained classification accuracy  (the Wong and Dras (2010) score is the highest  score from the last column of their Table 3).
true	P13-2150.pdf#86.3	TREC, Error	- whole  Table 4: Question classification (6 classes).
true	P13-2150.pdf#42.9	TREC, Error	sent .  Table 4: Question classification (6 classes).
true	P13-2150.pdf#86.8	TREC, Error	whole  Table 4: Question classification (6 classes).
true	P13-2150.pdf#49.2	TREC, Error	- sent .  Table 4: Question classification (6 classes).
true	D15-1166.pdf#23.0(+2.1)	WMT 2014 EN-DE, BLEU	Our NMT systems BLEU  Table 1: WMT'14 English-German results -shown are the perplexities (ppl) and the tokenized BLEU  scores of various systems on newstest2014. We highlight the best system in bold and give progressive  improvements in italic between consecutive systems. local-p referes to the local attention with predictive  alignments. We indicate for each attention model the alignment score function used in pararentheses.
true	D15-1166.pdf#21.6	WMT 2014 EN-DE, BLEU	Existing NMT systems BLEU  Table 1: WMT'14 English-German results -shown are the perplexities (ppl) and the tokenized BLEU  scores of various systems on newstest2014. We highlight the best system in bold and give progressive  improvements in italic between consecutive systems. local-p referes to the local attention with predictive  alignments. We indicate for each attention model the alignment score function used in pararentheses.
true	D15-1166.pdf#29.2	WMT 2014 EN-DE, BLEU	BLEU  Table 3: WMT'15 German-English results - performances of various systems (similar to Ta- ble 1). The base system already includes source  reversing on which we add global attention,  dropout, input feeding, and unk replacement.
true	D15-1166.pdf#>7.0	WMT 2014 EN-DE, BLEU	Ppl  Table 4: Attentional Architectures -perfor- mances of different attentional models. We trained  two local-m (dot) models; both have ppl > 7.0.
true	D15-1166.pdf#5.9	WMT 2014 EN-DE, BLEU	Ppl x  Table 4: Attentional Architectures -perfor- mances of different attentional models. We trained  two local-m (dot) models; both have ppl > 7.0.
true	D15-1166.pdf#20.9(+1.9)	WMT 2014 EN-DE, BLEU	After unk x  Table 4: Attentional Architectures -perfor- mances of different attentional models. We trained  two local-m (dot) models; both have ppl > 7.0.
true	D15-1166.pdf#23.0(+2.1)	WMT 2014 EN-FR, BLEU	Our NMT systems BLEU  Table 1: WMT'14 English-German results -shown are the perplexities (ppl) and the tokenized BLEU  scores of various systems on newstest2014. We highlight the best system in bold and give progressive  improvements in italic between consecutive systems. local-p referes to the local attention with predictive  alignments. We indicate for each attention model the alignment score function used in pararentheses.
true	D15-1166.pdf#21.6	WMT 2014 EN-FR, BLEU	Existing NMT systems BLEU  Table 1: WMT'14 English-German results -shown are the perplexities (ppl) and the tokenized BLEU  scores of various systems on newstest2014. We highlight the best system in bold and give progressive  improvements in italic between consecutive systems. local-p referes to the local attention with predictive  alignments. We indicate for each attention model the alignment score function used in pararentheses.
true	D15-1166.pdf#29.2	WMT 2014 EN-FR, BLEU	BLEU  Table 3: WMT'15 German-English results - performances of various systems (similar to Ta- ble 1). The base system already includes source  reversing on which we add global attention,  dropout, input feeding, and unk replacement.
true	D15-1166.pdf#>7.0	WMT 2014 EN-FR, BLEU	Ppl  Table 4: Attentional Architectures -perfor- mances of different attentional models. We trained  two local-m (dot) models; both have ppl > 7.0.
true	D15-1166.pdf#5.9	WMT 2014 EN-FR, BLEU	Ppl x  Table 4: Attentional Architectures -perfor- mances of different attentional models. We trained  two local-m (dot) models; both have ppl > 7.0.
true	D15-1166.pdf#20.9(+1.9)	WMT 2014 EN-FR, BLEU	After unk x  Table 4: Attentional Architectures -perfor- mances of different attentional models. We trained  two local-m (dot) models; both have ppl > 7.0.
true	P15-2025.pdf#0.832	WMT 2014 EN-DE, BLEU	sys γ -  Table 1: Correlations with human judgment on  WMT data for Into-English and Out-of-English  task. Results are averaged on all test sets.
true	P15-2025.pdf#0.191	WMT 2014 EN-DE, BLEU	seg τ -  Table 1: Correlations with human judgment on  WMT data for Into-English and Out-of-English  task. Results are averaged on all test sets.
true	P15-2025.pdf#0.874	WMT 2014 EN-DE, BLEU	sys γ -  Table 1: Correlations with human judgment on  WMT data for Into-English and Out-of-English  task. Results are averaged on all test sets.
true	P15-2025.pdf#0.259	WMT 2014 EN-DE, BLEU	seg τ -  Table 1: Correlations with human judgment on  WMT data for Into-English and Out-of-English  task. Results are averaged on all test sets.
true	D13-1003.pdf#.220†*	New York Times Corpus, P@10%	gmap  Table 1: Ranking quality of extracted facts. Significance  (paired t-test, p < 0.05) w.r.t. MLE(*) and hier orig( †).
true	D13-1003.pdf#.426†*	New York Times Corpus, P@10%	p@5  Table 1: Ranking quality of extracted facts. Significance  (paired t-test, p < 0.05) w.r.t. MLE(*) and hier orig( †).
true	D13-1003.pdf#.353†*	New York Times Corpus, P@10%	p@10  Table 1: Ranking quality of extracted facts. Significance  (paired t-test, p < 0.05) w.r.t. MLE(*) and hier orig( †).
true	D13-1003.pdf#.344†*	New York Times Corpus, P@10%	map  Table 1: Ranking quality of extracted facts. Significance  (paired t-test, p < 0.05) w.r.t. MLE(*) and hier orig( †).
true	D13-1003.pdf#.220†*	New York Times Corpus, P@10%	gmap  Table 1: Ranking quality of extracted facts. Significance  (paired t-test, p < 0.05) w.r.t. MLE(*) and hier orig( †).
true	D13-1003.pdf#.3onheld-outdevelopment	New York Times Corpus, P@10%	deanu et al . , 2012 ) , and use the same reduced set ) . As the From this document collec - Roth et al . We filter out all candidates that are not con - We tune a single  Table 2: TAC Scores on (Surdeanu et al., 2012) queries.
true	D10-1037.pdf#39.2%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F 1  Table 3: The error rate on the multi-aspect sentiment  ranking. We report mean L 1 and L 2 between system  prediction and true values over all aspects. Marked  results are statistically significant with p < 0.05: *  over the previous model and  † over NoCM.
true	D10-1037.pdf#48.6%†*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Recall  Table 3: The error rate on the multi-aspect sentiment  ranking. We report mean L 1 and L 2 between system  prediction and true values over all aspects. Marked  results are statistically significant with p < 0.05: *  over the previous model and  † over NoCM.
true	D10-1037.pdf#32.9%†*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Prec .  Table 3: The error rate on the multi-aspect sentiment  ranking. We report mean L 1 and L 2 between system  prediction and true values over all aspects. Marked  results are statistically significant with p < 0.05: *  over the previous model and  † over NoCM.
true	D10-1037.pdf#44.4%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	2  Table 3: The error rate on the multi-aspect sentiment  ranking. We report mean L 1 and L 2 between system  prediction and true values over all aspects. Marked  results are statistically significant with p < 0.05: *  over the previous model and  † over NoCM.
true	D10-1037.pdf#48.6%†	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Recall  Table 3: The error rate on the multi-aspect sentiment  ranking. We report mean L 1 and L 2 between system  prediction and true values over all aspects. Marked  results are statistically significant with p < 0.05: *  over the previous model and  † over NoCM.
true	D10-1037.pdf#44.4%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	2  Table 4: Results for multi-aspect summarization on  the Yelp corpus. Marked precision and recall are  statistically significant with p < 0.05: * over the  previous model and  † over NoCM.
true	D10-1037.pdf#48.6%†	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Recall  Table 4: Results for multi-aspect summarization on  the Yelp corpus. Marked precision and recall are  statistically significant with p < 0.05: * over the  previous model and  † over NoCM.
true	D10-1037.pdf#39.2%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F 1  Table 4: Results for multi-aspect summarization on  the Yelp corpus. Marked precision and recall are  statistically significant with p < 0.05: * over the  previous model and  † over NoCM.
true	D10-1037.pdf#32.9%†*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Prec .  Table 4: Results for multi-aspect summarization on  the Yelp corpus. Marked precision and recall are  statistically significant with p < 0.05: * over the  previous model and  † over NoCM.
true	D10-1037.pdf#48.6%†*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Recall  Table 4: Results for multi-aspect summarization on  the Yelp corpus. Marked precision and recall are  statistically significant with p < 0.05: * over the  previous model and  † over NoCM.
true	D10-1037.pdf#22.8	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F Binary labels F  Table 5: Results for multi-aspect summarization on the Amazon corpus. Marked ROUGE, precision, and  recall are statistically significant with p < 0.05: * over the previous model and  † over NoCM.
true	D10-1037.pdf#28.2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	30 Binary labels ROUGE  Table 5: Results for multi-aspect summarization on the Amazon corpus. Marked ROUGE, precision, and  recall are statistically significant with p < 0.05: * over the previous model and  † over NoCM.
true	D10-1037.pdf#47.6%†*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Binary labels ROUGE  Table 5: Results for multi-aspect summarization on the Amazon corpus. Marked ROUGE, precision, and  recall are statistically significant with p < 0.05: * over the previous model and  † over NoCM.
true	D10-1037.pdf#57.1%†*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Binary labels Recall  Table 5: Results for multi-aspect summarization on the Amazon corpus. Marked ROUGE, precision, and  recall are statistically significant with p < 0.05: * over the previous model and  † over NoCM.
true	D10-1037.pdf#25.8%†*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Multi - label Prec .  Table 5: Results for multi-aspect summarization on the Amazon corpus. Marked ROUGE, precision, and  recall are statistically significant with p < 0.05: * over the previous model and  † over NoCM.
true	D10-1037.pdf#53.0%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Binary labels 2  Table 5: Results for multi-aspect summarization on the Amazon corpus. Marked ROUGE, precision, and  recall are statistically significant with p < 0.05: * over the previous model and  † over NoCM.
true	D10-1037.pdf#47.8%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Binary labels F 1  Table 5: Results for multi-aspect summarization on the Amazon corpus. Marked ROUGE, precision, and  recall are statistically significant with p < 0.05: * over the previous model and  † over NoCM.
true	D10-1037.pdf#45.3%†*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Binary labels Prec .  Table 5: Results for multi-aspect summarization on the Amazon corpus. Marked ROUGE, precision, and  recall are statistically significant with p < 0.05: * over the previous model and  † over NoCM.
true	D10-1037.pdf#26.0	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	30 Binary labels Recall  Table 5: Results for multi-aspect summarization on the Amazon corpus. Marked ROUGE, precision, and  recall are statistically significant with p < 0.05: * over the previous model and  † over NoCM.
true	D10-1037.pdf#28.2%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Multi - label F 1  Table 5: Results for multi-aspect summarization on the Amazon corpus. Marked ROUGE, precision, and  recall are statistically significant with p < 0.05: * over the previous model and  † over NoCM.
true	D10-1037.pdf#33.7%†*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Multi - label Recall  Table 5: Results for multi-aspect summarization on the Amazon corpus. Marked ROUGE, precision, and  recall are statistically significant with p < 0.05: * over the previous model and  † over NoCM.
true	D10-1037.pdf#31.3%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Multi - label 2  Table 5: Results for multi-aspect summarization on the Amazon corpus. Marked ROUGE, precision, and  recall are statistically significant with p < 0.05: * over the previous model and  † over NoCM.
true	P12-1002.pdf#26.09	WMT 2014 EN-DE, BLEU	1 devtest - nc  Table 2: BLEU-4 results for algorithms 1 (SGD), 2 (MixSGD), 3 (IterMixSDG), and 4 (IterSelSGD) on news- commentary (nc) data. Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and  rule shape (shape). Statistical significance at p-level < 0.05 of a result difference on the test set to a different algo- rithm applied to the same feature group is indicated by raised algorithm number.  † indicates statistically significant  differences to best result across features groups for same algorithm, indicated in bold face. @ indicates the optimal  number of epochs chosen on the devtest set.
true	P12-1002.pdf#25.88	WMT 2014 EN-DE, BLEU	devtest - nc  Table 2: BLEU-4 results for algorithms 1 (SGD), 2 (MixSGD), 3 (IterMixSDG), and 4 (IterSelSGD) on news- commentary (nc) data. Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and  rule shape (shape). Statistical significance at p-level < 0.05 of a result difference on the test set to a different algo- rithm applied to the same feature group is indicated by raised algorithm number.  † indicates statistically significant  differences to best result across features groups for same algorithm, indicated in bold face. @ indicates the optimal  number of epochs chosen on the devtest set.
true	P12-1002.pdf#26.8@8	WMT 2014 EN-DE, BLEU	3 devtest - nc  Table 2: BLEU-4 results for algorithms 1 (SGD), 2 (MixSGD), 3 (IterMixSDG), and 4 (IterSelSGD) on news- commentary (nc) data. Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and  rule shape (shape). Statistical significance at p-level < 0.05 of a result difference on the test set to a different algo- rithm applied to the same feature group is indicated by raised algorithm number.  † indicates statistically significant  differences to best result across features groups for same algorithm, indicated in bold face. @ indicates the optimal  number of epochs chosen on the devtest set.
true	P12-1002.pdf#28.1534	WMT 2014 EN-DE, BLEU	1 test - nc  Table 2: BLEU-4 results for algorithms 1 (SGD), 2 (MixSGD), 3 (IterMixSDG), and 4 (IterSelSGD) on news- commentary (nc) data. Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and  rule shape (shape). Statistical significance at p-level < 0.05 of a result difference on the test set to a different algo- rithm applied to the same feature group is indicated by raised algorithm number.  † indicates statistically significant  differences to best result across features groups for same algorithm, indicated in bold face. @ indicates the optimal  number of epochs chosen on the devtest set.
true	P12-1002.pdf#28.03134	WMT 2014 EN-DE, BLEU	1 test - nc  Table 2: BLEU-4 results for algorithms 1 (SGD), 2 (MixSGD), 3 (IterMixSDG), and 4 (IterSelSGD) on news- commentary (nc) data. Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and  rule shape (shape). Statistical significance at p-level < 0.05 of a result difference on the test set to a different algo- rithm applied to the same feature group is indicated by raised algorithm number.  † indicates statistically significant  differences to best result across features groups for same algorithm, indicated in bold face. @ indicates the optimal  number of epochs chosen on the devtest set.
true	P12-1002.pdf#26.42@9	WMT 2014 EN-DE, BLEU	3 devtest - nc  Table 2: BLEU-4 results for algorithms 1 (SGD), 2 (MixSGD), 3 (IterMixSDG), and 4 (IterSelSGD) on news- commentary (nc) data. Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and  rule shape (shape). Statistical significance at p-level < 0.05 of a result difference on the test set to a different algo- rithm applied to the same feature group is indicated by raised algorithm number.  † indicates statistically significant  differences to best result across features groups for same algorithm, indicated in bold face. @ indicates the optimal  number of epochs chosen on the devtest set.
true	P12-1002.pdf#28.81123	WMT 2014 EN-DE, BLEU	3 test - nc  Table 2: BLEU-4 results for algorithms 1 (SGD), 2 (MixSGD), 3 (IterMixSDG), and 4 (IterSelSGD) on news- commentary (nc) data. Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and  rule shape (shape). Statistical significance at p-level < 0.05 of a result difference on the test set to a different algo- rithm applied to the same feature group is indicated by raised algorithm number.  † indicates statistically significant  differences to best result across features groups for same algorithm, indicated in bold face. @ indicates the optimal  number of epochs chosen on the devtest set.
true	P12-1002.pdf#28.55124	WMT 2014 EN-DE, BLEU	3 test - nc  Table 2: BLEU-4 results for algorithms 1 (SGD), 2 (MixSGD), 3 (IterMixSDG), and 4 (IterSelSGD) on news- commentary (nc) data. Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and  rule shape (shape). Statistical significance at p-level < 0.05 of a result difference on the test set to a different algo- rithm applied to the same feature group is indicated by raised algorithm number.  † indicates statistically significant  differences to best result across features groups for same algorithm, indicated in bold face. @ indicates the optimal  number of epochs chosen on the devtest set.
true	P12-1002.pdf#15.39†	WMT 2014 EN-DE, BLEU	test - crawl10  Table 3: BLEU-4 results for algorithms 1 (SGD) and 4 (IterSelSGD) on Europarl (ep) and news crawl (crawl) test  data. Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and rule shape (shape).  Statistical significance at p-level < 0.05 of a result difference on the test set to a different algorithm applied to the  same feature group is indicated by raised algorithm number.  † indicates statistically significant differences to best  result across features groups for same algorithm, indicated in bold face. @ indicates the optimal number of epochs  chosen on the devtest set.
true	P12-1002.pdf#14.43†	WMT 2014 EN-DE, BLEU	test - crawl11  Table 3: BLEU-4 results for algorithms 1 (SGD) and 4 (IterSelSGD) on Europarl (ep) and news crawl (crawl) test  data. Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and rule shape (shape).  Statistical significance at p-level < 0.05 of a result difference on the test set to a different algorithm applied to the  same feature group is indicated by raised algorithm number.  † indicates statistically significant differences to best  result across features groups for same algorithm, indicated in bold face. @ indicates the optimal number of epochs  chosen on the devtest set.
true	P12-1002.pdf#25.62	WMT 2014 EN-DE, BLEU	devtest - ep  Table 3: BLEU-4 results for algorithms 1 (SGD) and 4 (IterSelSGD) on Europarl (ep) and news crawl (crawl) test  data. Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and rule shape (shape).  Statistical significance at p-level < 0.05 of a result difference on the test set to a different algorithm applied to the  same feature group is indicated by raised algorithm number.  † indicates statistically significant differences to best  result across features groups for same algorithm, indicated in bold face. @ indicates the optimal number of epochs  chosen on the devtest set.
true	P12-1002.pdf#26.42†	WMT 2014 EN-DE, BLEU	test - ep  Table 3: BLEU-4 results for algorithms 1 (SGD) and 4 (IterSelSGD) on Europarl (ep) and news crawl (crawl) test  data. Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and rule shape (shape).  Statistical significance at p-level < 0.05 of a result difference on the test set to a different algorithm applied to the  same feature group is indicated by raised algorithm number.  † indicates statistically significant differences to best  result across features groups for same algorithm, indicated in bold face. @ indicates the optimal number of epochs  chosen on the devtest set.
true	D13-1091.pdf#0.5421	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	CJ  Table 2: Polarity classification accuracies using excellent  and poor as seed words
true	D13-1091.pdf#0.5421	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	CD  Table 2: Polarity classification accuracies using excellent  and poor as seed words
true	D13-1091.pdf#0.5567	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	CD  Table 2: Polarity classification accuracies using excellent  and poor as seed words
true	D13-1091.pdf#0.5493	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	CP  Table 2: Polarity classification accuracies using excellent  and poor as seed words
true	D13-1091.pdf#0.5635	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	CP  Table 2: Polarity classification accuracies using excellent  and poor as seed words
true	D13-1091.pdf#0.5494	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	GD  Table 2: Polarity classification accuracies using excellent  and poor as seed words
true	D13-1091.pdf#0.5567	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	CJ  Table 2: Polarity classification accuracies using excellent  and poor as seed words
true	D13-1091.pdf#0.5635	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	GD  Table 2: Polarity classification accuracies using excellent  and poor as seed words
true	D13-1091.pdf#0.6921	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	CJ  Table 3: Polarity classification accuracies using the seed  word groups
true	D13-1091.pdf#0.6665	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	CJ  Table 3: Polarity classification accuracies using the seed  word groups
true	D13-1091.pdf#0.6921	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	CD  Table 3: Polarity classification accuracies using the seed  word groups
true	D13-1091.pdf#0.6599	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	GD  Table 3: Polarity classification accuracies using the seed  word groups
true	D13-1091.pdf#0.7273	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	CP  Table 3: Polarity classification accuracies using the seed  word groups
true	D13-1091.pdf#0.6383	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	GD  Table 3: Polarity classification accuracies using the seed  word groups
true	D13-1091.pdf#0.7001	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	CP  Table 3: Polarity classification accuracies using the seed  word groups
true	D13-1091.pdf#0.6665	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	CD  Table 3: Polarity classification accuracies using the seed  word groups
true	D15-1223.pdf#0.4421	DUC 2004 Task 1, ROUGE-1	Algorithm DUC ' 02 DUC ' 04 DUC ' 07 DUC ' 02 DUC ' 04 DUC ' 07 performed the other methods on all datasets ( us - the Wilcoxon matched pairs test . DUC 2004 is not statistically significant . Based on this result , document domain . ROUGE - 1 Recall  Table 4: Comparative results.
true	D15-1223.pdf#0.3440	DUC 2004 Task 1, ROUGE-1	Algorithm DUC ' 02 DUC ' 04 DUC ' 07 DUC ' 02 DUC ' 04 DUC ' 07 performed the other methods on all datasets ( us - the Wilcoxon matched pairs test . DUC 2004 is not statistically significant . Based on this result , document domain . ROUGE - 1 Recall  Table 4: Comparative results.
true	D15-1223.pdf#0.1981	DUC 2004 Task 1, ROUGE-1	Algorithm DUC ' 02 DUC ' 04 DUC ' 07 DUC ' 02 DUC ' 04 DUC ' 07 performed the other methods on all datasets ( us - the Wilcoxon matched pairs test . DUC 2004 is not statistically significant . Based on this result , Intuitively , it is a very logical ROUGE - 2 Recall  Table 4: Comparative results.
true	D15-1223.pdf#0.0829	DUC 2004 Task 1, ROUGE-1	Algorithm DUC ' 02 DUC ' 04 DUC ' 07 DUC ' 02 DUC ' 04 DUC ' 07 performed the other methods on all datasets ( us - Based on the DUC 2004 is not statistically significant . we can conclude Intuitively , it is a very logical ROUGE - 2 Recall  Table 4: Comparative results.
true	D15-1223.pdf#0.3959	DUC 2004 Task 1, ROUGE-1	Algorithm DUC ' 02 DUC ' 04 DUC ' 07 DUC ' 02 DUC ' 04 DUC ' 07 performed the other methods on all datasets ( us - the Wilcoxon matched pairs test . DUC 2004 is not statistically significant . Based on this result , Intuitively , it is a very logical ROUGE - 1 Recall  Table 4: Comparative results.
true	D15-1223.pdf#0.0942	DUC 2004 Task 1, ROUGE-1	Algorithm DUC ' 02 DUC ' 04 DUC ' 07 DUC ' 02 DUC ' 04 DUC ' 07 performed the other methods on all datasets ( us - Based on the On the we can conclude Intuitively , it is a very logical ROUGE - 2 Recall  Table 4: Comparative results.
true	D15-1223.pdf#0.4421	DUC 2004 Task 1, ROUGE-L	Algorithm DUC ' 02 DUC ' 04 DUC ' 07 DUC ' 02 DUC ' 04 DUC ' 07 performed the other methods on all datasets ( us - the Wilcoxon matched pairs test . DUC 2004 is not statistically significant . Based on this result , document domain . ROUGE - 1 Recall  Table 4: Comparative results.
true	D15-1223.pdf#0.3440	DUC 2004 Task 1, ROUGE-L	Algorithm DUC ' 02 DUC ' 04 DUC ' 07 DUC ' 02 DUC ' 04 DUC ' 07 performed the other methods on all datasets ( us - the Wilcoxon matched pairs test . DUC 2004 is not statistically significant . Based on this result , document domain . ROUGE - 1 Recall  Table 4: Comparative results.
true	D15-1223.pdf#0.1981	DUC 2004 Task 1, ROUGE-L	Algorithm DUC ' 02 DUC ' 04 DUC ' 07 DUC ' 02 DUC ' 04 DUC ' 07 performed the other methods on all datasets ( us - the Wilcoxon matched pairs test . DUC 2004 is not statistically significant . Based on this result , Intuitively , it is a very logical ROUGE - 2 Recall  Table 4: Comparative results.
true	D15-1223.pdf#0.0829	DUC 2004 Task 1, ROUGE-L	Algorithm DUC ' 02 DUC ' 04 DUC ' 07 DUC ' 02 DUC ' 04 DUC ' 07 performed the other methods on all datasets ( us - Based on the DUC 2004 is not statistically significant . we can conclude Intuitively , it is a very logical ROUGE - 2 Recall  Table 4: Comparative results.
true	D15-1223.pdf#0.3959	DUC 2004 Task 1, ROUGE-L	Algorithm DUC ' 02 DUC ' 04 DUC ' 07 DUC ' 02 DUC ' 04 DUC ' 07 performed the other methods on all datasets ( us - the Wilcoxon matched pairs test . DUC 2004 is not statistically significant . Based on this result , Intuitively , it is a very logical ROUGE - 1 Recall  Table 4: Comparative results.
true	D15-1223.pdf#0.0942	DUC 2004 Task 1, ROUGE-L	Algorithm DUC ' 02 DUC ' 04 DUC ' 07 DUC ' 02 DUC ' 04 DUC ' 07 performed the other methods on all datasets ( us - Based on the On the we can conclude Intuitively , it is a very logical ROUGE - 2 Recall  Table 4: Comparative results.
true	D15-1223.pdf#0.4421	DUC 2004 Task 1, ROUGE-2	Algorithm DUC ' 02 DUC ' 04 DUC ' 07 DUC ' 02 DUC ' 04 DUC ' 07 performed the other methods on all datasets ( us - the Wilcoxon matched pairs test . DUC 2004 is not statistically significant . Based on this result , document domain . ROUGE - 1 Recall  Table 4: Comparative results.
true	D15-1223.pdf#0.3440	DUC 2004 Task 1, ROUGE-2	Algorithm DUC ' 02 DUC ' 04 DUC ' 07 DUC ' 02 DUC ' 04 DUC ' 07 performed the other methods on all datasets ( us - the Wilcoxon matched pairs test . DUC 2004 is not statistically significant . Based on this result , document domain . ROUGE - 1 Recall  Table 4: Comparative results.
true	D15-1223.pdf#0.1981	DUC 2004 Task 1, ROUGE-2	Algorithm DUC ' 02 DUC ' 04 DUC ' 07 DUC ' 02 DUC ' 04 DUC ' 07 performed the other methods on all datasets ( us - the Wilcoxon matched pairs test . DUC 2004 is not statistically significant . Based on this result , Intuitively , it is a very logical ROUGE - 2 Recall  Table 4: Comparative results.
true	D15-1223.pdf#0.0829	DUC 2004 Task 1, ROUGE-2	Algorithm DUC ' 02 DUC ' 04 DUC ' 07 DUC ' 02 DUC ' 04 DUC ' 07 performed the other methods on all datasets ( us - Based on the DUC 2004 is not statistically significant . we can conclude Intuitively , it is a very logical ROUGE - 2 Recall  Table 4: Comparative results.
true	D15-1223.pdf#0.3959	DUC 2004 Task 1, ROUGE-2	Algorithm DUC ' 02 DUC ' 04 DUC ' 07 DUC ' 02 DUC ' 04 DUC ' 07 performed the other methods on all datasets ( us - the Wilcoxon matched pairs test . DUC 2004 is not statistically significant . Based on this result , Intuitively , it is a very logical ROUGE - 1 Recall  Table 4: Comparative results.
true	D15-1223.pdf#0.0942	DUC 2004 Task 1, ROUGE-2	Algorithm DUC ' 02 DUC ' 04 DUC ' 07 DUC ' 02 DUC ' 04 DUC ' 07 performed the other methods on all datasets ( us - Based on the On the we can conclude Intuitively , it is a very logical ROUGE - 2 Recall  Table 4: Comparative results.
true	P14-1111.pdf#5	SemEval 2013, F1	Possible values all word forms lower case word form 5 - char word form prefixes True , False top - 800 word form  Table 1: Word and edge properties in templates.
true	P14-1111.pdf#87.35	SemEval 2013, F1	Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#80.32	SemEval 2013, F1	ca  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#71.21	SemEval 2013, F1	es -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#72.48	SemEval 2013, F1	Avg . -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#80.46	SemEval 2013, F1	es  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#79.50	SemEval 2013, F1	de  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#76.16	SemEval 2013, F1	en -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#84.98	SemEval 2013, F1	Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#81.03	SemEval 2013, F1	cs -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#76.32	SemEval 2013, F1	zh -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#85.15	SemEval 2013, F1	ca  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#85.63	SemEval 2013, F1	en  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#84.40	SemEval 2013, F1	es  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#87.65	SemEval 2013, F1	Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#78.60	SemEval 2013, F1	zh  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#67.26	SemEval 2013, F1	de -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#71.55	SemEval 2013, F1	ca -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#86.54	SemEval 2013, F1	Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#85.41	SemEval 2013, F1	cs  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#79.71	SemEval 2013, F1	de  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#81.55	SemEval 2013, F1	Avg .  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#64.4	SemEval 2013, F1	Supervision cs WSJ ∞  Table 7: Unlabeled directed dependency accuracy  on CoNLL'09 test set in low-resource settings.  DMV models are trained on either POS tags (pos)  or Brown clusters (bc). *Indicates the supervised parser
true	P14-1111.pdf#5	Senseval 2, F1	Possible values all word forms lower case word form 5 - char word form prefixes True , False top - 800 word form  Table 1: Word and edge properties in templates.
true	P14-1111.pdf#87.35	Senseval 2, F1	Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#80.32	Senseval 2, F1	ca  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#71.21	Senseval 2, F1	es -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#72.48	Senseval 2, F1	Avg . -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#80.46	Senseval 2, F1	es  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#79.50	Senseval 2, F1	de  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#76.16	Senseval 2, F1	en -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#84.98	Senseval 2, F1	Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#81.03	Senseval 2, F1	cs -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#76.32	Senseval 2, F1	zh -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#85.15	Senseval 2, F1	ca  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#85.63	Senseval 2, F1	en  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#84.40	Senseval 2, F1	es  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#87.65	Senseval 2, F1	Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#78.60	Senseval 2, F1	zh  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#67.26	Senseval 2, F1	de -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#71.55	Senseval 2, F1	ca -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#86.54	Senseval 2, F1	Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#85.41	Senseval 2, F1	cs  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#79.71	Senseval 2, F1	de  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#81.55	Senseval 2, F1	Avg .  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#64.4	Senseval 2, F1	Supervision cs WSJ ∞  Table 7: Unlabeled directed dependency accuracy  on CoNLL'09 test set in low-resource settings.  DMV models are trained on either POS tags (pos)  or Brown clusters (bc). *Indicates the supervised parser
true	P14-1111.pdf#5	Senseval 3, F1	Possible values all word forms lower case word form 5 - char word form prefixes True , False top - 800 word form  Table 1: Word and edge properties in templates.
true	P14-1111.pdf#87.35	Senseval 3, F1	Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#80.32	Senseval 3, F1	ca  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#71.21	Senseval 3, F1	es -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#72.48	Senseval 3, F1	Avg . -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#80.46	Senseval 3, F1	es  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#79.50	Senseval 3, F1	de  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#76.16	Senseval 3, F1	en -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#84.98	Senseval 3, F1	Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#81.03	Senseval 3, F1	cs -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#76.32	Senseval 3, F1	zh -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#85.15	Senseval 3, F1	ca  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#85.63	Senseval 3, F1	en  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#84.40	Senseval 3, F1	es  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#87.65	Senseval 3, F1	Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#78.60	Senseval 3, F1	zh  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#67.26	Senseval 3, F1	de -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#71.55	Senseval 3, F1	ca -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#86.54	Senseval 3, F1	Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#85.41	Senseval 3, F1	cs  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#79.71	Senseval 3, F1	de  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#81.55	Senseval 3, F1	Avg .  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#64.4	Senseval 3, F1	Supervision cs WSJ ∞  Table 7: Unlabeled directed dependency accuracy  on CoNLL'09 test set in low-resource settings.  DMV models are trained on either POS tags (pos)  or Brown clusters (bc). *Indicates the supervised parser
true	P14-1111.pdf#5	SemEval 2007, F1	Possible values all word forms lower case word form 5 - char word form prefixes True , False top - 800 word form  Table 1: Word and edge properties in templates.
true	P14-1111.pdf#87.35	SemEval 2007, F1	Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#80.32	SemEval 2007, F1	ca  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#71.21	SemEval 2007, F1	es -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#72.48	SemEval 2007, F1	Avg . -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#80.46	SemEval 2007, F1	es  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#79.50	SemEval 2007, F1	de  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#76.16	SemEval 2007, F1	en -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#84.98	SemEval 2007, F1	Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#81.03	SemEval 2007, F1	cs -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#76.32	SemEval 2007, F1	zh -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#85.15	SemEval 2007, F1	ca  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#85.63	SemEval 2007, F1	en  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#84.40	SemEval 2007, F1	es  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#87.65	SemEval 2007, F1	Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#78.60	SemEval 2007, F1	zh  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#67.26	SemEval 2007, F1	de -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#71.55	SemEval 2007, F1	ca -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#86.54	SemEval 2007, F1	Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#85.41	SemEval 2007, F1	cs  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#79.71	SemEval 2007, F1	de  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#81.55	SemEval 2007, F1	Avg .  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#64.4	SemEval 2007, F1	Supervision cs WSJ ∞  Table 7: Unlabeled directed dependency accuracy  on CoNLL'09 test set in low-resource settings.  DMV models are trained on either POS tags (pos)  or Brown clusters (bc). *Indicates the supervised parser
true	P14-1111.pdf#5	SemEval 2015, F1	Possible values all word forms lower case word form 5 - char word form prefixes True , False top - 800 word form  Table 1: Word and edge properties in templates.
true	P14-1111.pdf#87.35	SemEval 2015, F1	Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#80.32	SemEval 2015, F1	ca  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#71.21	SemEval 2015, F1	es -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#72.48	SemEval 2015, F1	Avg . -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#80.46	SemEval 2015, F1	es  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#79.50	SemEval 2015, F1	de  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#76.16	SemEval 2015, F1	en -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#84.98	SemEval 2015, F1	Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#81.03	SemEval 2015, F1	cs -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#76.32	SemEval 2015, F1	zh -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#85.15	SemEval 2015, F1	ca  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#85.63	SemEval 2015, F1	en  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#84.40	SemEval 2015, F1	es  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#87.65	SemEval 2015, F1	Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#78.60	SemEval 2015, F1	zh  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#67.26	SemEval 2015, F1	de -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#71.55	SemEval 2015, F1	ca -  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#86.54	SemEval 2015, F1	Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#85.41	SemEval 2015, F1	cs  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#79.71	SemEval 2015, F1	de  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#81.55	SemEval 2015, F1	Avg .  Table 4: Test F1 for SRL and sense disambiguation on CoNLL'09 in high-resource and low-resource  settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are  ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
true	P14-1111.pdf#64.4	SemEval 2015, F1	Supervision cs WSJ ∞  Table 7: Unlabeled directed dependency accuracy  on CoNLL'09 test set in low-resource settings.  DMV models are trained on either POS tags (pos)  or Brown clusters (bc). *Indicates the supervised parser
true	N15-1129.pdf#17.7	WMT 2014 EN-FR, BLEU	Bleu WMT10  Table 2: F-and Bleu scores for Czech-English via  French. The joint training method outperforms all other  methods tested.
true	N15-1129.pdf#17.2	WMT 2014 EN-FR, BLEU	Bleu WMT09  Table 2: F-and Bleu scores for Czech-English via  French. The joint training method outperforms all other  methods tested.
true	N15-1129.pdf#70.1	WMT 2014 EN-FR, BLEU	F dev  Table 2: F-and Bleu scores for Czech-English via  French. The joint training method outperforms all other  methods tested.
true	N15-1129.pdf#17.7	WMT 2014 EN-DE, BLEU	Bleu WMT10  Table 2: F-and Bleu scores for Czech-English via  French. The joint training method outperforms all other  methods tested.
true	N15-1129.pdf#17.2	WMT 2014 EN-DE, BLEU	Bleu WMT09  Table 2: F-and Bleu scores for Czech-English via  French. The joint training method outperforms all other  methods tested.
true	N15-1129.pdf#70.1	WMT 2014 EN-DE, BLEU	F dev  Table 2: F-and Bleu scores for Czech-English via  French. The joint training method outperforms all other  methods tested.
true	P14-3010.pdf#88.4%	Senseval 3, F1	run . Table 3 shows the Accuracy  Table 3 shows the  number, percentage, and accuracy of decisions by  step in the classification procedure for the ctxt quad  run.
true	P14-3010.pdf#88.4%	Senseval 3, F1	ison . Table 3 shows the Accuracy Accuracy  Table 3 shows the  number, percentage, and accuracy of decisions by  step in the classification procedure for the ctxt quad  run.
true	P14-3010.pdf#88.4%	Senseval 3, F1	Accuracy  Table 2: Leading PP-attachment disambiguation  systems.
true	P14-3010.pdf#88.4%	SemEval 2013, F1	run . Table 3 shows the Accuracy  Table 3 shows the  number, percentage, and accuracy of decisions by  step in the classification procedure for the ctxt quad  run.
true	P14-3010.pdf#88.4%	SemEval 2013, F1	ison . Table 3 shows the Accuracy Accuracy  Table 3 shows the  number, percentage, and accuracy of decisions by  step in the classification procedure for the ctxt quad  run.
true	P14-3010.pdf#88.4%	SemEval 2013, F1	Accuracy  Table 2: Leading PP-attachment disambiguation  systems.
true	P14-3010.pdf#88.4%	Senseval 2, F1	run . Table 3 shows the Accuracy  Table 3 shows the  number, percentage, and accuracy of decisions by  step in the classification procedure for the ctxt quad  run.
true	P14-3010.pdf#88.4%	Senseval 2, F1	ison . Table 3 shows the Accuracy Accuracy  Table 3 shows the  number, percentage, and accuracy of decisions by  step in the classification procedure for the ctxt quad  run.
true	P14-3010.pdf#88.4%	Senseval 2, F1	Accuracy  Table 2: Leading PP-attachment disambiguation  systems.
true	P14-3010.pdf#88.4%	SemEval 2015, F1	run . Table 3 shows the Accuracy  Table 3 shows the  number, percentage, and accuracy of decisions by  step in the classification procedure for the ctxt quad  run.
true	P14-3010.pdf#88.4%	SemEval 2015, F1	ison . Table 3 shows the Accuracy Accuracy  Table 3 shows the  number, percentage, and accuracy of decisions by  step in the classification procedure for the ctxt quad  run.
true	P14-3010.pdf#88.4%	SemEval 2015, F1	Accuracy  Table 2: Leading PP-attachment disambiguation  systems.
true	P14-3010.pdf#88.4%	SemEval 2007, F1	run . Table 3 shows the Accuracy  Table 3 shows the  number, percentage, and accuracy of decisions by  step in the classification procedure for the ctxt quad  run.
true	P14-3010.pdf#88.4%	SemEval 2007, F1	ison . Table 3 shows the Accuracy Accuracy  Table 3 shows the  number, percentage, and accuracy of decisions by  step in the classification procedure for the ctxt quad  run.
true	P14-3010.pdf#88.4%	SemEval 2007, F1	Accuracy  Table 2: Leading PP-attachment disambiguation  systems.
true	P15-1121.pdf#78.45	New York Times Corpus, P@10%	55 MC160 dev Single  Table 3: Accuracy (%) of the different baselines (in italics) and our models. Single: questions requiring  single sentence to answer; Multi: questions requiring multiple sentences to answer. Sentence window  (k) = 4 for models 2 and 3. Best scores are shown in bold. Statistical significance (shown only for All  columns) of p < 0.05 using two-tailed paired t-test:  *  vs SWD,  † vs RTE.
true	P15-1121.pdf#73.23*†	New York Times Corpus, P@10%	55 MC160 test All  Table 3: Accuracy (%) of the different baselines (in italics) and our models. Single: questions requiring  single sentence to answer; Multi: questions requiring multiple sentences to answer. Sentence window  (k) = 4 for models 2 and 3. Best scores are shown in bold. Statistical significance (shown only for All  columns) of p < 0.05 using two-tailed paired t-test:  *  vs SWD,  † vs RTE.
true	P15-1121.pdf#74.41	New York Times Corpus, P@10%	55 MC500 dev Single  Table 3: Accuracy (%) of the different baselines (in italics) and our models. Single: questions requiring  single sentence to answer; Multi: questions requiring multiple sentences to answer. Sentence window  (k) = 4 for models 2 and 3. Best scores are shown in bold. Statistical significance (shown only for All  columns) of p < 0.05 using two-tailed paired t-test:  *  vs SWD,  † vs RTE.
true	P15-1121.pdf#59.9	New York Times Corpus, P@10%	55 MC500 test Multi  Table 3: Accuracy (%) of the different baselines (in italics) and our models. Single: questions requiring  single sentence to answer; Multi: questions requiring multiple sentences to answer. Sentence window  (k) = 4 for models 2 and 3. Best scores are shown in bold. Statistical significance (shown only for All  columns) of p < 0.05 using two-tailed paired t-test:  *  vs SWD,  † vs RTE.
true	P15-1121.pdf#65.23	New York Times Corpus, P@10%	55 MC160 test Multi  Table 3: Accuracy (%) of the different baselines (in italics) and our models. Single: questions requiring  single sentence to answer; Multi: questions requiring multiple sentences to answer. Sentence window  (k) = 4 for models 2 and 3. Best scores are shown in bold. Statistical significance (shown only for All  columns) of p < 0.05 using two-tailed paired t-test:  *  vs SWD,  † vs RTE.
true	P15-1121.pdf#70.58	New York Times Corpus, P@10%	55 MC500 test Single  Table 3: Accuracy (%) of the different baselines (in italics) and our models. Single: questions requiring  single sentence to answer; Multi: questions requiring multiple sentences to answer. Sentence window  (k) = 4 for models 2 and 3. Best scores are shown in bold. Statistical significance (shown only for All  columns) of p < 0.05 using two-tailed paired t-test:  *  vs SWD,  † vs RTE.
true	P15-1121.pdf#83.25	New York Times Corpus, P@10%	55 MC160 test Single  Table 3: Accuracy (%) of the different baselines (in italics) and our models. Single: questions requiring  single sentence to answer; Multi: questions requiring multiple sentences to answer. Sentence window  (k) = 4 for models 2 and 3. Best scores are shown in bold. Statistical significance (shown only for All  columns) of p < 0.05 using two-tailed paired t-test:  *  vs SWD,  † vs RTE.
true	P15-1121.pdf#68.47	New York Times Corpus, P@10%	55 MC160 dev All  Table 3: Accuracy (%) of the different baselines (in italics) and our models. Single: questions requiring  single sentence to answer; Multi: questions requiring multiple sentences to answer. Sentence window  (k) = 4 for models 2 and 3. Best scores are shown in bold. Statistical significance (shown only for All  columns) of p < 0.05 using two-tailed paired t-test:  *  vs SWD,  † vs RTE.
true	P15-1121.pdf#60.57	New York Times Corpus, P@10%	55 MC160 dev Multi  Table 3: Accuracy (%) of the different baselines (in italics) and our models. Single: questions requiring  single sentence to answer; Multi: questions requiring multiple sentences to answer. Sentence window  (k) = 4 for models 2 and 3. Best scores are shown in bold. Statistical significance (shown only for All  columns) of p < 0.05 using two-tailed paired t-test:  *  vs SWD,  † vs RTE.
true	P15-1121.pdf#63.75†	New York Times Corpus, P@10%	55 MC500 test All  Table 3: Accuracy (%) of the different baselines (in italics) and our models. Single: questions requiring  single sentence to answer; Multi: questions requiring multiple sentences to answer. Sentence window  (k) = 4 for models 2 and 3. Best scores are shown in bold. Statistical significance (shown only for All  columns) of p < 0.05 using two-tailed paired t-test:  *  vs SWD,  † vs RTE.
true	P15-1121.pdf#66.5*†	New York Times Corpus, P@10%	55 MC500 dev All  Table 3: Accuracy (%) of the different baselines (in italics) and our models. Single: questions requiring  single sentence to answer; Multi: questions requiring multiple sentences to answer. Sentence window  (k) = 4 for models 2 and 3. Best scores are shown in bold. Statistical significance (shown only for All  columns) of p < 0.05 using two-tailed paired t-test:  *  vs SWD,  † vs RTE.
true	P15-1121.pdf#59.9	New York Times Corpus, P@10%	55 MC500 test Multi  Table 3: Accuracy (%) of the different baselines (in italics) and our models. Single: questions requiring  single sentence to answer; Multi: questions requiring multiple sentences to answer. Sentence window  (k) = 4 for models 2 and 3. Best scores are shown in bold. Statistical significance (shown only for All  columns) of p < 0.05 using two-tailed paired t-test:  *  vs SWD,  † vs RTE.
true	P15-1121.pdf#61.4	New York Times Corpus, P@10%	55 MC500 dev Multi  Table 3: Accuracy (%) of the different baselines (in italics) and our models. Single: questions requiring  single sentence to answer; Multi: questions requiring multiple sentences to answer. Sentence window  (k) = 4 for models 2 and 3. Best scores are shown in bold. Statistical significance (shown only for All  columns) of p < 0.05 using two-tailed paired t-test:  *  vs SWD,  † vs RTE.
true	P15-1121.pdf#63.06	New York Times Corpus, P@10%	R @ 5 M3  Table 5: Recall (%) of relevant sentence(s) in the ranking by models 1, 2 and 3 compared with a match- frequency baseline (Freq) at various thresholds, for different question types in MC160. Question fre- quencies are in parentheses. Bold numbers represent best scores.
true	P15-1121.pdf#47.72	New York Times Corpus, P@10%	R @ 2 M2  Table 5: Recall (%) of relevant sentence(s) in the ranking by models 1, 2 and 3 compared with a match- frequency baseline (Freq) at various thresholds, for different question types in MC160. Question fre- quencies are in parentheses. Bold numbers represent best scores.
true	P15-1121.pdf#37.5	New York Times Corpus, P@10%	R @ 1 M1  Table 5: Recall (%) of relevant sentence(s) in the ranking by models 1, 2 and 3 compared with a match- frequency baseline (Freq) at various thresholds, for different question types in MC160. Question fre- quencies are in parentheses. Bold numbers represent best scores.
true	N12-1003.pdf#0.01level.	New York Times Corpus, P@10%	in tokens of sequences of one or more words labeled as Len indicates F 1 scores that are statistically  Table 1: Performance on the held-out test set, in terms of  precision (P), recall (R), F 1 measure, and average length  in tokens of sequences of one or more words labeled as  shell (Len).  *  indicates F 1 scores that are statistically  reliably different from CRF m+R at the p < 0.01 level.
true	P15-1008.pdf#0.521	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - VIDEO Rec . QUIZ - SUBMISSION Prec  Table 7: Representative rules from PSL-Joint model
true	P15-1008.pdf#0.550	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - SUBTITLES Prec . QUIZ - GRADING F1  Table 7: Representative rules from PSL-Joint model
true	P15-1008.pdf#0.410	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - CONTENT F1  Table 7: Representative rules from PSL-Joint model
true	P15-1008.pdf#0.461	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - SUBTITLES F1  Table 7: Representative rules from PSL-Joint model
true	P15-1008.pdf#0.36	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - VIDEO Prec . QUIZ - CONTENT F1  Table 7: Representative rules from PSL-Joint model
true	P15-1008.pdf#0.323	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - LECTURER F1  Table 7: Representative rules from PSL-Joint model
true	P15-1008.pdf#0.623	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - LECTURER Rec .  Table 7: Representative rules from PSL-Joint model
true	P15-1008.pdf#0.611	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - LECTURER Prec . QUIZ - DEADLINES F1  Table 7: Representative rules from PSL-Joint model
true	P15-1008.pdf#0.411	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - VIDEO Prec .  Table 7: Representative rules from PSL-Joint model
true	P15-1008.pdf#0.407	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - SUBTITLES Prec .  Table 7: Representative rules from PSL-Joint model
true	P15-1008.pdf#0.407	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - CONTENT Prec .  Table 7: Representative rules from PSL-Joint model
true	P15-1008.pdf#0.563	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - AUDIO F1 QUIZ - DEADLINES Rec .  Table 7: Representative rules from PSL-Joint model
true	P15-1008.pdf#0.572	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - LECTURER Rec . QUIZ - GRADING Prec .  Table 7: Representative rules from PSL-Joint model
true	P15-1008.pdf#0.591	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - VIDEO Rec .  Table 7: Representative rules from PSL-Joint model
true	P15-1008.pdf#0.485	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - VIDEO F1  Table 7: Representative rules from PSL-Joint model
true	P15-1008.pdf#0.405	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - CONTENT F1 QUIZ - CONTENT Rec .  Table 7: Representative rules from PSL-Joint model
true	P15-1008.pdf#0.531	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - LECTURER F1 QUIZ - GRADING Rec .  Table 7: Representative rules from PSL-Joint model
true	P15-1008.pdf#0.667	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - AUDIO Rec . QUIZ - DEADLINES Prec .  Table 7: Representative rules from PSL-Joint model
true	P15-1008.pdf#0.413	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - CONTENT Rec .  Table 7: Representative rules from PSL-Joint model
true	P15-1008.pdf#0.324	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - CONTENT Rec . QUIZ - CONTENT Prec  Table 7: Representative rules from PSL-Joint model
true	P15-1008.pdf#0.218	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - LECTURER Prec .  Table 7: Representative rules from PSL-Joint model
true	P15-1008.pdf#0.521	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - VIDEO Rec . QUIZ - SUBMISSION Prec  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.411	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - VIDEO Prec .  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.407	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - CONTENT Prec .  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.405	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - CONTENT F1 QUIZ - CONTENT Rec .  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.611	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - LECTURER Prec . QUIZ - DEADLINES F1  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.688	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 9 : Precision , recall and F1 scores for QUIZ fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - VIDEO F1 QUIZ - SUBMISSION Rec . QUIZ Rec .  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.667	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - AUDIO Rec . QUIZ - DEADLINES Prec .  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.413	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - CONTENT Rec .  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.659	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 9 : Precision , recall and F1 scores for QUIZ fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - SUBTITLES Prec . QUIZ - GRADING F1 SOCIAL F1  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.530	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 9 : Precision , recall and F1 scores for QUIZ fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - LECTURER F1 QUIZ - GRADING Rec . SOCIAL Rec .  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.410	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - CONTENT F1  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.218	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - LECTURER Prec .  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.550	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - SUBTITLES Prec . QUIZ - GRADING F1  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.563	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - AUDIO F1 QUIZ - DEADLINES Rec .  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.621	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 9 : Precision , recall and F1 scores for QUIZ fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - LECTURER Prec . QUIZ - DEADLINES F1 CERTIFICATE F1  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.323	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - LECTURER F1  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.36	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - VIDEO Prec . QUIZ - CONTENT F1  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.715	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 9 : Precision , recall and F1 scores for QUIZ fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - CONTENT F1 QUIZ - CONTENT Rec . LECTURE Rec .  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.623	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - LECTURER Rec .  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.324	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - CONTENT Rec . QUIZ - CONTENT Prec  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.407	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - SUBTITLES Prec .  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.485	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - VIDEO F1  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.706	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 9 : Precision , recall and F1 scores for QUIZ fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - AUDIO Prec . QUIZ - SUBMISSION F1 QUIZ F1  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.552	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 9 : Precision , recall and F1 scores for QUIZ fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - AUDIO Rec . QUIZ - DEADLINES Prec . CERTIFICATE Prec .  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.591	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - VIDEO Rec .  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.572	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - LECTURER Rec . QUIZ - GRADING Prec .  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.461	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 7 : Representative rules from PSL - Joint model LECTURE - SUBTITLES F1  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.531	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects Table 7 : Representative rules from PSL - Joint model LECTURE - LECTURER F1 QUIZ - GRADING Rec .  Table 8: Precision, recall and F1 scores for LECTURE fine aspects
true	P15-1008.pdf#0.611	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects QUIZ - DEADLINES F1  Table 9: Precision, recall and F1 scores for QUIZ fine aspects
true	P15-1008.pdf#0.530	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 9 : Precision , recall and F1 scores for QUIZ fine aspects Table 8 : Precision , recall and F1 scores for LECTURE fine aspects QUIZ - GRADING Rec . SOCIAL Rec .  Table 9: Precision, recall and F1 scores for QUIZ fine aspects
true	P15-1008.pdf#0.531	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects QUIZ - GRADING Rec .  Table 9: Precision, recall and F1 scores for QUIZ fine aspects
true	P15-1008.pdf#0.563	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects QUIZ - DEADLINES Rec .  Table 9: Precision, recall and F1 scores for QUIZ fine aspects
true	P15-1008.pdf#0.667	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects QUIZ - DEADLINES Prec .  Table 9: Precision, recall and F1 scores for QUIZ fine aspects
true	P15-1008.pdf#0.324	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects QUIZ - CONTENT Prec  Table 9: Precision, recall and F1 scores for QUIZ fine aspects
true	P15-1008.pdf#0.521	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects QUIZ - SUBMISSION Prec  Table 9: Precision, recall and F1 scores for QUIZ fine aspects
true	P15-1008.pdf#0.621	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 9 : Precision , recall and F1 scores for QUIZ fine aspects Table 8 : Precision , recall and F1 scores for LECTURE fine aspects QUIZ - DEADLINES F1 CERTIFICATE F1  Table 9: Precision, recall and F1 scores for QUIZ fine aspects
true	P15-1008.pdf#0.36	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects QUIZ - CONTENT F1  Table 9: Precision, recall and F1 scores for QUIZ fine aspects
true	P15-1008.pdf#0.688	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 9 : Precision , recall and F1 scores for QUIZ fine aspects Table 8 : Precision , recall and F1 scores for LECTURE fine aspects QUIZ - SUBMISSION Rec . QUIZ Rec .  Table 9: Precision, recall and F1 scores for QUIZ fine aspects
true	P15-1008.pdf#0.405	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects QUIZ - CONTENT Rec .  Table 9: Precision, recall and F1 scores for QUIZ fine aspects
true	P15-1008.pdf#0.550	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects QUIZ - GRADING F1  Table 9: Precision, recall and F1 scores for QUIZ fine aspects
true	P15-1008.pdf#0.572	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 8 : Precision , recall and F1 scores for LECTURE fine aspects QUIZ - GRADING Prec .  Table 9: Precision, recall and F1 scores for QUIZ fine aspects
true	P15-1008.pdf#0.715	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 9 : Precision , recall and F1 scores for QUIZ fine aspects Table 8 : Precision , recall and F1 scores for LECTURE fine aspects QUIZ - CONTENT Rec . LECTURE Rec .  Table 9: Precision, recall and F1 scores for QUIZ fine aspects
true	P15-1008.pdf#0.706	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 9 : Precision , recall and F1 scores for QUIZ fine aspects Table 8 : Precision , recall and F1 scores for LECTURE fine aspects QUIZ - SUBMISSION F1 QUIZ F1  Table 9: Precision, recall and F1 scores for QUIZ fine aspects
true	P15-1008.pdf#0.659	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 9 : Precision , recall and F1 scores for QUIZ fine aspects Table 8 : Precision , recall and F1 scores for LECTURE fine aspects QUIZ - GRADING F1 SOCIAL F1  Table 9: Precision, recall and F1 scores for QUIZ fine aspects
true	P15-1008.pdf#0.552	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 9 : Precision , recall and F1 scores for QUIZ fine aspects Table 8 : Precision , recall and F1 scores for LECTURE fine aspects QUIZ - DEADLINES Prec . CERTIFICATE Prec .  Table 9: Precision, recall and F1 scores for QUIZ fine aspects
true	P15-1008.pdf#0.621	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 9 : Precision , recall and F1 scores for QUIZ fine aspects CERTIFICATE F1  Table 10: Precision, recall and F1 scores for coarse aspects
true	P15-1008.pdf#0.688	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 9 : Precision , recall and F1 scores for QUIZ fine aspects QUIZ Rec .  Table 10: Precision, recall and F1 scores for coarse aspects
true	P15-1008.pdf#0.552	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 9 : Precision , recall and F1 scores for QUIZ fine aspects CERTIFICATE Prec .  Table 10: Precision, recall and F1 scores for coarse aspects
true	P15-1008.pdf#0.715	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 9 : Precision , recall and F1 scores for QUIZ fine aspects LECTURE Rec .  Table 10: Precision, recall and F1 scores for coarse aspects
true	P15-1008.pdf#0.530	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 9 : Precision , recall and F1 scores for QUIZ fine aspects SOCIAL Rec .  Table 10: Precision, recall and F1 scores for coarse aspects
true	P15-1008.pdf#0.706	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 9 : Precision , recall and F1 scores for QUIZ fine aspects QUIZ F1  Table 10: Precision, recall and F1 scores for coarse aspects
true	P15-1008.pdf#0.659	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 9 : Precision , recall and F1 scores for QUIZ fine aspects SOCIAL F1  Table 10: Precision, recall and F1 scores for coarse aspects
true	P15-1008.pdf#0.114	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 10 : Precision , recall and F1 scores for coarse aspects POSITIVE Prec  Table 11: Precision, recall and F1 scores for sentiment
true	P15-1008.pdf#0.666	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 10 : Precision , recall and F1 scores for coarse aspects NEGATIVE Rec .  Table 11: Precision, recall and F1 scores for sentiment
true	P15-1008.pdf#0.189	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 10 : Precision , recall and F1 scores for coarse aspects POSITIVE F1  Table 11: Precision, recall and F1 scores for sentiment
true	P15-1008.pdf#0.434	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 10 : Precision , recall and F1 scores for coarse aspects NEUTRAL F1  Table 11: Precision, recall and F1 scores for sentiment
true	P15-1008.pdf#0.664	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 10 : Precision , recall and F1 scores for coarse aspects NEUTRAL Prec .  Table 11: Precision, recall and F1 scores for sentiment
true	P15-1008.pdf#0.615	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 10 : Precision , recall and F1 scores for coarse aspects NEGATIVE F1  Table 11: Precision, recall and F1 scores for sentiment
true	P15-1008.pdf#0.322	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 10 : Precision , recall and F1 scores for coarse aspects NEUTRAL Rec .  Table 11: Precision, recall and F1 scores for sentiment
true	P10-2050.pdf#38.4	SearchQA, Unigram Acc	Table 2 : Performance of Opinion Extraction with Correct Polarity Attribute Positive f ( % ) High f ( % )  Table 2: Performance of Opinion Extraction with Correct Polarity Attribute
true	P10-2050.pdf#40.9	SearchQA, Unigram Acc	Table 2 : Performance of Opinion Extraction with Correct Polarity Attribute Neutral f ( % ) Medium f ( % )  Table 2: Performance of Opinion Extraction with Correct Polarity Attribute
true	P10-2050.pdf#43.1	SearchQA, Unigram Acc	Positive f ( % )  Table 2: Performance of Opinion Extraction with Correct Polarity Attribute
true	P10-2050.pdf#43.1	SearchQA, Unigram Acc	Neutral f ( % )  Table 2: Performance of Opinion Extraction with Correct Polarity Attribute
true	P10-2050.pdf#30.7	SearchQA, Unigram Acc	Table 2 : Performance of Opinion Extraction with Correct Polarity Attribute Negative f ( % ) Low f ( % )  Table 2: Performance of Opinion Extraction with Correct Polarity Attribute
true	P10-2050.pdf#52.8	SearchQA, Unigram Acc	Negative f ( % )  Table 2: Performance of Opinion Extraction with Correct Polarity Attribute
true	P10-2050.pdf#30.7	SearchQA, Unigram Acc	Table 2 : Performance of Opinion Extraction with Correct Polarity Attribute Low f ( % )  Table 3: Performance of Opinion Extraction with Correct Intensity Attribute
true	P10-2050.pdf#40.9	SearchQA, Unigram Acc	Table 2 : Performance of Opinion Extraction with Correct Polarity Attribute Medium f ( % )  Table 3: Performance of Opinion Extraction with Correct Intensity Attribute
true	P10-2050.pdf#38.4	SearchQA, Unigram Acc	Table 2 : Performance of Opinion Extraction with Correct Polarity Attribute High f ( % )  Table 3: Performance of Opinion Extraction with Correct Intensity Attribute
true	P10-2050.pdf#62.0	SearchQA, Unigram Acc	Table 3 : Performance of Opinion Extraction with Correct Intensity Attribute f ( % )  Table 4: Performance of Opinion Extraction
true	D15-1153.pdf#97.51	benchmark Vietnamese dependency treebank VnDT, UAS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#93.75	benchmark Vietnamese dependency treebank VnDT, UAS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#97.03	benchmark Vietnamese dependency treebank VnDT, UAS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#92.66	benchmark Vietnamese dependency treebank VnDT, UAS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#91.93	benchmark Vietnamese dependency treebank VnDT, UAS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#96.88	benchmark Vietnamese dependency treebank VnDT, UAS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#72.04	benchmark Vietnamese dependency treebank VnDT, UAS	- - reviews OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.60	benchmark Vietnamese dependency treebank VnDT, UAS	- - newsgroups OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#81.35	benchmark Vietnamese dependency treebank VnDT, UAS	- - newsgroups LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#88.68	benchmark Vietnamese dependency treebank VnDT, UAS	- - WSJ LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#84.64	benchmark Vietnamese dependency treebank VnDT, UAS	- - newsgroups UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#82.53	benchmark Vietnamese dependency treebank VnDT, UAS	- - reviews UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.97	benchmark Vietnamese dependency treebank VnDT, UAS	- - answers OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#80.08	benchmark Vietnamese dependency treebank VnDT, UAS	- - answers UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#75.18	benchmark Vietnamese dependency treebank VnDT, UAS	- - answers LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.66	benchmark Vietnamese dependency treebank VnDT, UAS	- - newsgroups OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.18	benchmark Vietnamese dependency treebank VnDT, UAS	- - answers OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#93.77	benchmark Vietnamese dependency treebank VnDT, UAS	- - WSJ OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#88.00	benchmark Vietnamese dependency treebank VnDT, UAS	- - WSJ OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#90.61	benchmark Vietnamese dependency treebank VnDT, UAS	- - WSJ UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#73.39	benchmark Vietnamese dependency treebank VnDT, UAS	- - reviews OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#78.15	benchmark Vietnamese dependency treebank VnDT, UAS	- - reviews LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#91.80	benchmark Vietnamese dependency treebank VnDT, UAS	UAS  Table 3: Main results on WSJ. All systems are de- terministic.
true	D15-1153.pdf#90.68	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 3: Main results on WSJ. All systems are de- terministic.
true	D15-1153.pdf#97.51	Penn Treebank, Test perplexity	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#93.75	Penn Treebank, Test perplexity	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#97.03	Penn Treebank, Test perplexity	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#92.66	Penn Treebank, Test perplexity	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#91.93	Penn Treebank, Test perplexity	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#96.88	Penn Treebank, Test perplexity	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#72.04	Penn Treebank, Test perplexity	- - reviews OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.60	Penn Treebank, Test perplexity	- - newsgroups OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#81.35	Penn Treebank, Test perplexity	- - newsgroups LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#88.68	Penn Treebank, Test perplexity	- - WSJ LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#84.64	Penn Treebank, Test perplexity	- - newsgroups UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#82.53	Penn Treebank, Test perplexity	- - reviews UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.97	Penn Treebank, Test perplexity	- - answers OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#80.08	Penn Treebank, Test perplexity	- - answers UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#75.18	Penn Treebank, Test perplexity	- - answers LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.66	Penn Treebank, Test perplexity	- - newsgroups OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.18	Penn Treebank, Test perplexity	- - answers OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#93.77	Penn Treebank, Test perplexity	- - WSJ OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#88.00	Penn Treebank, Test perplexity	- - WSJ OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#90.61	Penn Treebank, Test perplexity	- - WSJ UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#73.39	Penn Treebank, Test perplexity	- - reviews OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#78.15	Penn Treebank, Test perplexity	- - reviews LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#91.80	Penn Treebank, Test perplexity	UAS  Table 3: Main results on WSJ. All systems are de- terministic.
true	D15-1153.pdf#90.68	Penn Treebank, Test perplexity	LAS  Table 3: Main results on WSJ. All systems are de- terministic.
true	D15-1153.pdf#97.51	Penn Treebank, LAS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#93.75	Penn Treebank, LAS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#97.03	Penn Treebank, LAS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#92.66	Penn Treebank, LAS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#91.93	Penn Treebank, LAS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#96.88	Penn Treebank, LAS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#72.04	Penn Treebank, LAS	- - reviews OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.60	Penn Treebank, LAS	- - newsgroups OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#81.35	Penn Treebank, LAS	- - newsgroups LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#88.68	Penn Treebank, LAS	- - WSJ LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#84.64	Penn Treebank, LAS	- - newsgroups UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#82.53	Penn Treebank, LAS	- - reviews UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.97	Penn Treebank, LAS	- - answers OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#80.08	Penn Treebank, LAS	- - answers UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#75.18	Penn Treebank, LAS	- - answers LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.66	Penn Treebank, LAS	- - newsgroups OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.18	Penn Treebank, LAS	- - answers OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#93.77	Penn Treebank, LAS	- - WSJ OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#88.00	Penn Treebank, LAS	- - WSJ OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#90.61	Penn Treebank, LAS	- - WSJ UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#73.39	Penn Treebank, LAS	- - reviews OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#78.15	Penn Treebank, LAS	- - reviews LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#91.80	Penn Treebank, LAS	UAS  Table 3: Main results on WSJ. All systems are de- terministic.
true	D15-1153.pdf#90.68	Penn Treebank, LAS	LAS  Table 3: Main results on WSJ. All systems are de- terministic.
true	D15-1153.pdf#97.51	Penn Treebank, Number of params	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#93.75	Penn Treebank, Number of params	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#97.03	Penn Treebank, Number of params	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#92.66	Penn Treebank, Number of params	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#91.93	Penn Treebank, Number of params	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#96.88	Penn Treebank, Number of params	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#72.04	Penn Treebank, Number of params	- - reviews OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.60	Penn Treebank, Number of params	- - newsgroups OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#81.35	Penn Treebank, Number of params	- - newsgroups LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#88.68	Penn Treebank, Number of params	- - WSJ LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#84.64	Penn Treebank, Number of params	- - newsgroups UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#82.53	Penn Treebank, Number of params	- - reviews UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.97	Penn Treebank, Number of params	- - answers OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#80.08	Penn Treebank, Number of params	- - answers UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#75.18	Penn Treebank, Number of params	- - answers LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.66	Penn Treebank, Number of params	- - newsgroups OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.18	Penn Treebank, Number of params	- - answers OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#93.77	Penn Treebank, Number of params	- - WSJ OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#88.00	Penn Treebank, Number of params	- - WSJ OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#90.61	Penn Treebank, Number of params	- - WSJ UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#73.39	Penn Treebank, Number of params	- - reviews OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#78.15	Penn Treebank, Number of params	- - reviews LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#91.80	Penn Treebank, Number of params	UAS  Table 3: Main results on WSJ. All systems are de- terministic.
true	D15-1153.pdf#90.68	Penn Treebank, Number of params	LAS  Table 3: Main results on WSJ. All systems are de- terministic.
true	D15-1153.pdf#97.51	Penn Treebank, UAS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#93.75	Penn Treebank, UAS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#97.03	Penn Treebank, UAS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#92.66	Penn Treebank, UAS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#91.93	Penn Treebank, UAS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#96.88	Penn Treebank, UAS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#72.04	Penn Treebank, UAS	- - reviews OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.60	Penn Treebank, UAS	- - newsgroups OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#81.35	Penn Treebank, UAS	- - newsgroups LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#88.68	Penn Treebank, UAS	- - WSJ LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#84.64	Penn Treebank, UAS	- - newsgroups UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#82.53	Penn Treebank, UAS	- - reviews UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.97	Penn Treebank, UAS	- - answers OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#80.08	Penn Treebank, UAS	- - answers UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#75.18	Penn Treebank, UAS	- - answers LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.66	Penn Treebank, UAS	- - newsgroups OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.18	Penn Treebank, UAS	- - answers OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#93.77	Penn Treebank, UAS	- - WSJ OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#88.00	Penn Treebank, UAS	- - WSJ OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#90.61	Penn Treebank, UAS	- - WSJ UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#73.39	Penn Treebank, UAS	- - reviews OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#78.15	Penn Treebank, UAS	- - reviews LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#91.80	Penn Treebank, UAS	UAS  Table 3: Main results on WSJ. All systems are de- terministic.
true	D15-1153.pdf#90.68	Penn Treebank, UAS	LAS  Table 3: Main results on WSJ. All systems are de- terministic.
true	D15-1153.pdf#97.51	Penn Treebank, POS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#93.75	Penn Treebank, POS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#97.03	Penn Treebank, POS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#92.66	Penn Treebank, POS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#91.93	Penn Treebank, POS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#96.88	Penn Treebank, POS	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#72.04	Penn Treebank, POS	- - reviews OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.60	Penn Treebank, POS	- - newsgroups OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#81.35	Penn Treebank, POS	- - newsgroups LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#88.68	Penn Treebank, POS	- - WSJ LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#84.64	Penn Treebank, POS	- - newsgroups UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#82.53	Penn Treebank, POS	- - reviews UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.97	Penn Treebank, POS	- - answers OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#80.08	Penn Treebank, POS	- - answers UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#75.18	Penn Treebank, POS	- - answers LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.66	Penn Treebank, POS	- - newsgroups OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.18	Penn Treebank, POS	- - answers OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#93.77	Penn Treebank, POS	- - WSJ OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#88.00	Penn Treebank, POS	- - WSJ OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#90.61	Penn Treebank, POS	- - WSJ UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#73.39	Penn Treebank, POS	- - reviews OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#78.15	Penn Treebank, POS	- - reviews LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#91.80	Penn Treebank, POS	UAS  Table 3: Main results on WSJ. All systems are de- terministic.
true	D15-1153.pdf#90.68	Penn Treebank, POS	LAS  Table 3: Main results on WSJ. All systems are de- terministic.
true	D15-1153.pdf#97.51	Penn Treebank, Accuracy	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#93.75	Penn Treebank, Accuracy	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#97.03	Penn Treebank, Accuracy	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#92.66	Penn Treebank, Accuracy	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#91.93	Penn Treebank, Accuracy	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#96.88	Penn Treebank, Accuracy	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#72.04	Penn Treebank, Accuracy	- - reviews OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.60	Penn Treebank, Accuracy	- - newsgroups OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#81.35	Penn Treebank, Accuracy	- - newsgroups LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#88.68	Penn Treebank, Accuracy	- - WSJ LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#84.64	Penn Treebank, Accuracy	- - newsgroups UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#82.53	Penn Treebank, Accuracy	- - reviews UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.97	Penn Treebank, Accuracy	- - answers OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#80.08	Penn Treebank, Accuracy	- - answers UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#75.18	Penn Treebank, Accuracy	- - answers LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.66	Penn Treebank, Accuracy	- - newsgroups OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.18	Penn Treebank, Accuracy	- - answers OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#93.77	Penn Treebank, Accuracy	- - WSJ OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#88.00	Penn Treebank, Accuracy	- - WSJ OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#90.61	Penn Treebank, Accuracy	- - WSJ UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#73.39	Penn Treebank, Accuracy	- - reviews OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#78.15	Penn Treebank, Accuracy	- - reviews LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#91.80	Penn Treebank, Accuracy	UAS  Table 3: Main results on WSJ. All systems are de- terministic.
true	D15-1153.pdf#90.68	Penn Treebank, Accuracy	LAS  Table 3: Main results on WSJ. All systems are de- terministic.
true	D15-1153.pdf#97.51	Penn Treebank, F1	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#93.75	Penn Treebank, F1	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#97.03	Penn Treebank, F1	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#92.66	Penn Treebank, F1	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#91.93	Penn Treebank, F1	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#96.88	Penn Treebank, F1	TA  Table 1: Corpus statistics of our experiments,  where TA denotes POS tagging accuracy.
true	D15-1153.pdf#72.04	Penn Treebank, F1	- - reviews OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.60	Penn Treebank, F1	- - newsgroups OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#81.35	Penn Treebank, F1	- - newsgroups LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#88.68	Penn Treebank, F1	- - WSJ LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#84.64	Penn Treebank, F1	- - newsgroups UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#82.53	Penn Treebank, F1	- - reviews UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.97	Penn Treebank, F1	- - answers OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#80.08	Penn Treebank, F1	- - answers UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#75.18	Penn Treebank, F1	- - answers LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.66	Penn Treebank, F1	- - newsgroups OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#69.18	Penn Treebank, F1	- - answers OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#93.77	Penn Treebank, F1	- - WSJ OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#88.00	Penn Treebank, F1	- - WSJ OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#90.61	Penn Treebank, F1	- - WSJ UAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#73.39	Penn Treebank, F1	- - reviews OOV OOE  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#78.15	Penn Treebank, F1	- - reviews LAS  Table 2: Main results on SANCL. All systems are deterministic.
true	D15-1153.pdf#91.80	Penn Treebank, F1	UAS  Table 3: Main results on WSJ. All systems are de- terministic.
true	D15-1153.pdf#90.68	Penn Treebank, F1	LAS  Table 3: Main results on WSJ. All systems are de- terministic.
true	P11-2032.pdf#15.45	WMT 2014 EN-DE, BLEU	CE  Table 2: BLEU scores in translation experiments. E: En- glish, T: Turkish, C: Czech, A: Arabic.
true	P11-2032.pdf#16.41	WMT 2014 EN-DE, BLEU	AE  Table 2: BLEU scores in translation experiments. E: En- glish, T: Turkish, C: Czech, A: Arabic.
true	P11-2032.pdf#29.51	WMT 2014 EN-DE, BLEU	ET  Table 2: BLEU scores in translation experiments. E: En- glish, T: Turkish, C: Czech, A: Arabic.
true	P11-2032.pdf#10.85	WMT 2014 EN-DE, BLEU	EC  Table 2: BLEU scores in translation experiments. E: En- glish, T: Turkish, C: Czech, A: Arabic.
true	P11-2032.pdf#41.78	WMT 2014 EN-DE, BLEU	TE  Table 2: BLEU scores in translation experiments. E: En- glish, T: Turkish, C: Czech, A: Arabic.
true	P11-2032.pdf#16.02	WMT 2014 EN-DE, BLEU	EA  Table 2: BLEU scores in translation experiments. E: En- glish, T: Turkish, C: Czech, A: Arabic.
true	P11-2032.pdf#25.4K	WMT 2014 EN-DE, BLEU	ET 440K 439K  Table 4: Sizes of bilingual dictionaries induced by differ- ent alignment methods.
true	P11-2032.pdf#23.5K	WMT 2014 EN-DE, BLEU	TE 440K 439K  Table 4: Sizes of bilingual dictionaries induced by differ- ent alignment methods.
true	P11-2032.pdf#15.45	WMT 2014 EN-FR, BLEU	CE  Table 2: BLEU scores in translation experiments. E: En- glish, T: Turkish, C: Czech, A: Arabic.
true	P11-2032.pdf#16.41	WMT 2014 EN-FR, BLEU	AE  Table 2: BLEU scores in translation experiments. E: En- glish, T: Turkish, C: Czech, A: Arabic.
true	P11-2032.pdf#29.51	WMT 2014 EN-FR, BLEU	ET  Table 2: BLEU scores in translation experiments. E: En- glish, T: Turkish, C: Czech, A: Arabic.
true	P11-2032.pdf#10.85	WMT 2014 EN-FR, BLEU	EC  Table 2: BLEU scores in translation experiments. E: En- glish, T: Turkish, C: Czech, A: Arabic.
true	P11-2032.pdf#41.78	WMT 2014 EN-FR, BLEU	TE  Table 2: BLEU scores in translation experiments. E: En- glish, T: Turkish, C: Czech, A: Arabic.
true	P11-2032.pdf#16.02	WMT 2014 EN-FR, BLEU	EA  Table 2: BLEU scores in translation experiments. E: En- glish, T: Turkish, C: Czech, A: Arabic.
true	P11-2032.pdf#25.4K	WMT 2014 EN-FR, BLEU	ET 440K 439K  Table 4: Sizes of bilingual dictionaries induced by differ- ent alignment methods.
true	P11-2032.pdf#23.5K	WMT 2014 EN-FR, BLEU	TE 440K 439K  Table 4: Sizes of bilingual dictionaries induced by differ- ent alignment methods.
true	P11-1150.pdf#0.710	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2. On average,  our approach significantly outperforms Hu's method  and Wu' method in terms of F 1 -measure by over  5.87% and 3.27%, respectively. In particular, our  approach obtains high precision. Such results imply  that our approach can accurately identify the aspects  from consumer reviews by leveraging the Pros and  Cons reviews.
true	P11-1150.pdf#0.706	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2. On average,  our approach significantly outperforms Hu's method  and Wu' method in terms of F 1 -measure by over  5.87% and 3.27%, respectively. In particular, our  approach obtains high precision. Such results imply  that our approach can accurately identify the aspects  from consumer reviews by leveraging the Pros and  Cons reviews.
true	P11-1150.pdf#0.747	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2. On average,  our approach significantly outperforms Hu's method  and Wu' method in terms of F 1 -measure by over  5.87% and 3.27%, respectively. In particular, our  approach obtains high precision. Such results imply  that our approach can accurately identify the aspects  from consumer reviews by leveraging the Pros and  Cons reviews.
true	P11-1150.pdf#0.760	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2. On average,  our approach significantly outperforms Hu's method  and Wu' method in terms of F 1 -measure by over  5.87% and 3.27%, respectively. In particular, our  approach obtains high precision. Such results imply  that our approach can accurately identify the aspects  from consumer reviews by leveraging the Pros and  Cons reviews.
true	P11-1150.pdf#0.712	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2. On average,  our approach significantly outperforms Hu's method  and Wu' method in terms of F 1 -measure by over  5.87% and 3.27%, respectively. In particular, our  approach obtains high precision. Such results imply  that our approach can accurately identify the aspects  from consumer reviews by leveraging the Pros and  Cons reviews.
true	P11-1150.pdf#0.747	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2. On average,  our approach significantly outperforms Hu's method  and Wu' method in terms of F 1 -measure by over  5.87% and 3.27%, respectively. In particular, our  approach obtains high precision. Such results imply  that our approach can accurately identify the aspects  from consumer reviews by leveraging the Pros and  Cons reviews.
true	P11-1150.pdf#0.741	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2. On average,  our approach significantly outperforms Hu's method  and Wu' method in terms of F 1 -measure by over  5.87% and 3.27%, respectively. In particular, our  approach obtains high precision. Such results imply  that our approach can accurately identify the aspects  from consumer reviews by leveraging the Pros and  Cons reviews.
true	P11-1150.pdf#0.734	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2. On average,  our approach significantly outperforms Hu's method  and Wu' method in terms of F 1 -measure by over  5.87% and 3.27%, respectively. In particular, our  approach obtains high precision. Such results imply  that our approach can accurately identify the aspects  from consumer reviews by leveraging the Pros and  Cons reviews.
true	P11-1150.pdf#0.728	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2. On average,  our approach significantly outperforms Hu's method  and Wu' method in terms of F 1 -measure by over  5.87% and 3.27%, respectively. In particular, our  approach obtains high precision. Such results imply  that our approach can accurately identify the aspects  from consumer reviews by leveraging the Pros and  Cons reviews.
true	P11-1150.pdf#0.740	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2. On average,  our approach significantly outperforms Hu's method  and Wu' method in terms of F 1 -measure by over  5.87% and 3.27%, respectively. In particular, our  approach obtains high precision. Such results imply  that our approach can accurately identify the aspects  from consumer reviews by leveraging the Pros and  Cons reviews.
true	P11-1150.pdf#0.718	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2. On average,  our approach significantly outperforms Hu's method  and Wu' method in terms of F 1 -measure by over  5.87% and 3.27%, respectively. In particular, our  approach obtains high precision. Such results imply  that our approach can accurately identify the aspects  from consumer reviews by leveraging the Pros and  Cons reviews.
true	P11-1150.pdf#0.760	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2: Evaluations on Aspect Identification. * signifi- cant t-test, p-values<0.05.
true	P11-1150.pdf#0.718	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2: Evaluations on Aspect Identification. * signifi- cant t-test, p-values<0.05.
true	P11-1150.pdf#0.706	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2: Evaluations on Aspect Identification. * signifi- cant t-test, p-values<0.05.
true	P11-1150.pdf#0.747	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2: Evaluations on Aspect Identification. * signifi- cant t-test, p-values<0.05.
true	P11-1150.pdf#0.734	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2: Evaluations on Aspect Identification. * signifi- cant t-test, p-values<0.05.
true	P11-1150.pdf#0.712	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2: Evaluations on Aspect Identification. * signifi- cant t-test, p-values<0.05.
true	P11-1150.pdf#0.741	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2: Evaluations on Aspect Identification. * signifi- cant t-test, p-values<0.05.
true	P11-1150.pdf#0.747	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2: Evaluations on Aspect Identification. * signifi- cant t-test, p-values<0.05.
true	P11-1150.pdf#0.710	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2: Evaluations on Aspect Identification. * signifi- cant t-test, p-values<0.05.
true	P11-1150.pdf#0.728	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2: Evaluations on Aspect Identification. * signifi- cant t-test, p-values<0.05.
true	P11-1150.pdf#0.740	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our Method  Table 2: Evaluations on Aspect Identification. * signifi- cant t-test, p-values<0.05.
true	P11-1150.pdf#0.739	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SVM  Table 3. We  can see that supervised methods significantly outper- form unsupervised method. For example, the SVM  classifier outperforms the unsupervised method in  terms of average F 1 -measure by over 10.37%. Thus,  we can deduce from such results that the Pros and  Cons reviews are useful for sentiment classification.  In addition, among the supervised classifiers, SVM  classifier performs the best in most products, which  is consistent with the previous research (
true	P11-1150.pdf#0.805	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SVM  Table 3. We  can see that supervised methods significantly outper- form unsupervised method. For example, the SVM  classifier outperforms the unsupervised method in  terms of average F 1 -measure by over 10.37%. Thus,  we can deduce from such results that the Pros and  Cons reviews are useful for sentiment classification.  In addition, among the supervised classifiers, SVM  classifier performs the best in most products, which  is consistent with the previous research (
true	P11-1150.pdf#0.851	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SVM  Table 3. We  can see that supervised methods significantly outper- form unsupervised method. For example, the SVM  classifier outperforms the unsupervised method in  terms of average F 1 -measure by over 10.37%. Thus,  we can deduce from such results that the Pros and  Cons reviews are useful for sentiment classification.  In addition, among the supervised classifiers, SVM  classifier performs the best in most products, which  is consistent with the previous research (
true	P11-1150.pdf#0.719	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SVM  Table 3. We  can see that supervised methods significantly outper- form unsupervised method. For example, the SVM  classifier outperforms the unsupervised method in  terms of average F 1 -measure by over 10.37%. Thus,  we can deduce from such results that the Pros and  Cons reviews are useful for sentiment classification.  In addition, among the supervised classifiers, SVM  classifier performs the best in most products, which  is consistent with the previous research (
true	P11-1150.pdf#0.737	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SVM  Table 3. We  can see that supervised methods significantly outper- form unsupervised method. For example, the SVM  classifier outperforms the unsupervised method in  terms of average F 1 -measure by over 10.37%. Thus,  we can deduce from such results that the Pros and  Cons reviews are useful for sentiment classification.  In addition, among the supervised classifiers, SVM  classifier performs the best in most products, which  is consistent with the previous research (
true	P11-1150.pdf#0.829	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SVM  Table 3. We  can see that supervised methods significantly outper- form unsupervised method. For example, the SVM  classifier outperforms the unsupervised method in  terms of average F 1 -measure by over 10.37%. Thus,  we can deduce from such results that the Pros and  Cons reviews are useful for sentiment classification.  In addition, among the supervised classifiers, SVM  classifier performs the best in most products, which  is consistent with the previous research (
true	P11-1150.pdf#0.819	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	NB  Table 3. We  can see that supervised methods significantly outper- form unsupervised method. For example, the SVM  classifier outperforms the unsupervised method in  terms of average F 1 -measure by over 10.37%. Thus,  we can deduce from such results that the Pros and  Cons reviews are useful for sentiment classification.  In addition, among the supervised classifiers, SVM  classifier performs the best in most products, which  is consistent with the previous research (
true	P11-1150.pdf#0.849	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SVM  Table 3. We  can see that supervised methods significantly outper- form unsupervised method. For example, the SVM  classifier outperforms the unsupervised method in  terms of average F 1 -measure by over 10.37%. Thus,  we can deduce from such results that the Pros and  Cons reviews are useful for sentiment classification.  In addition, among the supervised classifiers, SVM  classifier performs the best in most products, which  is consistent with the previous research (
true	P11-1150.pdf#0.828	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SVM  Table 3. We  can see that supervised methods significantly outper- form unsupervised method. For example, the SVM  classifier outperforms the unsupervised method in  terms of average F 1 -measure by over 10.37%. Thus,  we can deduce from such results that the Pros and  Cons reviews are useful for sentiment classification.  In addition, among the supervised classifiers, SVM  classifier performs the best in most products, which  is consistent with the previous research (
true	P11-1150.pdf#0.723	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	NB  Table 3. We  can see that supervised methods significantly outper- form unsupervised method. For example, the SVM  classifier outperforms the unsupervised method in  terms of average F 1 -measure by over 10.37%. Thus,  we can deduce from such results that the Pros and  Cons reviews are useful for sentiment classification.  In addition, among the supervised classifiers, SVM  classifier performs the best in most products, which  is consistent with the previous research (
true	P11-1150.pdf#0.791	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SVM  Table 3. We  can see that supervised methods significantly outper- form unsupervised method. For example, the SVM  classifier outperforms the unsupervised method in  terms of average F 1 -measure by over 10.37%. Thus,  we can deduce from such results that the Pros and  Cons reviews are useful for sentiment classification.  In addition, among the supervised classifiers, SVM  classifier performs the best in most products, which  is consistent with the previous research (
true	P11-1150.pdf#0.719	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SVM  Table 3: Evaluations on Sentiment Classification. Senti  denotes the method based on SentiWordNet. * significant  t-test, p-values<0.05.
true	P11-1150.pdf#0.805	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SVM  Table 3: Evaluations on Sentiment Classification. Senti  denotes the method based on SentiWordNet. * significant  t-test, p-values<0.05.
true	P11-1150.pdf#0.819	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	NB  Table 3: Evaluations on Sentiment Classification. Senti  denotes the method based on SentiWordNet. * significant  t-test, p-values<0.05.
true	P11-1150.pdf#0.829	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SVM  Table 3: Evaluations on Sentiment Classification. Senti  denotes the method based on SentiWordNet. * significant  t-test, p-values<0.05.
true	P11-1150.pdf#0.828	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SVM  Table 3: Evaluations on Sentiment Classification. Senti  denotes the method based on SentiWordNet. * significant  t-test, p-values<0.05.
true	P11-1150.pdf#0.723	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	NB  Table 3: Evaluations on Sentiment Classification. Senti  denotes the method based on SentiWordNet. * significant  t-test, p-values<0.05.
true	P11-1150.pdf#0.737	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SVM  Table 3: Evaluations on Sentiment Classification. Senti  denotes the method based on SentiWordNet. * significant  t-test, p-values<0.05.
true	P11-1150.pdf#0.739	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SVM  Table 3: Evaluations on Sentiment Classification. Senti  denotes the method based on SentiWordNet. * significant  t-test, p-values<0.05.
true	P11-1150.pdf#0.791	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SVM  Table 3: Evaluations on Sentiment Classification. Senti  denotes the method based on SentiWordNet. * significant  t-test, p-values<0.05.
true	P11-1150.pdf#0.851	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SVM  Table 3: Evaluations on Sentiment Classification. Senti  denotes the method based on SentiWordNet. * significant  t-test, p-values<0.05.
true	P11-1150.pdf#0.849	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SVM  Table 3: Evaluations on Sentiment Classification. Senti  denotes the method based on SentiWordNet. * significant  t-test, p-values<0.05.
true	P11-1150.pdf#0.903	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.826	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.968	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.781	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.815	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.854	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.811	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.862	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.874	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.829	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.959	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.874	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.772	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.944	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.731	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.785	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.794	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.834	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.779	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.796	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.801	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.815	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.851	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.801	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.863	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.748	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.860	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.824	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.902	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.814	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.760	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.948	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P11-1150.pdf#0.716	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	@15  Table 4: Evaluations on Aspect Ranking. @5, @10, @15 denote the evaluation metrics of NDCG@5, NDCG@10,  and NDCG@15, respectively. * significant t-test, p-values<0.05.
true	P15-1032.pdf#92.46	Penn Treebank, POS	Graph - based QTB  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.05	Penn Treebank, POS	Graph - based Method Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#94.26	Penn Treebank, POS	1 Method Transition - based 8  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#87.00	Penn Treebank, POS	1 Web Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.25	Penn Treebank, POS	Graph - based News Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#93.99	Penn Treebank, POS	Graph - based Method Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.41	Penn Treebank, POS	1 Method Transition - based 8  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#86.44	Penn Treebank, POS	Graph - based Web Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.62	Penn Treebank, POS	1 News Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#93.05	Penn Treebank, POS	1 QTB Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#±0.12	Penn Treebank, POS	§22  Table 3: Impact of network architecture on UAS for greedy  inference. We select the best model from 32 random restarts  based on the tune set and show the resulting dev set accuracy.  We also show the standard deviation across the 32 restarts.
true	P15-1032.pdf#92.58	Penn Treebank, POS	WSJ  Table 3: Impact of network architecture on UAS for greedy  inference. We select the best model from 32 random restarts  based on the tune set and show the resulting dev set accuracy.  We also show the standard deviation across the 32 restarts.
true	P15-1032.pdf#92.83	Penn Treebank, POS	2048  Table 4: Increasing hidden layer size increases WSJ Dev  UAS. Shown is the average WSJ Dev UAS across hyperpa- rameter tuning and early stopping with 3 random restarts with  a greedy model.
true	P15-1032.pdf#93.13	Penn Treebank, POS	2048  Table 4: Increasing hidden layer size increases WSJ Dev  UAS. Shown is the average WSJ Dev UAS across hyperpa- rameter tuning and early stopping with 3 random restarts with  a greedy model.
true	P15-1032.pdf#93.09	Penn Treebank, POS	32  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.37	Penn Treebank, POS	WSJ Only 4  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#94.33	Penn Treebank, POS	Tri - training 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.25	Penn Treebank, POS	WSJ Only 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.58	Penn Treebank, POS	WSJ Only 32  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.87	Penn Treebank, POS	Tri - training 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.47	Penn Treebank, POS	WSJ Only Tri - training  Table 6: Utilizing all intermediate representations improves  performance on the WSJ dev set. All results are with B = 8.
true	P15-1032.pdf#94.33	Penn Treebank, POS	WSJ Only Tri - training  Table 6: Utilizing all intermediate representations improves  performance on the WSJ dev set. All results are with B = 8.
true	P15-1032.pdf#92.46	Penn Treebank, UAS	Graph - based QTB  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.05	Penn Treebank, UAS	Graph - based Method Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#94.26	Penn Treebank, UAS	1 Method Transition - based 8  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#87.00	Penn Treebank, UAS	1 Web Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.25	Penn Treebank, UAS	Graph - based News Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#93.99	Penn Treebank, UAS	Graph - based Method Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.41	Penn Treebank, UAS	1 Method Transition - based 8  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#86.44	Penn Treebank, UAS	Graph - based Web Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.62	Penn Treebank, UAS	1 News Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#93.05	Penn Treebank, UAS	1 QTB Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#±0.12	Penn Treebank, UAS	§22  Table 3: Impact of network architecture on UAS for greedy  inference. We select the best model from 32 random restarts  based on the tune set and show the resulting dev set accuracy.  We also show the standard deviation across the 32 restarts.
true	P15-1032.pdf#92.58	Penn Treebank, UAS	WSJ  Table 3: Impact of network architecture on UAS for greedy  inference. We select the best model from 32 random restarts  based on the tune set and show the resulting dev set accuracy.  We also show the standard deviation across the 32 restarts.
true	P15-1032.pdf#92.83	Penn Treebank, UAS	2048  Table 4: Increasing hidden layer size increases WSJ Dev  UAS. Shown is the average WSJ Dev UAS across hyperpa- rameter tuning and early stopping with 3 random restarts with  a greedy model.
true	P15-1032.pdf#93.13	Penn Treebank, UAS	2048  Table 4: Increasing hidden layer size increases WSJ Dev  UAS. Shown is the average WSJ Dev UAS across hyperpa- rameter tuning and early stopping with 3 random restarts with  a greedy model.
true	P15-1032.pdf#93.09	Penn Treebank, UAS	32  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.37	Penn Treebank, UAS	WSJ Only 4  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#94.33	Penn Treebank, UAS	Tri - training 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.25	Penn Treebank, UAS	WSJ Only 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.58	Penn Treebank, UAS	WSJ Only 32  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.87	Penn Treebank, UAS	Tri - training 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.47	Penn Treebank, UAS	WSJ Only Tri - training  Table 6: Utilizing all intermediate representations improves  performance on the WSJ dev set. All results are with B = 8.
true	P15-1032.pdf#94.33	Penn Treebank, UAS	WSJ Only Tri - training  Table 6: Utilizing all intermediate representations improves  performance on the WSJ dev set. All results are with B = 8.
true	P15-1032.pdf#92.46	benchmark Vietnamese dependency treebank VnDT, UAS	Graph - based QTB  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.05	benchmark Vietnamese dependency treebank VnDT, UAS	Graph - based Method Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#94.26	benchmark Vietnamese dependency treebank VnDT, UAS	1 Method Transition - based 8  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#87.00	benchmark Vietnamese dependency treebank VnDT, UAS	1 Web Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.25	benchmark Vietnamese dependency treebank VnDT, UAS	Graph - based News Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#93.99	benchmark Vietnamese dependency treebank VnDT, UAS	Graph - based Method Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.41	benchmark Vietnamese dependency treebank VnDT, UAS	1 Method Transition - based 8  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#86.44	benchmark Vietnamese dependency treebank VnDT, UAS	Graph - based Web Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.62	benchmark Vietnamese dependency treebank VnDT, UAS	1 News Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#93.05	benchmark Vietnamese dependency treebank VnDT, UAS	1 QTB Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#±0.12	benchmark Vietnamese dependency treebank VnDT, UAS	§22  Table 3: Impact of network architecture on UAS for greedy  inference. We select the best model from 32 random restarts  based on the tune set and show the resulting dev set accuracy.  We also show the standard deviation across the 32 restarts.
true	P15-1032.pdf#92.58	benchmark Vietnamese dependency treebank VnDT, UAS	WSJ  Table 3: Impact of network architecture on UAS for greedy  inference. We select the best model from 32 random restarts  based on the tune set and show the resulting dev set accuracy.  We also show the standard deviation across the 32 restarts.
true	P15-1032.pdf#92.83	benchmark Vietnamese dependency treebank VnDT, UAS	2048  Table 4: Increasing hidden layer size increases WSJ Dev  UAS. Shown is the average WSJ Dev UAS across hyperpa- rameter tuning and early stopping with 3 random restarts with  a greedy model.
true	P15-1032.pdf#93.13	benchmark Vietnamese dependency treebank VnDT, UAS	2048  Table 4: Increasing hidden layer size increases WSJ Dev  UAS. Shown is the average WSJ Dev UAS across hyperpa- rameter tuning and early stopping with 3 random restarts with  a greedy model.
true	P15-1032.pdf#93.09	benchmark Vietnamese dependency treebank VnDT, UAS	32  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.37	benchmark Vietnamese dependency treebank VnDT, UAS	WSJ Only 4  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#94.33	benchmark Vietnamese dependency treebank VnDT, UAS	Tri - training 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.25	benchmark Vietnamese dependency treebank VnDT, UAS	WSJ Only 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.58	benchmark Vietnamese dependency treebank VnDT, UAS	WSJ Only 32  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.87	benchmark Vietnamese dependency treebank VnDT, UAS	Tri - training 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.47	benchmark Vietnamese dependency treebank VnDT, UAS	WSJ Only Tri - training  Table 6: Utilizing all intermediate representations improves  performance on the WSJ dev set. All results are with B = 8.
true	P15-1032.pdf#94.33	benchmark Vietnamese dependency treebank VnDT, UAS	WSJ Only Tri - training  Table 6: Utilizing all intermediate representations improves  performance on the WSJ dev set. All results are with B = 8.
true	P15-1032.pdf#92.46	Penn Treebank, LAS	Graph - based QTB  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.05	Penn Treebank, LAS	Graph - based Method Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#94.26	Penn Treebank, LAS	1 Method Transition - based 8  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#87.00	Penn Treebank, LAS	1 Web Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.25	Penn Treebank, LAS	Graph - based News Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#93.99	Penn Treebank, LAS	Graph - based Method Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.41	Penn Treebank, LAS	1 Method Transition - based 8  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#86.44	Penn Treebank, LAS	Graph - based Web Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.62	Penn Treebank, LAS	1 News Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#93.05	Penn Treebank, LAS	1 QTB Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#±0.12	Penn Treebank, LAS	§22  Table 3: Impact of network architecture on UAS for greedy  inference. We select the best model from 32 random restarts  based on the tune set and show the resulting dev set accuracy.  We also show the standard deviation across the 32 restarts.
true	P15-1032.pdf#92.58	Penn Treebank, LAS	WSJ  Table 3: Impact of network architecture on UAS for greedy  inference. We select the best model from 32 random restarts  based on the tune set and show the resulting dev set accuracy.  We also show the standard deviation across the 32 restarts.
true	P15-1032.pdf#92.83	Penn Treebank, LAS	2048  Table 4: Increasing hidden layer size increases WSJ Dev  UAS. Shown is the average WSJ Dev UAS across hyperpa- rameter tuning and early stopping with 3 random restarts with  a greedy model.
true	P15-1032.pdf#93.13	Penn Treebank, LAS	2048  Table 4: Increasing hidden layer size increases WSJ Dev  UAS. Shown is the average WSJ Dev UAS across hyperpa- rameter tuning and early stopping with 3 random restarts with  a greedy model.
true	P15-1032.pdf#93.09	Penn Treebank, LAS	32  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.37	Penn Treebank, LAS	WSJ Only 4  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#94.33	Penn Treebank, LAS	Tri - training 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.25	Penn Treebank, LAS	WSJ Only 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.58	Penn Treebank, LAS	WSJ Only 32  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.87	Penn Treebank, LAS	Tri - training 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.47	Penn Treebank, LAS	WSJ Only Tri - training  Table 6: Utilizing all intermediate representations improves  performance on the WSJ dev set. All results are with B = 8.
true	P15-1032.pdf#94.33	Penn Treebank, LAS	WSJ Only Tri - training  Table 6: Utilizing all intermediate representations improves  performance on the WSJ dev set. All results are with B = 8.
true	P15-1032.pdf#92.46	Penn Treebank, F1	Graph - based QTB  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.05	Penn Treebank, F1	Graph - based Method Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#94.26	Penn Treebank, F1	1 Method Transition - based 8  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#87.00	Penn Treebank, F1	1 Web Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.25	Penn Treebank, F1	Graph - based News Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#93.99	Penn Treebank, F1	Graph - based Method Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.41	Penn Treebank, F1	1 Method Transition - based 8  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#86.44	Penn Treebank, F1	Graph - based Web Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.62	Penn Treebank, F1	1 News Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#93.05	Penn Treebank, F1	1 QTB Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#±0.12	Penn Treebank, F1	§22  Table 3: Impact of network architecture on UAS for greedy  inference. We select the best model from 32 random restarts  based on the tune set and show the resulting dev set accuracy.  We also show the standard deviation across the 32 restarts.
true	P15-1032.pdf#92.58	Penn Treebank, F1	WSJ  Table 3: Impact of network architecture on UAS for greedy  inference. We select the best model from 32 random restarts  based on the tune set and show the resulting dev set accuracy.  We also show the standard deviation across the 32 restarts.
true	P15-1032.pdf#92.83	Penn Treebank, F1	2048  Table 4: Increasing hidden layer size increases WSJ Dev  UAS. Shown is the average WSJ Dev UAS across hyperpa- rameter tuning and early stopping with 3 random restarts with  a greedy model.
true	P15-1032.pdf#93.13	Penn Treebank, F1	2048  Table 4: Increasing hidden layer size increases WSJ Dev  UAS. Shown is the average WSJ Dev UAS across hyperpa- rameter tuning and early stopping with 3 random restarts with  a greedy model.
true	P15-1032.pdf#93.09	Penn Treebank, F1	32  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.37	Penn Treebank, F1	WSJ Only 4  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#94.33	Penn Treebank, F1	Tri - training 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.25	Penn Treebank, F1	WSJ Only 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.58	Penn Treebank, F1	WSJ Only 32  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.87	Penn Treebank, F1	Tri - training 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.47	Penn Treebank, F1	WSJ Only Tri - training  Table 6: Utilizing all intermediate representations improves  performance on the WSJ dev set. All results are with B = 8.
true	P15-1032.pdf#94.33	Penn Treebank, F1	WSJ Only Tri - training  Table 6: Utilizing all intermediate representations improves  performance on the WSJ dev set. All results are with B = 8.
true	P12-1075.pdf#0.736	Senseval 2, F1	System Pairwise Prec .  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.248	Senseval 2, F1	System B 3 Rec .  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.704	Senseval 2, F1	System B 3 Prec .  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.156	Senseval 2, F1	System Pairwise Rec .  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.422	Senseval 2, F1	System Pairwise F - 0 . 5  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.314	Senseval 2, F1	System Pairwise MCC  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.490	Senseval 2, F1	System B 3 F - 0 . 5  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.736	SemEval 2013, F1	System Pairwise Prec .  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.248	SemEval 2013, F1	System B 3 Rec .  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.704	SemEval 2013, F1	System B 3 Prec .  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.156	SemEval 2013, F1	System Pairwise Rec .  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.422	SemEval 2013, F1	System Pairwise F - 0 . 5  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.314	SemEval 2013, F1	System Pairwise MCC  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.490	SemEval 2013, F1	System B 3 F - 0 . 5  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.736	Senseval 3, F1	System Pairwise Prec .  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.248	Senseval 3, F1	System B 3 Rec .  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.704	Senseval 3, F1	System B 3 Prec .  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.156	Senseval 3, F1	System Pairwise Rec .  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.422	Senseval 3, F1	System Pairwise F - 0 . 5  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.314	Senseval 3, F1	System Pairwise MCC  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.490	Senseval 3, F1	System B 3 F - 0 . 5  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.736	SemEval 2015, F1	System Pairwise Prec .  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.248	SemEval 2015, F1	System B 3 Rec .  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.704	SemEval 2015, F1	System B 3 Prec .  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.156	SemEval 2015, F1	System Pairwise Rec .  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.422	SemEval 2015, F1	System Pairwise F - 0 . 5  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.314	SemEval 2015, F1	System Pairwise MCC  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.490	SemEval 2015, F1	System B 3 F - 0 . 5  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.736	SemEval 2007, F1	System Pairwise Prec .  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.248	SemEval 2007, F1	System B 3 Rec .  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.704	SemEval 2007, F1	System B 3 Prec .  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.156	SemEval 2007, F1	System Pairwise Rec .  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.422	SemEval 2007, F1	System Pairwise F - 0 . 5  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.314	SemEval 2007, F1	System Pairwise MCC  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	P12-1075.pdf#0.490	SemEval 2007, F1	System B 3 F - 0 . 5  Table 3: Pairwise and B 3 evaluation for various systems. Since our systems predict more fine-grained clusters than  Freebase, the recall measure is underestimated.
true	D10-1005.pdf#1.46	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Dictionary  Table 1: Mean squared error on a film review corpus.  All results are on the same German test data, varying the  training data. Over-fitting prevents the model learning on  the German data alone; adding English data to the mix  allows the model to make better predictions.
true	D10-1005.pdf#1.39	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Flat  Table 1: Mean squared error on a film review corpus.  All results are on the same German test data, varying the  training data. Over-fitting prevents the model learning on  the German data alone; adding English data to the mix  allows the model to make better predictions.
true	D10-1005.pdf#1.17	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	GermaNet  Table 1: Mean squared error on a film review corpus.  All results are on the same German test data, varying the  training data. Over-fitting prevents the model learning on  the German data alone; adding English data to the mix  allows the model to make better predictions.
true	P15-1060.pdf#0.682	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F 1  Table 2: Aspect identification results in terms of  precision, recall, and F 1 scores on the restaurant  reviews dataset
true	P15-1060.pdf#0.872	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F 1  Table 2: Aspect identification results in terms of  precision, recall, and F 1 scores on the restaurant  reviews dataset
true	P15-1060.pdf#0.677	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Recall  Table 2: Aspect identification results in terms of  precision, recall, and F 1 scores on the restaurant  reviews dataset
true	P15-1060.pdf#0.585	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Recall  Table 2: Aspect identification results in terms of  precision, recall, and F 1 scores on the restaurant  reviews dataset
true	P15-1060.pdf#0.819	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Precision  Table 2: Aspect identification results in terms of  precision, recall, and F 1 scores on the restaurant  reviews dataset
true	P15-1060.pdf#0.898	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Precision  Table 2: Aspect identification results in terms of  precision, recall, and F 1 scores on the restaurant  reviews dataset
true	P15-1060.pdf#0.680	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F 1  Table 2: Aspect identification results in terms of  precision, recall, and F 1 scores on the restaurant  reviews dataset
true	P15-1060.pdf#0.805	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Precision  Table 2: Aspect identification results in terms of  precision, recall, and F 1 scores on the restaurant  reviews dataset
true	P15-1060.pdf#0.854	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Recall  Table 2: Aspect identification results in terms of  precision, recall, and F 1 scores on the restaurant  reviews dataset
true	P15-1060.pdf#86.2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	73 neg .  Table 5: Accuracy for SERBM and JST
true	P15-1060.pdf#92.0	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	73 pos .  Table 5: Accuracy for SERBM and JST
true	P15-1060.pdf#89.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	73 overall  Table 5: Accuracy for SERBM and JST
true	P15-1060.pdf#0.682	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	F 1  Table 2: Aspect identification results in terms of  precision, recall, and F 1 scores on the restaurant  reviews dataset
true	P15-1060.pdf#0.872	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	F 1  Table 2: Aspect identification results in terms of  precision, recall, and F 1 scores on the restaurant  reviews dataset
true	P15-1060.pdf#0.677	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Recall  Table 2: Aspect identification results in terms of  precision, recall, and F 1 scores on the restaurant  reviews dataset
true	P15-1060.pdf#0.585	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Recall  Table 2: Aspect identification results in terms of  precision, recall, and F 1 scores on the restaurant  reviews dataset
true	P15-1060.pdf#0.819	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Precision  Table 2: Aspect identification results in terms of  precision, recall, and F 1 scores on the restaurant  reviews dataset
true	P15-1060.pdf#0.898	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Precision  Table 2: Aspect identification results in terms of  precision, recall, and F 1 scores on the restaurant  reviews dataset
true	P15-1060.pdf#0.680	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	F 1  Table 2: Aspect identification results in terms of  precision, recall, and F 1 scores on the restaurant  reviews dataset
true	P15-1060.pdf#0.805	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Precision  Table 2: Aspect identification results in terms of  precision, recall, and F 1 scores on the restaurant  reviews dataset
true	P15-1060.pdf#0.854	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Recall  Table 2: Aspect identification results in terms of  precision, recall, and F 1 scores on the restaurant  reviews dataset
true	P15-1060.pdf#86.2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	73 neg .  Table 5: Accuracy for SERBM and JST
true	P15-1060.pdf#92.0	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	73 pos .  Table 5: Accuracy for SERBM and JST
true	P15-1060.pdf#89.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	73 overall  Table 5: Accuracy for SERBM and JST
true	P11-1139.pdf#98.17	Penn Treebank, Validation perplexity	Seg  Table 7: F-score performance on the test data.
true	P11-1139.pdf#94.02	Penn Treebank, Validation perplexity	Seg&Tag  Table 7: F-score performance on the test data.
true	P11-1139.pdf#98.17	Penn Treebank, Accuracy	Seg  Table 7: F-score performance on the test data.
true	P11-1139.pdf#94.02	Penn Treebank, Accuracy	Seg&Tag  Table 7: F-score performance on the test data.
true	P11-1139.pdf#98.17	Penn Treebank, Number of params	Seg  Table 7: F-score performance on the test data.
true	P11-1139.pdf#94.02	Penn Treebank, Number of params	Seg&Tag  Table 7: F-score performance on the test data.
true	P11-1139.pdf#98.17	Chinese Treebank 6, F1	Seg  Table 7: F-score performance on the test data.
true	P11-1139.pdf#94.02	Chinese Treebank 6, F1	Seg&Tag  Table 7: F-score performance on the test data.
true	P15-2142.pdf#89.41	Penn Treebank, POS	-  Table 3: Parsing accuracies for dependency  parsers on the WSJ test set, CoNLL dependencies.
true	P15-2142.pdf#91.11	Penn Treebank, POS	-  Table 3: Parsing accuracies for dependency  parsers on the WSJ test set, CoNLL dependencies.
true	P15-2142.pdf#132.2	Penn Treebank, POS	Perplexity  Table 5: Sentences of length 20 or greater generated by the neural generative dependency model.
true	P15-2142.pdf#111.8	Penn Treebank, POS	Perplexity  Table 5: Sentences of length 20 or greater generated by the neural generative dependency model.
true	P15-2142.pdf#111.8	Penn Treebank, POS	Perplexity  Table 4: WSJ Language modelling test results.  We compare our model, with and without unsu- pervised tuning, to n-gram baselines.
true	P15-2142.pdf#132.2	Penn Treebank, POS	Perplexity  Table 4: WSJ Language modelling test results.  We compare our model, with and without unsu- pervised tuning, to n-gram baselines.
true	P15-2142.pdf#89.41	Penn Treebank, Test perplexity	-  Table 3: Parsing accuracies for dependency  parsers on the WSJ test set, CoNLL dependencies.
true	P15-2142.pdf#91.11	Penn Treebank, Test perplexity	-  Table 3: Parsing accuracies for dependency  parsers on the WSJ test set, CoNLL dependencies.
true	P15-2142.pdf#132.2	Penn Treebank, Test perplexity	Perplexity  Table 5: Sentences of length 20 or greater generated by the neural generative dependency model.
true	P15-2142.pdf#111.8	Penn Treebank, Test perplexity	Perplexity  Table 5: Sentences of length 20 or greater generated by the neural generative dependency model.
true	P15-2142.pdf#111.8	Penn Treebank, Test perplexity	Perplexity  Table 4: WSJ Language modelling test results.  We compare our model, with and without unsu- pervised tuning, to n-gram baselines.
true	P15-2142.pdf#132.2	Penn Treebank, Test perplexity	Perplexity  Table 4: WSJ Language modelling test results.  We compare our model, with and without unsu- pervised tuning, to n-gram baselines.
true	P15-2142.pdf#89.41	Penn Treebank, UAS	-  Table 3: Parsing accuracies for dependency  parsers on the WSJ test set, CoNLL dependencies.
true	P15-2142.pdf#91.11	Penn Treebank, UAS	-  Table 3: Parsing accuracies for dependency  parsers on the WSJ test set, CoNLL dependencies.
true	P15-2142.pdf#132.2	Penn Treebank, UAS	Perplexity  Table 5: Sentences of length 20 or greater generated by the neural generative dependency model.
true	P15-2142.pdf#111.8	Penn Treebank, UAS	Perplexity  Table 5: Sentences of length 20 or greater generated by the neural generative dependency model.
true	P15-2142.pdf#111.8	Penn Treebank, UAS	Perplexity  Table 4: WSJ Language modelling test results.  We compare our model, with and without unsu- pervised tuning, to n-gram baselines.
true	P15-2142.pdf#132.2	Penn Treebank, UAS	Perplexity  Table 4: WSJ Language modelling test results.  We compare our model, with and without unsu- pervised tuning, to n-gram baselines.
true	P15-2142.pdf#89.41	Penn Treebank, Number of params	-  Table 3: Parsing accuracies for dependency  parsers on the WSJ test set, CoNLL dependencies.
true	P15-2142.pdf#91.11	Penn Treebank, Number of params	-  Table 3: Parsing accuracies for dependency  parsers on the WSJ test set, CoNLL dependencies.
true	P15-2142.pdf#132.2	Penn Treebank, Number of params	Perplexity  Table 5: Sentences of length 20 or greater generated by the neural generative dependency model.
true	P15-2142.pdf#111.8	Penn Treebank, Number of params	Perplexity  Table 5: Sentences of length 20 or greater generated by the neural generative dependency model.
true	P15-2142.pdf#111.8	Penn Treebank, Number of params	Perplexity  Table 4: WSJ Language modelling test results.  We compare our model, with and without unsu- pervised tuning, to n-gram baselines.
true	P15-2142.pdf#132.2	Penn Treebank, Number of params	Perplexity  Table 4: WSJ Language modelling test results.  We compare our model, with and without unsu- pervised tuning, to n-gram baselines.
true	P15-2142.pdf#89.41	Penn Treebank, LAS	-  Table 3: Parsing accuracies for dependency  parsers on the WSJ test set, CoNLL dependencies.
true	P15-2142.pdf#91.11	Penn Treebank, LAS	-  Table 3: Parsing accuracies for dependency  parsers on the WSJ test set, CoNLL dependencies.
true	P15-2142.pdf#132.2	Penn Treebank, LAS	Perplexity  Table 5: Sentences of length 20 or greater generated by the neural generative dependency model.
true	P15-2142.pdf#111.8	Penn Treebank, LAS	Perplexity  Table 5: Sentences of length 20 or greater generated by the neural generative dependency model.
true	P15-2142.pdf#111.8	Penn Treebank, LAS	Perplexity  Table 4: WSJ Language modelling test results.  We compare our model, with and without unsu- pervised tuning, to n-gram baselines.
true	P15-2142.pdf#132.2	Penn Treebank, LAS	Perplexity  Table 4: WSJ Language modelling test results.  We compare our model, with and without unsu- pervised tuning, to n-gram baselines.
true	D15-1068.pdf#80.55	SearchQA, Unigram Acc	Global Inference Classifiers 6 F 1  Table 3: Good-vs-Bad classification.  ‡ and  †  mark statistically significant differences in accu- racy compared to the baseline MaxEnt classifier  with confidence levels of 99% and 95%, respec- tively (randomized test).
true	D15-1068.pdf#79.80‡	SearchQA, Unigram Acc	Global Inference Classifiers 6 Acc  Table 3: Good-vs-Bad classification.  ‡ and  †  mark statistically significant differences in accu- racy compared to the baseline MaxEnt classifier  with confidence levels of 99% and 95%, respec- tively (randomized test).
true	P15-1162.pdf#81.1	SST-2, Accuracy	- RT fine - - - - - -  Table 1: DANs achieve comparable sentiment accu- racies to syntactic functions (bottom third of table)  but require much less training time (measured as  time of a single epoch on the SST fine-grained task).  Asterisked models are initialized either with differ- ent pretrained embeddings or randomly.
true	P15-1162.pdf#50.6	SST-2, Accuracy	- SST fine - - - -  Table 1: DANs achieve comparable sentiment accu- racies to syntactic functions (bottom third of table)  but require much less training time (measured as  time of a single epoch on the SST fine-grained task).  Asterisked models are initialized either with differ- ent pretrained embeddings or randomly.
true	P15-1162.pdf#88.1	SST-2, Accuracy	- SST bin - - - - - -  Table 1: DANs achieve comparable sentiment accu- racies to syntactic functions (bottom third of table)  but require much less training time (measured as  time of a single epoch on the SST fine-grained task).  Asterisked models are initialized either with differ- ent pretrained embeddings or randomly.
true	P15-1162.pdf#92.6	SST-2, Accuracy	- IMDB bin - - - - - -  Table 1: DANs achieve comparable sentiment accu- racies to syntactic functions (bottom third of table)  but require much less training time (measured as  time of a single epoch on the SST fine-grained task).  Asterisked models are initialized either with differ- ent pretrained embeddings or randomly.
true	P15-1162.pdf#54.8	SST-2, Accuracy	N / A Pos 1  Table 2: The DAN achieves slightly lower accu- racies than the more complex QANTA in much  less training time, even at early sentence posi- tions where compositionality plays a bigger role.  When Wikipedia is added to the training set (bot- tom half of table), the DAN outperforms QANTA
true	P15-1162.pdf#77.5	SST-2, Accuracy	18 Full  Table 2: The DAN achieves slightly lower accu- racies than the more complex QANTA in much  less training time, even at early sentence posi- tions where compositionality plays a bigger role.  When Wikipedia is added to the training set (bot- tom half of table), the DAN outperforms QANTA
true	P15-1162.pdf#76.6	SST-2, Accuracy	18 Pos 2  Table 2: The DAN achieves slightly lower accu- racies than the more complex QANTA in much  less training time, even at early sentence posi- tions where compositionality plays a bigger role.  When Wikipedia is added to the training set (bot- tom half of table), the DAN outperforms QANTA
true	P15-1162.pdf#81.1	IMDb, Accuracy	- RT fine - - - - - -  Table 1: DANs achieve comparable sentiment accu- racies to syntactic functions (bottom third of table)  but require much less training time (measured as  time of a single epoch on the SST fine-grained task).  Asterisked models are initialized either with differ- ent pretrained embeddings or randomly.
true	P15-1162.pdf#50.6	IMDb, Accuracy	- SST fine - - - -  Table 1: DANs achieve comparable sentiment accu- racies to syntactic functions (bottom third of table)  but require much less training time (measured as  time of a single epoch on the SST fine-grained task).  Asterisked models are initialized either with differ- ent pretrained embeddings or randomly.
true	P15-1162.pdf#88.1	IMDb, Accuracy	- SST bin - - - - - -  Table 1: DANs achieve comparable sentiment accu- racies to syntactic functions (bottom third of table)  but require much less training time (measured as  time of a single epoch on the SST fine-grained task).  Asterisked models are initialized either with differ- ent pretrained embeddings or randomly.
true	P15-1162.pdf#92.6	IMDb, Accuracy	- IMDB bin - - - - - -  Table 1: DANs achieve comparable sentiment accu- racies to syntactic functions (bottom third of table)  but require much less training time (measured as  time of a single epoch on the SST fine-grained task).  Asterisked models are initialized either with differ- ent pretrained embeddings or randomly.
true	P15-1162.pdf#54.8	IMDb, Accuracy	N / A Pos 1  Table 2: The DAN achieves slightly lower accu- racies than the more complex QANTA in much  less training time, even at early sentence posi- tions where compositionality plays a bigger role.  When Wikipedia is added to the training set (bot- tom half of table), the DAN outperforms QANTA
true	P15-1162.pdf#77.5	IMDb, Accuracy	18 Full  Table 2: The DAN achieves slightly lower accu- racies than the more complex QANTA in much  less training time, even at early sentence posi- tions where compositionality plays a bigger role.  When Wikipedia is added to the training set (bot- tom half of table), the DAN outperforms QANTA
true	P15-1162.pdf#76.6	IMDb, Accuracy	18 Pos 2  Table 2: The DAN achieves slightly lower accu- racies than the more complex QANTA in much  less training time, even at early sentence posi- tions where compositionality plays a bigger role.  When Wikipedia is added to the training set (bot- tom half of table), the DAN outperforms QANTA
true	P13-1170.pdf#77.0	New York Times Corpus, P@10%	F1  Table 6: Ablation test results for causal relation  recognition (%)
true	P13-1170.pdf#71.1	New York Times Corpus, P@10%	F1  Table 6: Ablation test results for causal relation  recognition (%)
true	P13-1170.pdf#83.8	New York Times Corpus, P@10%	F1  Table 6: Ablation test results for causal relation  recognition (%)
true	P13-1170.pdf#41.8	New York Times Corpus, P@10%	P@1  Table 7: Why-QA results (%)
true	P13-1170.pdf#41.0	New York Times Corpus, P@10%	MAP  Table 7: Why-QA results (%)
true	P13-1170.pdf#41.8	New York Times Corpus, P@10%	P@1  Table 8: Results with/without intra-and inter- sentential causal relations (%)
true	P13-1170.pdf#41.0	New York Times Corpus, P@10%	MAP  Table 8: Results with/without intra-and inter- sentential causal relations (%)
true	P13-1170.pdf#41.8	New York Times Corpus, P@10%	P@1  Table 9: Ablation test results for why-QA (%)
true	P13-1170.pdf#41.0	New York Times Corpus, P@10%	MAP  Table 9: Ablation test results for why-QA (%)
true	P14-2132.pdf#92.1	Penn Treebank, Accuracy	45 tag set  Table 2: Tagging accuracy of different approaches on English Penn Treebank. Columns labeled 973k  train describe models trained on the subset of 973k tokens used by
true	P14-2132.pdf#96.8	Penn Treebank, Accuracy	- 17 tag set 973k train  Table 2: Tagging accuracy of different approaches on English Penn Treebank. Columns labeled 973k  train describe models trained on the subset of 973k tokens used by
true	P14-2132.pdf#92.8	Penn Treebank, Accuracy	45 tag set  Table 2: Tagging accuracy of different approaches on English Penn Treebank. Columns labeled 973k  train describe models trained on the subset of 973k tokens used by
true	P14-2132.pdf#93.9	Penn Treebank, Accuracy	17 tag set  Table 2: Tagging accuracy of different approaches on English Penn Treebank. Columns labeled 973k  train describe models trained on the subset of 973k tokens used by
true	P15-2029.pdf#2	SST-2, Accuracy	ancestor paths pattern ( s ) s  Table 1: Tree-based convolution patterns. Word concatenation always starts with m, while h, g, and g 2  denote parent, grand parent, and great-grand parent, etc., and " " denotes words excluded in convolution.
true	P15-2029.pdf#3	SST-2, Accuracy	ancestor paths n s s  Table 1: Tree-based convolution patterns. Word concatenation always starts with m, while h, g, and g 2  denote parent, grand parent, and great-grand parent, etc., and " " denotes words excluded in convolution.
true	P15-2029.pdf#2	SST-2, Accuracy	ancestor paths pattern ( s ) s s  Table 1: Tree-based convolution patterns. Word concatenation always starts with m, while h, g, and g 2  denote parent, grand parent, and great-grand parent, etc., and " " denotes words excluded in convolution.
true	P15-2029.pdf#81.9	SST-2, Accuracy	- MR - -  Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets.  TREC-2 is TREC with fine grained labels.  † Results generated by GPU (all others generated by CPU).   *  Results generated from Kim (2014)'s implementation.
true	P15-2029.pdf#95.6†	SST-2, Accuracy	TREC  Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets.  TREC-2 is TREC with fine grained labels.  † Results generated by GPU (all others generated by CPU).   *  Results generated from Kim (2014)'s implementation.
true	P15-2029.pdf#49.8	SST-2, Accuracy	- SST - 1 -  Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets.  TREC-2 is TREC with fine grained labels.  † Results generated by GPU (all others generated by CPU).   *  Results generated from Kim (2014)'s implementation.
true	P15-2029.pdf#81.9	SST-2, Accuracy	MR  Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets.  TREC-2 is TREC with fine grained labels.  † Results generated by GPU (all others generated by CPU).   *  Results generated from Kim (2014)'s implementation.
true	P15-2029.pdf#90.8	SST-2, Accuracy	- TREC - 2 - - - -  Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets.  TREC-2 is TREC with fine grained labels.  † Results generated by GPU (all others generated by CPU).   *  Results generated from Kim (2014)'s implementation.
true	P15-2029.pdf#81	SST-2, Accuracy	MR - -  Table 2: Results on Movie Review (MR), Stanford Sentiment Treeba  TREC-2 is TREC with fine grained labels.  † Results generated by GPU   *  Results generated from Kim (2014)'s implementation.
true	P15-2029.pdf#81	SST-2, Accuracy	MR  Table 2: Results on Movie Review (MR), Stanford Sentiment Treeba  TREC-2 is TREC with fine grained labels.  † Results generated by GPU   *  Results generated from Kim (2014)'s implementation.
true	P15-2029.pdf#2	TREC, Error	ancestor paths pattern ( s ) s  Table 1: Tree-based convolution patterns. Word concatenation always starts with m, while h, g, and g 2  denote parent, grand parent, and great-grand parent, etc., and " " denotes words excluded in convolution.
true	P15-2029.pdf#3	TREC, Error	ancestor paths n s s  Table 1: Tree-based convolution patterns. Word concatenation always starts with m, while h, g, and g 2  denote parent, grand parent, and great-grand parent, etc., and " " denotes words excluded in convolution.
true	P15-2029.pdf#2	TREC, Error	ancestor paths pattern ( s ) s s  Table 1: Tree-based convolution patterns. Word concatenation always starts with m, while h, g, and g 2  denote parent, grand parent, and great-grand parent, etc., and " " denotes words excluded in convolution.
true	P15-2029.pdf#81.9	TREC, Error	- MR - -  Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets.  TREC-2 is TREC with fine grained labels.  † Results generated by GPU (all others generated by CPU).   *  Results generated from Kim (2014)'s implementation.
true	P15-2029.pdf#95.6†	TREC, Error	TREC  Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets.  TREC-2 is TREC with fine grained labels.  † Results generated by GPU (all others generated by CPU).   *  Results generated from Kim (2014)'s implementation.
true	P15-2029.pdf#49.8	TREC, Error	- SST - 1 -  Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets.  TREC-2 is TREC with fine grained labels.  † Results generated by GPU (all others generated by CPU).   *  Results generated from Kim (2014)'s implementation.
true	P15-2029.pdf#81.9	TREC, Error	MR  Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets.  TREC-2 is TREC with fine grained labels.  † Results generated by GPU (all others generated by CPU).   *  Results generated from Kim (2014)'s implementation.
true	P15-2029.pdf#90.8	TREC, Error	- TREC - 2 - - - -  Table 1: Results on Movie Review (MR), Stanford Sentiment Treebank (SST-1), and TREC datasets.  TREC-2 is TREC with fine grained labels.  † Results generated by GPU (all others generated by CPU).   *  Results generated from Kim (2014)'s implementation.
true	P15-2029.pdf#81	TREC, Error	MR - -  Table 2: Results on Movie Review (MR), Stanford Sentiment Treeba  TREC-2 is TREC with fine grained labels.  † Results generated by GPU   *  Results generated from Kim (2014)'s implementation.
true	P15-2029.pdf#81	TREC, Error	MR  Table 2: Results on Movie Review (MR), Stanford Sentiment Treeba  TREC-2 is TREC with fine grained labels.  † Results generated by GPU   *  Results generated from Kim (2014)'s implementation.
true	P13-1160.pdf#10.8	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F1  Table 3: Results in terms of macro-averaged pre- cision, recall and F1.
true	P13-1160.pdf#13.8	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Recall  Table 3: Results in terms of macro-averaged pre- cision, recall and F1.
true	P13-1160.pdf#16.5	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Precision  Table 3: Results in terms of macro-averaged pre- cision, recall and F1.
true	P13-1160.pdf#9.3	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Unmarked  Table 4: Separate evaluation (F1) of the "marked"  and the "unmarked" EDUs.
true	P13-1160.pdf#11.5	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Marked  Table 4: Separate evaluation (F1) of the "marked"  and the "unmarked" EDUs.
true	P13-1160.pdf#59.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	sentiment ( 3 classes )  Table 7: Supervised learning at the EDU level (accuracy)
true	P13-1160.pdf#39.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	aspect+sentiment ( 28 classes )  Table 7: Supervised learning at the EDU level (accuracy)
true	P13-1160.pdf#29.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Marked only sentiment+aspect ( 28 classes )  Table 7: Supervised learning at the EDU level (accuracy)
true	P13-1160.pdf#52.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	aspect ( 10 classes )  Table 7: Supervised learning at the EDU level (accuracy)
true	P14-2011.pdf#67.4	New York Times Corpus, P@10%	R ( % )
true	P14-2011.pdf#73.7(+5.4%)	New York Times Corpus, P@10%	F
true	P14-2011.pdf#81.2	New York Times Corpus, P@10%	P ( % )
true	P14-2011.pdf#67.4	New York Times Corpus, P@10%	P ( % ) R ( % )  Table 3. Comparison of different systems on the  ACE RDC 2004 corpus
true	P14-2011.pdf#81.1	New York Times Corpus, P@10%	CTK with CSPT P ( % ) R ( % )  Table 3. Comparison of different systems on the  ACE RDC 2004 corpus
true	P14-2011.pdf#72.9	New York Times Corpus, P@10%	CTK with CSPT F  Table 3. Comparison of different systems on the  ACE RDC 2004 corpus
true	P14-2011.pdf#81.2	New York Times Corpus, P@10%	P ( % ) R ( % )  Table 3. Comparison of different systems on the  ACE RDC 2004 corpus
true	P14-2011.pdf#66.2	New York Times Corpus, P@10%	CTK with CSPT P ( % ) R ( % )  Table 3. Comparison of different systems on the  ACE RDC 2004 corpus
true	P14-2011.pdf#73.7	New York Times Corpus, P@10%	F  Table 3. Comparison of different systems on the  ACE RDC 2004 corpus
true	N10-3001.pdf#2.92%	Penn Treebank, F1	Conjunctions  Table 1: Biomedical corpora that provide coordination  structures compared with the Penn Treebank corpus.
true	N10-3001.pdf#1.95%	Penn Treebank, F1	Conjunctions  Table 1: Biomedical corpora that provide coordination  structures compared with the Penn Treebank corpus.
true	N10-3001.pdf#46.40	Penn Treebank, F1	Conjunction  Table 2: Coordination resolution results at the conjunct  and conjunction levels as F-Measure.
true	N10-3001.pdf#64.64	Penn Treebank, F1	Conjunct  Table 2: Coordination resolution results at the conjunct  and conjunction levels as F-Measure.
true	N15-1124.pdf#40	WMT 2014 EN-FR, BLEU	
true	N15-1124.pdf#19	WMT 2014 EN-FR, BLEU	
true	N15-1124.pdf#15	WMT 2014 EN-FR, BLEU	
true	N15-1124.pdf#14	WMT 2014 EN-FR, BLEU	
true	N15-1124.pdf#40	WMT 2014 EN-DE, BLEU	
true	N15-1124.pdf#19	WMT 2014 EN-DE, BLEU	
true	N15-1124.pdf#15	WMT 2014 EN-DE, BLEU	
true	N15-1124.pdf#14	WMT 2014 EN-DE, BLEU	
true	P11-1125.pdf#29.17	WMT 2014 EN-FR, BLEU	fr - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#30.45	WMT 2014 EN-FR, BLEU	es - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#29.32	WMT 2014 EN-FR, BLEU	fr - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#30.46	WMT 2014 EN-FR, BLEU	es - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#24.13	WMT 2014 EN-FR, BLEU	cz - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#24.14	WMT 2014 EN-FR, BLEU	cz - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#30.52	WMT 2014 EN-FR, BLEU	es - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#30.41	WMT 2014 EN-FR, BLEU	es - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#23.82	WMT 2014 EN-FR, BLEU	cz - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#29.57	WMT 2014 EN-FR, BLEU	fr - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#29.15	WMT 2014 EN-FR, BLEU	fr - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#24.58	WMT 2014 EN-FR, BLEU	de - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#23.77	WMT 2014 EN-FR, BLEU	cz - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#24.01	WMT 2014 EN-FR, BLEU	cz - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#23.93	WMT 2014 EN-FR, BLEU	cz - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#29.17	WMT 2014 EN-DE, BLEU	fr - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#30.45	WMT 2014 EN-DE, BLEU	es - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#29.32	WMT 2014 EN-DE, BLEU	fr - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#30.46	WMT 2014 EN-DE, BLEU	es - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#24.13	WMT 2014 EN-DE, BLEU	cz - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#24.14	WMT 2014 EN-DE, BLEU	cz - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#30.52	WMT 2014 EN-DE, BLEU	es - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#30.41	WMT 2014 EN-DE, BLEU	es - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#23.82	WMT 2014 EN-DE, BLEU	cz - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#29.57	WMT 2014 EN-DE, BLEU	fr - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#29.15	WMT 2014 EN-DE, BLEU	fr - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#24.58	WMT 2014 EN-DE, BLEU	de - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#23.77	WMT 2014 EN-DE, BLEU	cz - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#24.01	WMT 2014 EN-DE, BLEU	cz - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	P11-1125.pdf#23.93	WMT 2014 EN-DE, BLEU	cz - en  Table 2: Translation results in lower-case BLEU.  CN for confusion network and CF for confusion  forest with different vertical (v) and horizontal (h)  Markovization order.
true	D12-1133.pdf#74.47	Penn Treebank, POS	Parser TLAS  Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and  the score threshold α. Beam parameters fixed at b 1 = 40, b 2 = 4.
true	D12-1133.pdf#86.43	Penn Treebank, POS	English POS TLAS  Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and  the score threshold α. Beam parameters fixed at b 1 = 40, b 2 = 4.
true	D12-1133.pdf#86.35	Penn Treebank, POS	German POS TLAS  Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and  the score threshold α. Beam parameters fixed at b 1 = 40, b 2 = 4.
true	D12-1133.pdf#82.85	Penn Treebank, POS	Czech POS TLAS  Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and  the score threshold α. Beam parameters fixed at b 1 = 40, b 2 = 4.
true	D12-1133.pdf#74.47	benchmark Vietnamese dependency treebank VnDT, UAS	Parser TLAS  Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and  the score threshold α. Beam parameters fixed at b 1 = 40, b 2 = 4.
true	D12-1133.pdf#86.43	benchmark Vietnamese dependency treebank VnDT, UAS	English POS TLAS  Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and  the score threshold α. Beam parameters fixed at b 1 = 40, b 2 = 4.
true	D12-1133.pdf#86.35	benchmark Vietnamese dependency treebank VnDT, UAS	German POS TLAS  Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and  the score threshold α. Beam parameters fixed at b 1 = 40, b 2 = 4.
true	D12-1133.pdf#82.85	benchmark Vietnamese dependency treebank VnDT, UAS	Czech POS TLAS  Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and  the score threshold α. Beam parameters fixed at b 1 = 40, b 2 = 4.
true	D12-1133.pdf#74.47	benchmark Vietnamese dependency treebank VnDT, LAS	Parser TLAS  Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and  the score threshold α. Beam parameters fixed at b 1 = 40, b 2 = 4.
true	D12-1133.pdf#86.43	benchmark Vietnamese dependency treebank VnDT, LAS	English POS TLAS  Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and  the score threshold α. Beam parameters fixed at b 1 = 40, b 2 = 4.
true	D12-1133.pdf#86.35	benchmark Vietnamese dependency treebank VnDT, LAS	German POS TLAS  Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and  the score threshold α. Beam parameters fixed at b 1 = 40, b 2 = 4.
true	D12-1133.pdf#82.85	benchmark Vietnamese dependency treebank VnDT, LAS	Czech POS TLAS  Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and  the score threshold α. Beam parameters fixed at b 1 = 40, b 2 = 4.
true	D12-1133.pdf#74.47	Penn Treebank, UAS	Parser TLAS  Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and  the score threshold α. Beam parameters fixed at b 1 = 40, b 2 = 4.
true	D12-1133.pdf#86.43	Penn Treebank, UAS	English POS TLAS  Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and  the score threshold α. Beam parameters fixed at b 1 = 40, b 2 = 4.
true	D12-1133.pdf#86.35	Penn Treebank, UAS	German POS TLAS  Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and  the score threshold α. Beam parameters fixed at b 1 = 40, b 2 = 4.
true	D12-1133.pdf#82.85	Penn Treebank, UAS	Czech POS TLAS  Table 1: Accuracy scores for the CoNLL 2009 shared task development sets as a function of the number of tags k and  the score threshold α. Beam parameters fixed at b 1 = 40, b 2 = 4.
true	P14-1032.pdf#.4	Chinese Treebank 6, F1	LEX&CONT DP  Table 3: The results of convolutional method vs. the results of non-convolutional methods.
true	P14-1032.pdf#.5	Chinese Treebank 6, F1	LEX&CONT DP  Table 3: The results of convolutional method vs. the results of non-convolutional methods.
true	P14-1032.pdf#.6	Chinese Treebank 6, F1	LEX&CONT DP  Table 3: The results of convolutional method vs. the results of non-convolutional methods.
true	P14-1032.pdf#.7	Chinese Treebank 6, F1	LEX DP  Table 3: The results of convolutional method vs. the results of non-convolutional methods.
true	P14-1032.pdf#.8	Chinese Treebank 6, F1	DP - HITS DP  Table 3: The results of convolutional method vs. the results of non-convolutional methods.
true	P14-1032.pdf#.9	Chinese Treebank 6, F1	Table 3: The results of convolutional method vs. the results of non-convolutional methods.
true	D13-1093.pdf#92.41	Penn Treebank, Accuracy	200 Penn - YM LAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#93.50	Penn Treebank, Accuracy	200 Penn - YM UAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#91.28	Penn Treebank, Accuracy	200 Penn - S LAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#86.24	Penn Treebank, Accuracy	200 CTB - 5 LAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#93.64	Penn Treebank, Accuracy	200 Penn - S UAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#87.87	Penn Treebank, Accuracy	200 CTB - 5 UAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#92.41	Penn Treebank, Number of params	200 Penn - YM LAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#93.50	Penn Treebank, Number of params	200 Penn - YM UAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#91.28	Penn Treebank, Number of params	200 Penn - S LAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#86.24	Penn Treebank, Number of params	200 CTB - 5 LAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#93.64	Penn Treebank, Number of params	200 Penn - S UAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#87.87	Penn Treebank, Number of params	200 CTB - 5 UAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#92.41	Penn Treebank, Validation perplexity	200 Penn - YM LAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#93.50	Penn Treebank, Validation perplexity	200 Penn - YM UAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#91.28	Penn Treebank, Validation perplexity	200 Penn - S LAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#86.24	Penn Treebank, Validation perplexity	200 CTB - 5 LAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#93.64	Penn Treebank, Validation perplexity	200 Penn - S UAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#87.87	Penn Treebank, Validation perplexity	200 CTB - 5 UAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#92.41	Penn Treebank, F1	200 Penn - YM LAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#93.50	Penn Treebank, F1	200 Penn - YM UAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#91.28	Penn Treebank, F1	200 Penn - S LAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#86.24	Penn Treebank, F1	200 CTB - 5 LAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#93.64	Penn Treebank, F1	200 Penn - S UAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	D13-1093.pdf#87.87	Penn Treebank, F1	200 CTB - 5 UAS - - - - -  Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except  punctuations. We also include the tokens per second numbers for different parsers whenever available, although the  numbers from other papers were obtained on different machines. Speed numbers marked with  † were converted from  sentences per second.
true	P14-1050.pdf#0.582	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	400  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.859	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	100  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.699	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	500  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.612	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	300  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.571	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	400  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.717	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	300  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.755	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	100  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.741	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	300  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.808	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	200  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.907	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	100  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.652	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	200  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.543	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	500  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.613	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	300  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.662	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	400  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.548	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	500  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.680	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	200  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.759	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	200  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.743	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	100  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.632	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	500  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.723	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	400  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.582	Chinese Treebank 6, F1	400  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.859	Chinese Treebank 6, F1	100  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.699	Chinese Treebank 6, F1	500  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.612	Chinese Treebank 6, F1	300  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.571	Chinese Treebank 6, F1	400  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.717	Chinese Treebank 6, F1	300  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.755	Chinese Treebank 6, F1	100  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.741	Chinese Treebank 6, F1	300  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.808	Chinese Treebank 6, F1	200  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.907	Chinese Treebank 6, F1	100  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.652	Chinese Treebank 6, F1	200  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.543	Chinese Treebank 6, F1	500  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.613	Chinese Treebank 6, F1	300  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.662	Chinese Treebank 6, F1	400  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.548	Chinese Treebank 6, F1	500  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.680	Chinese Treebank 6, F1	200  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.759	Chinese Treebank 6, F1	200  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.743	Chinese Treebank 6, F1	100  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.632	Chinese Treebank 6, F1	500  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P14-1050.pdf#0.723	Chinese Treebank 6, F1	400  Table 4: Results with vs. without likelihood ratio test (LRT).
true	P13-1072.pdf#0.757‡	Chinese Treebank 6, F1	Pre  Table 2: Performance comparison on the CWS  task. The two bottom-most rows show upper  bound performance. ' ‡'('  *  ') in the top four lines  indicates statistical significance at p < 0.001  (0.05) when compared with the previous row.  Symbols in the bottom two lines indicate signifi- cant difference between upper bound systems and  their corresponding counterparts.
true	P13-1072.pdf#0.778*	Chinese Treebank 6, F1	F1  Table 2: Performance comparison on the CWS  task. The two bottom-most rows show upper  bound performance. ' ‡'('  *  ') in the top four lines  indicates statistical significance at p < 0.001  (0.05) when compared with the previous row.  Symbols in the bottom two lines indicate signifi- cant difference between upper bound systems and  their corresponding counterparts.
true	P13-1072.pdf#0.801‡	Chinese Treebank 6, F1	Rec  Table 2: Performance comparison on the CWS  task. The two bottom-most rows show upper  bound performance. ' ‡'('  *  ') in the top four lines  indicates statistical significance at p < 0.001  (0.05) when compared with the previous row.  Symbols in the bottom two lines indicate signifi- cant difference between upper bound systems and  their corresponding counterparts.
true	P13-1072.pdf#0.633*	Chinese Treebank 6, F1	OOVR  Table 2: Performance comparison on the CWS  task. The two bottom-most rows show upper  bound performance. ' ‡'('  *  ') in the top four lines  indicates statistical significance at p < 0.001  (0.05) when compared with the previous row.  Symbols in the bottom two lines indicate signifi- cant difference between upper bound systems and  their corresponding counterparts.
true	P13-1072.pdf#0.655*	Chinese Treebank 6, F1	Rec  Table 3: Performance comparison on the IWR  task. ' ‡' or '  *  ' in the top four rows indicates sta- tistical significance at p < 0.001 or < 0.05 com- pared with the previous row. Symbols in the bot- tom two rows indicate differences between upper  bound systems and their counterparts.
true	P13-1072.pdf#0.877*	Chinese Treebank 6, F1	Pre  Table 3: Performance comparison on the IWR  task. ' ‡' or '  *  ' in the top four rows indicates sta- tistical significance at p < 0.001 or < 0.05 com- pared with the previous row. Symbols in the bot- tom two rows indicate differences between upper  bound systems and their counterparts.
true	P13-1072.pdf#0.750*	Chinese Treebank 6, F1	F1  Table 3: Performance comparison on the IWR  task. ' ‡' or '  *  ' in the top four rows indicates sta- tistical significance at p < 0.001 or < 0.05 com- pared with the previous row. Symbols in the bot- tom two rows indicate differences between upper  bound systems and their counterparts.
true	P10-1138.pdf#16.1intheSensevalset),andWord-	SemEval 2013, F1	Table 1 : Coverage of Search Results : Wikipedia vs . WordNet Wikipedia 
true	P10-1138.pdf#(25.2	SemEval 2013, F1	Table 1 : Coverage of Search Results : Wikipedia vs . WordNet Wikipedia 
true	P10-1138.pdf#.69	SemEval 2013, F1	Table 4 : Classification Results Precision  Table 4: Classification Results
true	P10-1138.pdf#.67	SemEval 2013, F1	Table 4 : Classification Results Precision  Table 4: Classification Results
true	P10-1138.pdf#4.75	SemEval 2013, F1	49% Table 5 : Enhancement of Search Results Diversity 
true	P10-1138.pdf#16.1intheSensevalset),andWord-	SemEval 2015, F1	Table 1 : Coverage of Search Results : Wikipedia vs . WordNet Wikipedia 
true	P10-1138.pdf#(25.2	SemEval 2015, F1	Table 1 : Coverage of Search Results : Wikipedia vs . WordNet Wikipedia 
true	P10-1138.pdf#.69	SemEval 2015, F1	Table 4 : Classification Results Precision  Table 4: Classification Results
true	P10-1138.pdf#.67	SemEval 2015, F1	Table 4 : Classification Results Precision  Table 4: Classification Results
true	P10-1138.pdf#4.75	SemEval 2015, F1	49% Table 5 : Enhancement of Search Results Diversity 
true	P10-1138.pdf#16.1intheSensevalset),andWord-	Senseval 3, F1	Table 1 : Coverage of Search Results : Wikipedia vs . WordNet Wikipedia 
true	P10-1138.pdf#(25.2	Senseval 3, F1	Table 1 : Coverage of Search Results : Wikipedia vs . WordNet Wikipedia 
true	P10-1138.pdf#.69	Senseval 3, F1	Table 4 : Classification Results Precision  Table 4: Classification Results
true	P10-1138.pdf#.67	Senseval 3, F1	Table 4 : Classification Results Precision  Table 4: Classification Results
true	P10-1138.pdf#4.75	Senseval 3, F1	49% Table 5 : Enhancement of Search Results Diversity 
true	P10-1138.pdf#16.1intheSensevalset),andWord-	SemEval 2007, F1	Table 1 : Coverage of Search Results : Wikipedia vs . WordNet Wikipedia 
true	P10-1138.pdf#(25.2	SemEval 2007, F1	Table 1 : Coverage of Search Results : Wikipedia vs . WordNet Wikipedia 
true	P10-1138.pdf#.69	SemEval 2007, F1	Table 4 : Classification Results Precision  Table 4: Classification Results
true	P10-1138.pdf#.67	SemEval 2007, F1	Table 4 : Classification Results Precision  Table 4: Classification Results
true	P10-1138.pdf#4.75	SemEval 2007, F1	49% Table 5 : Enhancement of Search Results Diversity 
true	P10-1138.pdf#16.1intheSensevalset),andWord-	Senseval 2, F1	Table 1 : Coverage of Search Results : Wikipedia vs . WordNet Wikipedia 
true	P10-1138.pdf#(25.2	Senseval 2, F1	Table 1 : Coverage of Search Results : Wikipedia vs . WordNet Wikipedia 
true	P10-1138.pdf#.69	Senseval 2, F1	Table 4 : Classification Results Precision  Table 4: Classification Results
true	P10-1138.pdf#.67	Senseval 2, F1	Table 4 : Classification Results Precision  Table 4: Classification Results
true	P10-1138.pdf#4.75	Senseval 2, F1	49% Table 5 : Enhancement of Search Results Diversity 
true	P15-2027.pdf#65.6%	IMDb, Accuracy	lengths actually bias the learning greatly . indicating variant document α - NCE  Table 1: Comparison of classification accuracy on  the 20 Newsgroups dataset using RSMs.
true	P15-2027.pdf#64.8%	IMDb, Accuracy	lengths actually bias the learning greatly . indicating variant document α - NCE  Table 1: Comparison of classification accuracy on  the 20 Newsgroups dataset using RSMs.
true	P15-2027.pdf#87.09%	IMDb, Accuracy	Accuracy  Table 2: The performance of sentiment classifica- tion accuracy on the IMDB dataset using RSMs  compared to other BoW-based approaches.
true	P15-2027.pdf#87.81%	IMDb, Accuracy	Accuracy  Table 2: The performance of sentiment classifica- tion accuracy on the IMDB dataset using RSMs  compared to other BoW-based approaches.
true	D10-1019.pdf#94.31	Chinese Treebank 6, F1	sources Table 4 : Comparison with other systems on shallow pars - F1  Table 4: Comparison with other systems on shallow pars- ing task
true	D10-1019.pdf#90.88	Chinese Treebank 6, F1	sets and resources . candidate reranking 100 Results for Chinese word segmentation and F1  Table 8: Comparison of word segmentation and POS tag- ging, such comparison is indirect due to different data  sets and resources.  Model  F1  Pipeline (ours)  90.40  100-Best Reranking (ours)  90.53  Hybrid CRFs (ours)  90.88  Pipeline (Shi and Wang, 2007)  91.67  20-Best Reranking (Shi and Wang,  2007)
true	D10-1019.pdf#0.01)	Chinese Treebank 6, F1	POS tagging task , Hybrid CRFs significantly outperform candidate reranking 80 Results for Chinese word segmentation and  Table 8: Comparison of word segmentation and POS tag- ging, such comparison is indirect due to different data  sets and resources.  Model  F1  Pipeline (ours)  90.40  100-Best Reranking (ours)  90.53  Hybrid CRFs (ours)  90.88  Pipeline (Shi and Wang, 2007)  91.67  20-Best Reranking (Shi and Wang,  2007)
true	D10-1019.pdf#90.40	Chinese Treebank 6, F1	sets and resources . candidate reranking 100 Results for Chinese word segmentation and F1  Table 8: Comparison of word segmentation and POS tag- ging, such comparison is indirect due to different data  sets and resources.  Model  F1  Pipeline (ours)  90.40  100-Best Reranking (ours)  90.53  Hybrid CRFs (ours)  90.88  Pipeline (Shi and Wang, 2007)  91.67  20-Best Reranking (Shi and Wang,  2007)
true	D10-1019.pdf#91.67	Chinese Treebank 6, F1	sets and resources . candidate reranking 100 Results for Chinese word segmentation and F1  Table 8: Comparison of word segmentation and POS tag- ging, such comparison is indirect due to different data  sets and resources.  Model  F1  Pipeline (ours)  90.40  100-Best Reranking (ours)  90.53  Hybrid CRFs (ours)  90.88  Pipeline (Shi and Wang, 2007)  91.67  20-Best Reranking (Shi and Wang,  2007)
true	D10-1019.pdf#90.53	Chinese Treebank 6, F1	sets and resources . candidate reranking 100 Results for Chinese word segmentation and F1  Table 8: Comparison of word segmentation and POS tag- ging, such comparison is indirect due to different data  sets and resources.  Model  F1  Pipeline (ours)  90.40  100-Best Reranking (ours)  90.53  Hybrid CRFs (ours)  90.88  Pipeline (Shi and Wang, 2007)  91.67  20-Best Reranking (Shi and Wang,  2007)
true	P15-1080.pdf#39.45+	New York Times Corpus, P@10%	38 . 40 ( - 0 . 01 ) MT04 + + +  Table 4: BLEU4 in percentage for comparing of systems using different network structures (HPB refers  to the baseline hierarchical phrase-based system. TLayer, TDN, GN refer to the standard 2-layer network,  Two-Degree Hidden Layer Network, Grouped Network, respectively. Subscript of TLayer indicates the  number of nodes in the hidden layer.) + ,  *  marks results that are significant better than the baseline  system with p < 0.01 and p < 0.05.
true	P15-1080.pdf#38.51	New York Times Corpus, P@10%	38 . 40 ( - 0 . 01 ) MT05 + + +  Table 4: BLEU4 in percentage for comparing of systems using different network structures (HPB refers  to the baseline hierarchical phrase-based system. TLayer, TDN, GN refer to the standard 2-layer network,  Two-Degree Hidden Layer Network, Grouped Network, respectively. Subscript of TLayer indicates the  number of nodes in the hidden layer.) + ,  *  marks results that are significant better than the baseline  system with p < 0.01 and p < 0.05.
true	P15-1080.pdf#39.41+	New York Times Corpus, P@10%	38 . 40 ( - 0 . 01 ) MT02 ( dev ) + + +  Table 4: BLEU4 in percentage for comparing of systems using different network structures (HPB refers  to the baseline hierarchical phrase-based system. TLayer, TDN, GN refer to the standard 2-layer network,  Two-Degree Hidden Layer Network, Grouped Network, respectively. Subscript of TLayer indicates the  number of nodes in the hidden layer.) + ,  *  marks results that are significant better than the baseline  system with p < 0.01 and p < 0.05.
true	P15-1080.pdf#39.73+	New York Times Corpus, P@10%	38 . 40 ( - 0 . 01 ) MT03 ( train ) + + +  Table 4: BLEU4 in percentage for comparing of systems using different network structures (HPB refers  to the baseline hierarchical phrase-based system. TLayer, TDN, GN refer to the standard 2-layer network,  Two-Degree Hidden Layer Network, Grouped Network, respectively. Subscript of TLayer indicates the  number of nodes in the hidden layer.) + ,  *  marks results that are significant better than the baseline  system with p < 0.01 and p < 0.05.
true	P15-1080.pdf#200	New York Times Corpus, P@10%	Systems HPB 1 , 301  Table 5: Comparison of network scales and training time of different systems, including the number of  nodes in the hidden layer, the number of parameters, the average training time per iteration (15 iterations).  The notations of systems are the same as in Table4.
true	P15-1080.pdf#100	New York Times Corpus, P@10%	Systems HPB  Table 5: Comparison of network scales and training time of different systems, including the number of  nodes in the hidden layer, the number of parameters, the average training time per iteration (15 iterations).  The notations of systems are the same as in Table4.
true	D15-1141.pdf#95.7	PKU, F1	Length Dropout rate=20% F  Table 2: Performances of LSTM-1 with the different context lengths and dropout rates on PKU test set.
true	D15-1141.pdf#95.5	PKU, F1	Length Dropout rate=20% R  Table 2: Performances of LSTM-1 with the different context lengths and dropout rates on PKU test set.
true	D15-1141.pdf#95.8	PKU, F1	Length Dropout rate=20% P  Table 2: Performances of LSTM-1 with the different context lengths and dropout rates on PKU test set.
true	D15-1141.pdf#95.8	PKU, F1	models Contextr Length = ( 0 , 2 ) P  Table 3: Performance on our four proposed models  on PKU test set.
true	D15-1141.pdf#95.7	PKU, F1	models Contextr Length = ( 0 , 2 ) F  Table 3: Performance on our four proposed models  on PKU test set.
true	D15-1141.pdf#95.5	PKU, F1	models Contextr Length = ( 0 , 2 ) R  Table 3: Performance on our four proposed models  on PKU test set.
true	D15-1141.pdf#95.7	PKU, F1	models PKU F  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.2	PKU, F1	models MSRA R  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.4	PKU, F1	+Pre - train+bigram MSRA F MSRA F -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.0	PKU, F1	models CTB6 P  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.3	PKU, F1	+Pre - train+bigram MSRA R MSRA R -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.7	PKU, F1	models MSRA P  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.5	PKU, F1	models PKU R  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.5	PKU, F1	+Pre - train+bigram PKU F PKU F -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.6	PKU, F1	+Pre - train+bigram PKU P PKU P -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.8	PKU, F1	models PKU P  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.2	PKU, F1	+Pre - train+bigram CTB6 P CTB6 P -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.4	PKU, F1	models MSRA F  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#94.9	PKU, F1	models CTB6 F  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#94.8	PKU, F1	models CTB6 R  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.8	PKU, F1	+Pre - train+bigram CTB6 R CTB6 R -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.0	PKU, F1	+Pre - train+bigram CTB6 F CTB6 F -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.4	PKU, F1	+Pre - train+bigram PKU R PKU R -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.5	PKU, F1	+Pre - train+bigram MSRA P MSRA P -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.2	PKU, F1	+Pre - train+bigram CTB6 P -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.8	PKU, F1	+Pre - train+bigram CTB6 R -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.4	PKU, F1	+Pre - train+bigram PKU R -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.5	PKU, F1	+Pre - train+bigram PKU F -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.3	PKU, F1	+Pre - train+bigram MSRA R -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.0	PKU, F1	+Pre - train+bigram CTB6 F -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.4	PKU, F1	+Pre - train+bigram MSRA F -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.6	PKU, F1	+Pre - train+bigram PKU P -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.5	PKU, F1	+Pre - train+bigram MSRA P -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.4	PKU, F1	- MSRA  Table 6: Comparison of our model with state-of- the-art methods on three test sets.
true	D15-1141.pdf#96.0	PKU, F1	- CTB6  Table 6: Comparison of our model with state-of- the-art methods on three test sets.
true	D15-1141.pdf#96.5	PKU, F1	- PKU  Table 6: Comparison of our model with state-of- the-art methods on three test sets.
true	D15-1141.pdf#95.7	MSR, F1	Length Dropout rate=20% F  Table 2: Performances of LSTM-1 with the different context lengths and dropout rates on PKU test set.
true	D15-1141.pdf#95.5	MSR, F1	Length Dropout rate=20% R  Table 2: Performances of LSTM-1 with the different context lengths and dropout rates on PKU test set.
true	D15-1141.pdf#95.8	MSR, F1	Length Dropout rate=20% P  Table 2: Performances of LSTM-1 with the different context lengths and dropout rates on PKU test set.
true	D15-1141.pdf#95.8	MSR, F1	models Contextr Length = ( 0 , 2 ) P  Table 3: Performance on our four proposed models  on PKU test set.
true	D15-1141.pdf#95.7	MSR, F1	models Contextr Length = ( 0 , 2 ) F  Table 3: Performance on our four proposed models  on PKU test set.
true	D15-1141.pdf#95.5	MSR, F1	models Contextr Length = ( 0 , 2 ) R  Table 3: Performance on our four proposed models  on PKU test set.
true	D15-1141.pdf#95.7	MSR, F1	models PKU F  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.2	MSR, F1	models MSRA R  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.4	MSR, F1	+Pre - train+bigram MSRA F MSRA F -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.0	MSR, F1	models CTB6 P  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.3	MSR, F1	+Pre - train+bigram MSRA R MSRA R -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.7	MSR, F1	models MSRA P  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.5	MSR, F1	models PKU R  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.5	MSR, F1	+Pre - train+bigram PKU F PKU F -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.6	MSR, F1	+Pre - train+bigram PKU P PKU P -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.8	MSR, F1	models PKU P  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.2	MSR, F1	+Pre - train+bigram CTB6 P CTB6 P -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.4	MSR, F1	models MSRA F  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#94.9	MSR, F1	models CTB6 F  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#94.8	MSR, F1	models CTB6 R  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.8	MSR, F1	+Pre - train+bigram CTB6 R CTB6 R -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.0	MSR, F1	+Pre - train+bigram CTB6 F CTB6 F -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.4	MSR, F1	+Pre - train+bigram PKU R PKU R -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.5	MSR, F1	+Pre - train+bigram MSRA P MSRA P -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.2	MSR, F1	+Pre - train+bigram CTB6 P -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.8	MSR, F1	+Pre - train+bigram CTB6 R -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.4	MSR, F1	+Pre - train+bigram PKU R -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.5	MSR, F1	+Pre - train+bigram PKU F -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.3	MSR, F1	+Pre - train+bigram MSRA R -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.0	MSR, F1	+Pre - train+bigram CTB6 F -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.4	MSR, F1	+Pre - train+bigram MSRA F -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.6	MSR, F1	+Pre - train+bigram PKU P -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.5	MSR, F1	+Pre - train+bigram MSRA P -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.4	MSR, F1	- MSRA  Table 6: Comparison of our model with state-of- the-art methods on three test sets.
true	D15-1141.pdf#96.0	MSR, F1	- CTB6  Table 6: Comparison of our model with state-of- the-art methods on three test sets.
true	D15-1141.pdf#96.5	MSR, F1	- PKU  Table 6: Comparison of our model with state-of- the-art methods on three test sets.
true	D15-1141.pdf#95.7	Chinese Treebank 6, F1	Length Dropout rate=20% F  Table 2: Performances of LSTM-1 with the different context lengths and dropout rates on PKU test set.
true	D15-1141.pdf#95.5	Chinese Treebank 6, F1	Length Dropout rate=20% R  Table 2: Performances of LSTM-1 with the different context lengths and dropout rates on PKU test set.
true	D15-1141.pdf#95.8	Chinese Treebank 6, F1	Length Dropout rate=20% P  Table 2: Performances of LSTM-1 with the different context lengths and dropout rates on PKU test set.
true	D15-1141.pdf#95.8	Chinese Treebank 6, F1	models Contextr Length = ( 0 , 2 ) P  Table 3: Performance on our four proposed models  on PKU test set.
true	D15-1141.pdf#95.7	Chinese Treebank 6, F1	models Contextr Length = ( 0 , 2 ) F  Table 3: Performance on our four proposed models  on PKU test set.
true	D15-1141.pdf#95.5	Chinese Treebank 6, F1	models Contextr Length = ( 0 , 2 ) R  Table 3: Performance on our four proposed models  on PKU test set.
true	D15-1141.pdf#95.7	Chinese Treebank 6, F1	models PKU F  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.2	Chinese Treebank 6, F1	models MSRA R  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.4	Chinese Treebank 6, F1	+Pre - train+bigram MSRA F MSRA F -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.0	Chinese Treebank 6, F1	models CTB6 P  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.3	Chinese Treebank 6, F1	+Pre - train+bigram MSRA R MSRA R -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.7	Chinese Treebank 6, F1	models MSRA P  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.5	Chinese Treebank 6, F1	models PKU R  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.5	Chinese Treebank 6, F1	+Pre - train+bigram PKU F PKU F -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.6	Chinese Treebank 6, F1	+Pre - train+bigram PKU P PKU P -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.8	Chinese Treebank 6, F1	models PKU P  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.2	Chinese Treebank 6, F1	+Pre - train+bigram CTB6 P CTB6 P -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.4	Chinese Treebank 6, F1	models MSRA F  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#94.9	Chinese Treebank 6, F1	models CTB6 F  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#94.8	Chinese Treebank 6, F1	models CTB6 R  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.8	Chinese Treebank 6, F1	+Pre - train+bigram CTB6 R CTB6 R -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.0	Chinese Treebank 6, F1	+Pre - train+bigram CTB6 F CTB6 F -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.4	Chinese Treebank 6, F1	+Pre - train+bigram PKU R PKU R -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.5	Chinese Treebank 6, F1	+Pre - train+bigram MSRA P MSRA P -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.2	Chinese Treebank 6, F1	+Pre - train+bigram CTB6 P -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.8	Chinese Treebank 6, F1	+Pre - train+bigram CTB6 R -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.4	Chinese Treebank 6, F1	+Pre - train+bigram PKU R -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.5	Chinese Treebank 6, F1	+Pre - train+bigram PKU F -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.3	Chinese Treebank 6, F1	+Pre - train+bigram MSRA R -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.0	Chinese Treebank 6, F1	+Pre - train+bigram CTB6 F -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.4	Chinese Treebank 6, F1	+Pre - train+bigram MSRA F -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.6	Chinese Treebank 6, F1	+Pre - train+bigram PKU P -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.5	Chinese Treebank 6, F1	+Pre - train+bigram MSRA P -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.4	Chinese Treebank 6, F1	- MSRA  Table 6: Comparison of our model with state-of- the-art methods on three test sets.
true	D15-1141.pdf#96.0	Chinese Treebank 6, F1	- CTB6  Table 6: Comparison of our model with state-of- the-art methods on three test sets.
true	D15-1141.pdf#96.5	Chinese Treebank 6, F1	- PKU  Table 6: Comparison of our model with state-of- the-art methods on three test sets.
true	D15-1087.pdf#47.5	New York Times Corpus, P@10%	( a ) Results obtained using gold spatial elements . OLINK True F OLINK True  Table 3: Results for extracting spatial relations.
true	D15-1087.pdf#98.4	New York Times Corpus, P@10%	MOVELINK False F  Table 3: Results for extracting spatial relations.
true	D15-1087.pdf#66.4	New York Times Corpus, P@10%	( a ) Results obtained using gold spatial elements . OVERALL F OVERALL F  Table 3: Results for extracting spatial relations.
true	D15-1087.pdf#17.8	New York Times Corpus, P@10%	( a ) Results obtained using gold spatial elements . QSLINK True F QSLINK True F  Table 3: Results for extracting spatial relations.
true	D15-1087.pdf#52.3	New York Times Corpus, P@10%	QSLINK True F  Table 3: Results for extracting spatial relations.
true	D15-1087.pdf#79.7	New York Times Corpus, P@10%	OVERALL True F  Table 3: Results for extracting spatial relations.
true	D15-1087.pdf#99.9	New York Times Corpus, P@10%	( a ) Results obtained using gold spatial elements . QSLINK False F QSLINK False  Table 3: Results for extracting spatial relations.
true	D15-1087.pdf#34.2	New York Times Corpus, P@10%	( a ) Results obtained using gold spatial elements . OVERALL True F OVERALL True F  Table 3: Results for extracting spatial relations.
true	D15-1087.pdf#99.9	New York Times Corpus, P@10%	( a ) Results obtained using gold spatial elements . QSLINK False F QSLINK False F  Table 3: Results for extracting spatial relations.
true	D15-1087.pdf#99.6	New York Times Corpus, P@10%	QSLINK False F  Table 3: Results for extracting spatial relations.
true	D15-1087.pdf#83.4	New York Times Corpus, P@10%	OVERALL F  Table 3: Results for extracting spatial relations.
true	D15-1087.pdf#66.9	New York Times Corpus, P@10%	OLINK True F  Table 3: Results for extracting spatial relations.
true	D15-1087.pdf#47.5	New York Times Corpus, P@10%	( a ) Results obtained using gold spatial elements . OLINK True F OLINK True F  Table 3: Results for extracting spatial relations.
true	D15-1087.pdf#99.7	New York Times Corpus, P@10%	OLINK False  Table 3: Results for extracting spatial relations.
true	D15-1087.pdf#65.8	New York Times Corpus, P@10%	( a ) Results obtained using gold spatial elements . MOVELINK False F MOVELINK False F  Table 3: Results for extracting spatial relations.
true	D15-1087.pdf#47.5	New York Times Corpus, P@10%	( a ) Results obtained using gold spatial elements . OLINK True F OLINK True F  Table 3: Results for extracting spatial relations.
true	D15-1087.pdf#99.7	New York Times Corpus, P@10%	OLINK False F  Table 3: Results for extracting spatial relations.
true	D15-1087.pdf#99.7	New York Times Corpus, P@10%	OLINK False F  Table 3: Results for extracting spatial relations.
true	D15-1087.pdf#99.9	New York Times Corpus, P@10%	( a ) Results obtained using gold spatial elements . QSLINK False F QSLINK False F  Table 3: Results for extracting spatial relations.
true	D15-1087.pdf#66.9	New York Times Corpus, P@10%	OLINK True  Table 3: Results for extracting spatial relations.
true	D15-1087.pdf#24.2	New York Times Corpus, P@10%	( a ) Results obtained using gold spatial elements . source F source F  Table 4: Results for extracting MOVELINK optional roles.
true	D15-1087.pdf#56.8	New York Times Corpus, P@10%	goal  Table 4: Results for extracting MOVELINK optional roles.
true	D15-1087.pdf#40.0	New York Times Corpus, P@10%	source F  Table 4: Results for extracting MOVELINK optional roles.
true	D15-1087.pdf#22.7	New York Times Corpus, P@10%	( a ) Results obtained using gold spatial elements . midpoint F midpoint F  Table 4: Results for extracting MOVELINK optional roles.
true	D15-1087.pdf#26.3	New York Times Corpus, P@10%	( a ) Results obtained using gold spatial elements . motion - signal F motion signal F  Table 4: Results for extracting MOVELINK optional roles.
true	D15-1087.pdf#28.4	New York Times Corpus, P@10%	( a ) Results obtained using gold spatial elements . goal F goal F  Table 4: Results for extracting MOVELINK optional roles.
true	D15-1087.pdf#6.4	New York Times Corpus, P@10%	( a ) Results obtained using gold spatial elements . landmark F landmark F  Table 4: Results for extracting MOVELINK optional roles.
true	D15-1087.pdf#60.0	New York Times Corpus, P@10%	OVERALL F  Table 4: Results for extracting MOVELINK optional roles.
true	D15-1087.pdf#54.4	New York Times Corpus, P@10%	path F  Table 4: Results for extracting MOVELINK optional roles.
true	D15-1087.pdf#14.6	New York Times Corpus, P@10%	( a ) Results obtained using gold spatial elements . path F path F  Table 4: Results for extracting MOVELINK optional roles.
true	D15-1087.pdf#72.8	New York Times Corpus, P@10%	motion - signal F  Table 4: Results for extracting MOVELINK optional roles.
true	D15-1087.pdf#24.3	New York Times Corpus, P@10%	( a ) Results obtained using gold spatial elements . OVERALL F OVERALL F  Table 4: Results for extracting MOVELINK optional roles.
true	D15-1087.pdf#43.0	New York Times Corpus, P@10%	midpoint F  Table 4: Results for extracting MOVELINK optional roles.
true	D15-1087.pdf#24.2	New York Times Corpus, P@10%	landmark  Table 4: Results for extracting MOVELINK optional roles.
true	D15-1087.pdf#48.0	New York Times Corpus, P@10%	Gold F  Table 5: Results for extracting entire MOVELINKs  using gold and extracted elements.
true	D15-1087.pdf#20.3	New York Times Corpus, P@10%	Extracted F  Table 5: Results for extracting entire MOVELINKs  using gold and extracted elements.
true	P13-2019.pdf#92.34	Penn Treebank, F1	- ≤ 40  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#91.59	Penn Treebank, F1	- Full  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#89.62	Penn Treebank, F1	- Full  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#91.48	Penn Treebank, F1	- ≤ 40  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#87.5	Penn Treebank, F1	- UAS  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#86.52	Penn Treebank, F1	- DA  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#85.88	Penn Treebank, F1	- UAS  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#86.70	Penn Treebank, F1	- DA  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#92.34	benchmark Vietnamese dependency treebank VnDT, UAS	- ≤ 40  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#91.59	benchmark Vietnamese dependency treebank VnDT, UAS	- Full  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#89.62	benchmark Vietnamese dependency treebank VnDT, UAS	- Full  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#91.48	benchmark Vietnamese dependency treebank VnDT, UAS	- ≤ 40  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#87.5	benchmark Vietnamese dependency treebank VnDT, UAS	- UAS  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#86.52	benchmark Vietnamese dependency treebank VnDT, UAS	- DA  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#85.88	benchmark Vietnamese dependency treebank VnDT, UAS	- UAS  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#86.70	benchmark Vietnamese dependency treebank VnDT, UAS	- DA  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#92.34	Chinese Treebank 6, F1	- ≤ 40  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#91.59	Chinese Treebank 6, F1	- Full  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#89.62	Chinese Treebank 6, F1	- Full  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#91.48	Chinese Treebank 6, F1	- ≤ 40  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#87.5	Chinese Treebank 6, F1	- UAS  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#86.52	Chinese Treebank 6, F1	- DA  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#85.88	Chinese Treebank 6, F1	- UAS  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#86.70	Chinese Treebank 6, F1	- DA  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#92.34	Penn Treebank, UAS	- ≤ 40  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#91.59	Penn Treebank, UAS	- Full  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#89.62	Penn Treebank, UAS	- Full  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#91.48	Penn Treebank, UAS	- ≤ 40  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#87.5	Penn Treebank, UAS	- UAS  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#86.52	Penn Treebank, UAS	- DA  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#85.88	Penn Treebank, UAS	- UAS  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#86.70	Penn Treebank, UAS	- DA  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#92.34	Penn Treebank, POS	- ≤ 40  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#91.59	Penn Treebank, POS	- Full  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#89.62	Penn Treebank, POS	- Full  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#91.48	Penn Treebank, POS	- ≤ 40  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#87.5	Penn Treebank, POS	- UAS  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#86.52	Penn Treebank, POS	- DA  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#85.88	Penn Treebank, POS	- UAS  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#86.70	Penn Treebank, POS	- DA  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#92.34	Penn Treebank, LAS	- ≤ 40  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#91.59	Penn Treebank, LAS	- Full  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#89.62	Penn Treebank, LAS	- Full  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#91.48	Penn Treebank, LAS	- ≤ 40  Table 2: Comparison of different approach on  CTB4 test set using UAS metric. MaltParser =  Hall et al. (2006); MST M alt =Nivre and McDon-
true	P13-2019.pdf#87.5	Penn Treebank, LAS	- UAS  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#86.52	Penn Treebank, LAS	- DA  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#85.88	Penn Treebank, LAS	- UAS  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	P13-2019.pdf#86.70	Penn Treebank, LAS	- DA  Table 3: Comparison of different approaches on  CTB5 test set. Abbreviations D, C, H and S are as  in Table 2.
true	N13-1107.pdf#0.549	New York Times Corpus, P@10%	( Binary ) Table 2 : Relation extraction results on Treebank set F - score  Table 3: Relation extraction results on Treebank set  (Triple)
true	N13-1107.pdf#0.764	New York Times Corpus, P@10%	( Binary ) Table 2 : Relation extraction results on Treebank set F - score F - score  Table 3: Relation extraction results on Treebank set  (Triple)
true	N13-1107.pdf#0.536	New York Times Corpus, P@10%	( Binary ) Table 2 : Relation extraction results on Treebank set F - score F - score F - score  Table 3: Relation extraction results on Treebank set  (Triple)
true	N13-1107.pdf#0.721	New York Times Corpus, P@10%	( Binary ) Table 2 : Relation extraction results on Treebank set F - score F - score  Table 3: Relation extraction results on Treebank set  (Triple)
true	N13-1107.pdf#0.592	New York Times Corpus, P@10%	( Binary ) Table 2 : Relation extraction results on Treebank set F - score  Table 3: Relation extraction results on Treebank set  (Triple)
true	N13-1107.pdf#0.781	New York Times Corpus, P@10%	F - score  Table 4: Relation extraction results on REVERB set  (Triple).
true	N13-1107.pdf#0.793	New York Times Corpus, P@10%	F - score  Table 5: Relation extraction results on OLLIE set  (Triple).
true	D12-1128.pdf#54.3	SemEval 2015, F1	P  Table 1: Performance on SemEval-2010 all-words do- main WSD (nouns only subset). Best results for each  measure are bolded.  † indicates statistically significant  differences with respect to the monolingual setting.
true	D12-1128.pdf#51.2	SemEval 2015, F1	R  Table 1: Performance on SemEval-2010 all-words do- main WSD (nouns only subset). Best results for each  measure are bolded.  † indicates statistically significant  differences with respect to the monolingual setting.
true	D12-1128.pdf#52.2	SemEval 2015, F1	F 1  Table 1: Performance on SemEval-2010 all-words do- main WSD (nouns only subset). Best results for each  measure are bolded.  † indicates statistically significant  differences with respect to the monolingual setting.
true	D12-1128.pdf#61.4	SemEval 2015, F1	F 1  Table 2: Performance on SemEval-2010 all-words do- main WSD (nouns only subset) using the most frequent  sense assigned by the system as back-off strategy when  no sense assignment is attempted.
true	D12-1128.pdf#60.4	SemEval 2015, F1	R  Table 2: Performance on SemEval-2010 all-words do- main WSD (nouns only subset) using the most frequent  sense assigned by the system as back-off strategy when  no sense assignment is attempted.
true	D12-1128.pdf#62.5	SemEval 2015, F1	P  Table 2: Performance on SemEval-2010 all-words do- main WSD (nouns only subset) using the most frequent  sense assigned by the system as back-off strategy when  no sense assignment is attempted.
true	D12-1128.pdf#24.61	SemEval 2015, F1	French P / R / F 1  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#23.65	SemEval 2015, F1	Spanish P / R / F  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#19.05	SemEval 2015, F1	Italian P / R / F  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#18.26	SemEval 2015, F1	German P / R / F  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#54.3	SemEval 2007, F1	P  Table 1: Performance on SemEval-2010 all-words do- main WSD (nouns only subset). Best results for each  measure are bolded.  † indicates statistically significant  differences with respect to the monolingual setting.
true	D12-1128.pdf#51.2	SemEval 2007, F1	R  Table 1: Performance on SemEval-2010 all-words do- main WSD (nouns only subset). Best results for each  measure are bolded.  † indicates statistically significant  differences with respect to the monolingual setting.
true	D12-1128.pdf#52.2	SemEval 2007, F1	F 1  Table 1: Performance on SemEval-2010 all-words do- main WSD (nouns only subset). Best results for each  measure are bolded.  † indicates statistically significant  differences with respect to the monolingual setting.
true	D12-1128.pdf#61.4	SemEval 2007, F1	F 1  Table 2: Performance on SemEval-2010 all-words do- main WSD (nouns only subset) using the most frequent  sense assigned by the system as back-off strategy when  no sense assignment is attempted.
true	D12-1128.pdf#60.4	SemEval 2007, F1	R  Table 2: Performance on SemEval-2010 all-words do- main WSD (nouns only subset) using the most frequent  sense assigned by the system as back-off strategy when  no sense assignment is attempted.
true	D12-1128.pdf#62.5	SemEval 2007, F1	P  Table 2: Performance on SemEval-2010 all-words do- main WSD (nouns only subset) using the most frequent  sense assigned by the system as back-off strategy when  no sense assignment is attempted.
true	D12-1128.pdf#24.61	SemEval 2007, F1	French P / R / F 1  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#23.65	SemEval 2007, F1	Spanish P / R / F  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#19.05	SemEval 2007, F1	Italian P / R / F  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#18.26	SemEval 2007, F1	German P / R / F  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#54.3	Senseval 2, F1	P  Table 1: Performance on SemEval-2010 all-words do- main WSD (nouns only subset). Best results for each  measure are bolded.  † indicates statistically significant  differences with respect to the monolingual setting.
true	D12-1128.pdf#51.2	Senseval 2, F1	R  Table 1: Performance on SemEval-2010 all-words do- main WSD (nouns only subset). Best results for each  measure are bolded.  † indicates statistically significant  differences with respect to the monolingual setting.
true	D12-1128.pdf#52.2	Senseval 2, F1	F 1  Table 1: Performance on SemEval-2010 all-words do- main WSD (nouns only subset). Best results for each  measure are bolded.  † indicates statistically significant  differences with respect to the monolingual setting.
true	D12-1128.pdf#61.4	Senseval 2, F1	F 1  Table 2: Performance on SemEval-2010 all-words do- main WSD (nouns only subset) using the most frequent  sense assigned by the system as back-off strategy when  no sense assignment is attempted.
true	D12-1128.pdf#60.4	Senseval 2, F1	R  Table 2: Performance on SemEval-2010 all-words do- main WSD (nouns only subset) using the most frequent  sense assigned by the system as back-off strategy when  no sense assignment is attempted.
true	D12-1128.pdf#62.5	Senseval 2, F1	P  Table 2: Performance on SemEval-2010 all-words do- main WSD (nouns only subset) using the most frequent  sense assigned by the system as back-off strategy when  no sense assignment is attempted.
true	D12-1128.pdf#24.61	Senseval 2, F1	French P / R / F 1  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#23.65	Senseval 2, F1	Spanish P / R / F  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#19.05	Senseval 2, F1	Italian P / R / F  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#18.26	Senseval 2, F1	German P / R / F  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#54.3	Senseval 3, F1	P  Table 1: Performance on SemEval-2010 all-words do- main WSD (nouns only subset). Best results for each  measure are bolded.  † indicates statistically significant  differences with respect to the monolingual setting.
true	D12-1128.pdf#51.2	Senseval 3, F1	R  Table 1: Performance on SemEval-2010 all-words do- main WSD (nouns only subset). Best results for each  measure are bolded.  † indicates statistically significant  differences with respect to the monolingual setting.
true	D12-1128.pdf#52.2	Senseval 3, F1	F 1  Table 1: Performance on SemEval-2010 all-words do- main WSD (nouns only subset). Best results for each  measure are bolded.  † indicates statistically significant  differences with respect to the monolingual setting.
true	D12-1128.pdf#61.4	Senseval 3, F1	F 1  Table 2: Performance on SemEval-2010 all-words do- main WSD (nouns only subset) using the most frequent  sense assigned by the system as back-off strategy when  no sense assignment is attempted.
true	D12-1128.pdf#60.4	Senseval 3, F1	R  Table 2: Performance on SemEval-2010 all-words do- main WSD (nouns only subset) using the most frequent  sense assigned by the system as back-off strategy when  no sense assignment is attempted.
true	D12-1128.pdf#62.5	Senseval 3, F1	P  Table 2: Performance on SemEval-2010 all-words do- main WSD (nouns only subset) using the most frequent  sense assigned by the system as back-off strategy when  no sense assignment is attempted.
true	D12-1128.pdf#24.61	Senseval 3, F1	French P / R / F 1  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#23.65	Senseval 3, F1	Spanish P / R / F  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#19.05	Senseval 3, F1	Italian P / R / F  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#18.26	Senseval 3, F1	German P / R / F  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#54.3	SemEval 2013, F1	P  Table 1: Performance on SemEval-2010 all-words do- main WSD (nouns only subset). Best results for each  measure are bolded.  † indicates statistically significant  differences with respect to the monolingual setting.
true	D12-1128.pdf#51.2	SemEval 2013, F1	R  Table 1: Performance on SemEval-2010 all-words do- main WSD (nouns only subset). Best results for each  measure are bolded.  † indicates statistically significant  differences with respect to the monolingual setting.
true	D12-1128.pdf#52.2	SemEval 2013, F1	F 1  Table 1: Performance on SemEval-2010 all-words do- main WSD (nouns only subset). Best results for each  measure are bolded.  † indicates statistically significant  differences with respect to the monolingual setting.
true	D12-1128.pdf#61.4	SemEval 2013, F1	F 1  Table 2: Performance on SemEval-2010 all-words do- main WSD (nouns only subset) using the most frequent  sense assigned by the system as back-off strategy when  no sense assignment is attempted.
true	D12-1128.pdf#60.4	SemEval 2013, F1	R  Table 2: Performance on SemEval-2010 all-words do- main WSD (nouns only subset) using the most frequent  sense assigned by the system as back-off strategy when  no sense assignment is attempted.
true	D12-1128.pdf#62.5	SemEval 2013, F1	P  Table 2: Performance on SemEval-2010 all-words do- main WSD (nouns only subset) using the most frequent  sense assigned by the system as back-off strategy when  no sense assignment is attempted.
true	D12-1128.pdf#24.61	SemEval 2013, F1	French P / R / F 1  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#23.65	SemEval 2013, F1	Spanish P / R / F  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#19.05	SemEval 2013, F1	Italian P / R / F  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#18.26	SemEval 2013, F1	German P / R / F  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#54.3	SemEval-2010 Task 8, F1	P  Table 1: Performance on SemEval-2010 all-words do- main WSD (nouns only subset). Best results for each  measure are bolded.  † indicates statistically significant  differences with respect to the monolingual setting.
true	D12-1128.pdf#51.2	SemEval-2010 Task 8, F1	R  Table 1: Performance on SemEval-2010 all-words do- main WSD (nouns only subset). Best results for each  measure are bolded.  † indicates statistically significant  differences with respect to the monolingual setting.
true	D12-1128.pdf#52.2	SemEval-2010 Task 8, F1	F 1  Table 1: Performance on SemEval-2010 all-words do- main WSD (nouns only subset). Best results for each  measure are bolded.  † indicates statistically significant  differences with respect to the monolingual setting.
true	D12-1128.pdf#61.4	SemEval-2010 Task 8, F1	F 1  Table 2: Performance on SemEval-2010 all-words do- main WSD (nouns only subset) using the most frequent  sense assigned by the system as back-off strategy when  no sense assignment is attempted.
true	D12-1128.pdf#60.4	SemEval-2010 Task 8, F1	R  Table 2: Performance on SemEval-2010 all-words do- main WSD (nouns only subset) using the most frequent  sense assigned by the system as back-off strategy when  no sense assignment is attempted.
true	D12-1128.pdf#62.5	SemEval-2010 Task 8, F1	P  Table 2: Performance on SemEval-2010 all-words do- main WSD (nouns only subset) using the most frequent  sense assigned by the system as back-off strategy when  no sense assignment is attempted.
true	D12-1128.pdf#24.61	SemEval-2010 Task 8, F1	French P / R / F 1  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#23.65	SemEval-2010 Task 8, F1	Spanish P / R / F  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#19.05	SemEval-2010 Task 8, F1	Italian P / R / F  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	D12-1128.pdf#18.26	SemEval-2010 Task 8, F1	German P / R / F  Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).
true	P15-1102.pdf#0.752	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Avg .  Table 1: The semi-supervised classification accu- racy of ten systems.
true	P15-1102.pdf#0.780	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	ELEC KITC  Table 1: The semi-supervised classification accu- racy of ten systems.
true	P15-1102.pdf#0.769	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	ELEC KITC  Table 1: The semi-supervised classification accu- racy of ten systems.
true	P15-1102.pdf#0.721	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	BOOK  Table 1: The semi-supervised classification accu- racy of ten systems.
true	P15-1102.pdf#0.738	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	DVD  Table 1: The semi-supervised classification accu- racy of ten systems.
true	D11-1006.pdf#82.5	benchmark Vietnamese dependency treebank VnDT, UAS	Source Training Language en  Table 2: UAS for all source-target language pairs. Each column represents which source language was used to train a  delexicalized parser and each row represents which target language test data was used. Bold numbers are when source  equals target and underlined numbers are the single best UAS for a target language. Results are for all sentence lengths  without punctuation.
true	D11-1006.pdf#84.8	benchmark Vietnamese dependency treebank VnDT, UAS	Source Training Language sv  Table 2: UAS for all source-target language pairs. Each column represents which source language was used to train a  delexicalized parser and each row represents which target language test data was used. Bold numbers are when source  equals target and underlined numbers are the single best UAS for a target language. Results are for all sentence lengths  without punctuation.
true	D11-1006.pdf#79.2	benchmark Vietnamese dependency treebank VnDT, UAS	Source Training Language  Table 2: UAS for all source-target language pairs. Each column represents which source language was used to train a  delexicalized parser and each row represents which target language test data was used. Bold numbers are when source  equals target and underlined numbers are the single best UAS for a target language. Results are for all sentence lengths  without punctuation.
true	D11-1006.pdf#83.9	benchmark Vietnamese dependency treebank VnDT, UAS	Source Training Language de  Table 2: UAS for all source-target language pairs. Each column represents which source language was used to train a  delexicalized parser and each row represents which target language test data was used. Bold numbers are when source  equals target and underlined numbers are the single best UAS for a target language. Results are for all sentence lengths  without punctuation.
true	D11-1006.pdf#84.6	benchmark Vietnamese dependency treebank VnDT, UAS	Source Training Language pt  Table 2: UAS for all source-target language pairs. Each column represents which source language was used to train a  delexicalized parser and each row represents which target language test data was used. Bold numbers are when source  equals target and underlined numbers are the single best UAS for a target language. Results are for all sentence lengths  without punctuation.
true	D11-1006.pdf#79.3	benchmark Vietnamese dependency treebank VnDT, UAS	Source Training Language it  Table 2: UAS for all source-target language pairs. Each column represents which source language was used to train a  delexicalized parser and each row represents which target language test data was used. Bold numbers are when source  equals target and underlined numbers are the single best UAS for a target language. Results are for all sentence lengths  without punctuation.
true	D11-1006.pdf#73.6	benchmark Vietnamese dependency treebank VnDT, UAS	Source Training Language nl  Table 2: UAS for all source-target language pairs. Each column represents which source language was used to train a  delexicalized parser and each row represents which target language test data was used. Bold numbers are when source  equals target and underlined numbers are the single best UAS for a target language. Results are for all sentence lengths  without punctuation.
true	D11-1006.pdf#77.5	benchmark Vietnamese dependency treebank VnDT, UAS	Source Training Language el  Table 2: UAS for all source-target language pairs. Each column represents which source language was used to train a  delexicalized parser and each row represents which target language test data was used. Bold numbers are when source  equals target and underlined numbers are the single best UAS for a target language. Results are for all sentence lengths  without punctuation.
true	D11-1006.pdf#79.7	benchmark Vietnamese dependency treebank VnDT, UAS	Source Training Language es  Table 2: UAS for all source-target language pairs. Each column represents which source language was used to train a  delexicalized parser and each row represents which target language test data was used. Bold numbers are when source  equals target and underlined numbers are the single best UAS for a target language. Results are for all sentence lengths  without punctuation.
true	D11-1006.pdf#82.5	Penn Treebank, UAS	Source Training Language en  Table 2: UAS for all source-target language pairs. Each column represents which source language was used to train a  delexicalized parser and each row represents which target language test data was used. Bold numbers are when source  equals target and underlined numbers are the single best UAS for a target language. Results are for all sentence lengths  without punctuation.
true	D11-1006.pdf#84.8	Penn Treebank, UAS	Source Training Language sv  Table 2: UAS for all source-target language pairs. Each column represents which source language was used to train a  delexicalized parser and each row represents which target language test data was used. Bold numbers are when source  equals target and underlined numbers are the single best UAS for a target language. Results are for all sentence lengths  without punctuation.
true	D11-1006.pdf#79.2	Penn Treebank, UAS	Source Training Language  Table 2: UAS for all source-target language pairs. Each column represents which source language was used to train a  delexicalized parser and each row represents which target language test data was used. Bold numbers are when source  equals target and underlined numbers are the single best UAS for a target language. Results are for all sentence lengths  without punctuation.
true	D11-1006.pdf#83.9	Penn Treebank, UAS	Source Training Language de  Table 2: UAS for all source-target language pairs. Each column represents which source language was used to train a  delexicalized parser and each row represents which target language test data was used. Bold numbers are when source  equals target and underlined numbers are the single best UAS for a target language. Results are for all sentence lengths  without punctuation.
true	D11-1006.pdf#84.6	Penn Treebank, UAS	Source Training Language pt  Table 2: UAS for all source-target language pairs. Each column represents which source language was used to train a  delexicalized parser and each row represents which target language test data was used. Bold numbers are when source  equals target and underlined numbers are the single best UAS for a target language. Results are for all sentence lengths  without punctuation.
true	D11-1006.pdf#79.3	Penn Treebank, UAS	Source Training Language it  Table 2: UAS for all source-target language pairs. Each column represents which source language was used to train a  delexicalized parser and each row represents which target language test data was used. Bold numbers are when source  equals target and underlined numbers are the single best UAS for a target language. Results are for all sentence lengths  without punctuation.
true	D11-1006.pdf#73.6	Penn Treebank, UAS	Source Training Language nl  Table 2: UAS for all source-target language pairs. Each column represents which source language was used to train a  delexicalized parser and each row represents which target language test data was used. Bold numbers are when source  equals target and underlined numbers are the single best UAS for a target language. Results are for all sentence lengths  without punctuation.
true	D11-1006.pdf#77.5	Penn Treebank, UAS	Source Training Language el  Table 2: UAS for all source-target language pairs. Each column represents which source language was used to train a  delexicalized parser and each row represents which target language test data was used. Bold numbers are when source  equals target and underlined numbers are the single best UAS for a target language. Results are for all sentence lengths  without punctuation.
true	D11-1006.pdf#79.7	Penn Treebank, UAS	Source Training Language es  Table 2: UAS for all source-target language pairs. Each column represents which source language was used to train a  delexicalized parser and each row represents which target language test data was used. Bold numbers are when source  equals target and underlined numbers are the single best UAS for a target language. Results are for all sentence lengths  without punctuation.
true	P13-2060.pdf#23.89	WMT 2014 EN-FR, BLEU	WMAX  Table 2: Testset BLEU scores when applying stacking on the devset only (using no specific training set).
true	P13-2060.pdf#23.55	WMT 2014 EN-FR, BLEU	PROD  Table 2: Testset BLEU scores when applying stacking on the devset only (using no specific training set).
true	P13-2060.pdf#22.48	WMT 2014 EN-FR, BLEU	WMAX  Table 2: Testset BLEU scores when applying stacking on the devset only (using no specific training set).
true	P13-2060.pdf#21.62	WMT 2014 EN-FR, BLEU	WSUM  Table 2: Testset BLEU scores when applying stacking on the devset only (using no specific training set).
true	P13-2060.pdf#22.91	WMT 2014 EN-FR, BLEU	PROD  Table 2: Testset BLEU scores when applying stacking on the devset only (using no specific training set).
true	P13-2060.pdf#23.78	WMT 2014 EN-FR, BLEU	PROD  Table 2: Testset BLEU scores when applying stacking on the devset only (using no specific training set).
true	P13-2060.pdf#22.92	WMT 2014 EN-FR, BLEU	SW : SUM  Table 2: Testset BLEU scores when applying stacking on the devset only (using no specific training set).
true	P13-2060.pdf#34.21	WMT 2014 EN-FR, BLEU	WSUM  Table 3: Testset BLEU scores when using 10k and 100k sentence training sets along with the devset.
true	P13-2060.pdf#29.87	WMT 2014 EN-FR, BLEU	WSUM  Table 3: Testset BLEU scores when using 10k and 100k sentence training sets along with the devset.
true	P13-2060.pdf#34.22	WMT 2014 EN-FR, BLEU	SW : SUM  Table 3: Testset BLEU scores when using 10k and 100k sentence training sets along with the devset.
true	P13-2060.pdf#34.07	WMT 2014 EN-FR, BLEU	WMAX  Table 3: Testset BLEU scores when using 10k and 100k sentence training sets along with the devset.
true	P13-2060.pdf#29.59	WMT 2014 EN-FR, BLEU	WSUM  Table 3: Testset BLEU scores when using 10k and 100k sentence training sets along with the devset.
true	P13-2060.pdf#23.89	WMT 2014 EN-DE, BLEU	WMAX  Table 2: Testset BLEU scores when applying stacking on the devset only (using no specific training set).
true	P13-2060.pdf#23.55	WMT 2014 EN-DE, BLEU	PROD  Table 2: Testset BLEU scores when applying stacking on the devset only (using no specific training set).
true	P13-2060.pdf#22.48	WMT 2014 EN-DE, BLEU	WMAX  Table 2: Testset BLEU scores when applying stacking on the devset only (using no specific training set).
true	P13-2060.pdf#21.62	WMT 2014 EN-DE, BLEU	WSUM  Table 2: Testset BLEU scores when applying stacking on the devset only (using no specific training set).
true	P13-2060.pdf#22.91	WMT 2014 EN-DE, BLEU	PROD  Table 2: Testset BLEU scores when applying stacking on the devset only (using no specific training set).
true	P13-2060.pdf#23.78	WMT 2014 EN-DE, BLEU	PROD  Table 2: Testset BLEU scores when applying stacking on the devset only (using no specific training set).
true	P13-2060.pdf#22.92	WMT 2014 EN-DE, BLEU	SW : SUM  Table 2: Testset BLEU scores when applying stacking on the devset only (using no specific training set).
true	P13-2060.pdf#34.21	WMT 2014 EN-DE, BLEU	WSUM  Table 3: Testset BLEU scores when using 10k and 100k sentence training sets along with the devset.
true	P13-2060.pdf#29.87	WMT 2014 EN-DE, BLEU	WSUM  Table 3: Testset BLEU scores when using 10k and 100k sentence training sets along with the devset.
true	P13-2060.pdf#34.22	WMT 2014 EN-DE, BLEU	SW : SUM  Table 3: Testset BLEU scores when using 10k and 100k sentence training sets along with the devset.
true	P13-2060.pdf#34.07	WMT 2014 EN-DE, BLEU	WMAX  Table 3: Testset BLEU scores when using 10k and 100k sentence training sets along with the devset.
true	P13-2060.pdf#29.59	WMT 2014 EN-DE, BLEU	WSUM  Table 3: Testset BLEU scores when using 10k and 100k sentence training sets along with the devset.
true	N10-1099.pdf#1.0	Text8, Number of params	task configuration ) 2008 , L ( n=64 ) R FEATURES L ( n=48 ) P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#43	Text8, Number of params	task configuration ) 2008 , L ( n=64 ) F FEATURES L ( n=48 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#86	Text8, Number of params	task configuration ) 2008 , L+H ( n=260 ) P FEATURES L+H ( n=258 ) P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#68	Text8, Number of params	summary task configuration ) 2008 , L ( n=64 ) P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#68	Text8, Number of params	summary task configuration ) 2008 , L ( n=64 ) P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#44	Text8, Number of params	task configuration ) 2008 , L ( n=64 ) F FEATURES L ( n=48 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#60	Text8, Number of params	task configuration ) 2008 , L ( n=64 ) F FEATURES L ( n=48 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#90	Text8, Number of params	summary task configuration ) 2008 , ( n=196 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#70	Text8, Number of params	summary task configuration ) 2008 , L ( n=64 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#94	Text8, Number of params	task configuration ) 2008 , ( n=196 ) F FEATURES H ( n=210 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#91	Text8, Number of params	SOX_TT+ 2008 , L+H ( n=260 ) P FEATURES L+H ( n=258 ) P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#71	Text8, Number of params	task configuration ) 2008 , L ( n=64 ) R FEATURES L ( n=48 ) P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#66	Text8, Number of params	SOX_TT+ 2008 , H P FEATURES L ( n=48 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#0.598	Text8, Number of params	GUMS+O_TT_Shellnouns+ EMNLP L ( n=64 ) WK FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#60	Text8, Number of params	task configuration ) 2008 , H P FEATURES L ( n=48 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#85	Text8, Number of params	GUMS+O_TT_Shellnouns+ 2008 , L+H ( n=260 ) P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#0.580	Text8, Number of params	task configuration ) 2008 , L ( n=64 ) WK FEATURES L ( n=48 ) WK FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#76	Text8, Number of params	summary task configuration ) 2008 , L ( n=64 ) P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#90	Text8, Number of params	summary task configuration ) 2008 , H P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#70	Text8, Number of params	GUMS+O_TT_Shellnouns+ 2008 , L ( n=64 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#88	Text8, Number of params	task configuration ) 2008 , L+H ( n=260 ) P FEATURES L+H ( n=258 ) P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#49	Text8, Number of params	SOX_TT+ 2008 , L ( n=64 ) F FEATURES L ( n=48 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#90	Text8, Number of params	task configuration ) 2008 , L+H ( n=260 ) R FEATURES L+H ( n=258 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#72	Text8, Number of params	summary task configuration ) 2008 , L ( n=64 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#83	Text8, Number of params	summary task configuration ) 2008 , L+H ( n=260 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#90	Text8, Number of params	summary task configuration ) 2008 , ( n=196 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#91	Text8, Number of params	SOX_TT+ 2008 , L+H ( n=260 ) R FEATURES L+H ( n=258 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#90	Text8, Number of params	summary task configuration ) 2008 , ( n=196 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#54	Text8, Number of params	task configuration ) 2008 , H P FEATURES L ( n=48 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#91	Text8, Number of params	SOX_TT+ 2008 , L+H ( n=260 ) F FEATURES L+H ( n=258 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#89	Text8, Number of params	summary task configuration ) 2008 , ( n=196 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#88	Text8, Number of params	task configuration ) 2008 , L+H ( n=260 ) F FEATURES L+H ( n=258 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#69	Text8, Number of params	GUMS+O_TT_Shellnouns+ 2008 , L ( n=64 ) P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#86	Text8, Number of params	task configuration ) 2008 , L+H ( n=260 ) F FEATURES L+H ( n=258 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#0.595	Text8, Number of params	summary task configuration ) EMNLP L ( n=64 ) WK FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#45	Text8, Number of params	summary task configuration ) 2008 , L ( n=64 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#70	Text8, Number of params	task configuration ) 2008 , L ( n=64 ) R FEATURES L ( n=48 ) P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#69	Text8, Number of params	summary task configuration ) 2008 , L ( n=64 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#0.547	Text8, Number of params	task configuration ) 2008 , L ( n=64 ) WK FEATURES L ( n=48 ) WK FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#1.0	Text8, Number of params	SOX_TT+ 2008 , ( n=196 ) F FEATURES H ( n=210 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#95	Text8, Number of params	summary task configuration ) 2008 , ( n=196 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#88	Text8, Number of params	task configuration ) 2008 , L+H ( n=260 ) R FEATURES L+H ( n=258 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#0.610	Text8, Number of params	SOX_TT+ 2008 , L ( n=64 ) WK FEATURES L ( n=48 ) WK FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#91	Text8, Number of params	summary task configuration ) 2008 , H P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#57	Text8, Number of params	summary task configuration ) 2008 , L ( n=64 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#89	Text8, Number of params	summary task configuration ) 2008 , ( n=196 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#89	Text8, Number of params	task configuration ) 2008 , H R FEATURES H ( n=210 ) P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#86	Text8, Number of params	task configuration ) 2008 , L+H ( n=260 ) R FEATURES L+H ( n=258 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#90	Text8, Number of params	GUMS+O_TT_Shellnouns+ 2008 , ( n=196 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#85	Text8, Number of params	GUMS+O_TT_Shellnouns+ 2008 , L+H ( n=260 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#85	Text8, Number of params	summary task configuration ) 2008 , L+H ( n=260 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#91	Text8, Number of params	task configuration ) 2008 , H R FEATURES H ( n=210 ) P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#88	Text8, Number of params	task configuration ) 2008 , H R FEATURES H ( n=210 ) P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#65	Text8, Number of params	task configuration ) 2008 , H P FEATURES L ( n=48 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#1.0	Text8, Number of params	task configuration ) 2008 , ( n=196 ) R FEATURES H ( n=210 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#96	Text8, Number of params	task configuration ) 2008 , ( n=196 ) F FEATURES H ( n=210 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#0.472	Text8, Number of params	summary task configuration ) EMNLP L ( n=64 ) WK FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#85	Text8, Number of params	summary task configuration ) 2008 , L+H ( n=260 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#1.0	Text8, Number of params	SOX_TT+ 2008 , L ( n=64 ) R FEATURES L ( n=48 ) P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#90	Text8, Number of params	GUMS+O_TT_Shellnouns+ 2008 , H P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#70	Text8, Number of params	summary task configuration ) 2008 , L ( n=64 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#92	Text8, Number of params	task configuration ) 2008 , L+H ( n=260 ) P FEATURES H ( n=210 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#95	Text8, Number of params	SOX_TT+ 2008 , L+H ( n=260 ) P FEATURES H ( n=210 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#85	Text8, Number of params	GUMS+O_TT_Shellnouns+ 2008 , L+H ( n=260 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#90	Text8, Number of params	task configuration ) 2008 , L+H ( n=260 ) P FEATURES L+H ( n=258 ) P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#83	Text8, Number of params	summary task configuration ) 2008 , L+H ( n=260 ) P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#90	Text8, Number of params	GUMS+O_TT_Shellnouns+ 2008 , ( n=196 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#93	Text8, Number of params	task configuration ) 2008 , L+H ( n=260 ) P FEATURES H ( n=210 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#90	Text8, Number of params	task configuration ) 2008 , L+H ( n=260 ) F FEATURES L+H ( n=258 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#85	Text8, Number of params	summary task configuration ) 2008 , L+H ( n=260 ) P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#83	Text8, Number of params	summary task configuration ) 2008 , L+H ( n=260 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#90	Text8, Number of params	SOX_TT+ 2008 , H R FEATURES H ( n=210 ) P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#94	Text8, Number of params	task configuration ) 2008 , L+H ( n=260 ) P FEATURES H ( n=210 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#84	Text8, Number of params	summary task configuration ) 2008 , H P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#85	Text8, Number of params	summary task configuration ) 2008 , L+H ( n=260 ) P FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#85	Text8, Number of params	summary task configuration ) 2008 , L+H ( n=260 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#85	Text8, Number of params	summary task configuration ) 2008 , L+H ( n=260 ) F FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#0.462	Text8, Number of params	task configuration ) 2008 , L ( n=64 ) WK FEATURES L ( n=48 ) WK FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#70	Text8, Number of params	GUMS+O_TT_Shellnouns+ 2008 , L ( n=64 ) R FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	N10-1099.pdf#0.590	Text8, Number of params	summary task configuration ) EMNLP L ( n=64 ) WK FEATURES  Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
true	D12-1036.pdf#0.851*	New York Times Corpus, P@10%	P  Table 2: Performance of question analysis.
true	D12-1036.pdf#0.763*	New York Times Corpus, P@10%	R  Table 2: Performance of question analysis.
true	D12-1036.pdf#0.805*	New York Times Corpus, P@10%	F 1  Table 2: Performance of question analysis.
true	D12-1036.pdf#0.805*	New York Times Corpus, P@10%	F 1  Table 3: Performance of aspect identification for question  analysis. * denotes the results (i.e. P , R, F 1 ) are tested  for statistical significance using T-Test, p-values<0.05.
true	D12-1036.pdf#0.763*	New York Times Corpus, P@10%	R  Table 3: Performance of aspect identification for question  analysis. * denotes the results (i.e. P , R, F 1 ) are tested  for statistical significance using T-Test, p-values<0.05.
true	D12-1036.pdf#0.851*	New York Times Corpus, P@10%	P  Table 3: Performance of aspect identification for question  analysis. * denotes the results (i.e. P , R, F 1 ) are tested  for statistical significance using T-Test, p-values<0.05.
true	D12-1036.pdf#0.726*	New York Times Corpus, P@10%	P  Table 4: Performance of implicit aspect identification for  question analysis. T-Test, p-values<0.05
true	D12-1036.pdf#0.643*	New York Times Corpus, P@10%	R  Table 4: Performance of implicit aspect identification for  question analysis. T-Test, p-values<0.05
true	D12-1036.pdf#0.682*	New York Times Corpus, P@10%	F 1  Table 4: Performance of implicit aspect identification for  question analysis. T-Test, p-values<0.05
true	D12-1036.pdf#0.364*	New York Times Corpus, P@10%	ROUGE1  Table 5: Performance of answer generation. T-Test, p- values<0.05.
true	D12-1036.pdf#0.138*	New York Times Corpus, P@10%	ROUGE - SU4  Table 5: Performance of answer generation. T-Test, p- values<0.05.
true	D12-1036.pdf#0.137*	New York Times Corpus, P@10%	ROUGE2  Table 5: Performance of answer generation. T-Test, p- values<0.05.
true	P15-2124.pdf#0.640	New York Times Corpus, P@10%	F  Table 1: Features of our sarcasm detection system
true	P15-2124.pdf#0.8876	New York Times Corpus, P@10%	Our system F Riloff et al . ( 2013 )  Table 2: Comparative results for Tweet-A using  rule-based algorithm and statistical classifiers us- ing our feature combinations
true	P15-2124.pdf#0.640	New York Times Corpus, P@10%	F  Table 3: Comparative results for Discussion-A us- ing our feature combinations
true	D15-1215.pdf#92.6	Penn Treebank, UAS	UAS  Table 2: Performance of different models on PTB3  dataset. UAS: unlabeled attachment score. LAS:  labeled attachment score.
true	D15-1215.pdf#91.9	Penn Treebank, UAS	LAS  Table 2: Performance of different models on PTB3  dataset. UAS: unlabeled attachment score. LAS:  labeled attachment score.
true	D15-1215.pdf#91.6	Penn Treebank, UAS	LAS  Table 2: Performance of different models on PTB3  dataset. UAS: unlabeled attachment score. LAS:  labeled attachment score.
true	D15-1215.pdf#92.8	Penn Treebank, UAS	UAS  Table 2: Performance of different models on PTB3  dataset. UAS: unlabeled attachment score. LAS:  labeled attachment score.
true	D15-1215.pdf#91.9	Penn Treebank, UAS	LAS  Table 2: Performance of different models on PTB3  dataset. UAS: unlabeled attachment score. LAS:  labeled attachment score.
true	D15-1215.pdf#84.6	Penn Treebank, UAS	UAS  Table 3: Performance of different models on  CTB5 dataset. UAS: unlabeled attachment score.  LAS: labeled attachment score.
true	D15-1215.pdf#83.6	Penn Treebank, UAS	LAS  Table 3: Performance of different models on  CTB5 dataset. UAS: unlabeled attachment score.  LAS: labeled attachment score.
true	D15-1215.pdf#84.7	Penn Treebank, UAS	UAS  Table 3: Performance of different models on  CTB5 dataset. UAS: unlabeled attachment score.  LAS: labeled attachment score.
true	D15-1215.pdf#83.7	Penn Treebank, UAS	LAS  Table 3: Performance of different models on  CTB5 dataset. UAS: unlabeled attachment score.  LAS: labeled attachment score.
true	D15-1215.pdf#92.6	benchmark Vietnamese dependency treebank VnDT, UAS	UAS  Table 2: Performance of different models on PTB3  dataset. UAS: unlabeled attachment score. LAS:  labeled attachment score.
true	D15-1215.pdf#91.9	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 2: Performance of different models on PTB3  dataset. UAS: unlabeled attachment score. LAS:  labeled attachment score.
true	D15-1215.pdf#91.6	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 2: Performance of different models on PTB3  dataset. UAS: unlabeled attachment score. LAS:  labeled attachment score.
true	D15-1215.pdf#92.8	benchmark Vietnamese dependency treebank VnDT, UAS	UAS  Table 2: Performance of different models on PTB3  dataset. UAS: unlabeled attachment score. LAS:  labeled attachment score.
true	D15-1215.pdf#91.9	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 2: Performance of different models on PTB3  dataset. UAS: unlabeled attachment score. LAS:  labeled attachment score.
true	D15-1215.pdf#84.6	benchmark Vietnamese dependency treebank VnDT, UAS	UAS  Table 3: Performance of different models on  CTB5 dataset. UAS: unlabeled attachment score.  LAS: labeled attachment score.
true	D15-1215.pdf#83.6	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 3: Performance of different models on  CTB5 dataset. UAS: unlabeled attachment score.  LAS: labeled attachment score.
true	D15-1215.pdf#84.7	benchmark Vietnamese dependency treebank VnDT, UAS	UAS  Table 3: Performance of different models on  CTB5 dataset. UAS: unlabeled attachment score.  LAS: labeled attachment score.
true	D15-1215.pdf#83.7	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 3: Performance of different models on  CTB5 dataset. UAS: unlabeled attachment score.  LAS: labeled attachment score.
true	D15-1215.pdf#92.6	Penn Treebank, LAS	UAS  Table 2: Performance of different models on PTB3  dataset. UAS: unlabeled attachment score. LAS:  labeled attachment score.
true	D15-1215.pdf#91.9	Penn Treebank, LAS	LAS  Table 2: Performance of different models on PTB3  dataset. UAS: unlabeled attachment score. LAS:  labeled attachment score.
true	D15-1215.pdf#91.6	Penn Treebank, LAS	LAS  Table 2: Performance of different models on PTB3  dataset. UAS: unlabeled attachment score. LAS:  labeled attachment score.
true	D15-1215.pdf#92.8	Penn Treebank, LAS	UAS  Table 2: Performance of different models on PTB3  dataset. UAS: unlabeled attachment score. LAS:  labeled attachment score.
true	D15-1215.pdf#91.9	Penn Treebank, LAS	LAS  Table 2: Performance of different models on PTB3  dataset. UAS: unlabeled attachment score. LAS:  labeled attachment score.
true	D15-1215.pdf#84.6	Penn Treebank, LAS	UAS  Table 3: Performance of different models on  CTB5 dataset. UAS: unlabeled attachment score.  LAS: labeled attachment score.
true	D15-1215.pdf#83.6	Penn Treebank, LAS	LAS  Table 3: Performance of different models on  CTB5 dataset. UAS: unlabeled attachment score.  LAS: labeled attachment score.
true	D15-1215.pdf#84.7	Penn Treebank, LAS	UAS  Table 3: Performance of different models on  CTB5 dataset. UAS: unlabeled attachment score.  LAS: labeled attachment score.
true	D15-1215.pdf#83.7	Penn Treebank, LAS	LAS  Table 3: Performance of different models on  CTB5 dataset. UAS: unlabeled attachment score.  LAS: labeled attachment score.
true	D15-1215.pdf#92.6	benchmark Vietnamese dependency treebank VnDT, LAS	UAS  Table 2: Performance of different models on PTB3  dataset. UAS: unlabeled attachment score. LAS:  labeled attachment score.
true	D15-1215.pdf#91.9	benchmark Vietnamese dependency treebank VnDT, LAS	LAS  Table 2: Performance of different models on PTB3  dataset. UAS: unlabeled attachment score. LAS:  labeled attachment score.
true	D15-1215.pdf#91.6	benchmark Vietnamese dependency treebank VnDT, LAS	LAS  Table 2: Performance of different models on PTB3  dataset. UAS: unlabeled attachment score. LAS:  labeled attachment score.
true	D15-1215.pdf#92.8	benchmark Vietnamese dependency treebank VnDT, LAS	UAS  Table 2: Performance of different models on PTB3  dataset. UAS: unlabeled attachment score. LAS:  labeled attachment score.
true	D15-1215.pdf#91.9	benchmark Vietnamese dependency treebank VnDT, LAS	LAS  Table 2: Performance of different models on PTB3  dataset. UAS: unlabeled attachment score. LAS:  labeled attachment score.
true	D15-1215.pdf#84.6	benchmark Vietnamese dependency treebank VnDT, LAS	UAS  Table 3: Performance of different models on  CTB5 dataset. UAS: unlabeled attachment score.  LAS: labeled attachment score.
true	D15-1215.pdf#83.6	benchmark Vietnamese dependency treebank VnDT, LAS	LAS  Table 3: Performance of different models on  CTB5 dataset. UAS: unlabeled attachment score.  LAS: labeled attachment score.
true	D15-1215.pdf#84.7	benchmark Vietnamese dependency treebank VnDT, LAS	UAS  Table 3: Performance of different models on  CTB5 dataset. UAS: unlabeled attachment score.  LAS: labeled attachment score.
true	D15-1215.pdf#83.7	benchmark Vietnamese dependency treebank VnDT, LAS	LAS  Table 3: Performance of different models on  CTB5 dataset. UAS: unlabeled attachment score.  LAS: labeled attachment score.
true	P12-2003.pdf#82.8	benchmark Vietnamese dependency treebank VnDT, LAS	556 : 51 Dev UAS  Table 3: Performance and efficiency for all parsers on CTB data: unlabeled and labeled attachment scores (UAS/LAS)  are for both development and test data sets; parsing times (minutes:seconds) are for the test data only and exclude gen- eration of basic Stanford dependencies (for constituent parsers) and part-of-speech tagging (for dependency parsers).
true	P12-2003.pdf#83.1	benchmark Vietnamese dependency treebank VnDT, LAS	556 : 51 Test UAS  Table 3: Performance and efficiency for all parsers on CTB data: unlabeled and labeled attachment scores (UAS/LAS)  are for both development and test data sets; parsing times (minutes:seconds) are for the test data only and exclude gen- eration of basic Stanford dependencies (for constituent parsers) and part-of-speech tagging (for dependency parsers).
true	P12-2003.pdf#78.2	benchmark Vietnamese dependency treebank VnDT, LAS	556 : 51 Dev LAS  Table 3: Performance and efficiency for all parsers on CTB data: unlabeled and labeled attachment scores (UAS/LAS)  are for both development and test data sets; parsing times (minutes:seconds) are for the test data only and exclude gen- eration of basic Stanford dependencies (for constituent parsers) and part-of-speech tagging (for dependency parsers).
true	P12-2003.pdf#78.1	benchmark Vietnamese dependency treebank VnDT, LAS	556 : 51 Test LAS  Table 3: Performance and efficiency for all parsers on CTB data: unlabeled and labeled attachment scores (UAS/LAS)  are for both development and test data sets; parsing times (minutes:seconds) are for the test data only and exclude gen- eration of basic Stanford dependencies (for constituent parsers) and part-of-speech tagging (for dependency parsers).
true	P12-2003.pdf#97.4	benchmark Vietnamese dependency treebank VnDT, LAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#87.1	benchmark Vietnamese dependency treebank VnDT, LAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#91.3	benchmark Vietnamese dependency treebank VnDT, LAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#85.6	benchmark Vietnamese dependency treebank VnDT, LAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#82.3	benchmark Vietnamese dependency treebank VnDT, LAS	Berkeley  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#84.2	benchmark Vietnamese dependency treebank VnDT, LAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#86.3	benchmark Vietnamese dependency treebank VnDT, LAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#94.3	benchmark Vietnamese dependency treebank VnDT, LAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#86.0	benchmark Vietnamese dependency treebank VnDT, LAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#94.3	benchmark Vietnamese dependency treebank VnDT, LAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#88.9	benchmark Vietnamese dependency treebank VnDT, LAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#84.4	benchmark Vietnamese dependency treebank VnDT, LAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#74.0	benchmark Vietnamese dependency treebank VnDT, LAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#70.3	benchmark Vietnamese dependency treebank VnDT, LAS	Berkeley  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#76.0	benchmark Vietnamese dependency treebank VnDT, LAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#82.8	benchmark Vietnamese dependency treebank VnDT, UAS	556 : 51 Dev UAS  Table 3: Performance and efficiency for all parsers on CTB data: unlabeled and labeled attachment scores (UAS/LAS)  are for both development and test data sets; parsing times (minutes:seconds) are for the test data only and exclude gen- eration of basic Stanford dependencies (for constituent parsers) and part-of-speech tagging (for dependency parsers).
true	P12-2003.pdf#83.1	benchmark Vietnamese dependency treebank VnDT, UAS	556 : 51 Test UAS  Table 3: Performance and efficiency for all parsers on CTB data: unlabeled and labeled attachment scores (UAS/LAS)  are for both development and test data sets; parsing times (minutes:seconds) are for the test data only and exclude gen- eration of basic Stanford dependencies (for constituent parsers) and part-of-speech tagging (for dependency parsers).
true	P12-2003.pdf#78.2	benchmark Vietnamese dependency treebank VnDT, UAS	556 : 51 Dev LAS  Table 3: Performance and efficiency for all parsers on CTB data: unlabeled and labeled attachment scores (UAS/LAS)  are for both development and test data sets; parsing times (minutes:seconds) are for the test data only and exclude gen- eration of basic Stanford dependencies (for constituent parsers) and part-of-speech tagging (for dependency parsers).
true	P12-2003.pdf#78.1	benchmark Vietnamese dependency treebank VnDT, UAS	556 : 51 Test LAS  Table 3: Performance and efficiency for all parsers on CTB data: unlabeled and labeled attachment scores (UAS/LAS)  are for both development and test data sets; parsing times (minutes:seconds) are for the test data only and exclude gen- eration of basic Stanford dependencies (for constituent parsers) and part-of-speech tagging (for dependency parsers).
true	P12-2003.pdf#97.4	benchmark Vietnamese dependency treebank VnDT, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#87.1	benchmark Vietnamese dependency treebank VnDT, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#91.3	benchmark Vietnamese dependency treebank VnDT, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#85.6	benchmark Vietnamese dependency treebank VnDT, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#82.3	benchmark Vietnamese dependency treebank VnDT, UAS	Berkeley  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#84.2	benchmark Vietnamese dependency treebank VnDT, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#86.3	benchmark Vietnamese dependency treebank VnDT, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#94.3	benchmark Vietnamese dependency treebank VnDT, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#86.0	benchmark Vietnamese dependency treebank VnDT, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#94.3	benchmark Vietnamese dependency treebank VnDT, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#88.9	benchmark Vietnamese dependency treebank VnDT, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#84.4	benchmark Vietnamese dependency treebank VnDT, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#74.0	benchmark Vietnamese dependency treebank VnDT, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#70.3	benchmark Vietnamese dependency treebank VnDT, UAS	Berkeley  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#76.0	benchmark Vietnamese dependency treebank VnDT, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#82.8	Penn Treebank, UAS	556 : 51 Dev UAS  Table 3: Performance and efficiency for all parsers on CTB data: unlabeled and labeled attachment scores (UAS/LAS)  are for both development and test data sets; parsing times (minutes:seconds) are for the test data only and exclude gen- eration of basic Stanford dependencies (for constituent parsers) and part-of-speech tagging (for dependency parsers).
true	P12-2003.pdf#83.1	Penn Treebank, UAS	556 : 51 Test UAS  Table 3: Performance and efficiency for all parsers on CTB data: unlabeled and labeled attachment scores (UAS/LAS)  are for both development and test data sets; parsing times (minutes:seconds) are for the test data only and exclude gen- eration of basic Stanford dependencies (for constituent parsers) and part-of-speech tagging (for dependency parsers).
true	P12-2003.pdf#78.2	Penn Treebank, UAS	556 : 51 Dev LAS  Table 3: Performance and efficiency for all parsers on CTB data: unlabeled and labeled attachment scores (UAS/LAS)  are for both development and test data sets; parsing times (minutes:seconds) are for the test data only and exclude gen- eration of basic Stanford dependencies (for constituent parsers) and part-of-speech tagging (for dependency parsers).
true	P12-2003.pdf#78.1	Penn Treebank, UAS	556 : 51 Test LAS  Table 3: Performance and efficiency for all parsers on CTB data: unlabeled and labeled attachment scores (UAS/LAS)  are for both development and test data sets; parsing times (minutes:seconds) are for the test data only and exclude gen- eration of basic Stanford dependencies (for constituent parsers) and part-of-speech tagging (for dependency parsers).
true	P12-2003.pdf#97.4	Penn Treebank, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#87.1	Penn Treebank, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#91.3	Penn Treebank, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#85.6	Penn Treebank, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#82.3	Penn Treebank, UAS	Berkeley  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#84.2	Penn Treebank, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#86.3	Penn Treebank, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#94.3	Penn Treebank, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#86.0	Penn Treebank, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#94.3	Penn Treebank, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#88.9	Penn Treebank, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#84.4	Penn Treebank, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#74.0	Penn Treebank, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#70.3	Penn Treebank, UAS	Berkeley  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	P12-2003.pdf#76.0	Penn Treebank, UAS	Mate  Table 4: Performance (F 1 scores) for the fifteen most- frequent dependency relations in the CTB 7.0 develop- ment data set attained by both Mate and Berkeley parsers.
true	D13-1018.pdf#23	New York Times Corpus, P@10%	terms BoW C  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#99	New York Times Corpus, P@10%	terms PTM P  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#95	New York Times Corpus, P@10%	terms PTM P  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#99	New York Times Corpus, P@10%	glosses PTM P  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#98	New York Times Corpus, P@10%	terms PTM C  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#46	New York Times Corpus, P@10%	terms BoW C  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#97	New York Times Corpus, P@10%	terms BoW P  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#43	New York Times Corpus, P@10%	terms PTM C  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#97	New York Times Corpus, P@10%	terms PTM P  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#98	New York Times Corpus, P@10%	glosses PTM P  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#95	New York Times Corpus, P@10%	terms PTM P  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#96	New York Times Corpus, P@10%	glosses PTM P  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#98	New York Times Corpus, P@10%	terms PTM P  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#96	New York Times Corpus, P@10%	glosses PTM P  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#94	New York Times Corpus, P@10%	terms PTM P  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#97	New York Times Corpus, P@10%	glosses PTM P  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#91	New York Times Corpus, P@10%	terms PTM P  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#98	New York Times Corpus, P@10%	terms PTM P  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#93	New York Times Corpus, P@10%	glosses PTM P  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#96	New York Times Corpus, P@10%	glosses PTM P  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#77	New York Times Corpus, P@10%	terms PTM C  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#89	New York Times Corpus, P@10%	glosses PTM P  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#89	New York Times Corpus, P@10%	terms PTM C  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#31	New York Times Corpus, P@10%	terms BoW C  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#29	New York Times Corpus, P@10%	terms PTM C  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#95	New York Times Corpus, P@10%	glosses PTM P  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#26	New York Times Corpus, P@10%	terms PTM C  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#95	New York Times Corpus, P@10%	glosses BoW P  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#43	New York Times Corpus, P@10%	terms BoW C  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	D13-1018.pdf#92	New York Times Corpus, P@10%	terms PTM P  Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
true	P15-1093.pdf#38.39	New York Times Corpus, P@10%	AC Joint † †  Table 3: F-measures of the three methods in the test sets. The bold values denote the highest F-measures  among all the three methods. Statistical significance with p < 0.05 is marked with  † compared with  Baseline,  ‡ compared with PC Joint, and ⋆ compared with AC Joint.
true	P15-1093.pdf#88.13	New York Times Corpus, P@10%	AC Joint  Table 3: F-measures of the three methods in the test sets. The bold values denote the highest F-measures  among all the three methods. Statistical significance with p < 0.05 is marked with  † compared with  Baseline,  ‡ compared with PC Joint, and ⋆ compared with AC Joint.
true	P15-1093.pdf#89.00	New York Times Corpus, P@10%	PC Joint † †  Table 3: F-measures of the three methods in the test sets. The bold values denote the highest F-measures  among all the three methods. Statistical significance with p < 0.05 is marked with  † compared with  Baseline,  ‡ compared with PC Joint, and ⋆ compared with AC Joint.
true	P15-1093.pdf#79.23	New York Times Corpus, P@10%	AC Joint † † † † †  Table 3: F-measures of the three methods in the test sets. The bold values denote the highest F-measures  among all the three methods. Statistical significance with p < 0.05 is marked with  † compared with  Baseline,  ‡ compared with PC Joint, and ⋆ compared with AC Joint.
true	P15-1093.pdf#93.09	New York Times Corpus, P@10%	PC Joint †  Table 3: F-measures of the three methods in the test sets. The bold values denote the highest F-measures  among all the three methods. Statistical significance with p < 0.05 is marked with  † compared with  Baseline,  ‡ compared with PC Joint, and ⋆ compared with AC Joint.
true	P15-1093.pdf#48.11	New York Times Corpus, P@10%	AC Joint  Table 3: F-measures of the three methods in the test sets. The bold values denote the highest F-measures  among all the three methods. Statistical significance with p < 0.05 is marked with  † compared with  Baseline,  ‡ compared with PC Joint, and ⋆ compared with AC Joint.
true	P15-1093.pdf#79.03	New York Times Corpus, P@10%	AC Joint †  Table 3: F-measures of the three methods in the test sets. The bold values denote the highest F-measures  among all the three methods. Statistical significance with p < 0.05 is marked with  † compared with  Baseline,  ‡ compared with PC Joint, and ⋆ compared with AC Joint.
true	P15-1093.pdf#4.80	New York Times Corpus, P@10%	AC Joint † †  Table 3: F-measures of the three methods in the test sets. The bold values denote the highest F-measures  among all the three methods. Statistical significance with p < 0.05 is marked with  † compared with  Baseline,  ‡ compared with PC Joint, and ⋆ compared with AC Joint.
true	P15-1093.pdf#86.07	New York Times Corpus, P@10%	AC Joint † † † †  Table 3: F-measures of the three methods in the test sets. The bold values denote the highest F-measures  among all the three methods. Statistical significance with p < 0.05 is marked with  † compared with  Baseline,  ‡ compared with PC Joint, and ⋆ compared with AC Joint.
true	P15-1093.pdf#24.43	New York Times Corpus, P@10%	AC Joint † †  Table 3: F-measures of the three methods in the test sets. The bold values denote the highest F-measures  among all the three methods. Statistical significance with p < 0.05 is marked with  † compared with  Baseline,  ‡ compared with PC Joint, and ⋆ compared with AC Joint.
true	P15-1093.pdf#36.35	New York Times Corpus, P@10%	AC Joint † † †  Table 3: F-measures of the three methods in the test sets. The bold values denote the highest F-measures  among all the three methods. Statistical significance with p < 0.05 is marked with  † compared with  Baseline,  ‡ compared with PC Joint, and ⋆ compared with AC Joint.
true	P15-1093.pdf#44.09	New York Times Corpus, P@10%	AC Joint † † † †  Table 3: F-measures of the three methods in the test sets. The bold values denote the highest F-measures  among all the three methods. Statistical significance with p < 0.05 is marked with  † compared with  Baseline,  ‡ compared with PC Joint, and ⋆ compared with AC Joint.
true	N15-1133.pdf#3.21and	SemEval-2010 Task 8, F1	an 83% decrease in the number of tanh evaluations . MV - RNN d < 5 time running SemEval - 2010 Figure 5 : Training time measured by seconds . Dependency  Table 1: Results on SemEval 2010 relation classification task with the
true	N15-1133.pdf#1.95timesfasterforSemEval2010	SemEval-2010 Task 8, F1	an 83% decrease in the number of tanh evaluations . MV - RNN d < 5 time running SemEval - 2010 Figure 5 : Training time measured by seconds . more  Table 1: Results on SemEval 2010 relation classification task with the
true	N15-1133.pdf#3.21and	SemEval-2010 Task 8, F1	an 83% decrease in the number of tanh evaluations . MV - RNN d < 5 time running SemEval - 2010 Figure 5 : Training time measured by seconds . Dependency  Table 2. In this task, the goal is to extract inter- actions between drug mentions in text. The corpus
true	N15-1133.pdf#1.95timesfasterforSemEval2010	SemEval-2010 Task 8, F1	an 83% decrease in the number of tanh evaluations . MV - RNN d < 5 time running SemEval - 2010 Figure 5 : Training time measured by seconds . more  Table 2. In this task, the goal is to extract inter- actions between drug mentions in text. The corpus
true	N15-1133.pdf#68.64	SemEval-2010 Task 8, F1	F=measure  Table 2: Results on SemEval 2013 Drug-Drug Interaction task
true	N15-1133.pdf#66.19	SemEval-2010 Task 8, F1	Recall  Table 2: Results on SemEval 2013 Drug-Drug Interaction task
true	N15-1133.pdf#75.31	SemEval-2010 Task 8, F1	Precision  Table 2: Results on SemEval 2013 Drug-Drug Interaction task
true	P15-2018.pdf#0.31	Text8, Number of params	Figure 2 : Some example input images and the generated descriptions . a father feeding his child on the street a man in a black shirt and his little girl wearing orange are a man and a girl sit on the ground and eat Flickr8K CIDEr  Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics.
true	P15-2018.pdf#13.17	Text8, Number of params	Figure 2 : Some example input images and the generated descriptions . a black bear climbing a tree in forest area a bird standing on a tree branch in a wooded area a painted sign of a blue bird in a tree in the woods MS COCO METEOR  Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics.
true	P15-2018.pdf#10.06	Text8, Number of params	Figure 2 : Some example input images and the generated descriptions . a girl is skipping across the road in front of a white truck a girl jumps rope in a parking lot a girl is in a parking lot jumping rope Flickr30K METEOR  Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics.
true	P15-2018.pdf#11.57	Text8, Number of params	Figure 2 : Some example input images and the generated descriptions . a father feeding his child on the street a brown and white dog jumps over a dog hurdle a brown and white sheltie leap - ing over a rail Flickr8K METEOR  Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics.
true	P15-2018.pdf#0.20	Text8, Number of params	Figure 2 : Some example input images and the generated descriptions . a girl is skipping across the road in front of a white truck a girl jumps rope in a parking lot a girl is in a parking lot jumping rope Flickr30K CIDEr  Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics.
true	P15-2018.pdf#0.58	Text8, Number of params	Figure 2 : Some example input images and the generated descriptions . a black bear climbing a tree in forest area a bird standing on a tree branch in a wooded area a painted sign of a blue bird in a tree in the woods MS COCO CIDEr  Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics.
true	P15-2018.pdf#3.78	Text8, Number of params	Figure 2 : Some example input images and the generated descriptions . a father feeding his child on the street a brown and white dog jumps over a dog hurdle a brown and white sheltie leap - ing over a rail Flickr8K BLEU  Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics.
true	P15-2018.pdf#5.36	Text8, Number of params	Figure 2 : Some example input images and the generated descriptions . a girl is skipping across the road forest area a girl jumps rope in a parking in a wooded area a girl is in a parking lot jumping a tree in the woods MS COCO BLEU  Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics.
true	P15-2018.pdf#3.22	Text8, Number of params	Figure 2 : Some example input images and the generated descriptions . a father feeding his child on the street a man in a black shirt and his little girl wearing orange are a man and a girl sit on the ground and eat Flickr30K BLEU  Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics.
true	P15-2018.pdf#2.73	Text8, Number of params	Variance  Table 2: Human judgment scores on a scale of 1 to 5.
true	P15-2018.pdf#0.31	Text8, Bit per Character (BPC)	Figure 2 : Some example input images and the generated descriptions . a father feeding his child on the street a man in a black shirt and his little girl wearing orange are a man and a girl sit on the ground and eat Flickr8K CIDEr  Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics.
true	P15-2018.pdf#13.17	Text8, Bit per Character (BPC)	Figure 2 : Some example input images and the generated descriptions . a black bear climbing a tree in forest area a bird standing on a tree branch in a wooded area a painted sign of a blue bird in a tree in the woods MS COCO METEOR  Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics.
true	P15-2018.pdf#10.06	Text8, Bit per Character (BPC)	Figure 2 : Some example input images and the generated descriptions . a girl is skipping across the road in front of a white truck a girl jumps rope in a parking lot a girl is in a parking lot jumping rope Flickr30K METEOR  Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics.
true	P15-2018.pdf#11.57	Text8, Bit per Character (BPC)	Figure 2 : Some example input images and the generated descriptions . a father feeding his child on the street a brown and white dog jumps over a dog hurdle a brown and white sheltie leap - ing over a rail Flickr8K METEOR  Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics.
true	P15-2018.pdf#0.20	Text8, Bit per Character (BPC)	Figure 2 : Some example input images and the generated descriptions . a girl is skipping across the road in front of a white truck a girl jumps rope in a parking lot a girl is in a parking lot jumping rope Flickr30K CIDEr  Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics.
true	P15-2018.pdf#0.58	Text8, Bit per Character (BPC)	Figure 2 : Some example input images and the generated descriptions . a black bear climbing a tree in forest area a bird standing on a tree branch in a wooded area a painted sign of a blue bird in a tree in the woods MS COCO CIDEr  Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics.
true	P15-2018.pdf#3.78	Text8, Bit per Character (BPC)	Figure 2 : Some example input images and the generated descriptions . a father feeding his child on the street a brown and white dog jumps over a dog hurdle a brown and white sheltie leap - ing over a rail Flickr8K BLEU  Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics.
true	P15-2018.pdf#5.36	Text8, Bit per Character (BPC)	Figure 2 : Some example input images and the generated descriptions . a girl is skipping across the road forest area a girl jumps rope in a parking in a wooded area a girl is in a parking lot jumping a tree in the woods MS COCO BLEU  Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics.
true	P15-2018.pdf#3.22	Text8, Bit per Character (BPC)	Figure 2 : Some example input images and the generated descriptions . a father feeding his child on the street a man in a black shirt and his little girl wearing orange are a man and a girl sit on the ground and eat Flickr30K BLEU  Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics.
true	P15-2018.pdf#2.73	Text8, Bit per Character (BPC)	Variance  Table 2: Human judgment scores on a scale of 1 to 5.
true	P14-2015.pdf#86.7(-0.9)	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	( without entity status info ) Financial 90 / 86 / 88 status info ) 89 / 85 / 87 96 / 84 / 90 96 / 83 / 89  Table 1. Performance of different algorithms on three subsets of the corpus with a different status of  the target entity within the document.
true	P14-2015.pdf#89.6	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	from exp . 5 . 2 ) ) Medical 84 / 88 / 86 status info ) 87 / 81 / 84 99 / 84 / 91 95 / 85 / 90  Table 1. Performance of different algorithms on three subsets of the corpus with a different status of  the target entity within the document.
true	P14-2015.pdf#91.2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	from exp . 5 . 2 ) ) Financial 90 / 86 / 88 status info ) 89 / 85 / 87 96 / 84 / 90 96 / 83 / 89  Table 1. Performance of different algorithms on three subsets of the corpus with a different status of  the target entity within the document.
true	P14-2015.pdf#89.7(+0.1)	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	( without entity status info ) Financial 90 / 86 / 88 status info ) 89 / 85 / 87 96 / 84 / 90 96 / 83 / 89  Table 1. Performance of different algorithms on three subsets of the corpus with a different status of  the target entity within the document.
true	P14-2015.pdf#89.6	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	from exp . 5 . 2 ) ) Medical 84 / 88 / 86 status info ) 87 / 81 / 84 99 / 84 / 91 95 / 85 / 90  Table 2. Performance of different algorithms  on the different domains.
true	P14-2015.pdf#91.2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	from exp . 5 . 2 ) ) Financial 90 / 86 / 88 status info ) 89 / 85 / 87 96 / 84 / 90 96 / 83 / 89  Table 2. Performance of different algorithms  on the different domains.
true	P14-2015.pdf#86.7(-0.9)	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	( without entity status info ) Financial 90 / 86 / 88 status info ) 89 / 85 / 87 96 / 84 / 90 96 / 83 / 89  Table 2. Performance of different algorithms  on the different domains.
true	P14-2015.pdf#89.7(+0.1)	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	( without entity status info ) Financial 90 / 86 / 88 status info ) 89 / 85 / 87 96 / 84 / 90 96 / 83 / 89  Table 2. Performance of different algorithms  on the different domains.
true	P13-1124.pdf#38.55	Text8, Number of params	Acc MT08 nw BLEU  Table 4: The NIST MT08 results on newswire (nw) and we- blog (wb) genres. S2D is the baseline string-to-dependency  system (line 1), on top of which Two-Neighbor Orientation  Model 1 to 4 are employed (line 2-5). The best TER and  BLEU results on each genre are in bold. For BLEU, higher  scores are better, while for TER, lower scores are better.
true	P13-1124.pdf#28.44	Text8, Number of params	Acc MT08 wb BLEU  Table 4: The NIST MT08 results on newswire (nw) and we- blog (wb) genres. S2D is the baseline string-to-dependency  system (line 1), on top of which Two-Neighbor Orientation  Model 1 to 4 are employed (line 2-5). The best TER and  BLEU results on each genre are in bold. For BLEU, higher  scores are better, while for TER, lower scores are better.
true	P13-1124.pdf#55.82	Text8, Number of params	Acc MT08 wb TER  Table 4: The NIST MT08 results on newswire (nw) and we- blog (wb) genres. S2D is the baseline string-to-dependency  system (line 1), on top of which Two-Neighbor Orientation  Model 1 to 4 are employed (line 2-5). The best TER and  BLEU results on each genre are in bold. For BLEU, higher  scores are better, while for TER, lower scores are better.
true	P13-1124.pdf#52.41	Text8, Number of params	Acc MT08 nw TER  Table 4: The NIST MT08 results on newswire (nw) and we- blog (wb) genres. S2D is the baseline string-to-dependency  system (line 1), on top of which Two-Neighbor Orientation  Model 1 to 4 are employed (line 2-5). The best TER and  BLEU results on each genre are in bold. For BLEU, higher  scores are better, while for TER, lower scores are better.
true	D14-1181.pdf#5http://cogcomp.cs.illinois.edu/Data/QA/QC/	SUBJ, Accuracy	and Le and Mikolov ( 2014 ) . Thus the training set is an order |V | 17836 16185 21323 9592 5340 6246 words present in the set of pre - trained word vectors .  Table 1: Summary statistics for the datasets after tokeniza- tion. c: Number of target classes. l: Average sentence length.  N : Dataset size. |V |: Vocabulary size. |Vpre|: Number of  words present in the set of pre-trained word vectors. Test:  Test set size (CV means there was no standard train/test split  and thus 10-fold CV was used).
true	D14-1181.pdf#7	SUBJ, Accuracy	Table 1: Summary statistics for the datasets after tokeniza- tion. c: Number of target classes. l: Average sentence length.  N : Dataset size. |V |: Vocabulary size. |Vpre|: Number of  words present in the set of pre-trained word vectors. Test:  Test set size (CV means there was no standard train/test split  and thus 10-fold CV was used).
true	D14-1181.pdf#3https://www.cs.cornell.edu/people/pabo/movie-review-data/	SUBJ, Accuracy	itive / negative reviews ( Hu and Liu , 2004 ) . Test 2210 1821 CV 500 CV CV words present in the set of pre - trained word vectors .  Table 1: Summary statistics for the datasets after tokeniza- tion. c: Number of target classes. l: Average sentence length.  N : Dataset size. |V |: Vocabulary size. |Vpre|: Number of  words present in the set of pre-trained word vectors. Test:  Test set size (CV means there was no standard train/test split  and thus 10-fold CV was used).
true	D14-1181.pdf#4http://nlp.stanford.edu/sentiment/Dataisactuallyprovided	SUBJ, Accuracy	itive / negative reviews ( Hu and Liu , 2004 ) . Test 2210 1821 CV 500 CV CV Test :  Table 1: Summary statistics for the datasets after tokeniza- tion. c: Number of target classes. l: Average sentence length.  N : Dataset size. |V |: Vocabulary size. |Vpre|: Number of  words present in the set of pre-trained word vectors. Test:  Test set size (CV means there was no standard train/test split  and thus 10-fold CV was used).
true	D14-1181.pdf#6http://www.cs.uic.edu/	SUBJ, Accuracy	of magnitude larger than listed in table 1 . l 18 19 23 10 19 10606 words present in the set of pre - trained word vectors .  Table 1: Summary statistics for the datasets after tokeniza- tion. c: Number of target classes. l: Average sentence length.  N : Dataset size. |V |: Vocabulary size. |Vpre|: Number of  words present in the set of pre-trained word vectors. Test:  Test set size (CV means there was no standard train/test split  and thus 10-fold CV was used).
true	D14-1181.pdf#6	SUBJ, Accuracy	Customer reviews of various products Test 2210 1821 CV 500 CV CV Test :  Table 1: Summary statistics for the datasets after tokeniza- tion. c: Number of target classes. l: Average sentence length.  N : Dataset size. |V |: Vocabulary size. |Vpre|: Number of  words present in the set of pre-trained word vectors. Test:  Test set size (CV means there was no standard train/test split  and thus 10-fold CV was used).
true	D14-1181.pdf#5http://cogcomp.cs.illinois.edu/Data/QA/QC/	SearchQA, Unigram Acc	and Le and Mikolov ( 2014 ) . Thus the training set is an order |V | 17836 16185 21323 9592 5340 6246 words present in the set of pre - trained word vectors .  Table 1: Summary statistics for the datasets after tokeniza- tion. c: Number of target classes. l: Average sentence length.  N : Dataset size. |V |: Vocabulary size. |Vpre|: Number of  words present in the set of pre-trained word vectors. Test:  Test set size (CV means there was no standard train/test split  and thus 10-fold CV was used).
true	D14-1181.pdf#7	SearchQA, Unigram Acc	Table 1: Summary statistics for the datasets after tokeniza- tion. c: Number of target classes. l: Average sentence length.  N : Dataset size. |V |: Vocabulary size. |Vpre|: Number of  words present in the set of pre-trained word vectors. Test:  Test set size (CV means there was no standard train/test split  and thus 10-fold CV was used).
true	D14-1181.pdf#3https://www.cs.cornell.edu/people/pabo/movie-review-data/	SearchQA, Unigram Acc	itive / negative reviews ( Hu and Liu , 2004 ) . Test 2210 1821 CV 500 CV CV words present in the set of pre - trained word vectors .  Table 1: Summary statistics for the datasets after tokeniza- tion. c: Number of target classes. l: Average sentence length.  N : Dataset size. |V |: Vocabulary size. |Vpre|: Number of  words present in the set of pre-trained word vectors. Test:  Test set size (CV means there was no standard train/test split  and thus 10-fold CV was used).
true	D14-1181.pdf#4http://nlp.stanford.edu/sentiment/Dataisactuallyprovided	SearchQA, Unigram Acc	itive / negative reviews ( Hu and Liu , 2004 ) . Test 2210 1821 CV 500 CV CV Test :  Table 1: Summary statistics for the datasets after tokeniza- tion. c: Number of target classes. l: Average sentence length.  N : Dataset size. |V |: Vocabulary size. |Vpre|: Number of  words present in the set of pre-trained word vectors. Test:  Test set size (CV means there was no standard train/test split  and thus 10-fold CV was used).
true	D14-1181.pdf#6http://www.cs.uic.edu/	SearchQA, Unigram Acc	of magnitude larger than listed in table 1 . l 18 19 23 10 19 10606 words present in the set of pre - trained word vectors .  Table 1: Summary statistics for the datasets after tokeniza- tion. c: Number of target classes. l: Average sentence length.  N : Dataset size. |V |: Vocabulary size. |Vpre|: Number of  words present in the set of pre-trained word vectors. Test:  Test set size (CV means there was no standard train/test split  and thus 10-fold CV was used).
true	D14-1181.pdf#6	SearchQA, Unigram Acc	Customer reviews of various products Test 2210 1821 CV 500 CV CV Test :  Table 1: Summary statistics for the datasets after tokeniza- tion. c: Number of target classes. l: Average sentence length.  N : Dataset size. |V |: Vocabulary size. |Vpre|: Number of  words present in the set of pre-trained word vectors. Test:  Test set size (CV means there was no standard train/test split  and thus 10-fold CV was used).
true	D11-1063.pdf#0.605	Senseval 3, F1	F - score ( 0 / 0=0 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.408	Senseval 3, F1	F - score ( 0 / 0=0 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.500	Senseval 3, F1	F - score ( 0 / 0=0 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.697	Senseval 3, F1	F - score ( 0 / 0=0 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.500	Senseval 3, F1	F - score ( 0 / 0=1 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.697	Senseval 3, F1	F - score ( 0 / 0=0 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.605	Senseval 3, F1	F - score ( 0 / 0=0 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.500	Senseval 3, F1	F - score ( 0 / 0=0 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.500	Senseval 3, F1	F - score ( 0 / 0=1 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.408	Senseval 3, F1	F - score ( 0 / 0=0 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.500	Senseval 3, F1	F - score ( 0 / 0=1 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.408	Senseval 3, F1	F - score ( 0 / 0=0 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.697	Senseval 3, F1	F - score ( 0 / 0=0 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.605	Senseval 3, F1	F - score ( 0 / 0=0 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.500	Senseval 3, F1	F - score ( 0 / 0=0 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.500	Senseval 3, F1	F - score ( 0 / 0=1 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.629	Senseval 3, F1	F - score ( 0 / 0=1 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.408	Senseval 3, F1	F - score ( 0 / 0=0 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.500	Senseval 3, F1	F - score ( 0 / 0=0 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.605	Senseval 3, F1	Accuracy ( 0 / 0=0 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.605	SemEval 2015, F1	F - score ( 0 / 0=0 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.408	SemEval 2015, F1	F - score ( 0 / 0=0 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.500	SemEval 2015, F1	F - score ( 0 / 0=0 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.697	SemEval 2015, F1	F - score ( 0 / 0=0 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.500	SemEval 2015, F1	F - score ( 0 / 0=1 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.697	SemEval 2015, F1	F - score ( 0 / 0=0 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.605	SemEval 2015, F1	F - score ( 0 / 0=0 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.500	SemEval 2015, F1	F - score ( 0 / 0=0 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.500	SemEval 2015, F1	F - score ( 0 / 0=1 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.408	SemEval 2015, F1	F - score ( 0 / 0=0 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.500	SemEval 2015, F1	F - score ( 0 / 0=1 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.408	SemEval 2015, F1	F - score ( 0 / 0=0 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.697	SemEval 2015, F1	F - score ( 0 / 0=0 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.605	SemEval 2015, F1	F - score ( 0 / 0=0 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.500	SemEval 2015, F1	F - score ( 0 / 0=0 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.500	SemEval 2015, F1	F - score ( 0 / 0=1 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.629	SemEval 2015, F1	F - score ( 0 / 0=1 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.408	SemEval 2015, F1	F - score ( 0 / 0=0 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.500	SemEval 2015, F1	F - score ( 0 / 0=0 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.605	SemEval 2015, F1	Accuracy ( 0 / 0=0 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.605	Senseval 2, F1	F - score ( 0 / 0=0 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.408	Senseval 2, F1	F - score ( 0 / 0=0 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.500	Senseval 2, F1	F - score ( 0 / 0=0 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.697	Senseval 2, F1	F - score ( 0 / 0=0 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.500	Senseval 2, F1	F - score ( 0 / 0=1 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.697	Senseval 2, F1	F - score ( 0 / 0=0 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.605	Senseval 2, F1	F - score ( 0 / 0=0 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.500	Senseval 2, F1	F - score ( 0 / 0=0 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.500	Senseval 2, F1	F - score ( 0 / 0=1 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.408	Senseval 2, F1	F - score ( 0 / 0=0 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.500	Senseval 2, F1	F - score ( 0 / 0=1 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.408	Senseval 2, F1	F - score ( 0 / 0=0 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.697	Senseval 2, F1	F - score ( 0 / 0=0 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.605	Senseval 2, F1	F - score ( 0 / 0=0 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.500	Senseval 2, F1	F - score ( 0 / 0=0 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.500	Senseval 2, F1	F - score ( 0 / 0=1 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.629	Senseval 2, F1	F - score ( 0 / 0=1 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.408	Senseval 2, F1	F - score ( 0 / 0=0 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.500	Senseval 2, F1	F - score ( 0 / 0=0 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.605	Senseval 2, F1	Accuracy ( 0 / 0=0 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.605	SemEval 2007, F1	F - score ( 0 / 0=0 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.408	SemEval 2007, F1	F - score ( 0 / 0=0 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.500	SemEval 2007, F1	F - score ( 0 / 0=0 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.697	SemEval 2007, F1	F - score ( 0 / 0=0 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.500	SemEval 2007, F1	F - score ( 0 / 0=1 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.697	SemEval 2007, F1	F - score ( 0 / 0=0 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.605	SemEval 2007, F1	F - score ( 0 / 0=0 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.500	SemEval 2007, F1	F - score ( 0 / 0=0 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.500	SemEval 2007, F1	F - score ( 0 / 0=1 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.408	SemEval 2007, F1	F - score ( 0 / 0=0 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.500	SemEval 2007, F1	F - score ( 0 / 0=1 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.408	SemEval 2007, F1	F - score ( 0 / 0=0 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.697	SemEval 2007, F1	F - score ( 0 / 0=0 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.605	SemEval 2007, F1	F - score ( 0 / 0=0 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.500	SemEval 2007, F1	F - score ( 0 / 0=0 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.500	SemEval 2007, F1	F - score ( 0 / 0=1 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.629	SemEval 2007, F1	F - score ( 0 / 0=1 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.408	SemEval 2007, F1	F - score ( 0 / 0=0 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.500	SemEval 2007, F1	F - score ( 0 / 0=0 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.605	SemEval 2007, F1	Accuracy ( 0 / 0=0 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.605	SemEval 2013, F1	F - score ( 0 / 0=0 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.408	SemEval 2013, F1	F - score ( 0 / 0=0 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.500	SemEval 2013, F1	F - score ( 0 / 0=0 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.697	SemEval 2013, F1	F - score ( 0 / 0=0 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.500	SemEval 2013, F1	F - score ( 0 / 0=1 )  Table 5: The performance with known verbs.
true	D11-1063.pdf#0.697	SemEval 2013, F1	F - score ( 0 / 0=0 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.605	SemEval 2013, F1	F - score ( 0 / 0=0 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.500	SemEval 2013, F1	F - score ( 0 / 0=0 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.500	SemEval 2013, F1	F - score ( 0 / 0=1 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.408	SemEval 2013, F1	F - score ( 0 / 0=0 )  Table 5. The num- bers are in bold font when the performance of an  algorithm is significantly below the performance of  Concrete-Abstract. In no case is any score signifi- cantly above the performance of Concrete-Abstract,  at the 95% confidence level. NA indicates scores that  were not calculated by
true	D11-1063.pdf#0.500	SemEval 2013, F1	F - score ( 0 / 0=1 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.408	SemEval 2013, F1	F - score ( 0 / 0=0 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.697	SemEval 2013, F1	F - score ( 0 / 0=0 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.605	SemEval 2013, F1	F - score ( 0 / 0=0 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.500	SemEval 2013, F1	F - score ( 0 / 0=0 )  Table 6. The  numbers are in bold font when the performance of  an algorithm is significantly below the performance
true	D11-1063.pdf#0.500	SemEval 2013, F1	F - score ( 0 / 0=1 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.629	SemEval 2013, F1	F - score ( 0 / 0=1 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.408	SemEval 2013, F1	F - score ( 0 / 0=0 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.500	SemEval 2013, F1	F - score ( 0 / 0=0 )  Table 6: The performance with unknown verbs.
true	D11-1063.pdf#0.605	SemEval 2013, F1	Accuracy ( 0 / 0=0 )  Table 6: The performance with unknown verbs.
true	P13-1090.pdf#87.34	Penn Treebank, Accuracy	QBank POS  Table 2:  Domain Adaptation performance  in F-measure on Semantic Tagging on  Movie Target domain and POS tagging on  QBank:QuestionBank. Best performing models  are bolded.
true	P13-1090.pdf#82.24	Penn Treebank, Accuracy	Movie Domain All  Table 2:  Domain Adaptation performance  in F-measure on Semantic Tagging on  Movie Target domain and POS tagging on  QBank:QuestionBank. Best performing models  are bolded.
true	D11-1094.pdf#0.01comparedtothesecondbaseline.	New York Times Corpus, P@10%	for the English lexical substitution task . Scores marked model τ b ' are copied from the authors ' respective papers .  Table 1: Kendall's τ b and GAP paraphrase ranking scores  for the English lexical substitution task. Scores marked  with ' ' are copied from the authors' respective papers.  Scores marked with ' ' are statistically significant with  p < 0.01 compared to the second baseline.
true	D11-1094.pdf#49.02	New York Times Corpus, P@10%	GAP  Table 1: Kendall's τ b and GAP paraphrase ranking scores  for the English lexical substitution task. Scores marked  with ' ' are copied from the authors' respective papers.  Scores marked with ' ' are statistically significant with  p < 0.01 compared to the second baseline.
true	D11-1094.pdf#22.59	New York Times Corpus, P@10%	τ b  Table 1: Kendall's τ b and GAP paraphrase ranking scores  for the English lexical substitution task. Scores marked  with ' ' are copied from the authors' respective papers.  Scores marked with ' ' are statistically significant with  p < 0.01 compared to the second baseline.
true	D11-1094.pdf#22.68	New York Times Corpus, P@10%	difference between NMF dep and NMF c+d . ) , except for the n  Table 2: Kendall's τ b paraphrase ranking scores for the  English lexical substitution task across different parts of  speech
true	D11-1094.pdf#17.47	New York Times Corpus, P@10%	difference between NMF dep and NMF c+d . ) , except for the v  Table 2: Kendall's τ b paraphrase ranking scores for the  English lexical substitution task across different parts of  speech
true	D11-1094.pdf#28.66	New York Times Corpus, P@10%	difference between NMF dep and NMF c+d . ) , except for the r  Table 2: Kendall's τ b paraphrase ranking scores for the  English lexical substitution task across different parts of  speech
true	D11-1094.pdf#24.57	New York Times Corpus, P@10%	difference between NMF dep and NMF c+d . ) , except for the a  Table 2: Kendall's τ b paraphrase ranking scores for the  English lexical substitution task across different parts of  speech
true	D11-1094.pdf#8.96	New York Times Corpus, P@10%	best  Table 3: R best and P 10 paraphrase induction scores for  the English lexical substitution task
true	D11-1094.pdf#30.49	New York Times Corpus, P@10%	P 10  Table 3: R best and P 10 paraphrase induction scores for  the English lexical substitution task
true	D11-1094.pdf#25.99	New York Times Corpus, P@10%	v  Table 4: P 10 paraphrase induction scores for the English  lexical substitution task across different parts of speech.  Scores marked with ' ' and ' ' are statistically significant  with respectively p < 0.01 and p < 0.05 compared to the  baseline.
true	D11-1094.pdf#33.73	New York Times Corpus, P@10%	n  Table 4: P 10 paraphrase induction scores for the English  lexical substitution task across different parts of speech.  Scores marked with ' ' and ' ' are statistically significant  with respectively p < 0.01 and p < 0.05 compared to the  baseline.
true	D11-1094.pdf#44.96	New York Times Corpus, P@10%	performance on the paraphrase induction task . Table 5 presents the results for GAP  Table 5: Kendall's τ b and GAP paraphrase ranking scores  for the French lexical substitution task
true	D11-1094.pdf#18.63	New York Times Corpus, P@10%	performance on the paraphrase induction task . Table 5 presents the results for Kendall ' s τ  Table 5: Kendall's τ b and GAP paraphrase ranking scores  for the French lexical substitution task
true	D11-1094.pdf#10.71	New York Times Corpus, P@10%	R best  Table 6: R best and P 10 paraphrase induction scores for  the French lexical substitution task
true	D11-1094.pdf#35.32	New York Times Corpus, P@10%	P 10  Table 6: R best and P 10 paraphrase induction scores for  the French lexical substitution task
true	P13-1020.pdf#42.6	DUC 2004 Task 1, ROUGE-2	- Pyr -  Table 2: Results for compressive summarization.  Shown are the ROUGE-2 and ROUGE SU-4 re- calls with the default options from the ROUGE  toolkit (Lin, 2004); Pyramid scores (Nenkova and  Passonneau, 2004); and linguistic quality scores,  scored between 1 (very bad) to 5 (very good). For  Pyramid, the evaluation was performed by two  annotators, each evaluating half of the problems;  scores marked with  † were computed by different  annotators and are not directly comparable. Lin- guistic quality was evaluated by two linguists; we  show the average of the reported scores.
true	P13-1020.pdf#4.6	DUC 2004 Task 1, ROUGE-2	- LQ  Table 2: Results for compressive summarization.  Shown are the ROUGE-2 and ROUGE SU-4 re- calls with the default options from the ROUGE  toolkit (Lin, 2004); Pyramid scores (Nenkova and  Passonneau, 2004); and linguistic quality scores,  scored between 1 (very bad) to 5 (very good). For  Pyramid, the evaluation was performed by two  annotators, each evaluating half of the problems;  scores marked with  † were computed by different  annotators and are not directly comparable. Lin- guistic quality was evaluated by two linguists; we  show the average of the reported scores.
true	P13-1020.pdf#15.18	DUC 2004 Task 1, ROUGE-2	- R - SU4 -  Table 2: Results for compressive summarization.  Shown are the ROUGE-2 and ROUGE SU-4 re- calls with the default options from the ROUGE  toolkit (Lin, 2004); Pyramid scores (Nenkova and  Passonneau, 2004); and linguistic quality scores,  scored between 1 (very bad) to 5 (very good). For  Pyramid, the evaluation was performed by two  annotators, each evaluating half of the problems;  scores marked with  † were computed by different  annotators and are not directly comparable. Lin- guistic quality was evaluated by two linguists; we  show the average of the reported scores.
true	P13-1020.pdf#12.30	DUC 2004 Task 1, ROUGE-2	- R - 2 -  Table 2: Results for compressive summarization.  Shown are the ROUGE-2 and ROUGE SU-4 re- calls with the default options from the ROUGE  toolkit (Lin, 2004); Pyramid scores (Nenkova and  Passonneau, 2004); and linguistic quality scores,  scored between 1 (very bad) to 5 (very good). For  Pyramid, the evaluation was performed by two  annotators, each evaluating half of the problems;  scores marked with  † were computed by different  annotators and are not directly comparable. Lin- guistic quality was evaluated by two linguists; we  show the average of the reported scores.
true	P13-1020.pdf#42.6	DUC 2004 Task 1, ROUGE-1	- Pyr -  Table 2: Results for compressive summarization.  Shown are the ROUGE-2 and ROUGE SU-4 re- calls with the default options from the ROUGE  toolkit (Lin, 2004); Pyramid scores (Nenkova and  Passonneau, 2004); and linguistic quality scores,  scored between 1 (very bad) to 5 (very good). For  Pyramid, the evaluation was performed by two  annotators, each evaluating half of the problems;  scores marked with  † were computed by different  annotators and are not directly comparable. Lin- guistic quality was evaluated by two linguists; we  show the average of the reported scores.
true	P13-1020.pdf#4.6	DUC 2004 Task 1, ROUGE-1	- LQ  Table 2: Results for compressive summarization.  Shown are the ROUGE-2 and ROUGE SU-4 re- calls with the default options from the ROUGE  toolkit (Lin, 2004); Pyramid scores (Nenkova and  Passonneau, 2004); and linguistic quality scores,  scored between 1 (very bad) to 5 (very good). For  Pyramid, the evaluation was performed by two  annotators, each evaluating half of the problems;  scores marked with  † were computed by different  annotators and are not directly comparable. Lin- guistic quality was evaluated by two linguists; we  show the average of the reported scores.
true	P13-1020.pdf#15.18	DUC 2004 Task 1, ROUGE-1	- R - SU4 -  Table 2: Results for compressive summarization.  Shown are the ROUGE-2 and ROUGE SU-4 re- calls with the default options from the ROUGE  toolkit (Lin, 2004); Pyramid scores (Nenkova and  Passonneau, 2004); and linguistic quality scores,  scored between 1 (very bad) to 5 (very good). For  Pyramid, the evaluation was performed by two  annotators, each evaluating half of the problems;  scores marked with  † were computed by different  annotators and are not directly comparable. Lin- guistic quality was evaluated by two linguists; we  show the average of the reported scores.
true	P13-1020.pdf#12.30	DUC 2004 Task 1, ROUGE-1	- R - 2 -  Table 2: Results for compressive summarization.  Shown are the ROUGE-2 and ROUGE SU-4 re- calls with the default options from the ROUGE  toolkit (Lin, 2004); Pyramid scores (Nenkova and  Passonneau, 2004); and linguistic quality scores,  scored between 1 (very bad) to 5 (very good). For  Pyramid, the evaluation was performed by two  annotators, each evaluating half of the problems;  scores marked with  † were computed by different  annotators and are not directly comparable. Lin- guistic quality was evaluated by two linguists; we  show the average of the reported scores.
true	P13-1020.pdf#42.6	DUC 2004 Task 1, ROUGE-L	- Pyr -  Table 2: Results for compressive summarization.  Shown are the ROUGE-2 and ROUGE SU-4 re- calls with the default options from the ROUGE  toolkit (Lin, 2004); Pyramid scores (Nenkova and  Passonneau, 2004); and linguistic quality scores,  scored between 1 (very bad) to 5 (very good). For  Pyramid, the evaluation was performed by two  annotators, each evaluating half of the problems;  scores marked with  † were computed by different  annotators and are not directly comparable. Lin- guistic quality was evaluated by two linguists; we  show the average of the reported scores.
true	P13-1020.pdf#4.6	DUC 2004 Task 1, ROUGE-L	- LQ  Table 2: Results for compressive summarization.  Shown are the ROUGE-2 and ROUGE SU-4 re- calls with the default options from the ROUGE  toolkit (Lin, 2004); Pyramid scores (Nenkova and  Passonneau, 2004); and linguistic quality scores,  scored between 1 (very bad) to 5 (very good). For  Pyramid, the evaluation was performed by two  annotators, each evaluating half of the problems;  scores marked with  † were computed by different  annotators and are not directly comparable. Lin- guistic quality was evaluated by two linguists; we  show the average of the reported scores.
true	P13-1020.pdf#15.18	DUC 2004 Task 1, ROUGE-L	- R - SU4 -  Table 2: Results for compressive summarization.  Shown are the ROUGE-2 and ROUGE SU-4 re- calls with the default options from the ROUGE  toolkit (Lin, 2004); Pyramid scores (Nenkova and  Passonneau, 2004); and linguistic quality scores,  scored between 1 (very bad) to 5 (very good). For  Pyramid, the evaluation was performed by two  annotators, each evaluating half of the problems;  scores marked with  † were computed by different  annotators and are not directly comparable. Lin- guistic quality was evaluated by two linguists; we  show the average of the reported scores.
true	P13-1020.pdf#12.30	DUC 2004 Task 1, ROUGE-L	- R - 2 -  Table 2: Results for compressive summarization.  Shown are the ROUGE-2 and ROUGE SU-4 re- calls with the default options from the ROUGE  toolkit (Lin, 2004); Pyramid scores (Nenkova and  Passonneau, 2004); and linguistic quality scores,  scored between 1 (very bad) to 5 (very good). For  Pyramid, the evaluation was performed by two  annotators, each evaluating half of the problems;  scores marked with  † were computed by different  annotators and are not directly comparable. Lin- guistic quality was evaluated by two linguists; we  show the average of the reported scores.
true	D14-1109.pdf#92.67	benchmark Vietnamese dependency treebank VnDT, UAS	Exact 1st Full  Table 4: Results of our model and several state-of-the-art systems. "Best Published UAS" includes the  most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et  al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set  of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser  (Zhang et al., 2014) and tensor features (Lei et al., 2014).
true	D14-1109.pdf#93.25	benchmark Vietnamese dependency treebank VnDT, UAS	Exact 1st Full  Table 4: Results of our model and several state-of-the-art systems. "Best Published UAS" includes the  most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et  al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set  of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser  (Zhang et al., 2014) and tensor features (Lei et al., 2014).
true	D14-1109.pdf#93.80	benchmark Vietnamese dependency treebank VnDT, UAS	Our Model 3rd  Table 4: Results of our model and several state-of-the-art systems. "Best Published UAS" includes the  most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et  al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set  of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser  (Zhang et al., 2014) and tensor features (Lei et al., 2014).
true	D14-1109.pdf#87.06	benchmark Vietnamese dependency treebank VnDT, UAS	Our Model Fullw / o tensor  Table 4: Results of our model and several state-of-the-art systems. "Best Published UAS" includes the  most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et  al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set  of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser  (Zhang et al., 2014) and tensor features (Lei et al., 2014).
true	D14-1109.pdf#93.04	benchmark Vietnamese dependency treebank VnDT, UAS	Exact 1st Full  Table 4: Results of our model and several state-of-the-art systems. "Best Published UAS" includes the  most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et  al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set  of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser  (Zhang et al., 2014) and tensor features (Lei et al., 2014).
true	D14-1109.pdf#87.39	benchmark Vietnamese dependency treebank VnDT, UAS	Exact 1st Full  Table 4: Results of our model and several state-of-the-art systems. "Best Published UAS" includes the  most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et  al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set  of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser  (Zhang et al., 2014) and tensor features (Lei et al., 2014).
true	D14-1109.pdf#88.75	benchmark Vietnamese dependency treebank VnDT, UAS	Exact 1st Full  Table 4: Results of our model and several state-of-the-art systems. "Best Published UAS" includes the  most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et  al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set  of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser  (Zhang et al., 2014) and tensor features (Lei et al., 2014).
true	D14-1109.pdf#0.994	benchmark Vietnamese dependency treebank VnDT, UAS	len≤15 1  Table 6: Fractions (%) of the sentences that find  the best solution among 3,000 restarts within the  first 300 restarts.
true	D14-1109.pdf#800	benchmark Vietnamese dependency treebank VnDT, UAS	len>15 1  Table 6: Fractions (%) of the sentences that find  the best solution among 3,000 restarts within the  first 300 restarts.
true	D14-1109.pdf#400	benchmark Vietnamese dependency treebank VnDT, UAS	len>15 1  Table 6: Fractions (%) of the sentences that find  the best solution among 3,000 restarts within the  first 300 restarts.
true	D14-1109.pdf#600	benchmark Vietnamese dependency treebank VnDT, UAS	len>15 1  Table 6: Fractions (%) of the sentences that find  the best solution among 3,000 restarts within the  first 300 restarts.
true	D14-1109.pdf#0.998	benchmark Vietnamese dependency treebank VnDT, UAS	1  Table 6: Fractions (%) of the sentences that find  the best solution among 3,000 restarts within the  first 300 restarts.
true	D14-1109.pdf#0.996	benchmark Vietnamese dependency treebank VnDT, UAS	1  Table 6: Fractions (%) of the sentences that find  the best solution among 3,000 restarts within the  first 300 restarts.
true	D14-1109.pdf#200	benchmark Vietnamese dependency treebank VnDT, UAS	len>15 1  Table 6: Fractions (%) of the sentences that find  the best solution among 3,000 restarts within the  first 300 restarts.
true	D14-1109.pdf#92.67	Penn Treebank, UAS	Exact 1st Full  Table 4: Results of our model and several state-of-the-art systems. "Best Published UAS" includes the  most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et  al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set  of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser  (Zhang et al., 2014) and tensor features (Lei et al., 2014).
true	D14-1109.pdf#93.25	Penn Treebank, UAS	Exact 1st Full  Table 4: Results of our model and several state-of-the-art systems. "Best Published UAS" includes the  most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et  al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set  of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser  (Zhang et al., 2014) and tensor features (Lei et al., 2014).
true	D14-1109.pdf#93.80	Penn Treebank, UAS	Our Model 3rd  Table 4: Results of our model and several state-of-the-art systems. "Best Published UAS" includes the  most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et  al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set  of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser  (Zhang et al., 2014) and tensor features (Lei et al., 2014).
true	D14-1109.pdf#87.06	Penn Treebank, UAS	Our Model Fullw / o tensor  Table 4: Results of our model and several state-of-the-art systems. "Best Published UAS" includes the  most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et  al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set  of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser  (Zhang et al., 2014) and tensor features (Lei et al., 2014).
true	D14-1109.pdf#93.04	Penn Treebank, UAS	Exact 1st Full  Table 4: Results of our model and several state-of-the-art systems. "Best Published UAS" includes the  most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et  al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set  of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser  (Zhang et al., 2014) and tensor features (Lei et al., 2014).
true	D14-1109.pdf#87.39	Penn Treebank, UAS	Exact 1st Full  Table 4: Results of our model and several state-of-the-art systems. "Best Published UAS" includes the  most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et  al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set  of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser  (Zhang et al., 2014) and tensor features (Lei et al., 2014).
true	D14-1109.pdf#88.75	Penn Treebank, UAS	Exact 1st Full  Table 4: Results of our model and several state-of-the-art systems. "Best Published UAS" includes the  most accurate parsers among Martins et al. (2011), Martins et al. (2013), Koo et al. (2010), Zhang et  al. (2013), Lei et al. (2014) and Zhang et al. (2014). For the third-order model, we use the feature set  of TurboParser (Martins et al., 2013). The full model combines features of our sampling-based parser  (Zhang et al., 2014) and tensor features (Lei et al., 2014).
true	D14-1109.pdf#0.994	Penn Treebank, UAS	len≤15 1  Table 6: Fractions (%) of the sentences that find  the best solution among 3,000 restarts within the  first 300 restarts.
true	D14-1109.pdf#800	Penn Treebank, UAS	len>15 1  Table 6: Fractions (%) of the sentences that find  the best solution among 3,000 restarts within the  first 300 restarts.
true	D14-1109.pdf#400	Penn Treebank, UAS	len>15 1  Table 6: Fractions (%) of the sentences that find  the best solution among 3,000 restarts within the  first 300 restarts.
true	D14-1109.pdf#600	Penn Treebank, UAS	len>15 1  Table 6: Fractions (%) of the sentences that find  the best solution among 3,000 restarts within the  first 300 restarts.
true	D14-1109.pdf#0.998	Penn Treebank, UAS	1  Table 6: Fractions (%) of the sentences that find  the best solution among 3,000 restarts within the  first 300 restarts.
true	D14-1109.pdf#0.996	Penn Treebank, UAS	1  Table 6: Fractions (%) of the sentences that find  the best solution among 3,000 restarts within the  first 300 restarts.
true	D14-1109.pdf#200	Penn Treebank, UAS	len>15 1  Table 6: Fractions (%) of the sentences that find  the best solution among 3,000 restarts within the  first 300 restarts.
true	P15-1167.pdf#4	Chinese Treebank 6, F1	set split using official scoring script . shows the All evaluations in this PKU  Table 2: Corpus details of PKU and MSR
true	P15-1167.pdf#6	Chinese Treebank 6, F1	set split using official scoring script . shows the All evaluations in this PKU  Table 2: Corpus details of PKU and MSR
true	P15-1167.pdf#2	Chinese Treebank 6, F1	paper are conducted with official training / testing shows the All evaluations in this  Table 2: Corpus details of PKU and MSR
true	P15-1167.pdf#6	Chinese Treebank 6, F1	set split using official scoring script . shows the All evaluations in this MSR  Table 2: Corpus details of PKU and MSR
true	P15-1167.pdf#4	Chinese Treebank 6, F1	set split using official scoring script . shows the All evaluations in this MSR  Table 2: Corpus details of PKU and MSR
true	P15-1167.pdf#94.6	Chinese Treebank 6, F1	- PKU Corpus R  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#96.5	Chinese Treebank 6, F1	- MSR Corpus R  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#96.6	Chinese Treebank 6, F1	- MSR Corpus P  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#95.2	Chinese Treebank 6, F1	Models PKU Corpus F  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#±0.11	Chinese Treebank 6, F1	Models PKU Corpus P MSR CI  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#±0.14	Chinese Treebank 6, F1	Models PKU Corpus P MSR CI  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#±0.15	Chinese Treebank 6, F1	Models PKU Corpus P PKU CI  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#97.2	Chinese Treebank 6, F1	Models MSR Corpus F  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#87.2	Chinese Treebank 6, F1	- MSR Corpus R oov  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#95.5	Chinese Treebank 6, F1	- PKU Corpus P  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#76.0	Chinese Treebank 6, F1	- PKU Corpus R oov  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#±0.13	Chinese Treebank 6, F1	Models PKU Corpus P PKU CI  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#93.5	Chinese Treebank 6, F1	Models PKU F  Table 4: Significance test of closed-set results of  Pei et al (2014) and our model.
true	P15-1167.pdf#94.4	Chinese Treebank 6, F1	Models MSR F  Table 4: Significance test of closed-set results of  Pei et al (2014) and our model.
true	P15-1167.pdf#96.6	Chinese Treebank 6, F1	Models MSR F  Table 4: Significance test of closed-set results of  Pei et al (2014) and our model.
true	P15-1167.pdf#95.1	Chinese Treebank 6, F1	Models PKU F  Table 4: Significance test of closed-set results of  Pei et al (2014) and our model.
true	P15-1167.pdf#97.4	Chinese Treebank 6, F1	MSR  Table 5: Comparison with the state-of-the-art sys- tems. Results with  † used extra lexicon/raw cor- pora for training, i.e. in open-set setting. Best05  refers to the best closed-set results in 2nd SIGHAN  bakeoff.
true	P15-1167.pdf#96.1	Chinese Treebank 6, F1	PKU  Table 5: Comparison with the state-of-the-art sys- tems. Results with  † used extra lexicon/raw cor- pora for training, i.e. in open-set setting. Best05  refers to the best closed-set results in 2nd SIGHAN  bakeoff.
true	P15-1167.pdf#97.4	Chinese Treebank 6, F1	MSR  Table 5: Comparison with the state-of-the-art sys- tems. Results with  † used extra lexicon/raw cor- pora for training, i.e. in open-set setting. Best05  refers to the best closed-set results in 2nd SIGHAN  bakeoff.
true	P15-1167.pdf#95.1	Chinese Treebank 6, F1	F - score  Table 6: The influence of features. F-score in per- centage on the PKU corpus.
true	P15-1167.pdf#4	MSR, F1	set split using official scoring script . shows the All evaluations in this PKU  Table 2: Corpus details of PKU and MSR
true	P15-1167.pdf#6	MSR, F1	set split using official scoring script . shows the All evaluations in this PKU  Table 2: Corpus details of PKU and MSR
true	P15-1167.pdf#2	MSR, F1	paper are conducted with official training / testing shows the All evaluations in this  Table 2: Corpus details of PKU and MSR
true	P15-1167.pdf#6	MSR, F1	set split using official scoring script . shows the All evaluations in this MSR  Table 2: Corpus details of PKU and MSR
true	P15-1167.pdf#4	MSR, F1	set split using official scoring script . shows the All evaluations in this MSR  Table 2: Corpus details of PKU and MSR
true	P15-1167.pdf#94.6	MSR, F1	- PKU Corpus R  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#96.5	MSR, F1	- MSR Corpus R  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#96.6	MSR, F1	- MSR Corpus P  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#95.2	MSR, F1	Models PKU Corpus F  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#±0.11	MSR, F1	Models PKU Corpus P MSR CI  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#±0.14	MSR, F1	Models PKU Corpus P MSR CI  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#±0.15	MSR, F1	Models PKU Corpus P PKU CI  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#97.2	MSR, F1	Models MSR Corpus F  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#87.2	MSR, F1	- MSR Corpus R oov  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#95.5	MSR, F1	- PKU Corpus P  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#76.0	MSR, F1	- PKU Corpus R oov  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#±0.13	MSR, F1	Models PKU Corpus P PKU CI  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#93.5	MSR, F1	Models PKU F  Table 4: Significance test of closed-set results of  Pei et al (2014) and our model.
true	P15-1167.pdf#94.4	MSR, F1	Models MSR F  Table 4: Significance test of closed-set results of  Pei et al (2014) and our model.
true	P15-1167.pdf#96.6	MSR, F1	Models MSR F  Table 4: Significance test of closed-set results of  Pei et al (2014) and our model.
true	P15-1167.pdf#95.1	MSR, F1	Models PKU F  Table 4: Significance test of closed-set results of  Pei et al (2014) and our model.
true	P15-1167.pdf#97.4	MSR, F1	MSR  Table 5: Comparison with the state-of-the-art sys- tems. Results with  † used extra lexicon/raw cor- pora for training, i.e. in open-set setting. Best05  refers to the best closed-set results in 2nd SIGHAN  bakeoff.
true	P15-1167.pdf#96.1	MSR, F1	PKU  Table 5: Comparison with the state-of-the-art sys- tems. Results with  † used extra lexicon/raw cor- pora for training, i.e. in open-set setting. Best05  refers to the best closed-set results in 2nd SIGHAN  bakeoff.
true	P15-1167.pdf#97.4	MSR, F1	MSR  Table 5: Comparison with the state-of-the-art sys- tems. Results with  † used extra lexicon/raw cor- pora for training, i.e. in open-set setting. Best05  refers to the best closed-set results in 2nd SIGHAN  bakeoff.
true	P15-1167.pdf#95.1	MSR, F1	F - score  Table 6: The influence of features. F-score in per- centage on the PKU corpus.
true	P15-1167.pdf#4	PKU, F1	set split using official scoring script . shows the All evaluations in this PKU  Table 2: Corpus details of PKU and MSR
true	P15-1167.pdf#6	PKU, F1	set split using official scoring script . shows the All evaluations in this PKU  Table 2: Corpus details of PKU and MSR
true	P15-1167.pdf#2	PKU, F1	paper are conducted with official training / testing shows the All evaluations in this  Table 2: Corpus details of PKU and MSR
true	P15-1167.pdf#6	PKU, F1	set split using official scoring script . shows the All evaluations in this MSR  Table 2: Corpus details of PKU and MSR
true	P15-1167.pdf#4	PKU, F1	set split using official scoring script . shows the All evaluations in this MSR  Table 2: Corpus details of PKU and MSR
true	P15-1167.pdf#94.6	PKU, F1	- PKU Corpus R  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#96.5	PKU, F1	- MSR Corpus R  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#96.6	PKU, F1	- MSR Corpus P  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#95.2	PKU, F1	Models PKU Corpus F  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#±0.11	PKU, F1	Models PKU Corpus P MSR CI  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#±0.14	PKU, F1	Models PKU Corpus P MSR CI  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#±0.15	PKU, F1	Models PKU Corpus P PKU CI  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#97.2	PKU, F1	Models MSR Corpus F  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#87.2	PKU, F1	- MSR Corpus R oov  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#95.5	PKU, F1	- PKU Corpus P  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#76.0	PKU, F1	- PKU Corpus R oov  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#±0.13	PKU, F1	Models PKU Corpus P PKU CI  Table 3: Comparison with previous embedding based models. Numbers in percentage. Results with  †  used extra corpora for (pre-)training.
true	P15-1167.pdf#93.5	PKU, F1	Models PKU F  Table 4: Significance test of closed-set results of  Pei et al (2014) and our model.
true	P15-1167.pdf#94.4	PKU, F1	Models MSR F  Table 4: Significance test of closed-set results of  Pei et al (2014) and our model.
true	P15-1167.pdf#96.6	PKU, F1	Models MSR F  Table 4: Significance test of closed-set results of  Pei et al (2014) and our model.
true	P15-1167.pdf#95.1	PKU, F1	Models PKU F  Table 4: Significance test of closed-set results of  Pei et al (2014) and our model.
true	P15-1167.pdf#97.4	PKU, F1	MSR  Table 5: Comparison with the state-of-the-art sys- tems. Results with  † used extra lexicon/raw cor- pora for training, i.e. in open-set setting. Best05  refers to the best closed-set results in 2nd SIGHAN  bakeoff.
true	P15-1167.pdf#96.1	PKU, F1	PKU  Table 5: Comparison with the state-of-the-art sys- tems. Results with  † used extra lexicon/raw cor- pora for training, i.e. in open-set setting. Best05  refers to the best closed-set results in 2nd SIGHAN  bakeoff.
true	P15-1167.pdf#97.4	PKU, F1	MSR  Table 5: Comparison with the state-of-the-art sys- tems. Results with  † used extra lexicon/raw cor- pora for training, i.e. in open-set setting. Best05  refers to the best closed-set results in 2nd SIGHAN  bakeoff.
true	P15-1167.pdf#95.1	PKU, F1	F - score  Table 6: The influence of features. F-score in per- centage on the PKU corpus.
true	P10-1134.pdf#83.48	SQuAD, EM	A  Table 2: Performance on the Wikipedia dataset.
true	P10-1134.pdf#82.70	SQuAD, EM	R  Table 2: Performance on the Wikipedia dataset.
true	P10-1134.pdf#75.05	SQuAD, EM	F 1  Table 2: Performance on the Wikipedia dataset.
true	P10-1134.pdf#99.88	SQuAD, EM	P  Table 2: Performance on the Wikipedia dataset.
true	P10-1134.pdf#75.23	SQuAD, EM	F 1  Table 2: Performance on the Wikipedia dataset.
true	P10-1134.pdf#98.81	SQuAD, EM	P  Table 2: Performance on the Wikipedia dataset.
true	P10-1134.pdf#98.33	SQuAD, EM	P  Table 3: Performance on the ukWaC dataset ( † Re- call is estimated).
true	P10-1134.pdf#94.87	SQuAD, EM	P  Table 3: Performance on the ukWaC dataset ( † Re- call is estimated).
true	P10-1134.pdf#63.63	SQuAD, EM	R †  Table 3: Performance on the ukWaC dataset ( † Re- call is estimated).
true	P10-1134.pdf#89.27	SQuAD, EM	( 230 ) Full  Table 5: Precision in hypernym extraction on the  ukWaC dataset (number of hypernyms in paren- theses).
true	P10-1134.pdf#96.23	SQuAD, EM	Substring  Table 5: Precision in hypernym extraction on the  ukWaC dataset (number of hypernyms in paren- theses).
true	P10-1134.pdf#96.27	SQuAD, EM	( 230 ) Substring  Table 5: Precision in hypernym extraction on the  ukWaC dataset (number of hypernyms in paren- theses).
true	P10-1134.pdf#83.48	SQuAD, F1	A  Table 2: Performance on the Wikipedia dataset.
true	P10-1134.pdf#82.70	SQuAD, F1	R  Table 2: Performance on the Wikipedia dataset.
true	P10-1134.pdf#75.05	SQuAD, F1	F 1  Table 2: Performance on the Wikipedia dataset.
true	P10-1134.pdf#99.88	SQuAD, F1	P  Table 2: Performance on the Wikipedia dataset.
true	P10-1134.pdf#75.23	SQuAD, F1	F 1  Table 2: Performance on the Wikipedia dataset.
true	P10-1134.pdf#98.81	SQuAD, F1	P  Table 2: Performance on the Wikipedia dataset.
true	P10-1134.pdf#98.33	SQuAD, F1	P  Table 3: Performance on the ukWaC dataset ( † Re- call is estimated).
true	P10-1134.pdf#94.87	SQuAD, F1	P  Table 3: Performance on the ukWaC dataset ( † Re- call is estimated).
true	P10-1134.pdf#63.63	SQuAD, F1	R †  Table 3: Performance on the ukWaC dataset ( † Re- call is estimated).
true	P10-1134.pdf#89.27	SQuAD, F1	( 230 ) Full  Table 5: Precision in hypernym extraction on the  ukWaC dataset (number of hypernyms in paren- theses).
true	P10-1134.pdf#96.23	SQuAD, F1	Substring  Table 5: Precision in hypernym extraction on the  ukWaC dataset (number of hypernyms in paren- theses).
true	P10-1134.pdf#96.27	SQuAD, F1	( 230 ) Substring  Table 5: Precision in hypernym extraction on the  ukWaC dataset (number of hypernyms in paren- theses).
true	P10-1134.pdf#83.48	New York Times Corpus, P@10%	A  Table 2: Performance on the Wikipedia dataset.
true	P10-1134.pdf#82.70	New York Times Corpus, P@10%	R  Table 2: Performance on the Wikipedia dataset.
true	P10-1134.pdf#75.05	New York Times Corpus, P@10%	F 1  Table 2: Performance on the Wikipedia dataset.
true	P10-1134.pdf#99.88	New York Times Corpus, P@10%	P  Table 2: Performance on the Wikipedia dataset.
true	P10-1134.pdf#75.23	New York Times Corpus, P@10%	F 1  Table 2: Performance on the Wikipedia dataset.
true	P10-1134.pdf#98.81	New York Times Corpus, P@10%	P  Table 2: Performance on the Wikipedia dataset.
true	P10-1134.pdf#98.33	New York Times Corpus, P@10%	P  Table 3: Performance on the ukWaC dataset ( † Re- call is estimated).
true	P10-1134.pdf#94.87	New York Times Corpus, P@10%	P  Table 3: Performance on the ukWaC dataset ( † Re- call is estimated).
true	P10-1134.pdf#63.63	New York Times Corpus, P@10%	R †  Table 3: Performance on the ukWaC dataset ( † Re- call is estimated).
true	P10-1134.pdf#89.27	New York Times Corpus, P@10%	( 230 ) Full  Table 5: Precision in hypernym extraction on the  ukWaC dataset (number of hypernyms in paren- theses).
true	P10-1134.pdf#96.23	New York Times Corpus, P@10%	Substring  Table 5: Precision in hypernym extraction on the  ukWaC dataset (number of hypernyms in paren- theses).
true	P10-1134.pdf#96.27	New York Times Corpus, P@10%	( 230 ) Substring  Table 5: Precision in hypernym extraction on the  ukWaC dataset (number of hypernyms in paren- theses).
true	P11-1159.pdf#80.11	benchmark Vietnamese dependency treebank VnDT, UAS	++ LAS  Table 1: CORE12 with inflectional features, predicted input.
true	P11-1159.pdf#1.43	benchmark Vietnamese dependency treebank VnDT, UAS	++ LAS dif f  Table 1: CORE12 with inflectional features, predicted input.
true	P11-1159.pdf#1.14	benchmark Vietnamese dependency treebank VnDT, UAS	LAS dif f  Table 1: CORE12 with inflectional features, predicted input.
true	P11-1159.pdf#79.82	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 1: CORE12 with inflectional features, predicted input.
true	P11-1159.pdf#80.11	benchmark Vietnamese dependency treebank VnDT, UAS	++ LAS
true	P11-1159.pdf#1.43	benchmark Vietnamese dependency treebank VnDT, UAS	++ LAS dif f
true	P11-1159.pdf#79.82	benchmark Vietnamese dependency treebank VnDT, UAS	LAS
true	P11-1159.pdf#1.14	benchmark Vietnamese dependency treebank VnDT, UAS	LAS dif f
true	P11-1159.pdf#78.96+	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 3: Lexical features. Top part: Adding each feature
true	P11-1159.pdf#0.28	benchmark Vietnamese dependency treebank VnDT, UAS	LAS dif f  Table 3: Lexical features. Top part: Adding each feature
true	P11-1159.pdf#0.37	benchmark Vietnamese dependency treebank VnDT, UAS	LAS dif f  Table 3: Lexical features. Top part: Adding each feature
true	P11-1159.pdf#79.05++	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 3: Lexical features. Top part: Adding each feature
true	P11-1159.pdf#80.11++	benchmark Vietnamese dependency treebank VnDT, UAS	dif f  Table 4: Inflectional+lexical features together.
true	P11-1159.pdf#1.55	benchmark Vietnamese dependency treebank VnDT, UAS	dif f  Table 4: Inflectional+lexical features together.
true	P11-1159.pdf#80.23++	benchmark Vietnamese dependency treebank VnDT, UAS	dif f  Table 4: Inflectional+lexical features together.
true	P11-1159.pdf#80.13++	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 5: Extended inflectional features.
true	P11-1159.pdf#80.23++	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 5: Extended inflectional features.
true	P11-1159.pdf#80.83++	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 6: Functional features: gender, number, rationality.
true	P11-1159.pdf#74.40++	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 6: Functional features: gender, number, rationality.
true	P11-1159.pdf#80.53++	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 6: Functional features: gender, number, rationality.
true	P11-1159.pdf#78.46	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 7: Results on unseen test set for models which performed
true	P11-1159.pdf#81.81	benchmark Vietnamese dependency treebank VnDT, UAS	UAS  Table 7: Results on unseen test set for models which performed
true	D13-1097.pdf#82.1%	New York Times Corpus, P@10%	F 1  Table 3: Performance comparisons between the proposed  method and state-of-the-art methods. "MLN" represents  the method proposed in this work.
true	D13-1097.pdf#56.2%	New York Times Corpus, P@10%	Methods P P  Table 3: Performance comparisons between the proposed  method and state-of-the-art methods. "MLN" represents  the method proposed in this work.
true	D13-1097.pdf#63.5%	New York Times Corpus, P@10%	Methods F 1 F 1  Table 3: Performance comparisons between the proposed  method and state-of-the-art methods. "MLN" represents  the method proposed in this work.
true	D13-1097.pdf#89.7%	New York Times Corpus, P@10%	Methods R R  Table 3: Performance comparisons between the proposed  method and state-of-the-art methods. "MLN" represents  the method proposed in this work.
true	D13-1097.pdf#80.6%	New York Times Corpus, P@10%	R  Table 3: Performance comparisons between the proposed  method and state-of-the-art methods. "MLN" represents  the method proposed in this work.
true	D13-1097.pdf#85.3%	New York Times Corpus, P@10%	P  Table 3: Performance comparisons between the proposed  method and state-of-the-art methods. "MLN" represents  the method proposed in this work.
true	D13-1097.pdf#56.2%	New York Times Corpus, P@10%	Methods P P  Table 4. The first  row shows the result of the MLN based method with  all observed predicates and local formulas. From the  results we can observe that the observed predicates  which are not used in the local formulas for sub- jectivity classification also impact the performance  of subjectivity classification. We think that the per- formance is effected by the global formulas, which  combine the procedure of subjectivity classification
true	D13-1097.pdf#63.5%	New York Times Corpus, P@10%	Methods F 1 F 1  Table 4. The first  row shows the result of the MLN based method with  all observed predicates and local formulas. From the  results we can observe that the observed predicates  which are not used in the local formulas for sub- jectivity classification also impact the performance  of subjectivity classification. We think that the per- formance is effected by the global formulas, which  combine the procedure of subjectivity classification
true	D13-1097.pdf#85.3%	New York Times Corpus, P@10%	P  Table 4. The first  row shows the result of the MLN based method with  all observed predicates and local formulas. From the  results we can observe that the observed predicates  which are not used in the local formulas for sub- jectivity classification also impact the performance  of subjectivity classification. We think that the per- formance is effected by the global formulas, which  combine the procedure of subjectivity classification
true	D13-1097.pdf#89.7%	New York Times Corpus, P@10%	Methods R R  Table 4. The first  row shows the result of the MLN based method with  all observed predicates and local formulas. From the  results we can observe that the observed predicates  which are not used in the local formulas for sub- jectivity classification also impact the performance  of subjectivity classification. We think that the per- formance is effected by the global formulas, which  combine the procedure of subjectivity classification
true	D13-1097.pdf#80.6%	New York Times Corpus, P@10%	R  Table 4. The first  row shows the result of the MLN based method with  all observed predicates and local formulas. From the  results we can observe that the observed predicates  which are not used in the local formulas for sub- jectivity classification also impact the performance  of subjectivity classification. We think that the per- formance is effected by the global formulas, which  combine the procedure of subjectivity classification
true	D13-1097.pdf#82.1%	New York Times Corpus, P@10%	F 1  Table 4. The first  row shows the result of the MLN based method with  all observed predicates and local formulas. From the  results we can observe that the observed predicates  which are not used in the local formulas for sub- jectivity classification also impact the performance  of subjectivity classification. We think that the per- formance is effected by the global formulas, which  combine the procedure of subjectivity classification
true	P12-1098.pdf#0.236	WMT 2014 EN-FR, BLEU	Into - En seg .  Table 2: Correlations with human judgment on WMT
true	P12-1098.pdf#0.834	WMT 2014 EN-FR, BLEU	Into - En sys .  Table 2: Correlations with human judgment on WMT
true	P12-1098.pdf#0.242	WMT 2014 EN-FR, BLEU	Out - of - En seg .  Table 2: Correlations with human judgment on WMT
true	P12-1098.pdf#0.835	WMT 2014 EN-FR, BLEU	Out - of - En sys .  Table 2: Correlations with human judgment on WMT
true	P12-1098.pdf#27.2*	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) zh - en large Similarity of BLEU  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#30.3*	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) zh - en large Similarity of BLEU  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#55.7	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) fr - en Hans BLEU - tuned MTR  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#20.3	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) zh - en large Similarity of BLEU  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#69.8	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) fr - en Hans BLEU - tuned MTR  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#42.0	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) de - en WMT and PORT - tuned 1 - TER  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#59.0	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) fr - en Hans BLEU - tuned MTR  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#30.7	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) de - en WMT and PORT - tuned 1 - TER  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#53.2	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) en - de WMT and PORT - tuned PORT  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#56.0	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) fr - en Hans BLEU - tuned MTR  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#39.7	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) en - de WMT and PORT - tuned PORT  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#54.6	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) de - en WMT and PORT - tuned 1 - TER  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#50.0	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) en - de WMT and PORT - tuned PORT  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#30.3*	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) zh - en large Similarity of BLEU  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#59.0	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) fr - en Hans BLEU - tuned MTR  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#20.3	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) zh - en large Similarity of BLEU  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#54.6	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) de - en WMT and PORT - tuned 1 - TER  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#30.7	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) de - en WMT and PORT - tuned 1 - TER  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#69.8	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) fr - en Hans BLEU - tuned MTR  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#50.0	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) en - de WMT and PORT - tuned PORT  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#27.2*	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) zh - en large Similarity of BLEU  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#55.7	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) fr - en Hans BLEU - tuned MTR  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#53.2	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) en - de WMT and PORT - tuned PORT  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#56.0	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) fr - en Hans BLEU - tuned MTR  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#42.0	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) de - en WMT and PORT - tuned 1 - TER  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#39.7	WMT 2014 EN-FR, BLEU	Evaluation metrics ( % ) en - de WMT and PORT - tuned PORT  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#17	WMT 2014 EN-FR, BLEU	151  Table 5: Human preference for outputs from PORT- tuned vs. BLEU-tuned system.
true	P12-1098.pdf#38.8%	WMT 2014 EN-FR, BLEU	Table 5: Human preference for outputs from PORT- tuned vs. BLEU-tuned system.
true	P12-1098.pdf#45.3%	WMT 2014 EN-FR, BLEU	300 151  Table 5: Human preference for outputs from PORT- tuned vs. BLEU-tuned system.
true	P12-1098.pdf#69	WMT 2014 EN-FR, BLEU	Table 5: Human preference for outputs from PORT- tuned vs. BLEU-tuned system.
true	P12-1098.pdf#34	WMT 2014 EN-FR, BLEU	151  Table 5: Human preference for outputs from PORT- tuned vs. BLEU-tuned system.
true	P12-1098.pdf#39.5%	WMT 2014 EN-FR, BLEU	151  Table 5: Human preference for outputs from PORT- tuned vs. BLEU-tuned system.
true	P12-1098.pdf#45.7%	WMT 2014 EN-FR, BLEU	151  Table 5: Human preference for outputs from PORT- tuned vs. BLEU-tuned system.
true	P12-1098.pdf#19	WMT 2014 EN-FR, BLEU	Table 5: Human preference for outputs from PORT- tuned vs. BLEU-tuned system.
true	P12-1098.pdf#59.7%	WMT 2014 EN-FR, BLEU	151  Table 5: Human preference for outputs from PORT- tuned vs. BLEU-tuned system.
true	P12-1098.pdf#59.0	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#39.7	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment PORT PORT  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#38.4	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#20.3	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment BLEU BLEU  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#53.2	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment PORT PORT  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#48.2	WMT 2014 EN-FR, BLEU	PORT  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#54.6	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#25.3	WMT 2014 EN-FR, BLEU	BLEU  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#54.6	WMT 2014 EN-FR, BLEU	MTR  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#30.3	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment BLEU BLEU  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#39.7	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment PORT PORT  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#37.0	WMT 2014 EN-FR, BLEU	1 - TER  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#69.8	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#56.3	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#25.3	WMT 2014 EN-FR, BLEU	BLEU  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#42.0	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#54.6	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#20.3	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment BLEU BLEU  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#38.4	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#30.7	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#43.4	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#69.8	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#55.7	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#27.2	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment BLEU BLEU  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#38.2	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#50.0	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment PORT PORT  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#69.8	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#37.0	WMT 2014 EN-FR, BLEU	1 - TER  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#55.7	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#50.0	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment PORT PORT  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#54.6	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#43.4	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#54.6	WMT 2014 EN-FR, BLEU	MTR  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#25.3	WMT 2014 EN-FR, BLEU	BLEU  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#20.3	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment BLEU BLEU  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#30.7	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#30.3	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment BLEU BLEU  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#54.6	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#20.3	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment BLEU BLEU  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#38.4	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#69.8	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#42.0	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#48.2	WMT 2014 EN-FR, BLEU	PORT  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#25.3	WMT 2014 EN-FR, BLEU	BLEU  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#27.2	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment BLEU BLEU  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#56.3	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#38.2	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#59.0	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#39.7	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment PORT PORT  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#39.7	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment PORT PORT  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#38.4	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#53.2	WMT 2014 EN-FR, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment PORT PORT  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#0.918	WMT 2014 EN-FR, BLEU	ordering measures 1 - TER v  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#54.4	WMT 2014 EN-FR, BLEU	METEOR  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#0.981	WMT 2014 EN-FR, BLEU	ordering measures METEOR ρ  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#0.853	WMT 2014 EN-FR, BLEU	ordering measures 1 - TER v  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#0.929	WMT 2014 EN-FR, BLEU	ordering measures METEOR τ  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#25.3	WMT 2014 EN-FR, BLEU	BLEU  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#0.917	WMT 2014 EN-FR, BLEU	ordering measures 1 - TER v  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#37.0	WMT 2014 EN-FR, BLEU	1 - TER  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#0.975	WMT 2014 EN-FR, BLEU	ordering measures METEOR ρ  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#0.866	WMT 2014 EN-FR, BLEU	ordering measures METEOR τ  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#0.928	WMT 2014 EN-FR, BLEU	ordering measures METEOR τ  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#25.3	WMT 2014 EN-FR, BLEU	BLEU  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	P12-1098.pdf#54.4	WMT 2014 EN-FR, BLEU	METEOR  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	P12-1098.pdf#37.0	WMT 2014 EN-FR, BLEU	1 - TER  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	P12-1098.pdf#0.866	WMT 2014 EN-FR, BLEU	ordering measures METEOR τ  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	P12-1098.pdf#0.981	WMT 2014 EN-FR, BLEU	ordering measures METEOR ρ  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	P12-1098.pdf#0.853	WMT 2014 EN-FR, BLEU	ordering measures 1 - TER v  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	P12-1098.pdf#0.929	WMT 2014 EN-FR, BLEU	ordering measures METEOR τ  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	P12-1098.pdf#0.918	WMT 2014 EN-FR, BLEU	ordering measures 1 - TER v  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	P12-1098.pdf#0.917	WMT 2014 EN-FR, BLEU	ordering measures 1 - TER v  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	P12-1098.pdf#0.975	WMT 2014 EN-FR, BLEU	ordering measures METEOR ρ  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	P12-1098.pdf#0.928	WMT 2014 EN-FR, BLEU	ordering measures METEOR τ  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	P12-1098.pdf#0.236	WMT 2014 EN-DE, BLEU	Into - En seg .  Table 2: Correlations with human judgment on WMT
true	P12-1098.pdf#0.834	WMT 2014 EN-DE, BLEU	Into - En sys .  Table 2: Correlations with human judgment on WMT
true	P12-1098.pdf#0.242	WMT 2014 EN-DE, BLEU	Out - of - En seg .  Table 2: Correlations with human judgment on WMT
true	P12-1098.pdf#0.835	WMT 2014 EN-DE, BLEU	Out - of - En sys .  Table 2: Correlations with human judgment on WMT
true	P12-1098.pdf#27.2*	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) zh - en large Similarity of BLEU  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#30.3*	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) zh - en large Similarity of BLEU  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#55.7	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) fr - en Hans BLEU - tuned MTR  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#20.3	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) zh - en large Similarity of BLEU  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#69.8	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) fr - en Hans BLEU - tuned MTR  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#42.0	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) de - en WMT and PORT - tuned 1 - TER  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#59.0	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) fr - en Hans BLEU - tuned MTR  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#30.7	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) de - en WMT and PORT - tuned 1 - TER  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#53.2	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) en - de WMT and PORT - tuned PORT  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#56.0	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) fr - en Hans BLEU - tuned MTR  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#39.7	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) en - de WMT and PORT - tuned PORT  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#54.6	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) de - en WMT and PORT - tuned 1 - TER  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#50.0	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) en - de WMT and PORT - tuned PORT  Table 3: Similarity of BLEU-tuned and PORT-tuned  system outputs on test data.
true	P12-1098.pdf#30.3*	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) zh - en large Similarity of BLEU  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#59.0	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) fr - en Hans BLEU - tuned MTR  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#20.3	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) zh - en large Similarity of BLEU  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#54.6	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) de - en WMT and PORT - tuned 1 - TER  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#30.7	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) de - en WMT and PORT - tuned 1 - TER  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#69.8	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) fr - en Hans BLEU - tuned MTR  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#50.0	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) en - de WMT and PORT - tuned PORT  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#27.2*	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) zh - en large Similarity of BLEU  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#55.7	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) fr - en Hans BLEU - tuned MTR  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#53.2	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) en - de WMT and PORT - tuned PORT  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#56.0	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) fr - en Hans BLEU - tuned MTR  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#42.0	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) de - en WMT and PORT - tuned 1 - TER  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#39.7	WMT 2014 EN-DE, BLEU	Evaluation metrics ( % ) en - de WMT and PORT - tuned PORT  Table 4: Automatic evaluation scores on test data.  * indicates the results are significantly better than the  baseline (p<0.05).
true	P12-1098.pdf#17	WMT 2014 EN-DE, BLEU	151  Table 5: Human preference for outputs from PORT- tuned vs. BLEU-tuned system.
true	P12-1098.pdf#38.8%	WMT 2014 EN-DE, BLEU	Table 5: Human preference for outputs from PORT- tuned vs. BLEU-tuned system.
true	P12-1098.pdf#45.3%	WMT 2014 EN-DE, BLEU	300 151  Table 5: Human preference for outputs from PORT- tuned vs. BLEU-tuned system.
true	P12-1098.pdf#69	WMT 2014 EN-DE, BLEU	Table 5: Human preference for outputs from PORT- tuned vs. BLEU-tuned system.
true	P12-1098.pdf#34	WMT 2014 EN-DE, BLEU	151  Table 5: Human preference for outputs from PORT- tuned vs. BLEU-tuned system.
true	P12-1098.pdf#39.5%	WMT 2014 EN-DE, BLEU	151  Table 5: Human preference for outputs from PORT- tuned vs. BLEU-tuned system.
true	P12-1098.pdf#45.7%	WMT 2014 EN-DE, BLEU	151  Table 5: Human preference for outputs from PORT- tuned vs. BLEU-tuned system.
true	P12-1098.pdf#19	WMT 2014 EN-DE, BLEU	Table 5: Human preference for outputs from PORT- tuned vs. BLEU-tuned system.
true	P12-1098.pdf#59.7%	WMT 2014 EN-DE, BLEU	151  Table 5: Human preference for outputs from PORT- tuned vs. BLEU-tuned system.
true	P12-1098.pdf#59.0	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#39.7	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment PORT PORT  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#38.4	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#20.3	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment BLEU BLEU  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#53.2	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment PORT PORT  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#48.2	WMT 2014 EN-DE, BLEU	PORT  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#54.6	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#25.3	WMT 2014 EN-DE, BLEU	BLEU  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#54.6	WMT 2014 EN-DE, BLEU	MTR  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#30.3	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment BLEU BLEU  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#39.7	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment PORT PORT  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#37.0	WMT 2014 EN-DE, BLEU	1 - TER  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#69.8	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#56.3	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#25.3	WMT 2014 EN-DE, BLEU	BLEU  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#42.0	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#54.6	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#20.3	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment BLEU BLEU  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#38.4	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#30.7	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#43.4	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#69.8	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#55.7	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#27.2	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment BLEU BLEU  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#38.2	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#50.0	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment PORT PORT  Table 4 BLEU baseline because  the dev sets differ).
true	P12-1098.pdf#69.8	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#37.0	WMT 2014 EN-DE, BLEU	1 - TER  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#55.7	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#50.0	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment PORT PORT  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#54.6	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#43.4	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#54.6	WMT 2014 EN-DE, BLEU	MTR  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#25.3	WMT 2014 EN-DE, BLEU	BLEU  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#20.3	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment BLEU BLEU  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#30.7	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#30.3	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment BLEU BLEU  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#54.6	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#20.3	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment BLEU BLEU  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#38.4	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#69.8	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#42.0	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#48.2	WMT 2014 EN-DE, BLEU	PORT  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#25.3	WMT 2014 EN-DE, BLEU	BLEU  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#27.2	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment BLEU BLEU  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#56.3	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#38.2	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#59.0	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment MTR MTR  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#39.7	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment PORT PORT  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#39.7	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment PORT PORT  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#38.4	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment 1 - TER 1 - TER  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#53.2	WMT 2014 EN-DE, BLEU	Table 7 : PORT tuning - human & GIZA++ alignment PORT PORT  Table 7: PORT tuning -human & GIZA++ alignment
true	P12-1098.pdf#0.918	WMT 2014 EN-DE, BLEU	ordering measures 1 - TER v  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#54.4	WMT 2014 EN-DE, BLEU	METEOR  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#0.981	WMT 2014 EN-DE, BLEU	ordering measures METEOR ρ  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#0.853	WMT 2014 EN-DE, BLEU	ordering measures 1 - TER v  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#0.929	WMT 2014 EN-DE, BLEU	ordering measures METEOR τ  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#25.3	WMT 2014 EN-DE, BLEU	BLEU  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#0.917	WMT 2014 EN-DE, BLEU	ordering measures 1 - TER v  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#37.0	WMT 2014 EN-DE, BLEU	1 - TER  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#0.975	WMT 2014 EN-DE, BLEU	ordering measures METEOR ρ  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#0.866	WMT 2014 EN-DE, BLEU	ordering measures METEOR τ  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#0.928	WMT 2014 EN-DE, BLEU	ordering measures METEOR τ  Table 9: Comparison of the ordering measure: replacing  ν with ρ or τ in PORT.
true	P12-1098.pdf#25.3	WMT 2014 EN-DE, BLEU	BLEU  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	P12-1098.pdf#54.4	WMT 2014 EN-DE, BLEU	METEOR  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	P12-1098.pdf#37.0	WMT 2014 EN-DE, BLEU	1 - TER  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	P12-1098.pdf#0.866	WMT 2014 EN-DE, BLEU	ordering measures METEOR τ  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	P12-1098.pdf#0.981	WMT 2014 EN-DE, BLEU	ordering measures METEOR ρ  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	P12-1098.pdf#0.853	WMT 2014 EN-DE, BLEU	ordering measures 1 - TER v  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	P12-1098.pdf#0.929	WMT 2014 EN-DE, BLEU	ordering measures METEOR τ  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	P12-1098.pdf#0.918	WMT 2014 EN-DE, BLEU	ordering measures 1 - TER v  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	P12-1098.pdf#0.917	WMT 2014 EN-DE, BLEU	ordering measures 1 - TER v  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	P12-1098.pdf#0.975	WMT 2014 EN-DE, BLEU	ordering measures METEOR ρ  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	P12-1098.pdf#0.928	WMT 2014 EN-DE, BLEU	ordering measures METEOR τ  Table 10: Ordering scores (ρ, τ and v) for test sets NIST  2006, 2008 and CTB.
true	D15-1206.pdf#83.7	SemEval-2010 Task 8, F1	NomLex - Plus , Google n - gram , paraphrases , TextRunner F 1  Table 1: Comparison of relation classification systems. The " †" remark refers to special treatment for  the Other class.
true	D15-1206.pdf#84.1†	SemEval-2010 Task 8, F1	NomLex - Plus , Google n - gram , paraphrases , TextRunner F 1  Table 1: Comparison of relation classification systems. The " †" remark refers to special treatment for  the Other class.
true	N13-1089.pdf#0.7	AG News, Error	67 smt - news 91 γ=0 γ=1 γ=2 WTMF γ=1 WTMF+PK , γ=2 , δ=0 . 3 90 68  Table 1: Evaluation Results using Pearson Correlation on STS12 and LI06
true	N13-1089.pdf#90.17%	AG News, Error	LI06  Table 1: Evaluation Results using Pearson Correlation on STS12 and LI06
true	N13-1089.pdf#71.03%	AG News, Error	On - WN  Table 1: Evaluation Results using Pearson Correlation on STS12 and LI06
true	N13-1089.pdf#0.1	AG News, Error	67 smt - eur 91 γ=0 γ=1 γ=2 WTMF γ=1 WTMF+PK , γ=2 , δ=0 . 3 90 68  Table 1: Evaluation Results using Pearson Correlation on STS12 and LI06
true	N13-1089.pdf#75.29%	AG News, Error	STS12 tune  Table 1: Evaluation Results using Pearson Correlation on STS12 and LI06
true	N13-1089.pdf#0.3	AG News, Error	67 smt - news 91 γ=0 γ=1 γ=2 WTMF γ=1 WTMF+PK , γ=2 , δ=0 . 3 90 68  Table 1: Evaluation Results using Pearson Correlation on STS12 and LI06
true	N13-1089.pdf#7.2	AG News, Error	Figure 3 : Pearson correlation at different parameter settings Parameters 71 75 70 70 69 73 68 γ=1 γ=2 δ ( a ) STS12 tuning set ( K = 100 )  Table 1: Evaluation Results using Pearson Correlation on STS12 and LI06
true	N13-1089.pdf#46.77%	AG News, Error	msr - par  Table 1: Evaluation Results using Pearson Correlation on STS12 and LI06
true	N13-1089.pdf#41.66%	AG News, Error	smt - news  Table 1: Evaluation Results using Pearson Correlation on STS12 and LI06
true	N13-1089.pdf#70.70%	AG News, Error	STS12 test  Table 1: Evaluation Results using Pearson Correlation on STS12 and LI06
true	N13-1089.pdf#89.5	AG News, Error	67 On - WN 91 γ=0 γ=1 γ=2 WTMF γ=1 WTMF+PK , γ=2 , δ=0 . 3 90 68  Table 1: Evaluation Results using Pearson Correlation on STS12 and LI06
true	N13-1089.pdf#83.90%	AG News, Error	msr - vid  Table 1: Evaluation Results using Pearson Correlation on STS12 and LI06
true	N13-1089.pdf#90.5	AG News, Error	On - WN 91 γ=0 γ=1 γ=2  Table 1: Evaluation Results using Pearson Correlation on STS12 and LI06
true	N13-1089.pdf#0.5	AG News, Error	67 smt - news 91 γ=0 γ=1 γ=2 WTMF γ=1 WTMF+PK , γ=2 , δ=0 . 3 90 68  Table 1: Evaluation Results using Pearson Correlation on STS12 and LI06
true	N13-1089.pdf#50.89%	AG News, Error	smt - eur  Table 1: Evaluation Results using Pearson Correlation on STS12 and LI06
true	P11-2095.pdf#0.913	Text8, Bit per Character (BPC)	BF  Table 1: Results for the BR87 corpus.
true	P11-2095.pdf#0.949	Text8, Bit per Character (BPC)	BP  Table 1: Results for the BR87 corpus.
true	P11-2095.pdf#0.793	Text8, Bit per Character (BPC)	WP  Table 1: Results for the BR87 corpus.
true	P11-2095.pdf#0.897	Text8, Bit per Character (BPC)	BR  Table 1: Results for the BR87 corpus.
true	P11-2095.pdf#0.734	Text8, Bit per Character (BPC)	WR  Table 1: Results for the BR87 corpus.
true	P11-2095.pdf#0.762	Text8, Bit per Character (BPC)	WF  Table 1: Results for the BR87 corpus.
true	P11-2095.pdf#0.841	Text8, Bit per Character (BPC)	BP  Table 2: Results for the first 50,000 characters of 1984.
true	P11-2095.pdf#0.834	Text8, Bit per Character (BPC)	BF  Table 2: Results for the first 50,000 characters of 1984.
true	P11-2095.pdf#0.833	Text8, Bit per Character (BPC)	BR  Table 2: Results for the first 50,000 characters of 1984.
true	P11-2095.pdf#0.581	Text8, Bit per Character (BPC)	WF  Table 2: Results for the first 50,000 characters of 1984.
true	P11-2095.pdf#0.577	Text8, Bit per Character (BPC)	WR  Table 2: Results for the first 50,000 characters of 1984.
true	P11-2095.pdf#0.585	Text8, Bit per Character (BPC)	WP  Table 2: Results for the first 50,000 characters of 1984.
true	P11-2095.pdf#0.872	Text8, Bit per Character (BPC)	BF  Table 3: Results for a corpus of orthographic Chinese.
true	P11-2095.pdf#0.914	Text8, Bit per Character (BPC)	BR  Table 3: Results for a corpus of orthographic Chinese.
true	P11-2095.pdf#0.684	Text8, Bit per Character (BPC)	WF  Table 3: Results for a corpus of orthographic Chinese.
true	P11-2095.pdf#0.717	Text8, Bit per Character (BPC)	WR  Table 3: Results for a corpus of orthographic Chinese.
true	P11-2095.pdf#0.657	Text8, Bit per Character (BPC)	WP  Table 3: Results for a corpus of orthographic Chinese.
true	P11-2095.pdf#0.871	Text8, Bit per Character (BPC)	BP  Table 3: Results for a corpus of orthographic Chinese.
true	P11-2095.pdf#0.800	Text8, Bit per Character (BPC)	WF  Table 4: Results for a corpus of orthographic Thai.
true	P11-2095.pdf#0.916	Text8, Bit per Character (BPC)	BP  Table 4: Results for a corpus of orthographic Thai.
true	P11-2095.pdf#0.927	Text8, Bit per Character (BPC)	BF  Table 4: Results for a corpus of orthographic Thai.
true	P11-2095.pdf#0.767	Text8, Bit per Character (BPC)	WP  Table 4: Results for a corpus of orthographic Thai.
true	P11-2095.pdf#0.969	Text8, Bit per Character (BPC)	BR  Table 4: Results for a corpus of orthographic Thai.
true	P11-2095.pdf#0.836	Text8, Bit per Character (BPC)	WR  Table 4: Results for a corpus of orthographic Thai.
true	P11-2095.pdf#0.855	Text8, Bit per Character (BPC)	BR  Table 5: Results for a subset of the Switchboard corpus.
true	P11-2095.pdf#0.853	Text8, Bit per Character (BPC)	BF  Table 5: Results for a subset of the Switchboard corpus.
true	P11-2095.pdf#0.592	Text8, Bit per Character (BPC)	WR  Table 5: Results for a subset of the Switchboard corpus.
true	P11-2095.pdf#0.617	Text8, Bit per Character (BPC)	WF  Table 5: Results for a subset of the Switchboard corpus.
true	P11-2095.pdf#0.890	Text8, Bit per Character (BPC)	BP  Table 5: Results for a subset of the Switchboard corpus.
true	P11-2095.pdf#0.644	Text8, Bit per Character (BPC)	WP  Table 5: Results for a subset of the Switchboard corpus.
true	P11-2095.pdf#0.913	Text8, Number of params	BF  Table 1: Results for the BR87 corpus.
true	P11-2095.pdf#0.949	Text8, Number of params	BP  Table 1: Results for the BR87 corpus.
true	P11-2095.pdf#0.793	Text8, Number of params	WP  Table 1: Results for the BR87 corpus.
true	P11-2095.pdf#0.897	Text8, Number of params	BR  Table 1: Results for the BR87 corpus.
true	P11-2095.pdf#0.734	Text8, Number of params	WR  Table 1: Results for the BR87 corpus.
true	P11-2095.pdf#0.762	Text8, Number of params	WF  Table 1: Results for the BR87 corpus.
true	P11-2095.pdf#0.841	Text8, Number of params	BP  Table 2: Results for the first 50,000 characters of 1984.
true	P11-2095.pdf#0.834	Text8, Number of params	BF  Table 2: Results for the first 50,000 characters of 1984.
true	P11-2095.pdf#0.833	Text8, Number of params	BR  Table 2: Results for the first 50,000 characters of 1984.
true	P11-2095.pdf#0.581	Text8, Number of params	WF  Table 2: Results for the first 50,000 characters of 1984.
true	P11-2095.pdf#0.577	Text8, Number of params	WR  Table 2: Results for the first 50,000 characters of 1984.
true	P11-2095.pdf#0.585	Text8, Number of params	WP  Table 2: Results for the first 50,000 characters of 1984.
true	P11-2095.pdf#0.872	Text8, Number of params	BF  Table 3: Results for a corpus of orthographic Chinese.
true	P11-2095.pdf#0.914	Text8, Number of params	BR  Table 3: Results for a corpus of orthographic Chinese.
true	P11-2095.pdf#0.684	Text8, Number of params	WF  Table 3: Results for a corpus of orthographic Chinese.
true	P11-2095.pdf#0.717	Text8, Number of params	WR  Table 3: Results for a corpus of orthographic Chinese.
true	P11-2095.pdf#0.657	Text8, Number of params	WP  Table 3: Results for a corpus of orthographic Chinese.
true	P11-2095.pdf#0.871	Text8, Number of params	BP  Table 3: Results for a corpus of orthographic Chinese.
true	P11-2095.pdf#0.800	Text8, Number of params	WF  Table 4: Results for a corpus of orthographic Thai.
true	P11-2095.pdf#0.916	Text8, Number of params	BP  Table 4: Results for a corpus of orthographic Thai.
true	P11-2095.pdf#0.927	Text8, Number of params	BF  Table 4: Results for a corpus of orthographic Thai.
true	P11-2095.pdf#0.767	Text8, Number of params	WP  Table 4: Results for a corpus of orthographic Thai.
true	P11-2095.pdf#0.969	Text8, Number of params	BR  Table 4: Results for a corpus of orthographic Thai.
true	P11-2095.pdf#0.836	Text8, Number of params	WR  Table 4: Results for a corpus of orthographic Thai.
true	P11-2095.pdf#0.855	Text8, Number of params	BR  Table 5: Results for a subset of the Switchboard corpus.
true	P11-2095.pdf#0.853	Text8, Number of params	BF  Table 5: Results for a subset of the Switchboard corpus.
true	P11-2095.pdf#0.592	Text8, Number of params	WR  Table 5: Results for a subset of the Switchboard corpus.
true	P11-2095.pdf#0.617	Text8, Number of params	WF  Table 5: Results for a subset of the Switchboard corpus.
true	P11-2095.pdf#0.890	Text8, Number of params	BP  Table 5: Results for a subset of the Switchboard corpus.
true	P11-2095.pdf#0.644	Text8, Number of params	WP  Table 5: Results for a subset of the Switchboard corpus.
true	P11-2095.pdf#0.913	Chinese Treebank 6, F1	BF  Table 1: Results for the BR87 corpus.
true	P11-2095.pdf#0.949	Chinese Treebank 6, F1	BP  Table 1: Results for the BR87 corpus.
true	P11-2095.pdf#0.793	Chinese Treebank 6, F1	WP  Table 1: Results for the BR87 corpus.
true	P11-2095.pdf#0.897	Chinese Treebank 6, F1	BR  Table 1: Results for the BR87 corpus.
true	P11-2095.pdf#0.734	Chinese Treebank 6, F1	WR  Table 1: Results for the BR87 corpus.
true	P11-2095.pdf#0.762	Chinese Treebank 6, F1	WF  Table 1: Results for the BR87 corpus.
true	P11-2095.pdf#0.841	Chinese Treebank 6, F1	BP  Table 2: Results for the first 50,000 characters of 1984.
true	P11-2095.pdf#0.834	Chinese Treebank 6, F1	BF  Table 2: Results for the first 50,000 characters of 1984.
true	P11-2095.pdf#0.833	Chinese Treebank 6, F1	BR  Table 2: Results for the first 50,000 characters of 1984.
true	P11-2095.pdf#0.581	Chinese Treebank 6, F1	WF  Table 2: Results for the first 50,000 characters of 1984.
true	P11-2095.pdf#0.577	Chinese Treebank 6, F1	WR  Table 2: Results for the first 50,000 characters of 1984.
true	P11-2095.pdf#0.585	Chinese Treebank 6, F1	WP  Table 2: Results for the first 50,000 characters of 1984.
true	P11-2095.pdf#0.872	Chinese Treebank 6, F1	BF  Table 3: Results for a corpus of orthographic Chinese.
true	P11-2095.pdf#0.914	Chinese Treebank 6, F1	BR  Table 3: Results for a corpus of orthographic Chinese.
true	P11-2095.pdf#0.684	Chinese Treebank 6, F1	WF  Table 3: Results for a corpus of orthographic Chinese.
true	P11-2095.pdf#0.717	Chinese Treebank 6, F1	WR  Table 3: Results for a corpus of orthographic Chinese.
true	P11-2095.pdf#0.657	Chinese Treebank 6, F1	WP  Table 3: Results for a corpus of orthographic Chinese.
true	P11-2095.pdf#0.871	Chinese Treebank 6, F1	BP  Table 3: Results for a corpus of orthographic Chinese.
true	P11-2095.pdf#0.800	Chinese Treebank 6, F1	WF  Table 4: Results for a corpus of orthographic Thai.
true	P11-2095.pdf#0.916	Chinese Treebank 6, F1	BP  Table 4: Results for a corpus of orthographic Thai.
true	P11-2095.pdf#0.927	Chinese Treebank 6, F1	BF  Table 4: Results for a corpus of orthographic Thai.
true	P11-2095.pdf#0.767	Chinese Treebank 6, F1	WP  Table 4: Results for a corpus of orthographic Thai.
true	P11-2095.pdf#0.969	Chinese Treebank 6, F1	BR  Table 4: Results for a corpus of orthographic Thai.
true	P11-2095.pdf#0.836	Chinese Treebank 6, F1	WR  Table 4: Results for a corpus of orthographic Thai.
true	P11-2095.pdf#0.855	Chinese Treebank 6, F1	BR  Table 5: Results for a subset of the Switchboard corpus.
true	P11-2095.pdf#0.853	Chinese Treebank 6, F1	BF  Table 5: Results for a subset of the Switchboard corpus.
true	P11-2095.pdf#0.592	Chinese Treebank 6, F1	WR  Table 5: Results for a subset of the Switchboard corpus.
true	P11-2095.pdf#0.617	Chinese Treebank 6, F1	WF  Table 5: Results for a subset of the Switchboard corpus.
true	P11-2095.pdf#0.890	Chinese Treebank 6, F1	BP  Table 5: Results for a subset of the Switchboard corpus.
true	P11-2095.pdf#0.644	Chinese Treebank 6, F1	WP  Table 5: Results for a subset of the Switchboard corpus.
true	P15-1041.pdf#0.8518	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Domain DVD  Table 3: Macro performance of all approaches  in three domains. All values are accuracies and  Avg-Ac represents the average accuracy in three  domains.
true	P15-1041.pdf#0.8359	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Avg Ac  Table 3: Macro performance of all approaches  in three domains. All values are accuracies and  Avg-Ac represents the average accuracy in three  domains.
true	P15-1041.pdf#0.8093	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Music  Table 3: Macro performance of all approaches  in three domains. All values are accuracies and  Avg-Ac represents the average accuracy in three  domains.
true	P15-1041.pdf#0.8465	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Books  Table 3: Macro performance of all approaches  in three domains. All values are accuracies and  Avg-Ac represents the average accuracy in three  domains.
true	P15-1041.pdf#0.8853	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P P  Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.
true	P15-1041.pdf#0.8093	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Ac Ac Ac  Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.
true	P15-1041.pdf#0.8980	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Negative R Negative R  Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.
true	P15-1041.pdf#0.8536	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F1 F1  Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.
true	P15-1041.pdf#0.8572	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Positive R Positive R  Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.
true	P15-1041.pdf#0.8652	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P P  Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.
true	P15-1041.pdf#0.8881	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Positive R  Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.
true	P15-1041.pdf#0.8501	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F1  Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.
true	P15-1041.pdf#0.8465	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Ac  Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.
true	P15-1041.pdf#0.8708	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Positive R Positive R Positive R  Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.
true	P15-1041.pdf#0.7917	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F1 F1 F1  Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.
true	P15-1041.pdf#0.8787	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P P P  Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.
true	P15-1041.pdf#0.8825	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Negative R  Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.
true	P15-1041.pdf#0.8427	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F1  Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.
true	P15-1041.pdf#0.9035	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Negative R Negative R Negative R  Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.
true	P15-1041.pdf#0.8537	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.
true	P15-1041.pdf#0.8530	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F1 F1  Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.
true	P15-1041.pdf#0.9105	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.
true	P15-1041.pdf#0.8518	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Ac Ac  Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.
true	P15-1041.pdf#0.8928	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P P P  Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.
true	P15-1041.pdf#0.8241	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F1 F1 F1  Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F  measure, Ac: Accuracy, and -represents unknown. The model in BR2013 is unknown, thus its micro  performance is unavailable.
true	P15-1141.pdf#19.30	AG News, Error	IR SJMN - 2k  Table 1: Comparison of all models in the classi- fication task (accuracy in %) and the information  retrieval task (MAP scores in %) on small corpora.  Bold face indicates scores not significantly differ- ent from the best score (in italics) according to a  Wilcoxon signed rank test (p < 0.05).
true	P15-1141.pdf#19.37	AG News, Error	IR SJMN - 2k  Table 1: Comparison of all models in the classi- fication task (accuracy in %) and the information  retrieval task (MAP scores in %) on small corpora.  Bold face indicates scores not significantly differ- ent from the best score (in italics) according to a  Wilcoxon signed rank test (p < 0.05).
true	P15-1141.pdf#19.31	AG News, Error	IR SJMN - 2k  Table 1: Comparison of all models in the classi- fication task (accuracy in %) and the information  retrieval task (MAP scores in %) on small corpora.  Bold face indicates scores not significantly differ- ent from the best score (in italics) according to a  Wilcoxon signed rank test (p < 0.05).
true	P15-1141.pdf#73.15	AG News, Error	Classification MReview  Table 1: Comparison of all models in the classi- fication task (accuracy in %) and the information  retrieval task (MAP scores in %) on small corpora.  Bold face indicates scores not significantly differ- ent from the best score (in italics) according to a  Wilcoxon signed rank test (p < 0.05).
true	P15-1141.pdf#73.13	AG News, Error	Classification MReview  Table 1: Comparison of all models in the classi- fication task (accuracy in %) and the information  retrieval task (MAP scores in %) on small corpora.  Bold face indicates scores not significantly differ- ent from the best score (in italics) according to a  Wilcoxon signed rank test (p < 0.05).
true	P15-1141.pdf#73.14	AG News, Error	Classification MReview  Table 1: Comparison of all models in the classi- fication task (accuracy in %) and the information  retrieval task (MAP scores in %) on small corpora.  Bold face indicates scores not significantly differ- ent from the best score (in italics) according to a  Wilcoxon signed rank test (p < 0.05).
true	P15-1141.pdf#93.4	AG News, Error	TCM  Table 2: Classification accuracy (%) on larger  datasets. Bold face indicates scores not signifi- cantly different from the best score (in italics) ac- cording to a Wilcoxon signed rank test (p < 0.05).
true	P15-1141.pdf#87.9	AG News, Error	TCM  Table 2: Classification accuracy (%) on larger  datasets. Bold face indicates scores not signifi- cantly different from the best score (in italics) ac- cording to a Wilcoxon signed rank test (p < 0.05).
true	P15-1141.pdf#97.5	AG News, Error	PA  Table 2: Classification accuracy (%) on larger  datasets. Bold face indicates scores not signifi- cantly different from the best score (in italics) ac- cording to a Wilcoxon signed rank test (p < 0.05).
true	P15-1141.pdf#89.1	AG News, Error	Table 2: Classification accuracy (%) on larger  datasets. Bold face indicates scores not signifi- cantly different from the best score (in italics) ac- cording to a Wilcoxon signed rank test (p < 0.05).
true	P15-1141.pdf#89.2	AG News, Error	Table 2: Classification accuracy (%) on larger  datasets. Bold face indicates scores not signifi- cantly different from the best score (in italics) ac- cording to a Wilcoxon signed rank test (p < 0.05).
true	P15-1141.pdf#97.6	AG News, Error	TCM  Table 2: Classification accuracy (%) on larger  datasets. Bold face indicates scores not signifi- cantly different from the best score (in italics) ac- cording to a Wilcoxon signed rank test (p < 0.05).
true	P15-1141.pdf#89.2	AG News, Error	Table 2: Classification accuracy (%) on larger  datasets. Bold face indicates scores not signifi- cantly different from the best score (in italics) ac- cording to a Wilcoxon signed rank test (p < 0.05).
true	P15-1141.pdf#92.6	AG News, Error	TCM  Table 2: Classification accuracy (%) on larger  datasets. Bold face indicates scores not signifi- cantly different from the best score (in italics) ac- cording to a Wilcoxon signed rank test (p < 0.05).
true	P15-1141.pdf#21.2	AG News, Error	Table 3: Mean average Precision (MAP in %)  scores in the information retrieval task. Scores in  bold and italics are the significantly best MAP  scores according to a Wilcoxon signed rank test  (p < 0.05).
true	P15-1141.pdf#24.8	AG News, Error	TCM  Table 3: Mean average Precision (MAP in %)  scores in the information retrieval task. Scores in  bold and italics are the significantly best MAP  scores according to a Wilcoxon signed rank test  (p < 0.05).
true	P15-1141.pdf#79.7	AG News, Error	p ( t|w )  Table 4: The model precision (%) derived from the  intrusion detection experiments.
true	P15-1141.pdf#73.2	AG News, Error	p ( w|t )  Table 4: The model precision (%) derived from the  intrusion detection experiments.
true	P12-3016.pdf#0.5	Text8, Number of params	P  Table 3. Term mapping performance for English- Romanian.
true	P12-3016.pdf#0.357	Text8, Number of params	R  Table 3. Term mapping performance for English- Romanian.
true	P12-3016.pdf#0.904	Text8, Number of params	P  Table 3. Term mapping performance for English- Romanian.
true	P12-3016.pdf#0.511	Text8, Number of params	F - measure  Table 3. Term mapping performance for English- Romanian.
true	P12-3016.pdf#0.5	Text8, Number of params	Threshold  Table 4. Term mapping performance for English- Latvian.
true	P12-3016.pdf#0.306	Text8, Number of params	R  Table 4. Term mapping performance for English- Latvian.
true	P12-3016.pdf#0.947	Text8, Number of params	P  Table 4. Term mapping performance for English- Latvian.
true	P12-3016.pdf#0.463	Text8, Number of params	F - measure  Table 4. Term mapping performance for English- Latvian.
true	D12-1018.pdf#40.2	New York Times Corpus, P@10%	P%  Table 2: Top 10 positive and negative features according to the Pearson correlation score.
true	D12-1018.pdf#0.65	New York Times Corpus, P@10%	AUC  Table 2: Top 10 positive and negative features according to the Pearson correlation score.
true	D12-1018.pdf#71.0	New York Times Corpus, P@10%	R%  Table 2: Top 10 positive and negative features according to the Pearson correlation score.
true	D12-1018.pdf#0.51	New York Times Corpus, P@10%	F 1  Table 2: Top 10 positive and negative features according to the Pearson correlation score.
true	D12-1018.pdf#71.0	New York Times Corpus, P@10%	R%  Table 3: Average precision, recall, AUC and F 1 for our  method and the baselines.
true	D12-1018.pdf#0.65	New York Times Corpus, P@10%	AUC  Table 3: Average precision, recall, AUC and F 1 for our  method and the baselines.
true	D12-1018.pdf#0.51	New York Times Corpus, P@10%	F 1  Table 3: Average precision, recall, AUC and F 1 for our  method and the baselines.
true	D12-1018.pdf#40.2	New York Times Corpus, P@10%	P%  Table 3: Average precision, recall, AUC and F 1 for our  method and the baselines.
true	D14-1093.pdf#90.2	Chinese Treebank 6, F1	560 31 , 499 Unlabeled Wikipedia 32 , 023 34 , 355  Table 3: Statistics of data used in this paper.
true	D14-1093.pdf#90.1	Chinese Treebank 6, F1	560 31 , 499 Unlabeled Wikipedia 32 , 023 34 , 355  Table 3: Statistics of data used in this paper.
true	D14-1093.pdf#90.4	Chinese Treebank 6, F1	560 31 , 499 Unlabeled Wikipedia ZX 32 , 023 34 , 355  Table 3: Statistics of data used in this paper.
true	D14-1093.pdf#90.3	Chinese Treebank 6, F1	560 31 , 499 Unlabeled Wikipedia 32 , 023 34 , 355  Table 3: Statistics of data used in this paper.
true	D14-1093.pdf#90.63	Chinese Treebank 6, F1	ZX F  Table 4: Final result for adapting CTB to Zhuxian and adapting PD to the medicine and computer  domains, using partially annotated data (referred to as PA) obtained from unlabeled data and lexicons.
true	D14-1093.pdf#88.28	Chinese Treebank 6, F1	Zhang et al . Computer Roov Roov  Table 4: Final result for adapting CTB to Zhuxian and adapting PD to the medicine and computer  domains, using partially annotated data (referred to as PA) obtained from unlabeled data and lexicons.
true	D14-1093.pdf#95.00	Chinese Treebank 6, F1	Zhang et al . Computer Roov F  Table 4: Final result for adapting CTB to Zhuxian and adapting PD to the medicine and computer  domains, using partially annotated data (referred to as PA) obtained from unlabeled data and lexicons.
true	D14-1093.pdf#93.47	Chinese Treebank 6, F1	Computer F  Table 4: Final result for adapting CTB to Zhuxian and adapting PD to the medicine and computer  domains, using partially annotated data (referred to as PA) obtained from unlabeled data and lexicons.
true	D14-1093.pdf#91.68	Chinese Treebank 6, F1	Medicine F  Table 4: Final result for adapting CTB to Zhuxian and adapting PD to the medicine and computer  domains, using partially annotated data (referred to as PA) obtained from unlabeled data and lexicons.
true	D14-1093.pdf#74.99	Chinese Treebank 6, F1	Medicine Roov  Table 4: Final result for adapting CTB to Zhuxian and adapting PD to the medicine and computer  domains, using partially annotated data (referred to as PA) obtained from unlabeled data and lexicons.
true	D14-1093.pdf#84.88	Chinese Treebank 6, F1	ZX Roov  Table 4: Final result for adapting CTB to Zhuxian and adapting PD to the medicine and computer  domains, using partially annotated data (referred to as PA) obtained from unlabeled data and lexicons.
true	D14-1093.pdf#85.63	Chinese Treebank 6, F1	Computer Roov  Table 4: Final result for adapting CTB to Zhuxian and adapting PD to the medicine and computer  domains, using partially annotated data (referred to as PA) obtained from unlabeled data and lexicons.
true	D14-1093.pdf#76.84	Chinese Treebank 6, F1	Roov  Table 6: Experimental results on the SIGHAN Bakeoff 2010 data.
true	D14-1093.pdf#93.61	Chinese Treebank 6, F1	Avg - F  Table 6: Experimental results on the SIGHAN Bakeoff 2010 data.
true	D14-1093.pdf#93.34	Chinese Treebank 6, F1	dom 160K ) F  Table 6: Experimental results on the SIGHAN Bakeoff 2010 data.
true	D14-1093.pdf#88.53	Chinese Treebank 6, F1	Roov  Table 6: Experimental results on the SIGHAN Bakeoff 2010 data.
true	D14-1093.pdf#95.54	Chinese Treebank 6, F1	F  Table 6: Experimental results on the SIGHAN Bakeoff 2010 data.
true	D14-1093.pdf#78.28	Chinese Treebank 6, F1	Roov  Table 6: Experimental results on the SIGHAN Bakeoff 2010 data.
true	D14-1093.pdf#93.93	Chinese Treebank 6, F1	F  Table 6: Experimental results on the SIGHAN Bakeoff 2010 data.
true	D14-1093.pdf#87.53	Chinese Treebank 6, F1	Roov  Table 6: Experimental results on the SIGHAN Bakeoff 2010 data.
true	D14-1093.pdf#93.53	Chinese Treebank 6, F1	dom 160K ) F  Table 6: Experimental results on the SIGHAN Bakeoff 2010 data.
true	D15-1004.pdf#36.6	WMT 2014 EN-FR, BLEU	MT04  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#57.0	WMT 2014 EN-FR, BLEU	MT05  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#62.6	WMT 2014 EN-FR, BLEU	WMT12  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#32.1	WMT 2014 EN-FR, BLEU	MT04  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#32.5	WMT 2014 EN-FR, BLEU	MT05  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#31.8	WMT 2014 EN-FR, BLEU	MT04  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#29.8	WMT 2014 EN-FR, BLEU	WMT13  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#28.7	WMT 2014 EN-FR, BLEU	WMT12  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#23.3	WMT 2014 EN-FR, BLEU	WMT13  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#20.7	WMT 2014 EN-FR, BLEU	WMT12  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#35.3	WMT 2014 EN-FR, BLEU	MT05  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#59.0	WMT 2014 EN-FR, BLEU	WMT13  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#56.1	WMT 2014 EN-FR, BLEU	MT04  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#36.6	WMT 2014 EN-DE, BLEU	MT04  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#57.0	WMT 2014 EN-DE, BLEU	MT05  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#62.6	WMT 2014 EN-DE, BLEU	WMT12  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#32.1	WMT 2014 EN-DE, BLEU	MT04  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#32.5	WMT 2014 EN-DE, BLEU	MT05  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#31.8	WMT 2014 EN-DE, BLEU	MT04  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#29.8	WMT 2014 EN-DE, BLEU	WMT13  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#28.7	WMT 2014 EN-DE, BLEU	WMT12  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#23.3	WMT 2014 EN-DE, BLEU	WMT13  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#20.7	WMT 2014 EN-DE, BLEU	WMT12  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#35.3	WMT 2014 EN-DE, BLEU	MT05  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#59.0	WMT 2014 EN-DE, BLEU	WMT13  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	D15-1004.pdf#56.1	WMT 2014 EN-DE, BLEU	MT04  Table 2: Metric scores for all systems on Chinese-English (ZH-EN) and German-English (DE-EN) cor- pus. Each score is the average score over three MERT runs. Bold figures mean a system is significantly  better than Moses HPB at p ≤ 0.01. Moses HPB is significantly better than systems with  *  at p ≤ 0.01.
true	P15-1097.pdf#0.852	New York Times Corpus, P@10%	W 2V Vs Test F1  Table 1: Results on Paraphrasing Identification
true	P15-1097.pdf#79.13	New York Times Corpus, P@10%	W 2V Vs Test Acc ( % )  Table 1: Results on Paraphrasing Identification
true	P15-1097.pdf#67.00	New York Times Corpus, P@10%	Vs Test Acc ( % )  Table 2: Results on Textual Entailment Recognition
true	N13-1107v2.pdf#0.592	New York Times Corpus, P@10%	( Binary ) Table 2 : Relation extraction results on Treebank set F - score  Table 3: Relation extraction results on Treebank set  (Triple)
true	N13-1107v2.pdf#0.536	New York Times Corpus, P@10%	( Binary ) Table 2 : Relation extraction results on Treebank set F - score F - score F - score  Table 3: Relation extraction results on Treebank set  (Triple)
true	N13-1107v2.pdf#0.764	New York Times Corpus, P@10%	( Binary ) Table 2 : Relation extraction results on Treebank set F - score F - score  Table 3: Relation extraction results on Treebank set  (Triple)
true	N13-1107v2.pdf#0.53	New York Times Corpus, P@10%	( Binary ) Table 2 : Relation extraction results on Treebank set F - score  Table 3: Relation extraction results on Treebank set  (Triple)
true	N13-1107v2.pdf#0.549	New York Times Corpus, P@10%	( Binary ) Table 2 : Relation extraction results on Treebank set F - score  Table 3: Relation extraction results on Treebank set  (Triple)
true	N13-1107v2.pdf#0.721	New York Times Corpus, P@10%	( Binary ) Table 2 : Relation extraction results on Treebank set F - score F - score  Table 3: Relation extraction results on Treebank set  (Triple)
true	N13-1107v2.pdf#0.781	New York Times Corpus, P@10%	F - score  Table 4: Relation extraction results on REVERB set  (Triple).
true	N13-1107v2.pdf#0.793	New York Times Corpus, P@10%	F - score  Table 5: Relation extraction results on OLLIE set  (Triple).
true	D14-1203.pdf#34.1	New York Times Corpus, P@10%	NEL+Coref training Partitioned P  Table 1: Evaluation of different versions of the relation extraction system on the GORECO test set. For  nearly all systems, partitioning the model by argument types boosts F1, as does using NEL at either  training or extraction time, and using coreference at training time with type constraints (w/TC) raises F1  except with coreference at extraction time and when combined with type partitioning.
true	D14-1203.pdf#12.8	New York Times Corpus, P@10%	NEL+Coref training Single R  Table 1: Evaluation of different versions of the relation extraction system on the GORECO test set. For  nearly all systems, partitioning the model by argument types boosts F1, as does using NEL at either  training or extraction time, and using coreference at training time with type constraints (w/TC) raises F1  except with coreference at extraction time and when combined with type partitioning.
true	D14-1203.pdf#18.8	New York Times Corpus, P@10%	NEL training Partitioned F1  Table 1: Evaluation of different versions of the relation extraction system on the GORECO test set. For  nearly all systems, partitioning the model by argument types boosts F1, as does using NEL at either  training or extraction time, and using coreference at training time with type constraints (w/TC) raises F1  except with coreference at extraction time and when combined with type partitioning.
true	D14-1203.pdf#15.3	New York Times Corpus, P@10%	NEL training Partitioned R  Table 1: Evaluation of different versions of the relation extraction system on the GORECO test set. For  nearly all systems, partitioning the model by argument types boosts F1, as does using NEL at either  training or extraction time, and using coreference at training time with type constraints (w/TC) raises F1  except with coreference at extraction time and when combined with type partitioning.
true	D14-1203.pdf#33.3	New York Times Corpus, P@10%	NEL+Coref training Single P  Table 1: Evaluation of different versions of the relation extraction system on the GORECO test set. For  nearly all systems, partitioning the model by argument types boosts F1, as does using NEL at either  training or extraction time, and using coreference at training time with type constraints (w/TC) raises F1  except with coreference at extraction time and when combined with type partitioning.
true	D14-1203.pdf#18.5	New York Times Corpus, P@10%	NEL+Coref training Single F1  Table 1: Evaluation of different versions of the relation extraction system on the GORECO test set. For  nearly all systems, partitioning the model by argument types boosts F1, as does using NEL at either  training or extraction time, and using coreference at training time with type constraints (w/TC) raises F1  except with coreference at extraction time and when combined with type partitioning.
true	P15-1026.pdf#45.1	New York Times Corpus, P@10%	- P@1  Table 1: Evaluation results on the test split of WE-
true	P15-1026.pdf#40.8	New York Times Corpus, P@10%	- F1  Table 1: Evaluation results on the test split of WE-
true	P15-1026.pdf#40.8	New York Times Corpus, P@10%	F1  Table 2: Evaluation results of different set- tings on the test split of WEBQUESTIONS. w/o  path/type/context: without using the specific col- umn. w/o multi-column: tying parameters of mul- tiple columns. w/o paraphrase: without using  question paraphrases for training. 1-hop: using 1- hop paths to generate candidate answers.
true	P15-1026.pdf#45.1	New York Times Corpus, P@10%	P@1  Table 2: Evaluation results of different set- tings on the test split of WEBQUESTIONS. w/o  path/type/context: without using the specific col- umn. w/o multi-column: tying parameters of mul- tiple columns. w/o paraphrase: without using  question paraphrases for training. 1-hop: using 1- hop paths to generate candidate answers.
true	P13-2129.pdf#69.62	New York Times Corpus, P@10%	Datasets Sun et al .  Table 3: Results when using F3 (DA), F2 (pair of independent  frames) and F1 (single frame) features with Bhattacharyya  kernel on Joanis et al. and Sun et al. datasets
true	P13-2129.pdf#66.32	New York Times Corpus, P@10%	Datasets Joanis et al . 10  Table 3: Results when using F3 (DA), F2 (pair of independent  frames) and F1 (single frame) features with Bhattacharyya  kernel on Joanis et al. and Sun et al. datasets
true	P13-2129.pdf#53.79	New York Times Corpus, P@10%	Datasets Joanis et al . 8  Table 3: Results when using F3 (DA), F2 (pair of independent  frames) and F1 (single frame) features with Bhattacharyya  kernel on Joanis et al. and Sun et al. datasets
true	P13-2129.pdf#52.90	New York Times Corpus, P@10%	Datasets Joanis et al . 9  Table 3: Results when using F3 (DA), F2 (pair of independent  frames) and F1 (single frame) features with Bhattacharyya  kernel on Joanis et al. and Sun et al. datasets
true	P13-2129.pdf#50.97	New York Times Corpus, P@10%	Datasets Sun et al . 11  Table 3: Results when using F3 (DA), F2 (pair of independent  frames) and F1 (single frame) features with Bhattacharyya  kernel on Joanis et al. and Sun et al. datasets
true	P13-2129.pdf#56.36	New York Times Corpus, P@10%	Datasets Joanis et al . 7  Table 3: Results when using F3 (DA), F2 (pair of independent  frames) and F1 (single frame) features with Bhattacharyya  kernel on Joanis et al. and Sun et al. datasets
true	N10-1083.pdf#63.0	Chinese Treebank 6, F1	Eval Many - 1 Dir  Table 1: Locally normalized feature-based models outperform  all proposed baselines for all four tasks. LBFGS outperformed  EM in all cases where the algorithm was sufficiently fast to run.  Details of each experiment appear in the main text.
true	N10-1083.pdf#30.0	Chinese Treebank 6, F1	Eval Many - 1 Dir AER  Table 1: Locally normalized feature-based models outperform  all proposed baselines for all four tasks. LBFGS outperformed  EM in all cases where the algorithm was sufficiently fast to run.  Details of each experiment appear in the main text.
true	N10-1083.pdf#75.5(1.1)	Chinese Treebank 6, F1	Eval Many - 1  Table 1: Locally normalized feature-based models outperform  all proposed baselines for all four tasks. LBFGS outperformed  EM in all cases where the algorithm was sufficiently fast to run.  Details of each experiment appear in the main text.
true	N10-1083.pdf#88.0(0.1)	Chinese Treebank 6, F1	Eval Many - 1 Dir AER F1  Table 1: Locally normalized feature-based models outperform  all proposed baselines for all four tasks. LBFGS outperformed  EM in all cases where the algorithm was sufficiently fast to run.  Details of each experiment appear in the main text.
true	N10-1083.pdf#35.6	Chinese Treebank 6, F1	Eval Many - 1 Dir AER  Table 1: Locally normalized feature-based models outperform  all proposed baselines for all four tasks. LBFGS outperformed  EM in all cases where the algorithm was sufficiently fast to run.  Details of each experiment appear in the main text.
true	N10-1083.pdf#53.6	Chinese Treebank 6, F1	Eval Many - 1 Dir  Table 1: Locally normalized feature-based models outperform  all proposed baselines for all four tasks. LBFGS outperformed  EM in all cases where the algorithm was sufficiently fast to run.  Details of each experiment appear in the main text.
true	N12-1030.pdf#4.1	CCGBank, Accuracy	
true	N12-1030.pdf#4.1	CCGBank, Accuracy	
true	N12-1030.pdf#71.67	CCGBank, Accuracy	Table 2 : Shapes of categories LF  Table 2: Shapes of categories
true	N12-1030.pdf#71.40	CCGBank, Accuracy	- Table 2 : Shapes of categories LF  Table 2: Shapes of categories
true	N12-1030.pdf#72.74	CCGBank, Accuracy	- Table 2 : Shapes of categories LF  Table 2: Shapes of categories
true	N12-1030.pdf#71.67	CCGBank, Accuracy	Table 2 : Shapes of categories LF  Table 3: Dev set evaluation for P&K and C&C
true	N12-1030.pdf#72.74	CCGBank, Accuracy	- Table 2 : Shapes of categories LF  Table 3: Dev set evaluation for P&K and C&C
true	N12-1030.pdf#71.40	CCGBank, Accuracy	- Table 2 : Shapes of categories LF  Table 3: Dev set evaluation for P&K and C&C
true	N12-1030.pdf#71.40	CCGBank, Accuracy	- Table 2 : Shapes of categories LF  Table 3 shows the performance of P&K and C&C on  the three dev sets, and Table 4 only over sentences  parsed by both parsers. (A is the base release, B  includes the unary rule LCP → NP, and C also  collapses the N-NP distinction.) For P&K on corpus  A, F -score and supertagger accuracy increase mono- tonically as further split-merge iterations refine the  model. P&K on B and C overfits at 6 iterations, con- sistent with Fowler and Penn's findings for English.
true	N12-1030.pdf#72.74	CCGBank, Accuracy	- Table 2 : Shapes of categories LF  Table 3 shows the performance of P&K and C&C on  the three dev sets, and Table 4 only over sentences  parsed by both parsers. (A is the base release, B  includes the unary rule LCP → NP, and C also  collapses the N-NP distinction.) For P&K on corpus  A, F -score and supertagger accuracy increase mono- tonically as further split-merge iterations refine the  model. P&K on B and C overfits at 6 iterations, con- sistent with Fowler and Penn's findings for English.
true	N12-1030.pdf#71.67	CCGBank, Accuracy	Table 2 : Shapes of categories LF  Table 3 shows the performance of P&K and C&C on  the three dev sets, and Table 4 only over sentences  parsed by both parsers. (A is the base release, B  includes the unary rule LCP → NP, and C also  collapses the N-NP distinction.) For P&K on corpus  A, F -score and supertagger accuracy increase mono- tonically as further split-merge iterations refine the  model. P&K on B and C overfits at 6 iterations, con- sistent with Fowler and Penn's findings for English.
true	N12-1030.pdf#-0.01	CCGBank, Accuracy	∆LF  Table 8: Corrupting C&C gold  tags piecemeal on   6 dev set of corpus C. ∆LF is the change in
true	N12-1030.pdf#-0.12	CCGBank, Accuracy	∆LF  Table 8: Corrupting C&C gold  tags piecemeal on   6 dev set of corpus C. ∆LF is the change in
true	N12-1030.pdf#-1.50	CCGBank, Accuracy	∆LF  Table 8: Corrupting C&C gold  tags piecemeal on   6 dev set of corpus C. ∆LF is the change in
true	N12-1030.pdf#-1.75	CCGBank, Accuracy	∆LF  Table 8: Corrupting C&C gold  tags piecemeal on   6 dev set of corpus C. ∆LF is the change in
true	N12-1030.pdf#89.36	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches stag  Table 11: Summary of Chinese parsing approaches
true	N12-1030.pdf#65.42	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches LF  Table 11: Summary of Chinese parsing approaches
true	N12-1030.pdf#98.6	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches cov  Table 11: Summary of Chinese parsing approaches
true	N12-1030.pdf#70.67	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches LF  Table 11: Summary of Chinese parsing approaches
true	N12-1030.pdf#4.82	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches Lsa %  Table 11: Summary of Chinese parsing approaches
true	N12-1030.pdf#84.99	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches stag  Table 11: Summary of Chinese parsing approaches
true	N12-1030.pdf#83.73	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches stag  Table 11: Summary of Chinese parsing approaches
true	N12-1030.pdf#8.62	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches Lsa %  Table 11: Summary of Chinese parsing approaches
true	N12-1030.pdf#18.35	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches log C  Table 11: Summary of Chinese parsing approaches
true	N12-1030.pdf#93.8	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches cov  Table 11: Summary of Chinese parsing approaches
true	N12-1030.pdf#74.99	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches LF  Table 11: Summary of Chinese parsing approaches
true	N12-1030.pdf#97.9	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches cov  Table 11: Summary of Chinese parsing approaches
true	N12-1030.pdf#18.67	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches log C  Table 11: Summary of Chinese parsing approaches
true	N12-1030.pdf#7.42	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches Lsa %  Table 11: Summary of Chinese parsing approaches
true	N12-1030.pdf#65.42	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches LF  Table 12: Dev set evaluation for C&C over pro-drop  sentences only (and over full set in parentheses)
true	N12-1030.pdf#18.67	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches log C  Table 12: Dev set evaluation for C&C over pro-drop  sentences only (and over full set in parentheses)
true	N12-1030.pdf#7.42	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches Lsa %  Table 12: Dev set evaluation for C&C over pro-drop  sentences only (and over full set in parentheses)
true	N12-1030.pdf#83.73	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches stag  Table 12: Dev set evaluation for C&C over pro-drop  sentences only (and over full set in parentheses)
true	N12-1030.pdf#18.35	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches log C  Table 12: Dev set evaluation for C&C over pro-drop  sentences only (and over full set in parentheses)
true	N12-1030.pdf#74.99	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches LF  Table 12: Dev set evaluation for C&C over pro-drop  sentences only (and over full set in parentheses)
true	N12-1030.pdf#4.82	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches Lsa %  Table 12: Dev set evaluation for C&C over pro-drop  sentences only (and over full set in parentheses)
true	N12-1030.pdf#93.8	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches cov  Table 12: Dev set evaluation for C&C over pro-drop  sentences only (and over full set in parentheses)
true	N12-1030.pdf#98.6	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches cov  Table 12: Dev set evaluation for C&C over pro-drop  sentences only (and over full set in parentheses)
true	N12-1030.pdf#84.99	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches stag  Table 12: Dev set evaluation for C&C over pro-drop  sentences only (and over full set in parentheses)
true	N12-1030.pdf#89.36	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches stag  Table 12: Dev set evaluation for C&C over pro-drop  sentences only (and over full set in parentheses)
true	N12-1030.pdf#97.9	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches cov  Table 12: Dev set evaluation for C&C over pro-drop  sentences only (and over full set in parentheses)
true	N12-1030.pdf#8.62	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches Lsa %  Table 12: Dev set evaluation for C&C over pro-drop  sentences only (and over full set in parentheses)
true	N12-1030.pdf#70.67	CCGBank, Accuracy	Table 11 : Summary of Chinese parsing approaches LF  Table 12: Dev set evaluation for C&C over pro-drop  sentences only (and over full set in parentheses)
true	D12-1093.pdf#0.392	New York Times Corpus, P@10%	KB+Text  Table 2: Mean Reciprocal Rank (MRR) for different approaches under closed-world assumption. Here KB, Text and  KB+Text columns represent results obtained by training a PRA model with only the KB, only text, and both KB and  text. KB+Text[b] is the binarized PRA approach trained on both KB and text. The best performing system (results  shown in bold font) is significant at 0.0001 level over its nearest competitor according to a difference of proportions  significance test.
true	D12-1093.pdf#0.583	New York Times Corpus, P@10%	KB+Text  Table 2: Mean Reciprocal Rank (MRR) for different approaches under closed-world assumption. Here KB, Text and  KB+Text columns represent results obtained by training a PRA model with only the KB, only text, and both KB and  text. KB+Text[b] is the binarized PRA approach trained on both KB and text. The best performing system (results  shown in bold font) is significant at 0.0001 level over its nearest competitor according to a difference of proportions  significance test.
true	D12-1093.pdf#0.812	New York Times Corpus, P@10%	KB+Text  Table 2: Mean Reciprocal Rank (MRR) for different approaches under closed-world assumption. Here KB, Text and  KB+Text columns represent results obtained by training a PRA model with only the KB, only text, and both KB and  text. KB+Text[b] is the binarized PRA approach trained on both KB and text. The best performing system (results  shown in bold font) is significant at 0.0001 level over its nearest competitor according to a difference of proportions  significance test.
true	P15-4025.pdf#81.1%	Chinese Treebank 6, F1	Acurrary / F1 68M  Table 2: Evaluation of NiuParser on various tasks.  † beam search-based global training method.   ‡ classification-based method with Neural Networks. characters per second.  *  predicates per second.
true	P15-4025.pdf#68.4%	Chinese Treebank 6, F1	Acurrary / F1 68M  Table 2: Evaluation of NiuParser on various tasks.  † beam search-based global training method.   ‡ classification-based method with Neural Networks. characters per second.  *  predicates per second.
true	P15-4025.pdf#88.1%	Chinese Treebank 6, F1	Acurrary / F1 68M  Table 2: Evaluation of NiuParser on various tasks.  † beam search-based global training method.   ‡ classification-based method with Neural Networks. characters per second.  *  predicates per second.
true	P15-4025.pdf#82.1%	Chinese Treebank 6, F1	Acurrary / F1 68M  Table 2: Evaluation of NiuParser on various tasks.  † beam search-based global training method.   ‡ classification-based method with Neural Networks. characters per second.  *  predicates per second.
true	P15-4025.pdf#83.2%	Chinese Treebank 6, F1	Acurrary / F1 68M  Table 2: Evaluation of NiuParser on various tasks.  † beam search-based global training method.   ‡ classification-based method with Neural Networks. characters per second.  *  predicates per second.
true	P15-4025.pdf#82.4%	Chinese Treebank 6, F1	Acurrary / F1 68M  Table 2: Evaluation of NiuParser on various tasks.  † beam search-based global training method.   ‡ classification-based method with Neural Networks. characters per second.  *  predicates per second.
true	P15-4025.pdf#93.5%	Chinese Treebank 6, F1	Acurrary / F1  Table 2: Evaluation of NiuParser on various tasks.  † beam search-based global training method.   ‡ classification-based method with Neural Networks. characters per second.  *  predicates per second.
true	P15-4025.pdf#97.3%	Chinese Treebank 6, F1	Table 2: Evaluation of NiuParser on various tasks.  † beam search-based global training method.   ‡ classification-based method with Neural Networks. characters per second.  *  predicates per second.
true	P14-2107.pdf#93.71	Penn Treebank, Validation perplexity	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#90.64	Penn Treebank, Validation perplexity	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#93.01	Penn Treebank, Validation perplexity	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#93.02	Penn Treebank, Validation perplexity	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#86.34	Penn Treebank, Validation perplexity	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#91.37	Penn Treebank, Validation perplexity	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#87.96	Penn Treebank, Validation perplexity	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#93.57	Penn Treebank, Validation perplexity	Cube - pruned w / diversity  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#92.48	Penn Treebank, Validation perplexity	Cube - pruned w / diversity  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#94.40	Penn Treebank, Validation perplexity	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#67.00	Penn Treebank, Validation perplexity	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#94.91	Penn Treebank, Validation perplexity	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#82.16	Penn Treebank, Validation perplexity	w / o diversity  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#88.36	Penn Treebank, Validation perplexity	w / o diversity  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#85.26	Penn Treebank, Validation perplexity	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.95	Penn Treebank, Validation perplexity	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#93.26	Penn Treebank, Validation perplexity	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#77.14	Penn Treebank, Validation perplexity	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#80.29	Penn Treebank, Validation perplexity	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#89.54	Penn Treebank, Validation perplexity	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#83.57	Penn Treebank, Validation perplexity	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#80.14	Penn Treebank, Validation perplexity	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#87.57	Penn Treebank, Validation perplexity	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.67	Penn Treebank, Validation perplexity	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#89.87	Penn Treebank, Validation perplexity	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#84.95	Penn Treebank, Validation perplexity	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.85	Penn Treebank, Validation perplexity	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#69.98	Penn Treebank, Validation perplexity	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#83.22	Penn Treebank, Validation perplexity	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.88	Penn Treebank, Validation perplexity	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#89.52	Penn Treebank, Validation perplexity	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#88.50	Penn Treebank, Validation perplexity	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#94.02	Penn Treebank, Validation perplexity	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#92.23	Penn Treebank, Validation perplexity	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#78.45	Penn Treebank, Validation perplexity	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#80.56	Penn Treebank, Validation perplexity	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.21	Penn Treebank, Validation perplexity	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.27	Penn Treebank, Validation perplexity	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#81.79	Penn Treebank, Validation perplexity	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#87.98	Penn Treebank, Validation perplexity	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#92.03	Penn Treebank, Validation perplexity	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.55	Penn Treebank, Validation perplexity	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#77.44	Penn Treebank, Validation perplexity	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#71.94	Penn Treebank, Validation perplexity	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.30	Penn Treebank, Validation perplexity	PENN - S - 2 . 0 . 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#92.68	Penn Treebank, Validation perplexity	PENN - YM LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#86.28	Penn Treebank, Validation perplexity	CTB - 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.82	Penn Treebank, Validation perplexity	PENN - YM UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.77	Penn Treebank, Validation perplexity	PENN - S - 2 . 0 . 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.99	Penn Treebank, Validation perplexity	PENN - S - 2 . 0 . 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#87.91	Penn Treebank, Validation perplexity	CTB - 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#91.55	Penn Treebank, Validation perplexity	PENN - S - 2 . 0 . 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#92.74	Penn Treebank, Validation perplexity	PENN - YM LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#87.68	Penn Treebank, Validation perplexity	CTB - 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#86.08	Penn Treebank, Validation perplexity	CTB - 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.79	Penn Treebank, Validation perplexity	PENN - YM UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.71	Penn Treebank, UAS	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#90.64	Penn Treebank, UAS	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#93.01	Penn Treebank, UAS	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#93.02	Penn Treebank, UAS	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#86.34	Penn Treebank, UAS	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#91.37	Penn Treebank, UAS	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#87.96	Penn Treebank, UAS	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#93.57	Penn Treebank, UAS	Cube - pruned w / diversity  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#92.48	Penn Treebank, UAS	Cube - pruned w / diversity  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#94.40	Penn Treebank, UAS	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#67.00	Penn Treebank, UAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#94.91	Penn Treebank, UAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#82.16	Penn Treebank, UAS	w / o diversity  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#88.36	Penn Treebank, UAS	w / o diversity  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#85.26	Penn Treebank, UAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.95	Penn Treebank, UAS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#93.26	Penn Treebank, UAS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#77.14	Penn Treebank, UAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#80.29	Penn Treebank, UAS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#89.54	Penn Treebank, UAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#83.57	Penn Treebank, UAS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#80.14	Penn Treebank, UAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#87.57	Penn Treebank, UAS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.67	Penn Treebank, UAS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#89.87	Penn Treebank, UAS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#84.95	Penn Treebank, UAS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.85	Penn Treebank, UAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#69.98	Penn Treebank, UAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#83.22	Penn Treebank, UAS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.88	Penn Treebank, UAS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#89.52	Penn Treebank, UAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#88.50	Penn Treebank, UAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#94.02	Penn Treebank, UAS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#92.23	Penn Treebank, UAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#78.45	Penn Treebank, UAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#80.56	Penn Treebank, UAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.21	Penn Treebank, UAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.27	Penn Treebank, UAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#81.79	Penn Treebank, UAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#87.98	Penn Treebank, UAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#92.03	Penn Treebank, UAS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.55	Penn Treebank, UAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#77.44	Penn Treebank, UAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#71.94	Penn Treebank, UAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.30	Penn Treebank, UAS	PENN - S - 2 . 0 . 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#92.68	Penn Treebank, UAS	PENN - YM LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#86.28	Penn Treebank, UAS	CTB - 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.82	Penn Treebank, UAS	PENN - YM UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.77	Penn Treebank, UAS	PENN - S - 2 . 0 . 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.99	Penn Treebank, UAS	PENN - S - 2 . 0 . 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#87.91	Penn Treebank, UAS	CTB - 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#91.55	Penn Treebank, UAS	PENN - S - 2 . 0 . 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#92.74	Penn Treebank, UAS	PENN - YM LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#87.68	Penn Treebank, UAS	CTB - 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#86.08	Penn Treebank, UAS	CTB - 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.79	Penn Treebank, UAS	PENN - YM UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.71	Penn Treebank, POS	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#90.64	Penn Treebank, POS	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#93.01	Penn Treebank, POS	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#93.02	Penn Treebank, POS	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#86.34	Penn Treebank, POS	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#91.37	Penn Treebank, POS	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#87.96	Penn Treebank, POS	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#93.57	Penn Treebank, POS	Cube - pruned w / diversity  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#92.48	Penn Treebank, POS	Cube - pruned w / diversity  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#94.40	Penn Treebank, POS	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#67.00	Penn Treebank, POS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#94.91	Penn Treebank, POS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#82.16	Penn Treebank, POS	w / o diversity  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#88.36	Penn Treebank, POS	w / o diversity  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#85.26	Penn Treebank, POS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.95	Penn Treebank, POS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#93.26	Penn Treebank, POS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#77.14	Penn Treebank, POS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#80.29	Penn Treebank, POS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#89.54	Penn Treebank, POS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#83.57	Penn Treebank, POS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#80.14	Penn Treebank, POS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#87.57	Penn Treebank, POS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.67	Penn Treebank, POS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#89.87	Penn Treebank, POS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#84.95	Penn Treebank, POS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.85	Penn Treebank, POS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#69.98	Penn Treebank, POS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#83.22	Penn Treebank, POS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.88	Penn Treebank, POS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#89.52	Penn Treebank, POS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#88.50	Penn Treebank, POS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#94.02	Penn Treebank, POS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#92.23	Penn Treebank, POS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#78.45	Penn Treebank, POS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#80.56	Penn Treebank, POS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.21	Penn Treebank, POS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.27	Penn Treebank, POS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#81.79	Penn Treebank, POS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#87.98	Penn Treebank, POS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#92.03	Penn Treebank, POS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.55	Penn Treebank, POS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#77.44	Penn Treebank, POS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#71.94	Penn Treebank, POS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.30	Penn Treebank, POS	PENN - S - 2 . 0 . 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#92.68	Penn Treebank, POS	PENN - YM LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#86.28	Penn Treebank, POS	CTB - 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.82	Penn Treebank, POS	PENN - YM UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.77	Penn Treebank, POS	PENN - S - 2 . 0 . 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.99	Penn Treebank, POS	PENN - S - 2 . 0 . 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#87.91	Penn Treebank, POS	CTB - 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#91.55	Penn Treebank, POS	PENN - S - 2 . 0 . 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#92.74	Penn Treebank, POS	PENN - YM LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#87.68	Penn Treebank, POS	CTB - 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#86.08	Penn Treebank, POS	CTB - 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.79	Penn Treebank, POS	PENN - YM UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.71	Penn Treebank, LAS	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#90.64	Penn Treebank, LAS	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#93.01	Penn Treebank, LAS	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#93.02	Penn Treebank, LAS	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#86.34	Penn Treebank, LAS	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#91.37	Penn Treebank, LAS	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#87.96	Penn Treebank, LAS	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#93.57	Penn Treebank, LAS	Cube - pruned w / diversity  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#92.48	Penn Treebank, LAS	Cube - pruned w / diversity  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#94.40	Penn Treebank, LAS	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#67.00	Penn Treebank, LAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#94.91	Penn Treebank, LAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#82.16	Penn Treebank, LAS	w / o diversity  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#88.36	Penn Treebank, LAS	w / o diversity  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#85.26	Penn Treebank, LAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.95	Penn Treebank, LAS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#93.26	Penn Treebank, LAS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#77.14	Penn Treebank, LAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#80.29	Penn Treebank, LAS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#89.54	Penn Treebank, LAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#83.57	Penn Treebank, LAS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#80.14	Penn Treebank, LAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#87.57	Penn Treebank, LAS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.67	Penn Treebank, LAS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#89.87	Penn Treebank, LAS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#84.95	Penn Treebank, LAS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.85	Penn Treebank, LAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#69.98	Penn Treebank, LAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#83.22	Penn Treebank, LAS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.88	Penn Treebank, LAS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#89.52	Penn Treebank, LAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#88.50	Penn Treebank, LAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#94.02	Penn Treebank, LAS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#92.23	Penn Treebank, LAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#78.45	Penn Treebank, LAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#80.56	Penn Treebank, LAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.21	Penn Treebank, LAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.27	Penn Treebank, LAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#81.79	Penn Treebank, LAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#87.98	Penn Treebank, LAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#92.03	Penn Treebank, LAS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.55	Penn Treebank, LAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#77.44	Penn Treebank, LAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#71.94	Penn Treebank, LAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.30	Penn Treebank, LAS	PENN - S - 2 . 0 . 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#92.68	Penn Treebank, LAS	PENN - YM LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#86.28	Penn Treebank, LAS	CTB - 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.82	Penn Treebank, LAS	PENN - YM UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.77	Penn Treebank, LAS	PENN - S - 2 . 0 . 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.99	Penn Treebank, LAS	PENN - S - 2 . 0 . 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#87.91	Penn Treebank, LAS	CTB - 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#91.55	Penn Treebank, LAS	PENN - S - 2 . 0 . 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#92.74	Penn Treebank, LAS	PENN - YM LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#87.68	Penn Treebank, LAS	CTB - 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#86.08	Penn Treebank, LAS	CTB - 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.79	Penn Treebank, LAS	PENN - YM UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.71	Penn Treebank, F1	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#90.64	Penn Treebank, F1	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#93.01	Penn Treebank, F1	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#93.02	Penn Treebank, F1	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#86.34	Penn Treebank, F1	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#91.37	Penn Treebank, F1	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#87.96	Penn Treebank, F1	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#93.57	Penn Treebank, F1	Cube - pruned w / diversity  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#92.48	Penn Treebank, F1	Cube - pruned w / diversity  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#94.40	Penn Treebank, F1	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#67.00	Penn Treebank, F1	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#94.91	Penn Treebank, F1	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#82.16	Penn Treebank, F1	w / o diversity  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#88.36	Penn Treebank, F1	w / o diversity  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#85.26	Penn Treebank, F1	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.95	Penn Treebank, F1	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#93.26	Penn Treebank, F1	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#77.14	Penn Treebank, F1	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#80.29	Penn Treebank, F1	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#89.54	Penn Treebank, F1	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#83.57	Penn Treebank, F1	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#80.14	Penn Treebank, F1	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#87.57	Penn Treebank, F1	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.67	Penn Treebank, F1	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#89.87	Penn Treebank, F1	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#84.95	Penn Treebank, F1	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.85	Penn Treebank, F1	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#69.98	Penn Treebank, F1	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#83.22	Penn Treebank, F1	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.88	Penn Treebank, F1	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#89.52	Penn Treebank, F1	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#88.50	Penn Treebank, F1	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#94.02	Penn Treebank, F1	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#92.23	Penn Treebank, F1	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#78.45	Penn Treebank, F1	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#80.56	Penn Treebank, F1	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.21	Penn Treebank, F1	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.27	Penn Treebank, F1	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#81.79	Penn Treebank, F1	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#87.98	Penn Treebank, F1	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#92.03	Penn Treebank, F1	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.55	Penn Treebank, F1	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#77.44	Penn Treebank, F1	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#71.94	Penn Treebank, F1	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.30	Penn Treebank, F1	PENN - S - 2 . 0 . 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#92.68	Penn Treebank, F1	PENN - YM LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#86.28	Penn Treebank, F1	CTB - 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.82	Penn Treebank, F1	PENN - YM UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.77	Penn Treebank, F1	PENN - S - 2 . 0 . 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.99	Penn Treebank, F1	PENN - S - 2 . 0 . 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#87.91	Penn Treebank, F1	CTB - 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#91.55	Penn Treebank, F1	PENN - S - 2 . 0 . 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#92.74	Penn Treebank, F1	PENN - YM LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#87.68	Penn Treebank, F1	CTB - 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#86.08	Penn Treebank, F1	CTB - 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.79	Penn Treebank, F1	PENN - YM UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.71	benchmark Vietnamese dependency treebank VnDT, UAS	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#90.64	benchmark Vietnamese dependency treebank VnDT, UAS	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#93.01	benchmark Vietnamese dependency treebank VnDT, UAS	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#93.02	benchmark Vietnamese dependency treebank VnDT, UAS	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#86.34	benchmark Vietnamese dependency treebank VnDT, UAS	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#91.37	benchmark Vietnamese dependency treebank VnDT, UAS	Cube - pruned w / diversity LAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#87.96	benchmark Vietnamese dependency treebank VnDT, UAS	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#93.57	benchmark Vietnamese dependency treebank VnDT, UAS	Cube - pruned w / diversity  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#92.48	benchmark Vietnamese dependency treebank VnDT, UAS	Cube - pruned w / diversity  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#94.40	benchmark Vietnamese dependency treebank VnDT, UAS	Cube - pruned w / diversity UAS  Table 1: English and Chinese results for cube pruning dependency parsing with the enforcement of  structural diversity. PENN-S and CTB-5 are significant at p < 0.05. Penn-S-2.0.5 TurboParser result is  from
true	P14-2107.pdf#67.00	benchmark Vietnamese dependency treebank VnDT, UAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#94.91	benchmark Vietnamese dependency treebank VnDT, UAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#82.16	benchmark Vietnamese dependency treebank VnDT, UAS	w / o diversity  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#88.36	benchmark Vietnamese dependency treebank VnDT, UAS	w / o diversity  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#85.26	benchmark Vietnamese dependency treebank VnDT, UAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.95	benchmark Vietnamese dependency treebank VnDT, UAS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#93.26	benchmark Vietnamese dependency treebank VnDT, UAS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#77.14	benchmark Vietnamese dependency treebank VnDT, UAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#80.29	benchmark Vietnamese dependency treebank VnDT, UAS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#89.54	benchmark Vietnamese dependency treebank VnDT, UAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#83.57	benchmark Vietnamese dependency treebank VnDT, UAS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#80.14	benchmark Vietnamese dependency treebank VnDT, UAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#87.57	benchmark Vietnamese dependency treebank VnDT, UAS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.67	benchmark Vietnamese dependency treebank VnDT, UAS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#89.87	benchmark Vietnamese dependency treebank VnDT, UAS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#84.95	benchmark Vietnamese dependency treebank VnDT, UAS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.85	benchmark Vietnamese dependency treebank VnDT, UAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#69.98	benchmark Vietnamese dependency treebank VnDT, UAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#83.22	benchmark Vietnamese dependency treebank VnDT, UAS	w / o diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.88	benchmark Vietnamese dependency treebank VnDT, UAS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#89.52	benchmark Vietnamese dependency treebank VnDT, UAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#88.50	benchmark Vietnamese dependency treebank VnDT, UAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#94.02	benchmark Vietnamese dependency treebank VnDT, UAS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#92.23	benchmark Vietnamese dependency treebank VnDT, UAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#78.45	benchmark Vietnamese dependency treebank VnDT, UAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#80.56	benchmark Vietnamese dependency treebank VnDT, UAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.21	benchmark Vietnamese dependency treebank VnDT, UAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.27	benchmark Vietnamese dependency treebank VnDT, UAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#81.79	benchmark Vietnamese dependency treebank VnDT, UAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#87.98	benchmark Vietnamese dependency treebank VnDT, UAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#92.03	benchmark Vietnamese dependency treebank VnDT, UAS	w / o diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#86.55	benchmark Vietnamese dependency treebank VnDT, UAS	w / diversity UAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#77.44	benchmark Vietnamese dependency treebank VnDT, UAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#71.94	benchmark Vietnamese dependency treebank VnDT, UAS	w / diversity LAS  Table 2: Results for languages from CoNLL  2006/2007 shared tasks. When a language is in  both years, the 2006 set is used. Languages are  sorted by the number of unique arc labels.
true	P14-2107.pdf#91.30	benchmark Vietnamese dependency treebank VnDT, UAS	PENN - S - 2 . 0 . 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#92.68	benchmark Vietnamese dependency treebank VnDT, UAS	PENN - YM LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#86.28	benchmark Vietnamese dependency treebank VnDT, UAS	CTB - 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.82	benchmark Vietnamese dependency treebank VnDT, UAS	PENN - YM UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.77	benchmark Vietnamese dependency treebank VnDT, UAS	PENN - S - 2 . 0 . 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.99	benchmark Vietnamese dependency treebank VnDT, UAS	PENN - S - 2 . 0 . 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#87.91	benchmark Vietnamese dependency treebank VnDT, UAS	CTB - 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#91.55	benchmark Vietnamese dependency treebank VnDT, UAS	PENN - S - 2 . 0 . 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#92.74	benchmark Vietnamese dependency treebank VnDT, UAS	PENN - YM LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#87.68	benchmark Vietnamese dependency treebank VnDT, UAS	CTB - 5 UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#86.08	benchmark Vietnamese dependency treebank VnDT, UAS	CTB - 5 LAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P14-2107.pdf#93.79	benchmark Vietnamese dependency treebank VnDT, UAS	PENN - YM UAS  Table 3: Varying the degree of diversity by adjusting the secondary beam for labeled variants, with  different primary beams. When the size of the secondary beam is equal to the primary beam, the parser  degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller,  there is more structural diversity and less label diversity. Results are on development sets.
true	P13-1173.pdf#0.90	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.83	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.92	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.85	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.88	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.84	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.89	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.87	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.85	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.84	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.90	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.88	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.84	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.88	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.86	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.86	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.85	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.88	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.87	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.85	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.68	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.59	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.69	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.72	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.66	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.75	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.68	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.64	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.65	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.68	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.75	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.69	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.69	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.64	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.75	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.69	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.80	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.69	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.68	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.70	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.66	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.73	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.78	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.71	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.74	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.75	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.73	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.76	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.72	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.74	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.79	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.73	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.81	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.75	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.82	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.78	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.80	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.64	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.68	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.78	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.78	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.77	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.69	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.71	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.80	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.73	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.65	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.67	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	P  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.90	Text8, Number of params	P  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.83	Text8, Number of params	P  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.92	Text8, Number of params	P  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.85	Text8, Number of params	F  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.88	Text8, Number of params	R  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.84	Text8, Number of params	F  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.89	Text8, Number of params	F  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.87	Text8, Number of params	P  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.85	Text8, Number of params	F  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.84	Text8, Number of params	F  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.90	Text8, Number of params	P  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.88	Text8, Number of params	R  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.84	Text8, Number of params	F  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.88	Text8, Number of params	R  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.86	Text8, Number of params	F  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.86	Text8, Number of params	F  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.85	Text8, Number of params	R  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.88	Text8, Number of params	F  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.87	Text8, Number of params	R  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.85	Text8, Number of params	F  Table 2: Results of opinion target extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.68	Text8, Number of params	F  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.59	Text8, Number of params	P  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.69	Text8, Number of params	F  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.72	Text8, Number of params	P  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.66	Text8, Number of params	F  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.75	Text8, Number of params	R  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.68	Text8, Number of params	F  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.64	Text8, Number of params	P  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.65	Text8, Number of params	P  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.68	Text8, Number of params	F  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.75	Text8, Number of params	R  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.69	Text8, Number of params	F  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.69	Text8, Number of params	F  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.64	Text8, Number of params	P  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.75	Text8, Number of params	Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.69	Text8, Number of params	R  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.80	Text8, Number of params	R  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.69	Text8, Number of params	F  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.68	Text8, Number of params	F  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.70	Text8, Number of params	R  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.66	Text8, Number of params	P  Table 3: Results of opinion word extraction on the Customer Review Dataset.
true	P13-1173.pdf#0.73	Text8, Number of params	P  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.78	Text8, Number of params	P  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.71	Text8, Number of params	R  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.74	Text8, Number of params	F  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.75	Text8, Number of params	F  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.73	Text8, Number of params	R  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.76	Text8, Number of params	P  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.72	Text8, Number of params	F  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.74	Text8, Number of params	F  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.79	Text8, Number of params	F  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.73	Text8, Number of params	R  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.81	Text8, Number of params	R  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.75	Text8, Number of params	P  Table 4: Results of opinion targets extraction on Large and COAE08.
true	P13-1173.pdf#0.82	Text8, Number of params	P  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.78	Text8, Number of params	F  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.80	Text8, Number of params	F  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.64	Text8, Number of params	P  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.68	Text8, Number of params	F  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.78	Text8, Number of params	R  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.78	Text8, Number of params	R  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.77	Text8, Number of params	R  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.69	Text8, Number of params	R  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.71	Text8, Number of params	R  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.80	Text8, Number of params	P  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.73	Text8, Number of params	F  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.65	Text8, Number of params	F  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1173.pdf#0.67	Text8, Number of params	P  Table 5: Results of opinion words extraction on Large and COAE08.
true	P13-1088.pdf#87.2	SUBJ, Accuracy	MPQA  Table 3: Accuracy of sentiment classification on  the sentiment polarity (SP) and MPQA datasets.  For NB we only display the best result among a  larger group of models analysed in that paper.
true	P13-2090.pdf#5.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	L  Table 2: The original and the bootstrapped (high- lighted) lexicon term count (L I ⊂ L B ) with polar- ity across languages (thousands).
true	P13-2090.pdf#21.5	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	L  Table 2: The original and the bootstrapped (high- lighted) lexicon term count (L I ⊂ L B ) with polar- ity across languages (thousands).
true	P13-2090.pdf#10.8	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	L  Table 2: The original and the bootstrapped (high- lighted) lexicon term count (L I ⊂ L B ) with polar- ity across languages (thousands).
true	P13-2090.pdf#8.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	L  Table 2: The original and the bootstrapped (high- lighted) lexicon term count (L I ⊂ L B ) with polar- ity across languages (thousands).
true	P13-2090.pdf#3.7	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	L  Table 2: The original and the bootstrapped (high- lighted) lexicon term count (L I ⊂ L B ) with polar- ity across languages (thousands).
true	P13-2090.pdf#22.3	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	L  Table 2: The original and the bootstrapped (high- lighted) lexicon term count (L I ⊂ L B ) with polar- ity across languages (thousands).
true	D15-1084.pdf#0.3	Senseval 2, F1	Table 2 : Disambiguation precision for all KBs  Table 3: Coverage results (%) for all KBs
true	D15-1084.pdf#0.5	Senseval 2, F1	Table 2 : Disambiguation precision for all KBs  Table 3: Coverage results (%) for all KBs
true	D15-1084.pdf#0.8	Senseval 2, F1	Table 2 : Disambiguation precision for all KBs  Table 3: Coverage results (%) for all KBs
true	D15-1084.pdf#.864	Senseval 2, F1	- UNIFY  Table 4: Disambiguation results over NELL gold standard
true	D15-1084.pdf#0.5)	Senseval 2, F1	Dutta et al .  Table 4: Disambiguation results over NELL gold standard
true	D15-1084.pdf#.957	Senseval 2, F1	- UNIFY  Table 4: Disambiguation results over NELL gold standard
true	D15-1084.pdf#.875	Senseval 2, F1	- UNIFY  Table 4: Disambiguation results over NELL gold standard
true	D15-1084.pdf#.75	Senseval 2, F1	NELL - REVERB  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.74	Senseval 2, F1	PATTY - REVERB  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.80	Senseval 2, F1	PATTY - WISENET  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#1.00	Senseval 2, F1	88 PATTY - WISENET PATTY - NELL  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.87	Senseval 2, F1	88 NELL - REVERB WISENET - REVERB  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.84	Senseval 2, F1	88 PATTY - REVERB WISENET - NELL  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.87	Senseval 2, F1	88 NELL - REVERB WISENET - REVERB  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#1.00	Senseval 2, F1	88 PATTY - WISENET PATTY - NELL  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#.75	Senseval 2, F1	NELL - REVERB  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#.80	Senseval 2, F1	PATTY - WISENET  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#.84	Senseval 2, F1	88 PATTY - REVERB WISENET - NELL  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#.74	Senseval 2, F1	PATTY - REVERB  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#0.3	SemEval 2015, F1	Table 2 : Disambiguation precision for all KBs  Table 3: Coverage results (%) for all KBs
true	D15-1084.pdf#0.5	SemEval 2015, F1	Table 2 : Disambiguation precision for all KBs  Table 3: Coverage results (%) for all KBs
true	D15-1084.pdf#0.8	SemEval 2015, F1	Table 2 : Disambiguation precision for all KBs  Table 3: Coverage results (%) for all KBs
true	D15-1084.pdf#.864	SemEval 2015, F1	- UNIFY  Table 4: Disambiguation results over NELL gold standard
true	D15-1084.pdf#0.5)	SemEval 2015, F1	Dutta et al .  Table 4: Disambiguation results over NELL gold standard
true	D15-1084.pdf#.957	SemEval 2015, F1	- UNIFY  Table 4: Disambiguation results over NELL gold standard
true	D15-1084.pdf#.875	SemEval 2015, F1	- UNIFY  Table 4: Disambiguation results over NELL gold standard
true	D15-1084.pdf#.75	SemEval 2015, F1	NELL - REVERB  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.74	SemEval 2015, F1	PATTY - REVERB  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.80	SemEval 2015, F1	PATTY - WISENET  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#1.00	SemEval 2015, F1	88 PATTY - WISENET PATTY - NELL  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.87	SemEval 2015, F1	88 NELL - REVERB WISENET - REVERB  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.84	SemEval 2015, F1	88 PATTY - REVERB WISENET - NELL  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.87	SemEval 2015, F1	88 NELL - REVERB WISENET - REVERB  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#1.00	SemEval 2015, F1	88 PATTY - WISENET PATTY - NELL  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#.75	SemEval 2015, F1	NELL - REVERB  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#.80	SemEval 2015, F1	PATTY - WISENET  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#.84	SemEval 2015, F1	88 PATTY - REVERB WISENET - NELL  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#.74	SemEval 2015, F1	PATTY - REVERB  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#0.3	Senseval 3, F1	Table 2 : Disambiguation precision for all KBs  Table 3: Coverage results (%) for all KBs
true	D15-1084.pdf#0.5	Senseval 3, F1	Table 2 : Disambiguation precision for all KBs  Table 3: Coverage results (%) for all KBs
true	D15-1084.pdf#0.8	Senseval 3, F1	Table 2 : Disambiguation precision for all KBs  Table 3: Coverage results (%) for all KBs
true	D15-1084.pdf#.864	Senseval 3, F1	- UNIFY  Table 4: Disambiguation results over NELL gold standard
true	D15-1084.pdf#0.5)	Senseval 3, F1	Dutta et al .  Table 4: Disambiguation results over NELL gold standard
true	D15-1084.pdf#.957	Senseval 3, F1	- UNIFY  Table 4: Disambiguation results over NELL gold standard
true	D15-1084.pdf#.875	Senseval 3, F1	- UNIFY  Table 4: Disambiguation results over NELL gold standard
true	D15-1084.pdf#.75	Senseval 3, F1	NELL - REVERB  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.74	Senseval 3, F1	PATTY - REVERB  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.80	Senseval 3, F1	PATTY - WISENET  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#1.00	Senseval 3, F1	88 PATTY - WISENET PATTY - NELL  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.87	Senseval 3, F1	88 NELL - REVERB WISENET - REVERB  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.84	Senseval 3, F1	88 PATTY - REVERB WISENET - NELL  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.87	Senseval 3, F1	88 NELL - REVERB WISENET - REVERB  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#1.00	Senseval 3, F1	88 PATTY - WISENET PATTY - NELL  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#.75	Senseval 3, F1	NELL - REVERB  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#.80	Senseval 3, F1	PATTY - WISENET  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#.84	Senseval 3, F1	88 PATTY - REVERB WISENET - NELL  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#.74	Senseval 3, F1	PATTY - REVERB  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#0.3	SemEval 2013, F1	Table 2 : Disambiguation precision for all KBs  Table 3: Coverage results (%) for all KBs
true	D15-1084.pdf#0.5	SemEval 2013, F1	Table 2 : Disambiguation precision for all KBs  Table 3: Coverage results (%) for all KBs
true	D15-1084.pdf#0.8	SemEval 2013, F1	Table 2 : Disambiguation precision for all KBs  Table 3: Coverage results (%) for all KBs
true	D15-1084.pdf#.864	SemEval 2013, F1	- UNIFY  Table 4: Disambiguation results over NELL gold standard
true	D15-1084.pdf#0.5)	SemEval 2013, F1	Dutta et al .  Table 4: Disambiguation results over NELL gold standard
true	D15-1084.pdf#.957	SemEval 2013, F1	- UNIFY  Table 4: Disambiguation results over NELL gold standard
true	D15-1084.pdf#.875	SemEval 2013, F1	- UNIFY  Table 4: Disambiguation results over NELL gold standard
true	D15-1084.pdf#.75	SemEval 2013, F1	NELL - REVERB  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.74	SemEval 2013, F1	PATTY - REVERB  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.80	SemEval 2013, F1	PATTY - WISENET  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#1.00	SemEval 2013, F1	88 PATTY - WISENET PATTY - NELL  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.87	SemEval 2013, F1	88 NELL - REVERB WISENET - REVERB  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.84	SemEval 2013, F1	88 PATTY - REVERB WISENET - NELL  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.87	SemEval 2013, F1	88 NELL - REVERB WISENET - REVERB  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#1.00	SemEval 2013, F1	88 PATTY - WISENET PATTY - NELL  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#.75	SemEval 2013, F1	NELL - REVERB  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#.80	SemEval 2013, F1	PATTY - WISENET  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#.84	SemEval 2013, F1	88 PATTY - REVERB WISENET - NELL  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#.74	SemEval 2013, F1	PATTY - REVERB  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#0.3	SemEval 2007, F1	Table 2 : Disambiguation precision for all KBs  Table 3: Coverage results (%) for all KBs
true	D15-1084.pdf#0.5	SemEval 2007, F1	Table 2 : Disambiguation precision for all KBs  Table 3: Coverage results (%) for all KBs
true	D15-1084.pdf#0.8	SemEval 2007, F1	Table 2 : Disambiguation precision for all KBs  Table 3: Coverage results (%) for all KBs
true	D15-1084.pdf#.864	SemEval 2007, F1	- UNIFY  Table 4: Disambiguation results over NELL gold standard
true	D15-1084.pdf#0.5)	SemEval 2007, F1	Dutta et al .  Table 4: Disambiguation results over NELL gold standard
true	D15-1084.pdf#.957	SemEval 2007, F1	- UNIFY  Table 4: Disambiguation results over NELL gold standard
true	D15-1084.pdf#.875	SemEval 2007, F1	- UNIFY  Table 4: Disambiguation results over NELL gold standard
true	D15-1084.pdf#.75	SemEval 2007, F1	NELL - REVERB  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.74	SemEval 2007, F1	PATTY - REVERB  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.80	SemEval 2007, F1	PATTY - WISENET  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#1.00	SemEval 2007, F1	88 PATTY - WISENET PATTY - NELL  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.87	SemEval 2007, F1	88 NELL - REVERB WISENET - REVERB  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.84	SemEval 2007, F1	88 PATTY - REVERB WISENET - NELL  Table 7: Cross-resource alignment evaluation
true	D15-1084.pdf#.87	SemEval 2007, F1	88 NELL - REVERB WISENET - REVERB  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#1.00	SemEval 2007, F1	88 PATTY - WISENET PATTY - NELL  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#.75	SemEval 2007, F1	NELL - REVERB  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#.80	SemEval 2007, F1	PATTY - WISENET  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#.84	SemEval 2007, F1	88 PATTY - REVERB WISENET - NELL  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	D15-1084.pdf#.74	SemEval 2007, F1	PATTY - REVERB  Table 7. Our alignment al- gorithm shows high precision in all pairings where  δ align = 0.9. Alignment reliability decreases for  lower δ align , as relation pairs where r i is a gener- alization of r j (or vice versa) tend to have similar  centroids in V S . The same holds for pairs where r i  is the negation of r j (or vice versa). Even though  we could have utilized measures based on rela-tion string similarity (
true	P15-1118.pdf#89.8	Penn Treebank, UAS	BNC  Table 4: Comparison of hard and soft dual de- composition for joint parsing (development sec- tion, UAS).
true	P15-1118.pdf#90.9	Penn Treebank, UAS	Brown  Table 4: Comparison of hard and soft dual de- composition for joint parsing (development sec- tion, UAS).
true	P15-1118.pdf#94.0	Penn Treebank, UAS	WSJ  Table 4: Comparison of hard and soft dual de- composition for joint parsing (development sec- tion, UAS).
true	P15-1118.pdf#86.8	Penn Treebank, UAS	Avg  Table 4: Comparison of hard and soft dual de- composition for joint parsing (development sec- tion, UAS).
true	P15-1118.pdf#76.5	Penn Treebank, UAS	QB  Table 4: Comparison of hard and soft dual de- composition for joint parsing (development sec- tion, UAS).
true	P15-1118.pdf#90.9	Penn Treebank, UAS	Table 4: Comparison of hard and soft dual de- composition for joint parsing (development sec- tion, UAS).
true	P15-1118.pdf#84.2	Penn Treebank, UAS	QB  Table 5: UAS of joint parsing using the pair- finding scheme with various n values on the de- velopment portion. n = 1 is the baseline BLLIP  parser and n > 1 is BLLIP with pair-finding.
true	P15-1118.pdf#94.2	Penn Treebank, UAS	Table 5: UAS of joint parsing using the pair- finding scheme with various n values on the de- velopment portion. n = 1 is the baseline BLLIP  parser and n > 1 is BLLIP with pair-finding.
true	P15-1118.pdf#92.3	Penn Treebank, UAS	Brown  Table 5: UAS of joint parsing using the pair- finding scheme with various n values on the de- velopment portion. n = 1 is the baseline BLLIP  parser and n > 1 is BLLIP with pair-finding.
true	P15-1118.pdf#90.0	Penn Treebank, UAS	Avg  Table 5: UAS of joint parsing using the pair- finding scheme with various n values on the de- velopment portion. n = 1 is the baseline BLLIP  parser and n > 1 is BLLIP with pair-finding.
true	P15-1118.pdf#91.9	Penn Treebank, UAS	BNC  Table 5: UAS of joint parsing using the pair- finding scheme with various n values on the de- velopment portion. n = 1 is the baseline BLLIP  parser and n > 1 is BLLIP with pair-finding.
true	P15-1118.pdf#94.0	Penn Treebank, UAS	WSJ  Table 6: Effect of using logistic regression on  top of each method (UAS). Leave-one-out cross- validation is performed on the development data.  +X means augmenting the above system with X.
true	P15-1118.pdf#94.0	Penn Treebank, UAS	WSJ  Table 6: Effect of using logistic regression on  top of each method (UAS). Leave-one-out cross- validation is performed on the development data.  +X means augmenting the above system with X.
true	P15-1118.pdf#94.3	Penn Treebank, UAS	WSJ  Table 6: Effect of using logistic regression on  top of each method (UAS). Leave-one-out cross- validation is performed on the development data.  +X means augmenting the above system with X.
true	P15-1118.pdf#91.1	Penn Treebank, UAS	Avg  Table 6: Effect of using logistic regression on  top of each method (UAS). Leave-one-out cross- validation is performed on the development data.  +X means augmenting the above system with X.
true	P15-1118.pdf#92.6	Penn Treebank, UAS	Brown  Table 6: Effect of using logistic regression on  top of each method (UAS). Leave-one-out cross- validation is performed on the development data.  +X means augmenting the above system with X.
true	P15-1118.pdf#93.9	Penn Treebank, UAS	WSJ  Table 6: Effect of using logistic regression on  top of each method (UAS). Leave-one-out cross- validation is performed on the development data.  +X means augmenting the above system with X.
true	P15-1118.pdf#91.1	Penn Treebank, UAS	Brown  Table 6: Effect of using logistic regression on  top of each method (UAS). Leave-one-out cross- validation is performed on the development data.  +X means augmenting the above system with X.
true	P15-1118.pdf#85.2	Penn Treebank, UAS	QB  Table 6: Effect of using logistic regression on  top of each method (UAS). Leave-one-out cross- validation is performed on the development data.  +X means augmenting the above system with X.
true	P15-1118.pdf#76.5	Penn Treebank, UAS	QB  Table 6: Effect of using logistic regression on  top of each method (UAS). Leave-one-out cross- validation is performed on the development data.  +X means augmenting the above system with X.
true	P15-1118.pdf#76.5	Penn Treebank, UAS	QB  Table 6: Effect of using logistic regression on  top of each method (UAS). Leave-one-out cross- validation is performed on the development data.  +X means augmenting the above system with X.
true	P15-1118.pdf#86.9	Penn Treebank, UAS	Avg  Table 6: Effect of using logistic regression on  top of each method (UAS). Leave-one-out cross- validation is performed on the development data.  +X means augmenting the above system with X.
true	P15-1118.pdf#90.3	Penn Treebank, UAS	Avg  Table 6: Effect of using logistic regression on  top of each method (UAS). Leave-one-out cross- validation is performed on the development data.  +X means augmenting the above system with X.
true	P15-1118.pdf#92.3	Penn Treebank, UAS	Brown  Table 6: Effect of using logistic regression on  top of each method (UAS). Leave-one-out cross- validation is performed on the development data.  +X means augmenting the above system with X.
true	P15-1118.pdf#89.8	Penn Treebank, UAS	BNC  Table 6: Effect of using logistic regression on  top of each method (UAS). Leave-one-out cross- validation is performed on the development data.  +X means augmenting the above system with X.
true	P15-1118.pdf#93.5	Penn Treebank, UAS	BNC  Table 6: Effect of using logistic regression on  top of each method (UAS). Leave-one-out cross- validation is performed on the development data.  +X means augmenting the above system with X.
true	P15-1118.pdf#86.7	Penn Treebank, UAS	QB  Table 6: Effect of using logistic regression on  top of each method (UAS). Leave-one-out cross- validation is performed on the development data.  +X means augmenting the above system with X.
true	P15-1118.pdf#91.4	Penn Treebank, UAS	BNC  Table 6: Effect of using logistic regression on  top of each method (UAS). Leave-one-out cross- validation is performed on the development data.  +X means augmenting the above system with X.
true	N12-1006.pdf#20.66	MSR, F1	BLEU Egyptian - Web - Test  Table 4: A comparison of translation quality of Egyptian, Levantine, and MSA web text, using various training corpora.
true	N12-1006.pdf#2.27%	MSR, F1	OOV Levantine - Web - Test  Table 4: A comparison of translation quality of Egyptian, Levantine, and MSA web text, using various training corpora.
true	N12-1006.pdf#19.29	MSR, F1	BLEU Levantine - Web - Test  Table 4: A comparison of translation quality of Egyptian, Levantine, and MSA web text, using various training corpora.
true	N12-1006.pdf#27.85	MSR, F1	BLEU MSA - Web - Test  Table 4: A comparison of translation quality of Egyptian, Levantine, and MSA web text, using various training corpora.
true	N12-1006.pdf#0.45%	MSR, F1	OOV MSA - Web - Test  Table 4: A comparison of translation quality of Egyptian, Levantine, and MSA web text, using various training corpora.
true	N12-1006.pdf#2.04%	MSR, F1	OOV Egyptian - Web - Test  Table 4: A comparison of translation quality of Egyptian, Levantine, and MSA web text, using various training corpora.
true	N12-1006.pdf#19.29	MSR, F1	BLEU Levantine - Web - Test - -  Table 4: A comparison of translation quality of Egyptian, Levantine, and MSA web text, using various training corpora.
true	N12-1006.pdf#16.71	MSR, F1	BLEU  Table 6: Results on a truly independent test set, consisting of data harvested from Egyptian Facebook pages that are  entirely distinct from the our dialectal training set. The improvements over the MSA baseline are still considerable:  +2.9 BLEU points when no Facebook data is available for tuning and +2.7 with a Facebook tuning set.
true	N12-1006.pdf#18.50	MSR, F1	BLEU  Table 6: Results on a truly independent test set, consisting of data harvested from Egyptian Facebook pages that are  entirely distinct from the our dialectal training set. The improvements over the MSA baseline are still considerable:  +2.9 BLEU points when no Facebook data is available for tuning and +2.7 with a Facebook tuning set.
true	N12-1006.pdf#2.43%	MSR, F1	OOV  Table 6: Results on a truly independent test set, consisting of data harvested from Egyptian Facebook pages that are  entirely distinct from the our dialectal training set. The improvements over the MSA baseline are still considerable:  +2.9 BLEU points when no Facebook data is available for tuning and +2.7 with a Facebook tuning set.
true	N12-1006.pdf#2.43%	MSR, F1	OOV  Table 6: Results on a truly independent test set, consisting of data harvested from Egyptian Facebook pages that are  entirely distinct from the our dialectal training set. The improvements over the MSA baseline are still considerable:  +2.9 BLEU points when no Facebook data is available for tuning and +2.7 with a Facebook tuning set.
true	D15-1071.pdf#0.79	SST-2, Accuracy	represent the standard deviation of the negativity scores . pp ) ( in Capital  Table 3: Correlation coefficients between T1 evolution and sentiment scores.
true	D15-1071.pdf#-0.69	SST-2, Accuracy	represent the standard deviation of the negativity scores . pp ) ( in Capital  Table 3: Correlation coefficients between T1 evolution and sentiment scores.
true	D15-1071.pdf#0.86	SST-2, Accuracy	represent the standard deviation of the negativity scores . pp ) Mean ( Negativity score ) Capital  Table 3: Correlation coefficients between T1 evolution and sentiment scores.
true	D15-1071.pdf#0.89	SST-2, Accuracy	represent the standard deviation of the negativity scores . pp ) ( in Capital Negativity  Table 3: Correlation coefficients between T1 evolution and sentiment scores.
true	D15-1071.pdf#0.12	SST-2, Accuracy	represent the standard deviation of the negativity scores . pp ) ( in Capital Positivity  Table 3: Correlation coefficients between T1 evolution and sentiment scores.
true	D15-1071.pdf#0.85	SST-2, Accuracy	represent the standard deviation of the negativity scores . pp ) Mean ( Negativity score ) Capital Uncertainty  Table 3: Correlation coefficients between T1 evolution and sentiment scores.
true	D15-1071.pdf#1.000	SST-2, Accuracy	Recall U  Table 5: Overview of the results of the supervised classification experiment. Bold numbers indicate the  best results, U class UP, and D class DOWN.
true	D15-1071.pdf#0.491	SST-2, Accuracy	Recall D  Table 5: Overview of the results of the supervised classification experiment. Bold numbers indicate the  best results, U class UP, and D class DOWN.
true	D15-1071.pdf#1.000	SST-2, Accuracy	Recall U  Table 5: Overview of the results of the supervised classification experiment. Bold numbers indicate the  best results, U class UP, and D class DOWN.
true	D15-1071.pdf#0.810	SST-2, Accuracy	Precision U  Table 5: Overview of the results of the supervised classification experiment. Bold numbers indicate the  best results, U class UP, and D class DOWN.
true	D15-1071.pdf#1.000	SST-2, Accuracy	Recall U  Table 5: Overview of the results of the supervised classification experiment. Bold numbers indicate the  best results, U class UP, and D class DOWN.
true	D15-1071.pdf#1.000	SST-2, Accuracy	Recall U  Table 5: Overview of the results of the supervised classification experiment. Bold numbers indicate the  best results, U class UP, and D class DOWN.
true	D15-1071.pdf#0.792	SST-2, Accuracy	Accuracy  Table 5: Overview of the results of the supervised classification experiment. Bold numbers indicate the  best results, U class UP, and D class DOWN.
true	D15-1071.pdf#0.718	SST-2, Accuracy	Precision D  Table 5: Overview of the results of the supervised classification experiment. Bold numbers indicate the  best results, U class UP, and D class DOWN.
true	D15-1071.pdf#0.79	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	represent the standard deviation of the negativity scores . pp ) ( in Capital  Table 3: Correlation coefficients between T1 evolution and sentiment scores.
true	D15-1071.pdf#-0.69	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	represent the standard deviation of the negativity scores . pp ) ( in Capital  Table 3: Correlation coefficients between T1 evolution and sentiment scores.
true	D15-1071.pdf#0.86	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	represent the standard deviation of the negativity scores . pp ) Mean ( Negativity score ) Capital  Table 3: Correlation coefficients between T1 evolution and sentiment scores.
true	D15-1071.pdf#0.89	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	represent the standard deviation of the negativity scores . pp ) ( in Capital Negativity  Table 3: Correlation coefficients between T1 evolution and sentiment scores.
true	D15-1071.pdf#0.12	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	represent the standard deviation of the negativity scores . pp ) ( in Capital Positivity  Table 3: Correlation coefficients between T1 evolution and sentiment scores.
true	D15-1071.pdf#0.85	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	represent the standard deviation of the negativity scores . pp ) Mean ( Negativity score ) Capital Uncertainty  Table 3: Correlation coefficients between T1 evolution and sentiment scores.
true	D15-1071.pdf#1.000	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Recall U  Table 5: Overview of the results of the supervised classification experiment. Bold numbers indicate the  best results, U class UP, and D class DOWN.
true	D15-1071.pdf#0.491	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Recall D  Table 5: Overview of the results of the supervised classification experiment. Bold numbers indicate the  best results, U class UP, and D class DOWN.
true	D15-1071.pdf#1.000	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Recall U  Table 5: Overview of the results of the supervised classification experiment. Bold numbers indicate the  best results, U class UP, and D class DOWN.
true	D15-1071.pdf#0.810	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Precision U  Table 5: Overview of the results of the supervised classification experiment. Bold numbers indicate the  best results, U class UP, and D class DOWN.
true	D15-1071.pdf#1.000	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Recall U  Table 5: Overview of the results of the supervised classification experiment. Bold numbers indicate the  best results, U class UP, and D class DOWN.
true	D15-1071.pdf#1.000	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Recall U  Table 5: Overview of the results of the supervised classification experiment. Bold numbers indicate the  best results, U class UP, and D class DOWN.
true	D15-1071.pdf#0.792	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Accuracy  Table 5: Overview of the results of the supervised classification experiment. Bold numbers indicate the  best results, U class UP, and D class DOWN.
true	D15-1071.pdf#0.718	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Precision D  Table 5: Overview of the results of the supervised classification experiment. Bold numbers indicate the  best results, U class UP, and D class DOWN.
true	P15-1150.pdf#0.8676	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	( 0 . 0020 ) Pearson ' s r -  Table 3: Test set results on the SICK semantic relatedness subtask. For our experiments, we report mean  scores over 5 runs (standard deviations in parentheses). Results are grouped as follows: (1) SemEval  2014 submissions; (2) Our own baselines; (3) Sequential LSTMs; (4) Tree-structured LSTMs.
true	P15-1150.pdf#0.8083	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	( 0 . 0020 ) Spearman ' s ρ -  Table 3: Test set results on the SICK semantic relatedness subtask. For our experiments, we report mean  scores over 5 runs (standard deviations in parentheses). Results are grouped as follows: (1) SemEval  2014 submissions; (2) Our own baselines; (3) Sequential LSTMs; (4) Tree-structured LSTMs.
true	P15-1150.pdf#0.2532	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	( 0 . 0020 ) MSE -  Table 3: Test set results on the SICK semantic relatedness subtask. For our experiments, we report mean  scores over 5 runs (standard deviations in parentheses). Results are grouped as follows: (1) SemEval  2014 submissions; (2) Our own baselines; (3) Sequential LSTMs; (4) Tree-structured LSTMs.
true	P13-1075.pdf#1	Chinese Treebank 6, F1	erence sets are annotated according to a different Partition The  Table 2: Data partitioning for CTB 5.0.
true	P13-1075.pdf#97.2	Chinese Treebank 6, F1	Out - of - Domain Enhanced ( F% )  Table 3: Performance of the baseline classifier and  the classifier enhanced with natural annotations in  Chinese wikipedia.
true	P13-1075.pdf#97.5	Chinese Treebank 6, F1	Out - of - Domain Enhanced ( F% )  Table 3: Performance of the baseline classifier and  the classifier enhanced with natural annotations in  Chinese wikipedia.
true	P13-1075.pdf#97.1	Chinese Treebank 6, F1	Out - of - Domain Enhanced ( F% )  Table 3: Performance of the baseline classifier and  the classifier enhanced with natural annotations in  Chinese wikipedia.
true	P13-1075.pdf#97.3	Chinese Treebank 6, F1	Out - of - Domain Enhanced ( F% )  Table 3: Performance of the baseline classifier and  the classifier enhanced with natural annotations in  Chinese wikipedia.
true	P13-1075.pdf#97.4	Chinese Treebank 6, F1	Out - of - Domain Enhanced ( F% )  Table 3: Performance of the baseline classifier and  the classifier enhanced with natural annotations in  Chinese wikipedia.
true	P13-1075.pdf#97.5	Chinese Treebank 6, F1	Out - of - Domain Enhanced ( F% )  Table 3. On the  CTB testing set, training data from the Chinese
true	P13-1075.pdf#97.2	Chinese Treebank 6, F1	Out - of - Domain Enhanced ( F% )  Table 3. On the  CTB testing set, training data from the Chinese
true	P13-1075.pdf#97.4	Chinese Treebank 6, F1	Out - of - Domain Enhanced ( F% )  Table 3. On the  CTB testing set, training data from the Chinese
true	P13-1075.pdf#97.1	Chinese Treebank 6, F1	Out - of - Domain Enhanced ( F% )  Table 3. On the  CTB testing set, training data from the Chinese
true	P13-1075.pdf#97.3	Chinese Treebank 6, F1	Out - of - Domain Enhanced ( F% )  Table 3. On the  CTB testing set, training data from the Chinese
true	P13-1075.pdf#1	Penn Treebank, F1	erence sets are annotated according to a different Partition The  Table 2: Data partitioning for CTB 5.0.
true	P13-1075.pdf#97.2	Penn Treebank, F1	Out - of - Domain Enhanced ( F% )  Table 3: Performance of the baseline classifier and  the classifier enhanced with natural annotations in  Chinese wikipedia.
true	P13-1075.pdf#97.5	Penn Treebank, F1	Out - of - Domain Enhanced ( F% )  Table 3: Performance of the baseline classifier and  the classifier enhanced with natural annotations in  Chinese wikipedia.
true	P13-1075.pdf#97.1	Penn Treebank, F1	Out - of - Domain Enhanced ( F% )  Table 3: Performance of the baseline classifier and  the classifier enhanced with natural annotations in  Chinese wikipedia.
true	P13-1075.pdf#97.3	Penn Treebank, F1	Out - of - Domain Enhanced ( F% )  Table 3: Performance of the baseline classifier and  the classifier enhanced with natural annotations in  Chinese wikipedia.
true	P13-1075.pdf#97.4	Penn Treebank, F1	Out - of - Domain Enhanced ( F% )  Table 3: Performance of the baseline classifier and  the classifier enhanced with natural annotations in  Chinese wikipedia.
true	P13-1075.pdf#97.5	Penn Treebank, F1	Out - of - Domain Enhanced ( F% )  Table 3. On the  CTB testing set, training data from the Chinese
true	P13-1075.pdf#97.2	Penn Treebank, F1	Out - of - Domain Enhanced ( F% )  Table 3. On the  CTB testing set, training data from the Chinese
true	P13-1075.pdf#97.4	Penn Treebank, F1	Out - of - Domain Enhanced ( F% )  Table 3. On the  CTB testing set, training data from the Chinese
true	P13-1075.pdf#97.1	Penn Treebank, F1	Out - of - Domain Enhanced ( F% )  Table 3. On the  CTB testing set, training data from the Chinese
true	P13-1075.pdf#97.3	Penn Treebank, F1	Out - of - Domain Enhanced ( F% )  Table 3. On the  CTB testing set, training data from the Chinese
true	P14-2042.pdf#4	Chinese Treebank 6, F1	Figure 1 . A Word - character hybrid lattice of a Chinese sentence . Correct path is represented by blue  Table 3. Word representation with a 6-tag tagset: S, B, B 2 , B 3 , M, E
true	P14-2042.pdf#6	Chinese Treebank 6, F1	Figure 1 . A Word - character hybrid lattice of a Chinese sentence . Correct path is represented by blue  Table 3. Word representation with a 6-tag tagset: S, B, B 2 , B 3 , M, E
true	P14-2042.pdf#1	Chinese Treebank 6, F1	Figure 1 . A Word - character hybrid lattice of a Chinese sentence . Correct path is represented by blue  Table 3. Word representation with a 6-tag tagset: S, B, B 2 , B 3 , M, E
true	P14-2042.pdf#3	Chinese Treebank 6, F1	Figure 1 . A Word - character hybrid lattice of a Chinese sentence . Correct path is represented by blue  Table 3. Word representation with a 6-tag tagset: S, B, B 2 , B 3 , M, E
true	P14-2042.pdf#7ormore	Chinese Treebank 6, F1	Figure 1 . A Word - character hybrid lattice of a Chinese sentence . Correct path is represented by blue  Table 3. Word representation with a 6-tag tagset: S, B, B 2 , B 3 , M, E
true	P14-2042.pdf#5	Chinese Treebank 6, F1	Figure 1 . A Word - character hybrid lattice of a Chinese sentence . Correct path is represented by blue  Table 3. Word representation with a 6-tag tagset: S, B, B 2 , B 3 , M, E
true	P14-2042.pdf#2	Chinese Treebank 6, F1	Figure 1 . A Word - character hybrid lattice of a Chinese sentence . Correct path is represented by blue  Table 3. Word representation with a 6-tag tagset: S, B, B 2 , B 3 , M, E
true	N15-1011.pdf#7.14	IMDb, Accuracy	Elec  Table 2: Error rate (%) comparison with bag-of-n-gram-
true	N15-1011.pdf#7.67	IMDb, Accuracy	IMDB  Table 2: Error rate (%) comparison with bag-of-n-gram-
true	N15-1011.pdf#9.33	IMDb, Accuracy	RCV1  Table 2: Error rate (%) comparison with bag-of-n-gram-
true	N15-1011.pdf#64.8	IMDb, Accuracy	macro - F  Table 4: RCV1 micro-averaged and macro-averaged F-
true	N15-1011.pdf#84.0	IMDb, Accuracy	micro - F  Table 4: RCV1 micro-averaged and macro-averaged F-
true	D14-1027.pdf#0.240	WMT 2014 EN-DE, BLEU	Structured Kernel Learning es - en  Table 1: Kendall's (τ ) correlation with human judgements on WMT12 for each language pair.
true	D14-1027.pdf#0.210	WMT 2014 EN-DE, BLEU	Structured Kernel Learning cs - en  Table 1: Kendall's (τ ) correlation with human judgements on WMT12 for each language pair.
true	D14-1027.pdf#0.251	WMT 2014 EN-DE, BLEU	Structured Kernel Learning de - en  Table 1: Kendall's (τ ) correlation with human judgements on WMT12 for each language pair.
true	D14-1027.pdf#0.231	WMT 2014 EN-DE, BLEU	Structured Kernel Learning all  Table 1: Kendall's (τ ) correlation with human judgements on WMT12 for each language pair.
true	D14-1027.pdf#0.223	WMT 2014 EN-DE, BLEU	Structured Kernel Learning fr - en  Table 1: Kendall's (τ ) correlation with human judgements on WMT12 for each language pair.
true	D14-1027.pdf#0.237	WMT 2014 EN-DE, BLEU	Testing all  Table 2: Kendall's (τ ) on WMT12 for cross- language training with DIS+SYN.
true	D14-1027.pdf#0.258	WMT 2014 EN-DE, BLEU	Testing de - en  Table 2: Kendall's (τ ) on WMT12 for cross- language training with DIS+SYN.
true	D14-1027.pdf#0.231	WMT 2014 EN-DE, BLEU	Testing cs - en  Table 2: Kendall's (τ ) on WMT12 for cross- language training with DIS+SYN.
true	D14-1027.pdf#0.232	WMT 2014 EN-DE, BLEU	Testing fr - en  Table 2: Kendall's (τ ) on WMT12 for cross- language training with DIS+SYN.
true	D14-1027.pdf#0.240	WMT 2014 EN-DE, BLEU	Testing es - en  Table 2: Kendall's (τ ) on WMT12 for cross- language training with DIS+SYN.
true	D14-1027.pdf#0.240	WMT 2014 EN-FR, BLEU	Structured Kernel Learning es - en  Table 1: Kendall's (τ ) correlation with human judgements on WMT12 for each language pair.
true	D14-1027.pdf#0.210	WMT 2014 EN-FR, BLEU	Structured Kernel Learning cs - en  Table 1: Kendall's (τ ) correlation with human judgements on WMT12 for each language pair.
true	D14-1027.pdf#0.251	WMT 2014 EN-FR, BLEU	Structured Kernel Learning de - en  Table 1: Kendall's (τ ) correlation with human judgements on WMT12 for each language pair.
true	D14-1027.pdf#0.231	WMT 2014 EN-FR, BLEU	Structured Kernel Learning all  Table 1: Kendall's (τ ) correlation with human judgements on WMT12 for each language pair.
true	D14-1027.pdf#0.223	WMT 2014 EN-FR, BLEU	Structured Kernel Learning fr - en  Table 1: Kendall's (τ ) correlation with human judgements on WMT12 for each language pair.
true	D14-1027.pdf#0.237	WMT 2014 EN-FR, BLEU	Testing all  Table 2: Kendall's (τ ) on WMT12 for cross- language training with DIS+SYN.
true	D14-1027.pdf#0.258	WMT 2014 EN-FR, BLEU	Testing de - en  Table 2: Kendall's (τ ) on WMT12 for cross- language training with DIS+SYN.
true	D14-1027.pdf#0.231	WMT 2014 EN-FR, BLEU	Testing cs - en  Table 2: Kendall's (τ ) on WMT12 for cross- language training with DIS+SYN.
true	D14-1027.pdf#0.232	WMT 2014 EN-FR, BLEU	Testing fr - en  Table 2: Kendall's (τ ) on WMT12 for cross- language training with DIS+SYN.
true	D14-1027.pdf#0.240	WMT 2014 EN-FR, BLEU	Testing es - en  Table 2: Kendall's (τ ) on WMT12 for cross- language training with DIS+SYN.
true	D14-1176.pdf#47.30	Text8, Number of params	, − MT08+MT09 BLEU  Table 2: BLEU and TER scores for Arabic-English experiments. Statistically significant improvements  over the baseline (a) are marked at the p < .01 level and at the p < .05 level. Additionally, ·, and  ·, indicate significant improvements with respect to BiLM Lex•Lex (b). Since TER is an error rate, lower  scores are better.
true	D14-1176.pdf#43.45	Text8, Number of params	MT09 TER  Table 2: BLEU and TER scores for Arabic-English experiments. Statistically significant improvements  over the baseline (a) are marked at the p < .01 level and at the p < .05 level. Additionally, ·, and  ·, indicate significant improvements with respect to BiLM Lex•Lex (b). Since TER is an error rate, lower  scores are better.
true	D14-1176.pdf#49.30	Text8, Number of params	, MT09 BLEU  Table 2: BLEU and TER scores for Arabic-English experiments. Statistically significant improvements  over the baseline (a) are marked at the p < .01 level and at the p < .05 level. Additionally, ·, and  ·, indicate significant improvements with respect to BiLM Lex•Lex (b). Since TER is an error rate, lower  scores are better.
true	D14-1176.pdf#47.17	Text8, Number of params	MT08 TER  Table 2: BLEU and TER scores for Arabic-English experiments. Statistically significant improvements  over the baseline (a) are marked at the p < .01 level and at the p < .05 level. Additionally, ·, and  ·, indicate significant improvements with respect to BiLM Lex•Lex (b). Since TER is an error rate, lower  scores are better.
true	D14-1176.pdf#45.40	Text8, Number of params	MT08+MT09 TER  Table 2: BLEU and TER scores for Arabic-English experiments. Statistically significant improvements  over the baseline (a) are marked at the p < .01 level and at the p < .05 level. Additionally, ·, and  ·, indicate significant improvements with respect to BiLM Lex•Lex (b). Since TER is an error rate, lower  scores are better.
true	D14-1176.pdf#45.68	Text8, Number of params	, − MT08 BLEU  Table 2: BLEU and TER scores for Arabic-English experiments. Statistically significant improvements  over the baseline (a) are marked at the p < .01 level and at the p < .05 level. Additionally, ·, and  ·, indicate significant improvements with respect to BiLM Lex•Lex (b). Since TER is an error rate, lower  scores are better.
true	D14-1176.pdf#47.38	Text8, Number of params	, MT08+MT09 BLEU  Table 2: BLEU and TER scores for Arabic-English experiments. Statistically significant improvements  over the baseline (a) are marked at the p < .01 level and at the p < .05 level. Additionally, ·, and  ·, indicate significant improvements with respect to BiLM Lex•Lex (b). Since TER is an error rate, lower  scores are better.
true	D14-1176.pdf#59.79	Text8, Number of params	, − MT08 TER  Table 4: BLEU and TER scores for Chinese-English PBSMT baseline and BiLM pipelines. See Table 2  for the notation regarding statistical significance.
true	D14-1176.pdf#56.63	Text8, Number of params	, MT06 TER  Table 4: BLEU and TER scores for Chinese-English PBSMT baseline and BiLM pipelines. See Table 2  for the notation regarding statistical significance.
true	D14-1176.pdf#59.47	Text8, Number of params	, MT08 TER  Table 4: BLEU and TER scores for Chinese-English PBSMT baseline and BiLM pipelines. See Table 2  for the notation regarding statistical significance.
true	D14-1176.pdf#30.05	Text8, Number of params	, MT06+MT08 BLEU  Table 4: BLEU and TER scores for Chinese-English PBSMT baseline and BiLM pipelines. See Table 2  for the notation regarding statistical significance.
true	D14-1176.pdf#33.43	Text8, Number of params	, − MT06 BLEU  Table 4: BLEU and TER scores for Chinese-English PBSMT baseline and BiLM pipelines. See Table 2  for the notation regarding statistical significance.
true	D14-1176.pdf#26.50	Text8, Number of params	, − MT08 BLEU  Table 4: BLEU and TER scores for Chinese-English PBSMT baseline and BiLM pipelines. See Table 2  for the notation regarding statistical significance.
true	D14-1176.pdf#57.95	Text8, Number of params	, MT06+MT08 TER  Table 4: BLEU and TER scores for Chinese-English PBSMT baseline and BiLM pipelines. See Table 2  for the notation regarding statistical significance.
true	D14-1176.pdf#30.28	Text8, Number of params	, − MT06+MT08 BLEU  Table 4: BLEU and TER scores for Chinese-English PBSMT baseline and BiLM pipelines. See Table 2  for the notation regarding statistical significance.
true	D14-1176.pdf#15.87	Text8, Number of params	Ch - En MT06  Table 6: 4-gram precision scores for Arabic-English and Chinese-English baseline and BiLM systems.
true	D14-1176.pdf#28.90	Text8, Number of params	Ar - En MT08+MT09  Table 6: 4-gram precision scores for Arabic-English and Chinese-English baseline and BiLM systems.
true	D14-1176.pdf#13.77	Text8, Number of params	Ch - En MT06+MT08  Table 6: 4-gram precision scores for Arabic-English and Chinese-English baseline and BiLM systems.
true	D14-1176.pdf#14.07	Text8, Number of params	Ch - En MT06+MT08  Table 6: 4-gram precision scores for Arabic-English and Chinese-English baseline and BiLM systems.
true	D14-1176.pdf#11.70	Text8, Number of params	Ch - En MT08  Table 6: 4-gram precision scores for Arabic-English and Chinese-English baseline and BiLM systems.
true	D14-1176.pdf#31.27	Text8, Number of params	Ar - En MT09  Table 6: 4-gram precision scores for Arabic-English and Chinese-English baseline and BiLM systems.
true	D14-1176.pdf#15.42	Text8, Number of params	Ch - En MT06  Table 6: 4-gram precision scores for Arabic-English and Chinese-English baseline and BiLM systems.
true	D14-1176.pdf#11.85	Text8, Number of params	Ch - En MT08  Table 6: 4-gram precision scores for Arabic-English and Chinese-English baseline and BiLM systems.
true	D14-1176.pdf#26.91	Text8, Number of params	Ar - En MT08  Table 6: 4-gram precision scores for Arabic-English and Chinese-English baseline and BiLM systems.
true	D15-1209.pdf#0.583	WMT 2014 EN-DE, BLEU	WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.677	WMT 2014 EN-DE, BLEU	WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.665	WMT 2014 EN-DE, BLEU	WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.471	WMT 2014 EN-DE, BLEU	Japanese - English ( Kyoto Free Translation ) WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.689	WMT 2014 EN-DE, BLEU	WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.535	WMT 2014 EN-DE, BLEU	Japanese - English ( Kyoto Free Translation ) WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.461	WMT 2014 EN-DE, BLEU	WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.609	WMT 2014 EN-DE, BLEU	Japanese - English ( Kyoto Free Translation ) WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.612	WMT 2014 EN-DE, BLEU	WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.575	WMT 2014 EN-DE, BLEU	Japanese - English ( Kyoto Free Translation ) WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.648	WMT 2014 EN-DE, BLEU	Japanese - English ( Kyoto Free Translation ) WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.687	WMT 2014 EN-DE, BLEU	Japanese - English ( Kyoto Free Translation ) WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.774	WMT 2014 EN-DE, BLEU	WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.626	WMT 2014 EN-DE, BLEU	Japanese - English ( Kyoto Free Translation ) WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.628	WMT 2014 EN-DE, BLEU	Japanese - English ( Kyoto Free Translation ) WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.553	WMT 2014 EN-DE, BLEU	WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.618	WMT 2014 EN-DE, BLEU	Japanese - English ( Kyoto Free Translation ) WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.756	WMT 2014 EN-DE, BLEU	WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.693	WMT 2014 EN-DE, BLEU	Word Alignment Quality Leave - one - out ( prop . ) P  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.534	WMT 2014 EN-DE, BLEU	Word Alignment Quality Kneser - Ney Smooth . F1  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.678	WMT 2014 EN-DE, BLEU	Word Alignment Quality Leave - one - out ( prop . ) F1  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.501	WMT 2014 EN-DE, BLEU	Word Alignment Quality Kneser - Ney Smooth . R  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#32.70±	WMT 2014 EN-DE, BLEU	Translation Quality ( BLEU ) Leave - one - out ( prop . ) F1  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.722	WMT 2014 EN-DE, BLEU	Word Alignment Quality Leave - one - out ( prop . ) P  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.498	WMT 2014 EN-DE, BLEU	Word Alignment Quality Kneser - Ney Smooth . R  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.707	WMT 2014 EN-DE, BLEU	Word Alignment Quality Leave - one - out ( prop . ) P  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.522	WMT 2014 EN-DE, BLEU	Word Alignment Quality Kneser - Ney Smooth . F1  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.14	WMT 2014 EN-DE, BLEU	Translation Quality ( BLEU ) Leave - one - out ( prop . ) P  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.541	WMT 2014 EN-DE, BLEU	Word Alignment Quality Kneser - Ney Smooth . F1  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.615	WMT 2014 EN-DE, BLEU	Word Alignment Quality Leave - one - out ( prop . ) R  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.08	WMT 2014 EN-DE, BLEU	Translation Quality ( BLEU ) Leave - one - out ( prop . ) P  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.515	WMT 2014 EN-DE, BLEU	Word Alignment Quality Kneser - Ney Smooth . F1  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.755	WMT 2014 EN-DE, BLEU	Word Alignment Quality Leave - one - out ( prop . ) P  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.501	WMT 2014 EN-DE, BLEU	Word Alignment Quality Kneser - Ney Smooth . R  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.499	WMT 2014 EN-DE, BLEU	Word Alignment Quality Kneser - Ney Smooth . R  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#32.04±	WMT 2014 EN-DE, BLEU	Word Alignment Quality Leave - one - out ( prop . ) F1  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.733	WMT 2014 EN-DE, BLEU	Word Alignment Quality Leave - one - out ( prop . ) P  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.583	WMT 2014 EN-FR, BLEU	WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.677	WMT 2014 EN-FR, BLEU	WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.665	WMT 2014 EN-FR, BLEU	WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.471	WMT 2014 EN-FR, BLEU	Japanese - English ( Kyoto Free Translation ) WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.689	WMT 2014 EN-FR, BLEU	WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.535	WMT 2014 EN-FR, BLEU	Japanese - English ( Kyoto Free Translation ) WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.461	WMT 2014 EN-FR, BLEU	WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.609	WMT 2014 EN-FR, BLEU	Japanese - English ( Kyoto Free Translation ) WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.612	WMT 2014 EN-FR, BLEU	WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.575	WMT 2014 EN-FR, BLEU	Japanese - English ( Kyoto Free Translation ) WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.648	WMT 2014 EN-FR, BLEU	Japanese - English ( Kyoto Free Translation ) WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.687	WMT 2014 EN-FR, BLEU	Japanese - English ( Kyoto Free Translation ) WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.774	WMT 2014 EN-FR, BLEU	WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.626	WMT 2014 EN-FR, BLEU	Japanese - English ( Kyoto Free Translation ) WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.628	WMT 2014 EN-FR, BLEU	Japanese - English ( Kyoto Free Translation ) WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.553	WMT 2014 EN-FR, BLEU	WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.618	WMT 2014 EN-FR, BLEU	Japanese - English ( Kyoto Free Translation ) WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.756	WMT 2014 EN-FR, BLEU	WA , OpenMT )  Table 3: Word alignment accuracy measured by F 1 , precision and recall.
true	D15-1209.pdf#0.693	WMT 2014 EN-FR, BLEU	Word Alignment Quality Leave - one - out ( prop . ) P  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.534	WMT 2014 EN-FR, BLEU	Word Alignment Quality Kneser - Ney Smooth . F1  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.678	WMT 2014 EN-FR, BLEU	Word Alignment Quality Leave - one - out ( prop . ) F1  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.501	WMT 2014 EN-FR, BLEU	Word Alignment Quality Kneser - Ney Smooth . R  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#32.70±	WMT 2014 EN-FR, BLEU	Translation Quality ( BLEU ) Leave - one - out ( prop . ) F1  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.722	WMT 2014 EN-FR, BLEU	Word Alignment Quality Leave - one - out ( prop . ) P  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.498	WMT 2014 EN-FR, BLEU	Word Alignment Quality Kneser - Ney Smooth . R  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.707	WMT 2014 EN-FR, BLEU	Word Alignment Quality Leave - one - out ( prop . ) P  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.522	WMT 2014 EN-FR, BLEU	Word Alignment Quality Kneser - Ney Smooth . F1  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.14	WMT 2014 EN-FR, BLEU	Translation Quality ( BLEU ) Leave - one - out ( prop . ) P  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.541	WMT 2014 EN-FR, BLEU	Word Alignment Quality Kneser - Ney Smooth . F1  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.615	WMT 2014 EN-FR, BLEU	Word Alignment Quality Leave - one - out ( prop . ) R  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.08	WMT 2014 EN-FR, BLEU	Translation Quality ( BLEU ) Leave - one - out ( prop . ) P  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.515	WMT 2014 EN-FR, BLEU	Word Alignment Quality Kneser - Ney Smooth . F1  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.755	WMT 2014 EN-FR, BLEU	Word Alignment Quality Leave - one - out ( prop . ) P  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.501	WMT 2014 EN-FR, BLEU	Word Alignment Quality Kneser - Ney Smooth . R  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.499	WMT 2014 EN-FR, BLEU	Word Alignment Quality Kneser - Ney Smooth . R  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#32.04±	WMT 2014 EN-FR, BLEU	Word Alignment Quality Leave - one - out ( prop . ) F1  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	D15-1209.pdf#0.733	WMT 2014 EN-FR, BLEU	Word Alignment Quality Leave - one - out ( prop . ) P  Table 7: Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s
true	P14-2009.pdf#65.9	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Macro - F1  Table 1: Evaluation results on target-dependent  Twitter sentiment classification dataset. Our ap- proach outperforms the baseline methods.
true	P14-2009.pdf#66.3	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Macro - F1  Table 1: Evaluation results on target-dependent  Twitter sentiment classification dataset. Our ap- proach outperforms the baseline methods.
true	D15-1237.pdf#0.7633	SearchQA, Unigram Acc	QASENT MRR  Table 4: Baseline results on both QASENT and  WIKIQA datasets. Questions without correct an- swers in the candidate sentences are removed in  the WIKIQA dataset. The best results are in bold.
true	D15-1237.pdf#0.6520	SearchQA, Unigram Acc	WIKIQA MAP  Table 4: Baseline results on both QASENT and  WIKIQA datasets. Questions without correct an- swers in the candidate sentences are removed in  the WIKIQA dataset. The best results are in bold.
true	D15-1237.pdf#0.6652	SearchQA, Unigram Acc	WIKIQA MRR  Table 4: Baseline results on both QASENT and  WIKIQA datasets. Questions without correct an- swers in the candidate sentences are removed in  the WIKIQA dataset. The best results are in bold.
true	D15-1237.pdf#0.6954	SearchQA, Unigram Acc	QASENT MAP  Table 4: Baseline results on both QASENT and  WIKIQA datasets. Questions without correct an- swers in the candidate sentences are removed in  the WIKIQA dataset. The best results are in bold.
true	D11-1133.pdf#0.10	New York Times Corpus, P@10%	Pharmaceuticals of East Hanover , N . J . Sporanox is made by Janssen Pharmaceutica Inc . , Recall C  Table 8: Extraction Results on the 102 Document Test Set Annotated for All Instances of the Relations
true	D11-1133.pdf#0.13	New York Times Corpus, P@10%	Pharmaceuticals of East Hanover , N . J . Sporanox is made by Janssen Pharmaceutica Inc . , Precision HW  Table 8: Extraction Results on the 102 Document Test Set Annotated for All Instances of the Relations
true	D11-1133.pdf#0.15	New York Times Corpus, P@10%	Pharmaceuticals of East Hanover , N . J . Sporanox is made by Janssen Pharmaceutica Inc . , Recall C  Table 8: Extraction Results on the 102 Document Test Set Annotated for All Instances of the Relations
true	D11-1133.pdf#0.41	New York Times Corpus, P@10%	Pharmaceuticals of East Hanover , N . J . Sporanox is made by Janssen Pharmaceutica Inc . , Precision C  Table 8: Extraction Results on the 102 Document Test Set Annotated for All Instances of the Relations
true	D11-1133.pdf#0.31	New York Times Corpus, P@10%	Pharmaceuticals of East Hanover , N . J . Sporanox is made by Janssen Pharmaceutica Inc . , Precision HW  Table 8: Extraction Results on the 102 Document Test Set Annotated for All Instances of the Relations
true	D11-1133.pdf#0.21	New York Times Corpus, P@10%	Pharmaceuticals of East Hanover , N . J . Sporanox is made by Janssen Pharmaceutica Inc . , F C  Table 8: Extraction Results on the 102 Document Test Set Annotated for All Instances of the Relations
true	D11-1133.pdf#0.34	New York Times Corpus, P@10%	Pharmaceuticals of East Hanover , N . J . Sporanox is made by Janssen Pharmaceutica Inc . , Precision C  Table 8: Extraction Results on the 102 Document Test Set Annotated for All Instances of the Relations
true	D11-1133.pdf#0.17	New York Times Corpus, P@10%	Pharmaceuticals of East Hanover , N . J . Sporanox is made by Janssen Pharmaceutica Inc . , F C  Table 8: Extraction Results on the 102 Document Test Set Annotated for All Instances of the Relations
true	D11-1133.pdf#0.11	New York Times Corpus, P@10%	Pharmaceuticals of East Hanover , N . J . Sporanox is made by Janssen Pharmaceutica Inc . , F HW  Table 8: Extraction Results on the 102 Document Test Set Annotated for All Instances of the Relations
true	D15-1158.pdf#80.50	Penn Treebank, UAS	Email  Table 2: Parsing accuracies (UAS, excluding puctuation) of the different models on various corpora. All models are trained
true	D15-1158.pdf#87.29	Penn Treebank, UAS	Brown  Table 2: Parsing accuracies (UAS, excluding puctuation) of the different models on various corpora. All models are trained
true	D15-1158.pdf#82.58	Penn Treebank, UAS	Answers  Table 2: Parsing accuracies (UAS, excluding puctuation) of the different models on various corpora. All models are trained
true	D15-1158.pdf#92.61	Penn Treebank, UAS	Dev  Table 2: Parsing accuracies (UAS, excluding puctuation) of the different models on various corpora. All models are trained
true	D15-1158.pdf#87.42	Penn Treebank, UAS	Brown  Table 2: Parsing accuracies (UAS, excluding puctuation) of the different models on various corpora. All models are trained
true	D15-1158.pdf#83.78	Penn Treebank, UAS	News  Table 2: Parsing accuracies (UAS, excluding puctuation) of the different models on various corpora. All models are trained
true	D15-1158.pdf#84.26	Penn Treebank, UAS	Reviews  Table 2: Parsing accuracies (UAS, excluding puctuation) of the different models on various corpora. All models are trained
true	D15-1158.pdf#92.70	Penn Treebank, UAS	Dev  Table 2: Parsing accuracies (UAS, excluding puctuation) of the different models on various corpora. All models are trained
true	D15-1158.pdf#85.88	Penn Treebank, UAS	Blogs  Table 2: Parsing accuracies (UAS, excluding puctuation) of the different models on various corpora. All models are trained
true	D15-1158.pdf#84.18	Penn Treebank, UAS	Reviews  Table 2: Parsing accuracies (UAS, excluding puctuation) of the different models on various corpora. All models are trained
true	D15-1158.pdf#92.32	Penn Treebank, UAS	Test  Table 2: Parsing accuracies (UAS, excluding puctuation) of the different models on various corpora. All models are trained
true	D15-1158.pdf#92.45	Penn Treebank, UAS	Test  Table 2: Parsing accuracies (UAS, excluding puctuation) of the different models on various corpora. All models are trained
true	D15-1158.pdf#80.72	Penn Treebank, UAS	Email  Table 2: Parsing accuracies (UAS, excluding puctuation) of the different models on various corpora. All models are trained
true	D15-1158.pdf#86.08	Penn Treebank, UAS	Blogs  Table 2: Parsing accuracies (UAS, excluding puctuation) of the different models on various corpora. All models are trained
true	D15-1158.pdf#82.74	Penn Treebank, UAS	Answers  Table 2: Parsing accuracies (UAS, excluding puctuation) of the different models on various corpora. All models are trained
true	D15-1158.pdf#83.57	Penn Treebank, UAS	News  Table 2: Parsing accuracies (UAS, excluding puctuation) of the different models on various corpora. All models are trained
true	P14-2120.pdf#81.0	New York Times Corpus, P@10%	Table 3: Accuracy of our method, followed by  baselines and ablation tests.
true	P14-2120.pdf#50.9	New York Times Corpus, P@10%	Recall  Table 4: RTE-6 Experiment
true	P14-2120.pdf#46.8	New York Times Corpus, P@10%	F1 %  Table 4: RTE-6 Experiment
true	P14-2120.pdf#44.3	New York Times Corpus, P@10%	Table 4: RTE-6 Experiment
true	P13-2068.pdf#23.98	Text8, Number of params	MT08  Table 2: BLEU scores with various lexical co- hesion devices on the test sets MT06 and MT08.  "Base" is the traditonal hierarchical system, "Avg"  is the average BLEU score on the two test sets.
true	P13-2068.pdf#31.47	Text8, Number of params	MT06  Table 2: BLEU scores with various lexical co- hesion devices on the test sets MT06 and MT08.  "Base" is the traditonal hierarchical system, "Avg"  is the average BLEU score on the two test sets.
true	P13-2068.pdf#27.73	Text8, Number of params	Avg  Table 2: BLEU scores with various lexical co- hesion devices on the test sets MT06 and MT08.  "Base" is the traditonal hierarchical system, "Avg"  is the average BLEU score on the two test sets.
true	P13-2068.pdf#23.98	Text8, Bit per Character (BPC)	MT08  Table 2: BLEU scores with various lexical co- hesion devices on the test sets MT06 and MT08.  "Base" is the traditonal hierarchical system, "Avg"  is the average BLEU score on the two test sets.
true	P13-2068.pdf#31.47	Text8, Bit per Character (BPC)	MT06  Table 2: BLEU scores with various lexical co- hesion devices on the test sets MT06 and MT08.  "Base" is the traditonal hierarchical system, "Avg"  is the average BLEU score on the two test sets.
true	P13-2068.pdf#27.73	Text8, Bit per Character (BPC)	Avg  Table 2: BLEU scores with various lexical co- hesion devices on the test sets MT06 and MT08.  "Base" is the traditonal hierarchical system, "Avg"  is the average BLEU score on the two test sets.
true	D14-1133.pdf#43.4	WMT 2014 EN-FR, BLEU	of iterations and beam sizes . table and the number BLEU size  Table 2: Results on Medical for different training configurations, rewriting phrase tables and beam  sizes. opti denotes our optimal configuration for rewriter.
true	D14-1133.pdf#43.7	WMT 2014 EN-FR, BLEU	of iterations and beam sizes . table and the number BLEU size  Table 2: Results on Medical for different training configurations, rewriting phrase tables and beam  sizes. opti denotes our optimal configuration for rewriter.
true	D14-1133.pdf#40.4	WMT 2014 EN-FR, BLEU	of iterations and beam sizes . table and the number TER size  Table 2: Results on Medical for different training configurations, rewriting phrase tables and beam  sizes. opti denotes our optimal configuration for rewriter.
true	D14-1133.pdf#40.1	WMT 2014 EN-FR, BLEU	of iterations and beam sizes . table and the number TER size  Table 2: Results on Medical for different training configurations, rewriting phrase tables and beam  sizes. opti denotes our optimal configuration for rewriter.
true	D14-1133.pdf#39.2	WMT 2014 EN-FR, BLEU	test TER  Table 3: Results for the semi-oracle using opti.
true	D14-1133.pdf#44.9	WMT 2014 EN-FR, BLEU	test BLEU  Table 3: Results for the semi-oracle using opti.
true	D14-1133.pdf#44.9	WMT 2014 EN-FR, BLEU	test BLEU  Table 3: Results for the semi-oracle using opti.
true	D14-1133.pdf#39.0	WMT 2014 EN-FR, BLEU	test TER  Table 3: Results for the semi-oracle using opti.
true	D14-1133.pdf#33.7(+0.9)	WMT 2014 EN-FR, BLEU	en - fr BLEU  Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks.
true	D14-1133.pdf#34.2(+1.4)	WMT 2014 EN-FR, BLEU	en - fr BLEU  Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks.
true	D14-1133.pdf#34.1	WMT 2014 EN-FR, BLEU	fr - en BLEU  Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks.
true	D14-1133.pdf#49.3(−0.1)	WMT 2014 EN-FR, BLEU	en - fr TER  Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks.
true	D14-1133.pdf#47.4	WMT 2014 EN-FR, BLEU	fr - en TER  Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks.
true	D14-1133.pdf#48.6(−0.8)	WMT 2014 EN-FR, BLEU	en - fr TER  Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks.
true	D14-1133.pdf#33.4	WMT 2014 EN-FR, BLEU	fr - en BLEU  Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks.
true	D14-1133.pdf#46.6	WMT 2014 EN-FR, BLEU	fr - en TER  Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks.
true	D14-1133.pdf#43.4	WMT 2014 EN-DE, BLEU	of iterations and beam sizes . table and the number BLEU size  Table 2: Results on Medical for different training configurations, rewriting phrase tables and beam  sizes. opti denotes our optimal configuration for rewriter.
true	D14-1133.pdf#43.7	WMT 2014 EN-DE, BLEU	of iterations and beam sizes . table and the number BLEU size  Table 2: Results on Medical for different training configurations, rewriting phrase tables and beam  sizes. opti denotes our optimal configuration for rewriter.
true	D14-1133.pdf#40.4	WMT 2014 EN-DE, BLEU	of iterations and beam sizes . table and the number TER size  Table 2: Results on Medical for different training configurations, rewriting phrase tables and beam  sizes. opti denotes our optimal configuration for rewriter.
true	D14-1133.pdf#40.1	WMT 2014 EN-DE, BLEU	of iterations and beam sizes . table and the number TER size  Table 2: Results on Medical for different training configurations, rewriting phrase tables and beam  sizes. opti denotes our optimal configuration for rewriter.
true	D14-1133.pdf#39.2	WMT 2014 EN-DE, BLEU	test TER  Table 3: Results for the semi-oracle using opti.
true	D14-1133.pdf#44.9	WMT 2014 EN-DE, BLEU	test BLEU  Table 3: Results for the semi-oracle using opti.
true	D14-1133.pdf#44.9	WMT 2014 EN-DE, BLEU	test BLEU  Table 3: Results for the semi-oracle using opti.
true	D14-1133.pdf#39.0	WMT 2014 EN-DE, BLEU	test TER  Table 3: Results for the semi-oracle using opti.
true	D14-1133.pdf#33.7(+0.9)	WMT 2014 EN-DE, BLEU	en - fr BLEU  Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks.
true	D14-1133.pdf#34.2(+1.4)	WMT 2014 EN-DE, BLEU	en - fr BLEU  Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks.
true	D14-1133.pdf#34.1	WMT 2014 EN-DE, BLEU	fr - en BLEU  Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks.
true	D14-1133.pdf#49.3(−0.1)	WMT 2014 EN-DE, BLEU	en - fr TER  Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks.
true	D14-1133.pdf#47.4	WMT 2014 EN-DE, BLEU	fr - en TER  Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks.
true	D14-1133.pdf#48.6(−0.8)	WMT 2014 EN-DE, BLEU	en - fr TER  Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks.
true	D14-1133.pdf#33.4	WMT 2014 EN-DE, BLEU	fr - en BLEU  Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks.
true	D14-1133.pdf#46.6	WMT 2014 EN-DE, BLEU	fr - en TER  Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks.
true	N15-1071.pdf#0.05/p<	SST-2, Accuracy	supervised Node Acc . cross - lingual  Table 1: Filtering out objective phrases
true	N15-1071.pdf#0.850**	SST-2, Accuracy	supervised Node Acc .  Table 1: Filtering out objective phrases
true	N15-1071.pdf#0.005)	SST-2, Accuracy	supervised Root Acc . cross - lingual  Table 1: Filtering out objective phrases
true	N15-1071.pdf#0.774**	SST-2, Accuracy	supervised Root Acc .  Table 1: Filtering out objective phrases
true	N15-1071.pdf#0.850**	SST-2, Accuracy	supervised Node Acc .  Table 2: HeiST baseline, cross-lingual projection, SVM.
true	N15-1071.pdf#0.774**	SST-2, Accuracy	supervised Root Acc .  Table 2: HeiST baseline, cross-lingual projection, SVM.
true	N15-1071.pdf#0.005)	SST-2, Accuracy	supervised Root Acc .  Table 2: HeiST baseline, cross-lingual projection, SVM.
true	N15-1071.pdf#0.05/p<	SST-2, Accuracy	supervised Node Acc .  Table 2: HeiST baseline, cross-lingual projection, SVM.
true	N15-1071.pdf#0.807	SST-2, Accuracy	Vote - and - flip Node Acc .  Table 4: Lexicon-based phrase labeling
true	N15-1071.pdf#0.711	SST-2, Accuracy	Vote - only Root Acc .  Table 4: Lexicon-based phrase labeling
true	N15-1071.pdf#0.680	SST-2, Accuracy	Vote - and - flip Root Acc .  Table 4: Lexicon-based phrase labeling
true	N15-1071.pdf#0.815	SST-2, Accuracy	Vote - only Node Acc .  Table 4: Lexicon-based phrase labeling
true	N15-1071.pdf#0.815	SST-2, Accuracy	Vote - only Node Acc .  Table 4: Lexicon-based phrase labeling
true	N15-1071.pdf#0.730	SST-2, Accuracy	RNTN + split movie - review clusters Root Acc .  Table 5: Incorporating additional information
true	N15-1071.pdf#0.689	SST-2, Accuracy	RNTN + split movie - review clusters Root Acc .  Table 5: Incorporating additional information
true	N15-1071.pdf#0.677	SST-2, Accuracy	RNTN + clusters Root Acc .  Table 5: Incorporating additional information
true	N15-1071.pdf#0.780	SST-2, Accuracy	RNTN + clusters Node Acc .  Table 5: Incorporating additional information
true	N15-1071.pdf#0.807	SST-2, Accuracy	RNTN + split movie - review clusters Node Acc .  Table 5: Incorporating additional information
true	N15-1071.pdf#0.844	SST-2, Accuracy	RNTN + split movie - review clusters Node Acc .  Table 5: Incorporating additional information
true	N15-1071.pdf#0.05/p<	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	supervised Node Acc . cross - lingual  Table 1: Filtering out objective phrases
true	N15-1071.pdf#0.850**	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	supervised Node Acc .  Table 1: Filtering out objective phrases
true	N15-1071.pdf#0.005)	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	supervised Root Acc . cross - lingual  Table 1: Filtering out objective phrases
true	N15-1071.pdf#0.774**	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	supervised Root Acc .  Table 1: Filtering out objective phrases
true	N15-1071.pdf#0.850**	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	supervised Node Acc .  Table 2: HeiST baseline, cross-lingual projection, SVM.
true	N15-1071.pdf#0.774**	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	supervised Root Acc .  Table 2: HeiST baseline, cross-lingual projection, SVM.
true	N15-1071.pdf#0.005)	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	supervised Root Acc .  Table 2: HeiST baseline, cross-lingual projection, SVM.
true	N15-1071.pdf#0.05/p<	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	supervised Node Acc .  Table 2: HeiST baseline, cross-lingual projection, SVM.
true	N15-1071.pdf#0.807	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Vote - and - flip Node Acc .  Table 4: Lexicon-based phrase labeling
true	N15-1071.pdf#0.711	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Vote - only Root Acc .  Table 4: Lexicon-based phrase labeling
true	N15-1071.pdf#0.680	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Vote - and - flip Root Acc .  Table 4: Lexicon-based phrase labeling
true	N15-1071.pdf#0.815	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Vote - only Node Acc .  Table 4: Lexicon-based phrase labeling
true	N15-1071.pdf#0.815	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Vote - only Node Acc .  Table 4: Lexicon-based phrase labeling
true	N15-1071.pdf#0.730	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	RNTN + split movie - review clusters Root Acc .  Table 5: Incorporating additional information
true	N15-1071.pdf#0.689	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	RNTN + split movie - review clusters Root Acc .  Table 5: Incorporating additional information
true	N15-1071.pdf#0.677	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	RNTN + clusters Root Acc .  Table 5: Incorporating additional information
true	N15-1071.pdf#0.780	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	RNTN + clusters Node Acc .  Table 5: Incorporating additional information
true	N15-1071.pdf#0.807	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	RNTN + split movie - review clusters Node Acc .  Table 5: Incorporating additional information
true	N15-1071.pdf#0.844	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	RNTN + split movie - review clusters Node Acc .  Table 5: Incorporating additional information
true	P13-1097.pdf#2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Bridged Model : Term - Rate W×R Input :  Table 2. The algo- rithm employs document-rating, term-document  and term-rating matrices to infer the unknown  probabilities. This algorithm can be used with  both bridged or series models. Our goal is to in- fer the emotional vector for each word w that can  be obtained by the probability P(w|e). Note that,  this probability can be simply computed for the  SHEM model using P(d|e) as follows:
true	P13-1097.pdf#5	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Bridged Model : estimate the value of P ( e|w , d , r ) in Input : , e  Table 2. The algo- rithm employs document-rating, term-document  and term-rating matrices to infer the unknown  probabilities. This algorithm can be used with  both bridged or series models. Our goal is to in- fer the emotional vector for each word w that can  be obtained by the probability P(w|e). Note that,  this probability can be simply computed for the  SHEM model using P(d|e) as follows:
true	P13-1097.pdf#1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Bridged Model : Term - Rate W×R Input :  Table 2. The algo- rithm employs document-rating, term-document  and term-rating matrices to infer the unknown  probabilities. This algorithm can be used with  both bridged or series models. Our goal is to in- fer the emotional vector for each word w that can  be obtained by the probability P(w|e). Note that,  this probability can be simply computed for the  SHEM model using P(d|e) as follows:
true	P13-1097.pdf#1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Bridged Model : Term - Rate W×R Input :  Table 2. Constructing emotional vectors via P(w|e)
true	P13-1097.pdf#5	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Bridged Model : estimate the value of P ( e|w , d , r ) in Input : , e  Table 2. Constructing emotional vectors via P(w|e)
true	P13-1097.pdf#2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Bridged Model : Term - Rate W×R Input :  Table 2. Constructing emotional vectors via P(w|e)
true	P13-1097.pdf#69.39	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Precision  Table 3. Performance on SO prediction task
true	P13-1097.pdf#68.68	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F1  Table 3. Performance on SO prediction task
true	P13-1097.pdf#70.07	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Recall  Table 3. Performance on SO prediction task
true	P13-1097.pdf#67.15	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Rec .  Table 4. Performance on IQAP inference task
true	P13-1097.pdf#65.66	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F1  Table 4. Performance on IQAP inference task
true	P13-1097.pdf#66.95	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Prec .  Table 4. Performance on IQAP inference task
true	D15-1167.pdf#0.676	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2015 Accuracy  Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are  accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.671	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2014 Accuracy  Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are  accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.453	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	IMDB Accuracy  Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are  accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#2.71	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	IMDB MSE  Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are  accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.48	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2014 MSE  Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are  accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.651	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2013 Accuracy  Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are  accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.49	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2015 MSE  Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are  accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.50	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2013 MSE  Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are  accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.659	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2014 Accuracy  Table 3: Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu- racy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.430	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	IMDB Accuracy  Table 3: Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu- racy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.660	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2015 Accuracy  Table 3: Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu- racy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.56	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2013 MSE  Table 3: Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu- racy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#2.71	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	IMDB MSE  Table 3: Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu- racy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.637	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2013 Accuracy  Table 3: Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu- racy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.50	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2015 MSE  Table 3: Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu- racy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.51	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Yelp 2014 MSE  Table 3: Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu- racy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.676	IMDb, Accuracy	Yelp 2015 Accuracy  Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are  accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.671	IMDb, Accuracy	Yelp 2014 Accuracy  Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are  accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.453	IMDb, Accuracy	IMDB Accuracy  Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are  accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#2.71	IMDb, Accuracy	IMDB MSE  Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are  accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.48	IMDb, Accuracy	Yelp 2014 MSE  Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are  accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.651	IMDb, Accuracy	Yelp 2013 Accuracy  Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are  accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.49	IMDb, Accuracy	Yelp 2015 MSE  Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are  accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.50	IMDb, Accuracy	Yelp 2013 MSE  Table 2: Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are  accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.659	IMDb, Accuracy	Yelp 2014 Accuracy  Table 3: Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu- racy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.430	IMDb, Accuracy	IMDB Accuracy  Table 3: Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu- racy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.660	IMDb, Accuracy	Yelp 2015 Accuracy  Table 3: Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu- racy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.56	IMDb, Accuracy	Yelp 2013 MSE  Table 3: Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu- racy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#2.71	IMDb, Accuracy	IMDB MSE  Table 3: Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu- racy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.637	IMDb, Accuracy	Yelp 2013 Accuracy  Table 3: Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu- racy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.50	IMDb, Accuracy	Yelp 2015 MSE  Table 3: Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu- racy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D15-1167.pdf#0.51	IMDb, Accuracy	Yelp 2014 MSE  Table 3: Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu- racy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
true	D12-1050.pdf#0.35	SUBJ, Accuracy	V - Obj  Table 3: Correlation coefficients of model predictions  with subject similarity ratings (Spearman's ρ); columns  show dimensionality: fixed or varying (see Section 2.1),  composition method: + is additive vector composition,  is component-wise multiplicative vector composition,  RAE is Socher et al. (2011a)'s recursive auto-encoder.
true	D12-1050.pdf#0.50	SUBJ, Accuracy	N - N  Table 3: Correlation coefficients of model predictions  with subject similarity ratings (Spearman's ρ); columns  show dimensionality: fixed or varying (see Section 2.1),  composition method: + is additive vector composition,  is component-wise multiplicative vector composition,  RAE is Socher et al. (2011a)'s recursive auto-encoder.
true	D12-1050.pdf#0.48	SUBJ, Accuracy	Adj - N  Table 3: Correlation coefficients of model predictions  with subject similarity ratings (Spearman's ρ); columns  show dimensionality: fixed or varying (see Section 2.1),  composition method: + is additive vector composition,  is component-wise multiplicative vector composition,  RAE is Socher et al. (2011a)'s recursive auto-encoder.
true	D12-1050.pdf#81.28	SUBJ, Accuracy	NLM ( BNC ) ( con , other ) ( sub , other )  Table 4: Paraphrase classification accuracy in %. In- cluded features are in parentheses: "con" is sentence vec- tor concatenation, "sub" is sentence vector subtraction,  "other" stands for 4 other features (see Section 4)
true	D12-1050.pdf#73.51	SUBJ, Accuracy	DM ( 3 - BWC )  Table 4: Paraphrase classification accuracy in %. In- cluded features are in parentheses: "con" is sentence vec- tor concatenation, "sub" is sentence vector subtraction,  "other" stands for 4 other features (see Section 4)
true	D12-1050.pdf#82.33	SUBJ, Accuracy	SDS ( BNC ) ( other )  Table 4: Paraphrase classification accuracy in %. In- cluded features are in parentheses: "con" is sentence vec- tor concatenation, "sub" is sentence vector subtraction,  "other" stands for 4 other features (see Section 4)
true	D12-1050.pdf#82.16	SUBJ, Accuracy	DM ( 3 - BWC )  Table 4: Paraphrase classification accuracy in %. In- cluded features are in parentheses: "con" is sentence vec- tor concatenation, "sub" is sentence vector subtraction,  "other" stands for 4 other features (see Section 4)
true	D12-1050.pdf#73.04	SUBJ, Accuracy	SDS ( BNC ) ( other )  Table 4: Paraphrase classification accuracy in %. In- cluded features are in parentheses: "con" is sentence vec- tor concatenation, "sub" is sentence vector subtraction,  "other" stands for 4 other features (see Section 4)
true	D12-1050.pdf#70.26	SUBJ, Accuracy	NLM ( BNC ) ( con , other ) ( sub , other )  Table 4: Paraphrase classification accuracy in %. In- cluded features are in parentheses: "con" is sentence vec- tor concatenation, "sub" is sentence vector subtraction,  "other" stands for 4 other features (see Section 4)
true	D12-1050.pdf#82.3	SUBJ, Accuracy	F1  Table 6: Overview of results on the MSRCP (test corpus).  Accuracy differences of 3.3 or more are significant at the  0.01 level (using the χ 2 statistic).
true	D12-1050.pdf#73.0	SUBJ, Accuracy	Acc .  Table 6: Overview of results on the MSRCP (test corpus).  Accuracy differences of 3.3 or more are significant at the  0.01 level (using the χ 2 statistic).
true	D12-1050.pdf#73.5	SUBJ, Accuracy	Acc .  Table 6: Overview of results on the MSRCP (test corpus).  Accuracy differences of 3.3 or more are significant at the  0.01 level (using the χ 2 statistic).
true	D12-1050.pdf#82.2	SUBJ, Accuracy	F1  Table 6: Overview of results on the MSRCP (test corpus).  Accuracy differences of 3.3 or more are significant at the  0.01 level (using the χ 2 statistic).
true	P15-1078.pdf#29.70	WMT 2014 EN-FR, BLEU	NN using 4METRICS+ embedding vectors Kendall ' s τ AVG  Table 1: Kendall's tau (τ ) on the WMT12 dataset for various metrics. Notes: (i) the version of METEOR that took part in the
true	P15-1078.pdf#26.15	WMT 2014 EN-FR, BLEU	Kendall ' s τ AVG  Table 2: Kendall's τ on WMT12 for neural networks using BLEUCOMP, a decomposed version of BLEU. For comparison,
true	P15-1078.pdf#20.03	WMT 2014 EN-FR, BLEU	Kendall ' s τ cz  Table 2: Kendall's τ on WMT12 for neural networks using BLEUCOMP, a decomposed version of BLEU. For comparison,
true	P15-1078.pdf#24.05	WMT 2014 EN-FR, BLEU	Kendall ' s τ AVG  Table 2: Kendall's τ on WMT12 for neural networks using BLEUCOMP, a decomposed version of BLEU. For comparison,
true	P15-1078.pdf#27.07	WMT 2014 EN-FR, BLEU	Kendall ' s τ es  Table 2: Kendall's τ on WMT12 for neural networks using BLEUCOMP, a decomposed version of BLEU. For comparison,
true	P15-1078.pdf#25.95	WMT 2014 EN-FR, BLEU	Kendall ' s τ de  Table 2: Kendall's τ on WMT12 for neural networks using BLEUCOMP, a decomposed version of BLEU. For comparison,
true	P15-1078.pdf#23.16	WMT 2014 EN-FR, BLEU	Kendall ' s τ fr  Table 2: Kendall's τ on WMT12 for neural networks using BLEUCOMP, a decomposed version of BLEU. For comparison,
true	P15-1078.pdf#12.35	WMT 2014 EN-FR, BLEU	Alone  Table 3: Average Kendall's τ on WMT12 for semantic vec-
true	P15-1078.pdf#29.70	WMT 2014 EN-FR, BLEU	Comb .  Table 3: Average Kendall's τ on WMT12 for semantic vec-
true	P15-1078.pdf#29.90	WMT 2014 EN-FR, BLEU	Comb .  Table 3: Average Kendall's τ on WMT12 for semantic vec-
true	P15-1078.pdf#29.88	WMT 2014 EN-FR, BLEU	Comb .  Table 3: Average Kendall's τ on WMT12 for semantic vec-
true	P15-1078.pdf#11.41	WMT 2014 EN-FR, BLEU	Alone  Table 3: Average Kendall's τ on WMT12 for semantic vec-
true	P15-1078.pdf#10.01	WMT 2014 EN-FR, BLEU	Alone  Table 3: Average Kendall's τ on WMT12 for semantic vec-
true	P15-1078.pdf#12.16	WMT 2014 EN-FR, BLEU	Alone  Table 3: Average Kendall's τ on WMT12 for semantic vec-
true	P15-1078.pdf#29.70	WMT 2014 EN-FR, BLEU	Kendall ' s τ AVG  Table 4: Kendall's tau (τ ) on the WMT12 dataset for al-
true	P15-1078.pdf#29.92	WMT 2014 EN-FR, BLEU	Kendall ' s τ AVG  Table 5: Kendall's tau (τ ) on WMT12 for alternative cost
true	P15-1078.pdf#29.70	WMT 2014 EN-DE, BLEU	NN using 4METRICS+ embedding vectors Kendall ' s τ AVG  Table 1: Kendall's tau (τ ) on the WMT12 dataset for various metrics. Notes: (i) the version of METEOR that took part in the
true	P15-1078.pdf#26.15	WMT 2014 EN-DE, BLEU	Kendall ' s τ AVG  Table 2: Kendall's τ on WMT12 for neural networks using BLEUCOMP, a decomposed version of BLEU. For comparison,
true	P15-1078.pdf#20.03	WMT 2014 EN-DE, BLEU	Kendall ' s τ cz  Table 2: Kendall's τ on WMT12 for neural networks using BLEUCOMP, a decomposed version of BLEU. For comparison,
true	P15-1078.pdf#24.05	WMT 2014 EN-DE, BLEU	Kendall ' s τ AVG  Table 2: Kendall's τ on WMT12 for neural networks using BLEUCOMP, a decomposed version of BLEU. For comparison,
true	P15-1078.pdf#27.07	WMT 2014 EN-DE, BLEU	Kendall ' s τ es  Table 2: Kendall's τ on WMT12 for neural networks using BLEUCOMP, a decomposed version of BLEU. For comparison,
true	P15-1078.pdf#25.95	WMT 2014 EN-DE, BLEU	Kendall ' s τ de  Table 2: Kendall's τ on WMT12 for neural networks using BLEUCOMP, a decomposed version of BLEU. For comparison,
true	P15-1078.pdf#23.16	WMT 2014 EN-DE, BLEU	Kendall ' s τ fr  Table 2: Kendall's τ on WMT12 for neural networks using BLEUCOMP, a decomposed version of BLEU. For comparison,
true	P15-1078.pdf#12.35	WMT 2014 EN-DE, BLEU	Alone  Table 3: Average Kendall's τ on WMT12 for semantic vec-
true	P15-1078.pdf#29.70	WMT 2014 EN-DE, BLEU	Comb .  Table 3: Average Kendall's τ on WMT12 for semantic vec-
true	P15-1078.pdf#29.90	WMT 2014 EN-DE, BLEU	Comb .  Table 3: Average Kendall's τ on WMT12 for semantic vec-
true	P15-1078.pdf#29.88	WMT 2014 EN-DE, BLEU	Comb .  Table 3: Average Kendall's τ on WMT12 for semantic vec-
true	P15-1078.pdf#11.41	WMT 2014 EN-DE, BLEU	Alone  Table 3: Average Kendall's τ on WMT12 for semantic vec-
true	P15-1078.pdf#10.01	WMT 2014 EN-DE, BLEU	Alone  Table 3: Average Kendall's τ on WMT12 for semantic vec-
true	P15-1078.pdf#12.16	WMT 2014 EN-DE, BLEU	Alone  Table 3: Average Kendall's τ on WMT12 for semantic vec-
true	P15-1078.pdf#29.70	WMT 2014 EN-DE, BLEU	Kendall ' s τ AVG  Table 4: Kendall's tau (τ ) on the WMT12 dataset for al-
true	P15-1078.pdf#29.92	WMT 2014 EN-DE, BLEU	Kendall ' s τ AVG  Table 5: Kendall's tau (τ ) on WMT12 for alternative cost
true	D14-1118.pdf#0.7699	SearchQA, Unigram Acc	RCM H  Table 2: ACC of different methods for well- resolved questions.
true	D14-1118.pdf#0.7699	SearchQA, Unigram Acc	RCM Q  Table 2: ACC of different methods for well- resolved questions.
true	D14-1118.pdf#0.7371	SearchQA, Unigram Acc	RCM H  Table 2: ACC of different methods for well- resolved questions.
true	D14-1118.pdf#0.6609	SearchQA, Unigram Acc	RCM Q  Table 4: Averaged ACC of different methods for  cold-start questions.
true	D14-1118.pdf#0.7442	SearchQA, Unigram Acc	RCM Q  Table 4: Averaged ACC of different methods for  cold-start questions.
true	D10-1119.pdf#79.7	Text8, Number of params	System Spanish Rec .  Table 1: Performance across languages on Geo250 with  variable-free meaning representations.
true	D10-1119.pdf#92.0	Text8, Number of params	System English Pre . Japanese Pre .  Table 1: Performance across languages on Geo250 with  variable-free meaning representations.
true	D10-1119.pdf#95.2	Text8, Number of params	System Spanish Pre .  Table 1: Performance across languages on Geo250 with  variable-free meaning representations.
true	D10-1119.pdf#82.9	Text8, Number of params	System English F1 Japanese F1  Table 1: Performance across languages on Geo250 with  variable-free meaning representations.
true	D10-1119.pdf#78.6	Text8, Number of params	System Spanish F1 Turkish F1  Table 1: Performance across languages on Geo250 with  variable-free meaning representations.
true	D10-1119.pdf#80.5	Text8, Number of params	System English Rec . Japanese Rec .  Table 1: Performance across languages on Geo250 with  variable-free meaning representations.
true	D10-1119.pdf#82.7	Text8, Number of params	System English F1  Table 1: Performance across languages on Geo250 with  variable-free meaning representations.
true	D10-1119.pdf#74.2	Text8, Number of params	System Spanish Rec . Turkish Rec .  Table 1: Performance across languages on Geo250 with  variable-free meaning representations.
true	D10-1119.pdf#80.4	Text8, Number of params	System English Rec .  Table 1: Performance across languages on Geo250 with  variable-free meaning representations.
true	D10-1119.pdf#95.4	Text8, Number of params	System English Pre .  Table 1: Performance across languages on Geo250 with  variable-free meaning representations.
true	D10-1119.pdf#86.5	Text8, Number of params	System Spanish F1  Table 1: Performance across languages on Geo250 with  variable-free meaning representations.
true	D10-1119.pdf#97.0	Text8, Number of params	System Spanish Pre . Turkish Pre .  Table 1: Performance across languages on Geo250 with  variable-free meaning representations.
true	D10-1119.pdf#90.9	Text8, Number of params	System English Pre . Japanese Pre .  Table 2: Performance across languages on Geo250 with  lambda-calculus meaning representations.
true	D10-1119.pdf#78.1	Text8, Number of params	System Spanish F1 Turkish F1  Table 2: Performance across languages on Geo250 with  lambda-calculus meaning representations.
true	D10-1119.pdf#85.8	Text8, Number of params	System English F1 Japanese F1  Table 2: Performance across languages on Geo250 with  lambda-calculus meaning representations.
true	D10-1119.pdf#83.0	Text8, Number of params	System English Rec . Japanese Rec .  Table 2: Performance across languages on Geo250 with  lambda-calculus meaning representations.
true	D10-1119.pdf#81.4	Text8, Number of params	System Spanish Rec .  Table 2: Performance across languages on Geo250 with  lambda-calculus meaning representations.
true	D10-1119.pdf#81.8	Text8, Number of params	System English Rec .  Table 2: Performance across languages on Geo250 with  lambda-calculus meaning representations.
true	D10-1119.pdf#84.7	Text8, Number of params	System English F1  Table 2: Performance across languages on Geo250 with  lambda-calculus meaning representations.
true	D10-1119.pdf#71.8	Text8, Number of params	System Spanish Rec . Turkish Rec .  Table 2: Performance across languages on Geo250 with  lambda-calculus meaning representations.
true	D10-1119.pdf#93.4	Text8, Number of params	System Spanish Pre . Turkish Pre .  Table 2: Performance across languages on Geo250 with  lambda-calculus meaning representations.
true	D10-1119.pdf#93.4	Text8, Number of params	System Spanish Pre .  Table 2: Performance across languages on Geo250 with  lambda-calculus meaning representations.
true	D10-1119.pdf#78.1	Text8, Number of params	System Spanish F1 Turkish F1  Table 2: Performance across languages on Geo250 with  lambda-calculus meaning representations.
true	D10-1119.pdf#85.8	Text8, Number of params	System Spanish F1  Table 2: Performance across languages on Geo250 with  lambda-calculus meaning representations.
true	D10-1119.pdf#93.2	Text8, Number of params	System English Pre .  Table 2: Performance across languages on Geo250 with  lambda-calculus meaning representations.
true	D10-1119.pdf#85.2	Text8, Number of params	Cross Validation Results Variable Free F1 - -  Table 3: Performance on the Geo880 data set, with varied  meaning representations.
true	D10-1119.pdf#93.3	Text8, Number of params	Cross Validation Results Variable Free Pre .  Table 3: Performance on the Geo880 data set, with varied  meaning representations.
true	D10-1119.pdf#85.2	Text8, Number of params	Independent Test Set Variable Free F1 - - -  Table 3: Performance on the Geo880 data set, with varied  meaning representations.
true	D10-1119.pdf#96.3	Text8, Number of params	Independent Test Set Lambda Calculus Pre . - - -  Table 3: Performance on the Geo880 data set, with varied  meaning representations.
true	D10-1119.pdf#89.3	Text8, Number of params	Independent Test Set Lambda Calculus F1 - - -  Table 3: Performance on the Geo880 data set, with varied  meaning representations.
true	D10-1119.pdf#84.3	Text8, Number of params	Independent Test Set Variable Free Rec . - - -  Table 3: Performance on the Geo880 data set, with varied  meaning representations.
true	D10-1119.pdf#87.9	Text8, Number of params	Independent Test Set Lambda Calculus Rec . - - -  Table 3: Performance on the Geo880 data set, with varied  meaning representations.
true	D15-1039.pdf#§3.2fordefinitions	Penn Treebank, POS	100  Table 1: Statistics showing the accuracy for various definitions of projected trees: see  §3.2 for definitions  of P, P 100 etc. Columns labeled "Acc."
true	P15-1031.pdf#93.29	Penn Treebank, POS	14 UAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#91.37	Penn Treebank, POS	55 LAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#92.13	Penn Treebank, POS	14 LAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#92.29	Penn Treebank, POS	14 LAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#93.57	Penn Treebank, POS	14 UAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#92.82	Penn Treebank, POS	55 UAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#92.59	Penn Treebank, POS	55 UAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#91.48	Penn Treebank, POS	55 LAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#93.29	Penn Treebank, UAS	14 UAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#91.37	Penn Treebank, UAS	55 LAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#92.13	Penn Treebank, UAS	14 LAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#92.29	Penn Treebank, UAS	14 LAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#93.57	Penn Treebank, UAS	14 UAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#92.82	Penn Treebank, UAS	55 UAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#92.59	Penn Treebank, UAS	55 UAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#91.48	Penn Treebank, UAS	55 LAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#93.29	Penn Treebank, LAS	14 UAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#91.37	Penn Treebank, LAS	55 LAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#92.13	Penn Treebank, LAS	14 LAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#92.29	Penn Treebank, LAS	14 LAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#93.57	Penn Treebank, LAS	14 UAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#92.82	Penn Treebank, LAS	55 UAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#92.59	Penn Treebank, LAS	55 UAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#91.48	Penn Treebank, LAS	55 LAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#93.29	Penn Treebank, F1	14 UAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#91.37	Penn Treebank, F1	55 LAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#92.13	Penn Treebank, F1	14 LAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#92.29	Penn Treebank, F1	14 LAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#93.57	Penn Treebank, F1	14 UAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#92.82	Penn Treebank, F1	55 UAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#92.59	Penn Treebank, F1	55 UAS  Table 2: Comparison with conventional graph-based models.
true	P15-1031.pdf#91.48	Penn Treebank, F1	55 LAS  Table 2: Comparison with conventional graph-based models.
true	P14-1066.pdf#1	WMT 2014 EN-FR, BLEU	tively . Systems ∑ |í µí±|  Table 1.
true	P14-1066.pdf#1	WMT 2014 EN-FR, BLEU	tively . Systems  Table 1.
true	P14-1066.pdf#33.29β	WMT 2014 EN-FR, BLEU	WMT test2006  Table 1.
true	P14-1066.pdf#0.05)fromBaselineandCPTM,respec-	WMT 2014 EN-FR, BLEU	and β indicate statistically significant difference WMT test2006  Table 1.
true	P14-1066.pdf#33.91α	WMT 2014 EN-FR, BLEU	WMT test2006  Table 1.
true	P14-1066.pdf#33.15β	WMT 2014 EN-FR, BLEU	WMT test2006  Table 1.
true	P14-1066.pdf#33.06	WMT 2014 EN-FR, BLEU	WMT test2006  Table 1.
true	P14-1066.pdf#33.25β	WMT 2014 EN-FR, BLEU	WMT test2006  Table 1.
true	P14-1066.pdf#33.60αβ	WMT 2014 EN-FR, BLEU	WMT test2006  Table 1.
true	P14-1066.pdf#33.29β	WMT 2014 EN-FR, BLEU	WMT test2006  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#33.06	WMT 2014 EN-FR, BLEU	WMT test2006  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#33.91α	WMT 2014 EN-FR, BLEU	WMT test2006  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#1	WMT 2014 EN-FR, BLEU	tively . Systems ∑ |í µí±|  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#33.25β	WMT 2014 EN-FR, BLEU	WMT test2006  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#33.60αβ	WMT 2014 EN-FR, BLEU	WMT test2006  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#1	WMT 2014 EN-FR, BLEU	tively . Systems  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#33.15β	WMT 2014 EN-FR, BLEU	WMT test2006  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#0.05)fromBaselineandCPTM,respec-	WMT 2014 EN-FR, BLEU	and β indicate statistically significant difference WMT test2006  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#26.56	WMT 2014 EN-FR, BLEU	α news2011  Table 1. First of  all, it is more effective to model the phrase trans- lation directly than decomposing it into word- word translations in the CPTMs. Second, we see  that the nonlinear projection is able to generate  more effective features, leading to better results  than the linear projection.  We also compare the best version of the CPTM  i.e., CPTM, with three related models proposed  previously. We start the discussion with the re- sults on the WMT 2006 data set in
true	P14-1066.pdf#24.46αβ	WMT 2014 EN-FR, BLEU	α dev  Table 1. First of  all, it is more effective to model the phrase trans- lation directly than decomposing it into word- word translations in the CPTMs. Second, we see  that the nonlinear projection is able to generate  more effective features, leading to better results  than the linear projection.  We also compare the best version of the CPTM  i.e., CPTM, with three related models proposed  previously. We start the discussion with the re- sults on the WMT 2006 data set in
true	P14-1066.pdf#21.64αβ	WMT 2014 EN-FR, BLEU	α news2008  Table 1. First of  all, it is more effective to model the phrase trans- lation directly than decomposing it into word- word translations in the CPTMs. Second, we see  that the nonlinear projection is able to generate  more effective features, leading to better results  than the linear projection.  We also compare the best version of the CPTM  i.e., CPTM, with three related models proposed  previously. We start the discussion with the re- sults on the WMT 2006 data set in
true	P14-1066.pdf#25.22α	WMT 2014 EN-FR, BLEU	α newssyscomb2010  Table 1. First of  all, it is more effective to model the phrase trans- lation directly than decomposing it into word- word translations in the CPTMs. Second, we see  that the nonlinear projection is able to generate  more effective features, leading to better results  than the linear projection.  We also compare the best version of the CPTM  i.e., CPTM, with three related models proposed  previously. We start the discussion with the re- sults on the WMT 2006 data set in
true	P14-1066.pdf#25.52	WMT 2014 EN-FR, BLEU	α news2010  Table 1. First of  all, it is more effective to model the phrase trans- lation directly than decomposing it into word- word translations in the CPTMs. Second, we see  that the nonlinear projection is able to generate  more effective features, leading to better results  than the linear projection.  We also compare the best version of the CPTM  i.e., CPTM, with three related models proposed  previously. We start the discussion with the re- sults on the WMT 2006 data set in
true	P14-1066.pdf#26.56	WMT 2014 EN-FR, BLEU	α news2011  Table 1.
true	P14-1066.pdf#25.52	WMT 2014 EN-FR, BLEU	α news2010  Table 1.
true	P14-1066.pdf#24.46αβ	WMT 2014 EN-FR, BLEU	α dev  Table 1.
true	P14-1066.pdf#21.64αβ	WMT 2014 EN-FR, BLEU	α news2008  Table 1.
true	P14-1066.pdf#25.22α	WMT 2014 EN-FR, BLEU	α newssyscomb2010  Table 1.
true	P14-1066.pdf#26.56	WMT 2014 EN-FR, BLEU	α news2011  Table 2: BLEU results for the French to English task using translation models and systems built on  the WMT 2012 data set. The superscripts α and β indicate statistically significant difference (p <  0.05) from Baseline and MRFp, respectively.
true	P14-1066.pdf#25.52	WMT 2014 EN-FR, BLEU	α news2010  Table 2: BLEU results for the French to English task using translation models and systems built on  the WMT 2012 data set. The superscripts α and β indicate statistically significant difference (p <  0.05) from Baseline and MRFp, respectively.
true	P14-1066.pdf#21.64αβ	WMT 2014 EN-FR, BLEU	α news2008  Table 2: BLEU results for the French to English task using translation models and systems built on  the WMT 2012 data set. The superscripts α and β indicate statistically significant difference (p <  0.05) from Baseline and MRFp, respectively.
true	P14-1066.pdf#25.22α	WMT 2014 EN-FR, BLEU	α newssyscomb2010  Table 2: BLEU results for the French to English task using translation models and systems built on  the WMT 2012 data set. The superscripts α and β indicate statistically significant difference (p <  0.05) from Baseline and MRFp, respectively.
true	P14-1066.pdf#24.46αβ	WMT 2014 EN-FR, BLEU	α dev  Table 2: BLEU results for the French to English task using translation models and systems built on  the WMT 2012 data set. The superscripts α and β indicate statistically significant difference (p <  0.05) from Baseline and MRFp, respectively.
true	P14-1066.pdf#1	Text8, Number of params	tively . Systems ∑ |í µí±|  Table 1.
true	P14-1066.pdf#1	Text8, Number of params	tively . Systems  Table 1.
true	P14-1066.pdf#33.29β	Text8, Number of params	WMT test2006  Table 1.
true	P14-1066.pdf#0.05)fromBaselineandCPTM,respec-	Text8, Number of params	and β indicate statistically significant difference WMT test2006  Table 1.
true	P14-1066.pdf#33.91α	Text8, Number of params	WMT test2006  Table 1.
true	P14-1066.pdf#33.15β	Text8, Number of params	WMT test2006  Table 1.
true	P14-1066.pdf#33.06	Text8, Number of params	WMT test2006  Table 1.
true	P14-1066.pdf#33.25β	Text8, Number of params	WMT test2006  Table 1.
true	P14-1066.pdf#33.60αβ	Text8, Number of params	WMT test2006  Table 1.
true	P14-1066.pdf#33.29β	Text8, Number of params	WMT test2006  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#33.06	Text8, Number of params	WMT test2006  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#33.91α	Text8, Number of params	WMT test2006  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#1	Text8, Number of params	tively . Systems ∑ |í µí±|  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#33.25β	Text8, Number of params	WMT test2006  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#33.60αβ	Text8, Number of params	WMT test2006  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#1	Text8, Number of params	tively . Systems  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#33.15β	Text8, Number of params	WMT test2006  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#0.05)fromBaselineandCPTM,respec-	Text8, Number of params	and β indicate statistically significant difference WMT test2006  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#26.56	Text8, Number of params	α news2011  Table 1. First of  all, it is more effective to model the phrase trans- lation directly than decomposing it into word- word translations in the CPTMs. Second, we see  that the nonlinear projection is able to generate  more effective features, leading to better results  than the linear projection.  We also compare the best version of the CPTM  i.e., CPTM, with three related models proposed  previously. We start the discussion with the re- sults on the WMT 2006 data set in
true	P14-1066.pdf#24.46αβ	Text8, Number of params	α dev  Table 1. First of  all, it is more effective to model the phrase trans- lation directly than decomposing it into word- word translations in the CPTMs. Second, we see  that the nonlinear projection is able to generate  more effective features, leading to better results  than the linear projection.  We also compare the best version of the CPTM  i.e., CPTM, with three related models proposed  previously. We start the discussion with the re- sults on the WMT 2006 data set in
true	P14-1066.pdf#21.64αβ	Text8, Number of params	α news2008  Table 1. First of  all, it is more effective to model the phrase trans- lation directly than decomposing it into word- word translations in the CPTMs. Second, we see  that the nonlinear projection is able to generate  more effective features, leading to better results  than the linear projection.  We also compare the best version of the CPTM  i.e., CPTM, with three related models proposed  previously. We start the discussion with the re- sults on the WMT 2006 data set in
true	P14-1066.pdf#25.22α	Text8, Number of params	α newssyscomb2010  Table 1. First of  all, it is more effective to model the phrase trans- lation directly than decomposing it into word- word translations in the CPTMs. Second, we see  that the nonlinear projection is able to generate  more effective features, leading to better results  than the linear projection.  We also compare the best version of the CPTM  i.e., CPTM, with three related models proposed  previously. We start the discussion with the re- sults on the WMT 2006 data set in
true	P14-1066.pdf#25.52	Text8, Number of params	α news2010  Table 1. First of  all, it is more effective to model the phrase trans- lation directly than decomposing it into word- word translations in the CPTMs. Second, we see  that the nonlinear projection is able to generate  more effective features, leading to better results  than the linear projection.  We also compare the best version of the CPTM  i.e., CPTM, with three related models proposed  previously. We start the discussion with the re- sults on the WMT 2006 data set in
true	P14-1066.pdf#26.56	Text8, Number of params	α news2011  Table 1.
true	P14-1066.pdf#25.52	Text8, Number of params	α news2010  Table 1.
true	P14-1066.pdf#24.46αβ	Text8, Number of params	α dev  Table 1.
true	P14-1066.pdf#21.64αβ	Text8, Number of params	α news2008  Table 1.
true	P14-1066.pdf#25.22α	Text8, Number of params	α newssyscomb2010  Table 1.
true	P14-1066.pdf#26.56	Text8, Number of params	α news2011  Table 2: BLEU results for the French to English task using translation models and systems built on  the WMT 2012 data set. The superscripts α and β indicate statistically significant difference (p <  0.05) from Baseline and MRFp, respectively.
true	P14-1066.pdf#25.52	Text8, Number of params	α news2010  Table 2: BLEU results for the French to English task using translation models and systems built on  the WMT 2012 data set. The superscripts α and β indicate statistically significant difference (p <  0.05) from Baseline and MRFp, respectively.
true	P14-1066.pdf#21.64αβ	Text8, Number of params	α news2008  Table 2: BLEU results for the French to English task using translation models and systems built on  the WMT 2012 data set. The superscripts α and β indicate statistically significant difference (p <  0.05) from Baseline and MRFp, respectively.
true	P14-1066.pdf#25.22α	Text8, Number of params	α newssyscomb2010  Table 2: BLEU results for the French to English task using translation models and systems built on  the WMT 2012 data set. The superscripts α and β indicate statistically significant difference (p <  0.05) from Baseline and MRFp, respectively.
true	P14-1066.pdf#24.46αβ	Text8, Number of params	α dev  Table 2: BLEU results for the French to English task using translation models and systems built on  the WMT 2012 data set. The superscripts α and β indicate statistically significant difference (p <  0.05) from Baseline and MRFp, respectively.
true	P14-1066.pdf#1	WMT 2014 EN-DE, BLEU	tively . Systems ∑ |í µí±|  Table 1.
true	P14-1066.pdf#1	WMT 2014 EN-DE, BLEU	tively . Systems  Table 1.
true	P14-1066.pdf#33.29β	WMT 2014 EN-DE, BLEU	WMT test2006  Table 1.
true	P14-1066.pdf#0.05)fromBaselineandCPTM,respec-	WMT 2014 EN-DE, BLEU	and β indicate statistically significant difference WMT test2006  Table 1.
true	P14-1066.pdf#33.91α	WMT 2014 EN-DE, BLEU	WMT test2006  Table 1.
true	P14-1066.pdf#33.15β	WMT 2014 EN-DE, BLEU	WMT test2006  Table 1.
true	P14-1066.pdf#33.06	WMT 2014 EN-DE, BLEU	WMT test2006  Table 1.
true	P14-1066.pdf#33.25β	WMT 2014 EN-DE, BLEU	WMT test2006  Table 1.
true	P14-1066.pdf#33.60αβ	WMT 2014 EN-DE, BLEU	WMT test2006  Table 1.
true	P14-1066.pdf#33.29β	WMT 2014 EN-DE, BLEU	WMT test2006  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#33.06	WMT 2014 EN-DE, BLEU	WMT test2006  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#33.91α	WMT 2014 EN-DE, BLEU	WMT test2006  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#1	WMT 2014 EN-DE, BLEU	tively . Systems ∑ |í µí±|  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#33.25β	WMT 2014 EN-DE, BLEU	WMT test2006  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#33.60αβ	WMT 2014 EN-DE, BLEU	WMT test2006  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#1	WMT 2014 EN-DE, BLEU	tively . Systems  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#33.15β	WMT 2014 EN-DE, BLEU	WMT test2006  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#0.05)fromBaselineandCPTM,respec-	WMT 2014 EN-DE, BLEU	and β indicate statistically significant difference WMT test2006  Table 1: BLEU results for the English to French  task using translation models and systems built  on the WMT 2006 data set. The superscripts α  and β indicate statistically significant difference  (p < 0.05) from Baseline and CPTM, respec- tively.
true	P14-1066.pdf#26.56	WMT 2014 EN-DE, BLEU	α news2011  Table 1. First of  all, it is more effective to model the phrase trans- lation directly than decomposing it into word- word translations in the CPTMs. Second, we see  that the nonlinear projection is able to generate  more effective features, leading to better results  than the linear projection.  We also compare the best version of the CPTM  i.e., CPTM, with three related models proposed  previously. We start the discussion with the re- sults on the WMT 2006 data set in
true	P14-1066.pdf#24.46αβ	WMT 2014 EN-DE, BLEU	α dev  Table 1. First of  all, it is more effective to model the phrase trans- lation directly than decomposing it into word- word translations in the CPTMs. Second, we see  that the nonlinear projection is able to generate  more effective features, leading to better results  than the linear projection.  We also compare the best version of the CPTM  i.e., CPTM, with three related models proposed  previously. We start the discussion with the re- sults on the WMT 2006 data set in
true	P14-1066.pdf#21.64αβ	WMT 2014 EN-DE, BLEU	α news2008  Table 1. First of  all, it is more effective to model the phrase trans- lation directly than decomposing it into word- word translations in the CPTMs. Second, we see  that the nonlinear projection is able to generate  more effective features, leading to better results  than the linear projection.  We also compare the best version of the CPTM  i.e., CPTM, with three related models proposed  previously. We start the discussion with the re- sults on the WMT 2006 data set in
true	P14-1066.pdf#25.22α	WMT 2014 EN-DE, BLEU	α newssyscomb2010  Table 1. First of  all, it is more effective to model the phrase trans- lation directly than decomposing it into word- word translations in the CPTMs. Second, we see  that the nonlinear projection is able to generate  more effective features, leading to better results  than the linear projection.  We also compare the best version of the CPTM  i.e., CPTM, with three related models proposed  previously. We start the discussion with the re- sults on the WMT 2006 data set in
true	P14-1066.pdf#25.52	WMT 2014 EN-DE, BLEU	α news2010  Table 1. First of  all, it is more effective to model the phrase trans- lation directly than decomposing it into word- word translations in the CPTMs. Second, we see  that the nonlinear projection is able to generate  more effective features, leading to better results  than the linear projection.  We also compare the best version of the CPTM  i.e., CPTM, with three related models proposed  previously. We start the discussion with the re- sults on the WMT 2006 data set in
true	P14-1066.pdf#26.56	WMT 2014 EN-DE, BLEU	α news2011  Table 1.
true	P14-1066.pdf#25.52	WMT 2014 EN-DE, BLEU	α news2010  Table 1.
true	P14-1066.pdf#24.46αβ	WMT 2014 EN-DE, BLEU	α dev  Table 1.
true	P14-1066.pdf#21.64αβ	WMT 2014 EN-DE, BLEU	α news2008  Table 1.
true	P14-1066.pdf#25.22α	WMT 2014 EN-DE, BLEU	α newssyscomb2010  Table 1.
true	P14-1066.pdf#26.56	WMT 2014 EN-DE, BLEU	α news2011  Table 2: BLEU results for the French to English task using translation models and systems built on  the WMT 2012 data set. The superscripts α and β indicate statistically significant difference (p <  0.05) from Baseline and MRFp, respectively.
true	P14-1066.pdf#25.52	WMT 2014 EN-DE, BLEU	α news2010  Table 2: BLEU results for the French to English task using translation models and systems built on  the WMT 2012 data set. The superscripts α and β indicate statistically significant difference (p <  0.05) from Baseline and MRFp, respectively.
true	P14-1066.pdf#21.64αβ	WMT 2014 EN-DE, BLEU	α news2008  Table 2: BLEU results for the French to English task using translation models and systems built on  the WMT 2012 data set. The superscripts α and β indicate statistically significant difference (p <  0.05) from Baseline and MRFp, respectively.
true	P14-1066.pdf#25.22α	WMT 2014 EN-DE, BLEU	α newssyscomb2010  Table 2: BLEU results for the French to English task using translation models and systems built on  the WMT 2012 data set. The superscripts α and β indicate statistically significant difference (p <  0.05) from Baseline and MRFp, respectively.
true	P14-1066.pdf#24.46αβ	WMT 2014 EN-DE, BLEU	α dev  Table 2: BLEU results for the French to English task using translation models and systems built on  the WMT 2012 data set. The superscripts α and β indicate statistically significant difference (p <  0.05) from Baseline and MRFp, respectively.
true	P10-1156.pdf#62.13	Senseval 2, F1	Global F Measure  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#69.2	Senseval 2, F1	A  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#70.2	Senseval 2, F1	N  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#38.66	Senseval 2, F1	V  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#38.66	Senseval 2, F1	V  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#69.45	Senseval 2, F1	R  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#59.87	Senseval 2, F1	Global F Measure  Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#65.55	Senseval 2, F1	A  Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#49.2	Senseval 2, F1	V  Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#65.55	Senseval 2, F1	A  Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#58.35	Senseval 2, F1	Global F Measure  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#68.01	Senseval 2, F1	R  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#68.01	Senseval 2, F1	R  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#33.88	Senseval 2, F1	V  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#60.2	Senseval 2, F1	A  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#60.2	Senseval 2, F1	A  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#67.15	Senseval 2, F1	N  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#49.29	Senseval 2, F1	47 Global F Measure  Table 4: TransCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#59.52	Senseval 2, F1	Table 5 : F - measure % for all Combined experimental conditions on SV2AW Global F Measure Global F Measure  Table 5: F-measure % for all Combined experimental conditions on SV2AW
true	P10-1156.pdf#62.13	Senseval 2, F1	Global F Measure  Table 5: F-measure % for all Combined experimental conditions on SV2AW
true	P10-1156.pdf#64.58	Senseval 2, F1	Global F Measure  Table 5: F-measure % for all Combined experimental conditions on SV2AW
true	P10-1156.pdf#59.07	Senseval 2, F1	Table 5 : F - measure % for all Combined experimental conditions on SV2AW Global F Measure Global F Measure  Table 5: F-measure % for all Combined experimental conditions on SV2AW
true	P10-1156.pdf#62.13	SemEval 2007, F1	Global F Measure  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#69.2	SemEval 2007, F1	A  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#70.2	SemEval 2007, F1	N  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#38.66	SemEval 2007, F1	V  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#38.66	SemEval 2007, F1	V  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#69.45	SemEval 2007, F1	R  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#59.87	SemEval 2007, F1	Global F Measure  Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#65.55	SemEval 2007, F1	A  Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#49.2	SemEval 2007, F1	V  Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#65.55	SemEval 2007, F1	A  Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#58.35	SemEval 2007, F1	Global F Measure  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#68.01	SemEval 2007, F1	R  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#68.01	SemEval 2007, F1	R  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#33.88	SemEval 2007, F1	V  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#60.2	SemEval 2007, F1	A  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#60.2	SemEval 2007, F1	A  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#67.15	SemEval 2007, F1	N  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#49.29	SemEval 2007, F1	47 Global F Measure  Table 4: TransCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#59.52	SemEval 2007, F1	Table 5 : F - measure % for all Combined experimental conditions on SV2AW Global F Measure Global F Measure  Table 5: F-measure % for all Combined experimental conditions on SV2AW
true	P10-1156.pdf#62.13	SemEval 2007, F1	Global F Measure  Table 5: F-measure % for all Combined experimental conditions on SV2AW
true	P10-1156.pdf#64.58	SemEval 2007, F1	Global F Measure  Table 5: F-measure % for all Combined experimental conditions on SV2AW
true	P10-1156.pdf#59.07	SemEval 2007, F1	Table 5 : F - measure % for all Combined experimental conditions on SV2AW Global F Measure Global F Measure  Table 5: F-measure % for all Combined experimental conditions on SV2AW
true	P10-1156.pdf#62.13	SemEval 2015, F1	Global F Measure  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#69.2	SemEval 2015, F1	A  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#70.2	SemEval 2015, F1	N  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#38.66	SemEval 2015, F1	V  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#38.66	SemEval 2015, F1	V  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#69.45	SemEval 2015, F1	R  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#59.87	SemEval 2015, F1	Global F Measure  Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#65.55	SemEval 2015, F1	A  Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#49.2	SemEval 2015, F1	V  Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#65.55	SemEval 2015, F1	A  Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#58.35	SemEval 2015, F1	Global F Measure  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#68.01	SemEval 2015, F1	R  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#68.01	SemEval 2015, F1	R  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#33.88	SemEval 2015, F1	V  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#60.2	SemEval 2015, F1	A  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#60.2	SemEval 2015, F1	A  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#67.15	SemEval 2015, F1	N  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#49.29	SemEval 2015, F1	47 Global F Measure  Table 4: TransCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#59.52	SemEval 2015, F1	Table 5 : F - measure % for all Combined experimental conditions on SV2AW Global F Measure Global F Measure  Table 5: F-measure % for all Combined experimental conditions on SV2AW
true	P10-1156.pdf#62.13	SemEval 2015, F1	Global F Measure  Table 5: F-measure % for all Combined experimental conditions on SV2AW
true	P10-1156.pdf#64.58	SemEval 2015, F1	Global F Measure  Table 5: F-measure % for all Combined experimental conditions on SV2AW
true	P10-1156.pdf#59.07	SemEval 2015, F1	Table 5 : F - measure % for all Combined experimental conditions on SV2AW Global F Measure Global F Measure  Table 5: F-measure % for all Combined experimental conditions on SV2AW
true	P10-1156.pdf#62.13	Senseval 3, F1	Global F Measure  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#69.2	Senseval 3, F1	A  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#70.2	Senseval 3, F1	N  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#38.66	Senseval 3, F1	V  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#38.66	Senseval 3, F1	V  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#69.45	Senseval 3, F1	R  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#59.87	Senseval 3, F1	Global F Measure  Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#65.55	Senseval 3, F1	A  Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#49.2	Senseval 3, F1	V  Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#65.55	Senseval 3, F1	A  Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#58.35	Senseval 3, F1	Global F Measure  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#68.01	Senseval 3, F1	R  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#68.01	Senseval 3, F1	R  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#33.88	Senseval 3, F1	V  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#60.2	Senseval 3, F1	A  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#60.2	Senseval 3, F1	A  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#67.15	Senseval 3, F1	N  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#49.29	Senseval 3, F1	47 Global F Measure  Table 4: TransCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#59.52	Senseval 3, F1	Table 5 : F - measure % for all Combined experimental conditions on SV2AW Global F Measure Global F Measure  Table 5: F-measure % for all Combined experimental conditions on SV2AW
true	P10-1156.pdf#62.13	Senseval 3, F1	Global F Measure  Table 5: F-measure % for all Combined experimental conditions on SV2AW
true	P10-1156.pdf#64.58	Senseval 3, F1	Global F Measure  Table 5: F-measure % for all Combined experimental conditions on SV2AW
true	P10-1156.pdf#59.07	Senseval 3, F1	Table 5 : F - measure % for all Combined experimental conditions on SV2AW Global F Measure Global F Measure  Table 5: F-measure % for all Combined experimental conditions on SV2AW
true	P10-1156.pdf#62.13	SemEval 2013, F1	Global F Measure  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#69.2	SemEval 2013, F1	A  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#70.2	SemEval 2013, F1	N  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#38.66	SemEval 2013, F1	V  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#38.66	SemEval 2013, F1	V  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#69.45	SemEval 2013, F1	R  Table 1: RelCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#59.87	SemEval 2013, F1	Global F Measure  Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#65.55	SemEval 2013, F1	A  Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#49.2	SemEval 2013, F1	V  Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#65.55	SemEval 2013, F1	A  Table 2: RelCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#58.35	SemEval 2013, F1	Global F Measure  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#68.01	SemEval 2013, F1	R  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#68.01	SemEval 2013, F1	R  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#33.88	SemEval 2013, F1	V  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#60.2	SemEval 2013, F1	A  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#60.2	SemEval 2013, F1	A  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#67.15	SemEval 2013, F1	N  Table 3: TransCont F-measure results per POS tag per condition for SV2AW using WN 1.7.1.
true	P10-1156.pdf#49.29	SemEval 2013, F1	47 Global F Measure  Table 4: TransCont F-measure results per POS tag per condition for SV3AW using WN 1.7.1.
true	P10-1156.pdf#59.52	SemEval 2013, F1	Table 5 : F - measure % for all Combined experimental conditions on SV2AW Global F Measure Global F Measure  Table 5: F-measure % for all Combined experimental conditions on SV2AW
true	P10-1156.pdf#62.13	SemEval 2013, F1	Global F Measure  Table 5: F-measure % for all Combined experimental conditions on SV2AW
true	P10-1156.pdf#64.58	SemEval 2013, F1	Global F Measure  Table 5: F-measure % for all Combined experimental conditions on SV2AW
true	P10-1156.pdf#59.07	SemEval 2013, F1	Table 5 : F - measure % for all Combined experimental conditions on SV2AW Global F Measure Global F Measure  Table 5: F-measure % for all Combined experimental conditions on SV2AW
true	D11-1014.pdf#4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Corpus 10 , 662 31 , 675  Table 1: Statistics on the different datasets. K is the num- ber of classes. Distr. is the distribution of the different  classes (in the case of 2, the positive/negative classes, for  EP the rounded distribution of total votes in each class).  |W | is the average number of words per instance. We use  EP≥ 4, a subset of entries with at least 4 votes.
true	D11-1014.pdf#50.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Accuracy  Table 3: Accuracy of predicting the class with most votes.
true	D11-1014.pdf#77.7	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	MR  Table 4: Accuracy of sentiment classification on movie  review polarity (MR) and the MPQA dataset.
true	D11-1014.pdf#86.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	MPQA  Table 4: Accuracy of sentiment classification on movie  review polarity (MR) and the MPQA dataset.
true	D15-1180.pdf#88.6	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Test - 1657 431 140 - 2452 32 - 73 - 1  Table 1: Comparison between our model and other baseline methods on Stanford Sentiment Treebank.  The top block lists recursive neural network models, the second block are convolutional network mod- els and the third block contains other baseline methods, including the paragraph-vector model (Le and  Mikolov, 2014), the deep averaging network model (Iyyer et al., 2015) and our implementation of neural  bag-of-words. The training time of baseline methods is taken from (Iyyer et al., 2015) or directly from  the authors. For our implementations, timings were performed on a single core of a 2.6GHz Intel i7  processor.
true	D15-1180.pdf#51.2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Fine - grained Test - 1657 431 140 - 2452 32 - 73 - 1  Table 1: Comparison between our model and other baseline methods on Stanford Sentiment Treebank.  The top block lists recursive neural network models, the second block are convolutional network mod- els and the third block contains other baseline methods, including the paragraph-vector model (Le and  Mikolov, 2014), the deep averaging network model (Iyyer et al., 2015) and our implementation of neural  bag-of-words. The training time of baseline methods is taken from (Iyyer et al., 2015) or directly from  the authors. For our implementations, timings were performed on a single core of a 2.6GHz Intel i7  processor.
true	P15-2058.pdf#97.2	TREC, Error	Google Word2Vec Google News 300 3 , 000 , 000 TREC  Table 1: Details of word embeddings
true	P15-2058.pdf#85.1	TREC, Error	Google GloVe Wikipedia 50 400 , 000 Snippets  Table 1: Details of word embeddings
true	P15-2058.pdf#85.1	TREC, Error	Google GloVe Wikipedia 50 400 , 000 Snippets  Table 2: The classification accuracy of proposed  method against other models
true	P15-2058.pdf#97.2	TREC, Error	Google Word2Vec Google News 300 3 , 000 , 000 TREC  Table 2: The classification accuracy of proposed  method against other models
true	P13-1084.pdf#0.564	SearchQA, Unigram Acc	MAP  Table 3: Comparison with different methods for  question retrieval.
true	P13-1084.pdf#0.291	SearchQA, Unigram Acc	P@10  Table 3: Comparison with different methods for  question retrieval.
true	P13-1041.pdf#98.66	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Mar  Table 1: Classification accuracy for monolingual sentiment analysis. For English, results are reported on  two publicly available datasets based on Travel Domain (TD) and Product Domain (PD).
true	P13-1041.pdf#97.45	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	En - TD  Table 1: Classification accuracy for monolingual sentiment analysis. For English, results are reported on  two publicly available datasets based on Travel Domain (TD) and Product Domain (PD).
true	P13-1041.pdf#87.80	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	En - PD  Table 1: Classification accuracy for monolingual sentiment analysis. For English, results are reported on  two publicly available datasets based on Travel Domain (TD) and Product Domain (PD).
true	P13-1041.pdf#85.80	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Hi  Table 1: Classification accuracy for monolingual sentiment analysis. For English, results are reported on  two publicly available datasets based on Travel Domain (TD) and Product Domain (PD).
true	P13-1041.pdf#60.30	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	XC  Table 3: Cross-Lingual SA accuracy (%) on T =Hi and T =Mar with S=En for different approaches  (MT=Machine Translation, PS=Projection based on Sense, DCL=Direct Cluster Linking , XC=Cross- Lingual Clustering. There is no MT system available for (S=En, T =Mar).
true	P13-1041.pdf#66.16	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	XC  Table 3: Cross-Lingual SA accuracy (%) on T =Hi and T =Mar with S=En for different approaches  (MT=Machine Translation, PS=Projection based on Sense, DCL=Direct Cluster Linking , XC=Cross- Lingual Clustering. There is no MT system available for (S=En, T =Mar).
true	N15-1186.pdf#1	AG News, Error	Table 2: Examples of lexicalized morphological transformations evaluated in E n using rank and cosine.
true	N15-1186.pdf#2	AG News, Error	Table 2: Examples of lexicalized morphological transformations evaluated in E n using rank and cosine.
true	N15-1186.pdf#67.3	AG News, Error	System Spearman ρ FR RG - - - -  Table 4: Performance of previously proposed methods, compared to SG and SG+Morph trained on Wiki1b. LSM2013  uses exactly the same training data for EN, whereas BB2014 uses the same training data for DE, FR, ES.
true	N15-1186.pdf#41.8	AG News, Error	System Spearman ρ Language RW - - - -  Table 4: Performance of previously proposed methods, compared to SG and SG+Morph trained on Wiki1b. LSM2013  uses exactly the same training data for EN, whereas BB2014 uses the same training data for DE, FR, ES.
true	N15-1186.pdf#47.3	AG News, Error	System Spearman ρ ES WS - - - -  Table 4: Performance of previously proposed methods, compared to SG and SG+Morph trained on Wiki1b. LSM2013  uses exactly the same training data for EN, whereas BB2014 uses the same training data for DE, FR, ES.
true	N15-1186.pdf#71.2	AG News, Error	System Spearman ρ EN WS - - - -  Table 4: Performance of previously proposed methods, compared to SG and SG+Morph trained on Wiki1b. LSM2013  uses exactly the same training data for EN, whereas BB2014 uses the same training data for DE, FR, ES.
true	N15-1186.pdf#75.1	AG News, Error	System Spearman ρ EN RG - - - -  Table 4: Performance of previously proposed methods, compared to SG and SG+Morph trained on Wiki1b. LSM2013  uses exactly the same training data for EN, whereas BB2014 uses the same training data for DE, FR, ES.
true	N15-1186.pdf#64.1	AG News, Error	System Spearman ρ DE Gur - - - -  Table 4: Performance of previously proposed methods, compared to SG and SG+Morph trained on Wiki1b. LSM2013  uses exactly the same training data for EN, whereas BB2014 uses the same training data for DE, FR, ES.
true	N15-1186.pdf#43.1	AG News, Error	System Spearman ρ AR WS - - - -  Table 4: Performance of previously proposed methods, compared to SG and SG+Morph trained on Wiki1b. LSM2013  uses exactly the same training data for EN, whereas BB2014 uses the same training data for DE, FR, ES.
true	N15-1186.pdf#53.1	AG News, Error	System Spearman ρ RO WS - - - -  Table 4: Performance of previously proposed methods, compared to SG and SG+Morph trained on Wiki1b. LSM2013  uses exactly the same training data for EN, whereas BB2014 uses the same training data for DE, FR, ES.
true	N15-1186.pdf#25.0	AG News, Error	System Spearman ρ DE ZG - -  Table 4: Performance of previously proposed methods, compared to SG and SG+Morph trained on Wiki1b. LSM2013  uses exactly the same training data for EN, whereas BB2014 uses the same training data for DE, FR, ES.
true	P15-1148.pdf#-5.82	Penn Treebank, F1	θ  Table 3: Example of our feature projection. θ GP is  a weight vector with the GP, which collapses every  c. θ LF is with the LF, which collapses all elements  in
true	P15-1148.pdf#1.12	Penn Treebank, F1	· · · θ  Table 3: Example of our feature projection. θ GP is  a weight vector with the GP, which collapses every  c. θ LF is with the LF, which collapses all elements  in
true	P15-1148.pdf#1.12	Penn Treebank, F1	· · · θ  Table 4.
true	P15-1148.pdf#-5.82	Penn Treebank, F1	θ  Table 4.
true	P15-1148.pdf#90.7	Penn Treebank, F1	DP b=16 60 Comparison of parsing times between Span ( this work ) F1  Table 5: Results for the Penn Treebank develop- ment set. Z&C = feature set of
true	P15-1148.pdf#90.7	Penn Treebank, F1	DP b=16 60 Comparison of parsing times between Span ( this work ) F1  Table 5: Results for the Penn Treebank develop- ment set. Z&C = feature set of
true	P15-1148.pdf#90.9	Penn Treebank, F1	LR  Table 6: The final results for section 23 of the  Penn Treebank. The systems with  † are re- ported by authors running on different hardware.  We divide baseline state-of-the-art systems into  three categories: shift-reduce systems (Sagae and  Lavie, 2005; Sagae and Lavie, 2006; Zhu et  al., 2013), other chart-based systems (Petrov and  Klein, 2007; Socher et al., 2013), and the systems  with external semi supervised features or rerank- ing (Charniak and Johnson, 2005; McClosky et al.,  2006; Zhu et al., 2013).
true	P15-1148.pdf#91.1	Penn Treebank, F1	F1  Table 6: The final results for section 23 of the  Penn Treebank. The systems with  † are re- ported by authors running on different hardware.  We divide baseline state-of-the-art systems into  three categories: shift-reduce systems (Sagae and  Lavie, 2005; Sagae and Lavie, 2006; Zhu et  al., 2013), other chart-based systems (Petrov and  Klein, 2007; Socher et al., 2013), and the systems  with external semi supervised features or rerank- ing (Charniak and Johnson, 2005; McClosky et al.,  2006; Zhu et al., 2013).
true	P15-1148.pdf#91.2	Penn Treebank, F1	LP  Table 6: The final results for section 23 of the  Penn Treebank. The systems with  † are re- ported by authors running on different hardware.  We divide baseline state-of-the-art systems into  three categories: shift-reduce systems (Sagae and  Lavie, 2005; Sagae and Lavie, 2006; Zhu et  al., 2013), other chart-based systems (Petrov and  Klein, 2007; Socher et al., 2013), and the systems  with external semi supervised features or rerank- ing (Charniak and Johnson, 2005; McClosky et al.,  2006; Zhu et al., 2013).
true	P11-1138.pdf#93.59	Senseval 2, F1	P ( t|m ) +Local Wiki  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#87.02	Senseval 2, F1	P ( t|m ) +Local+Global MSNBC  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#97.83	Senseval 2, F1	P ( t|m ) +Local+Global ACE  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#95.91	Senseval 2, F1	P ( t|m ) +Global AQUAINT  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#88.51	Senseval 2, F1	P ( t|m ) +Global MSNBC  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#96.75	Senseval 2, F1	P ( t|m ) +Global ACE  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#95.57	Senseval 2, F1	P ( t|m ) +Local AQUAINT  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#94.18	Senseval 2, F1	P ( t|m ) +Local+Global Wiki  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#93.59	SemEval 2007, F1	P ( t|m ) +Local Wiki  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#87.02	SemEval 2007, F1	P ( t|m ) +Local+Global MSNBC  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#97.83	SemEval 2007, F1	P ( t|m ) +Local+Global ACE  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#95.91	SemEval 2007, F1	P ( t|m ) +Global AQUAINT  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#88.51	SemEval 2007, F1	P ( t|m ) +Global MSNBC  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#96.75	SemEval 2007, F1	P ( t|m ) +Global ACE  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#95.57	SemEval 2007, F1	P ( t|m ) +Local AQUAINT  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#94.18	SemEval 2007, F1	P ( t|m ) +Local+Global Wiki  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#93.59	Senseval 3, F1	P ( t|m ) +Local Wiki  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#87.02	Senseval 3, F1	P ( t|m ) +Local+Global MSNBC  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#97.83	Senseval 3, F1	P ( t|m ) +Local+Global ACE  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#95.91	Senseval 3, F1	P ( t|m ) +Global AQUAINT  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#88.51	Senseval 3, F1	P ( t|m ) +Global MSNBC  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#96.75	Senseval 3, F1	P ( t|m ) +Global ACE  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#95.57	Senseval 3, F1	P ( t|m ) +Local AQUAINT  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#94.18	Senseval 3, F1	P ( t|m ) +Local+Global Wiki  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#93.59	SemEval 2013, F1	P ( t|m ) +Local Wiki  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#87.02	SemEval 2013, F1	P ( t|m ) +Local+Global MSNBC  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#97.83	SemEval 2013, F1	P ( t|m ) +Local+Global ACE  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#95.91	SemEval 2013, F1	P ( t|m ) +Global AQUAINT  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#88.51	SemEval 2013, F1	P ( t|m ) +Global MSNBC  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#96.75	SemEval 2013, F1	P ( t|m ) +Global ACE  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#95.57	SemEval 2013, F1	P ( t|m ) +Local AQUAINT  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#94.18	SemEval 2013, F1	P ( t|m ) +Local+Global Wiki  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#93.59	SemEval 2015, F1	P ( t|m ) +Local Wiki  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#87.02	SemEval 2015, F1	P ( t|m ) +Local+Global MSNBC  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#97.83	SemEval 2015, F1	P ( t|m ) +Local+Global ACE  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#95.91	SemEval 2015, F1	P ( t|m ) +Global AQUAINT  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#88.51	SemEval 2015, F1	P ( t|m ) +Global MSNBC  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#96.75	SemEval 2015, F1	P ( t|m ) +Global ACE  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#95.57	SemEval 2015, F1	P ( t|m ) +Local AQUAINT  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	P11-1138.pdf#94.18	SemEval 2015, F1	P ( t|m ) +Local+Global Wiki  Table 4: Ranker Accuracy. Bold values indicate the  best performance in each feature group. The global ap- proaches marginally outperform the local approaches on  ranker accuracy , while combing the approaches leads to  further marginal performance improvement.
true	D11-1072.pdf#89.05	DBpedia, Error	Our Methods r - prior sim - k coh  Table 2: Experimental results on CoNLL (all values in %)
true	D11-1072.pdf#81.91	DBpedia, Error	Our Methods r - prior sim - k r - coh  Table 2: Experimental results on CoNLL (all values in %)
true	D11-1072.pdf#81.82	DBpedia, Error	Our Methods r - prior sim - k r - coh  Table 2: Experimental results on CoNLL (all values in %)
true	P12-2028.pdf#1	SemEval 2013, F1	financial  Table 1: Three possible hypotheses of latent vectors for
true	P12-2028.pdf#2	SemEval 2013, F1	financial 20  Table 1: Three possible hypotheses of latent vectors for
true	P12-2028.pdf#3	SemEval 2013, F1	financial 20 18  Table 1: Three possible hypotheses of latent vectors for
true	P12-2028.pdf#34.9	SemEval 2013, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#67.1	SemEval 2013, F1	Adv  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.64	SemEval 2013, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#70.8	SemEval 2013, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#48.8	SemEval 2013, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.5	SemEval 2013, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#61.5	SemEval 2013, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#43.9	SemEval 2013, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#51.72	SemEval 2013, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.5	SemEval 2013, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#57.4	SemEval 2013, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#69.7	SemEval 2013, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#41.2	SemEval 2013, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#67.1	SemEval 2013, F1	Adv  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#55.8	SemEval 2013, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#67.97	SemEval 2013, F1	Adv  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#45.1	SemEval 2013, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#70.64	SemEval 2013, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#53.5	SemEval 2013, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#41.2	SemEval 2013, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#71.45	SemEval 2013, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#60.5	SemEval 2013, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#45.5	SemEval 2013, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#39.9	SemEval 2013, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#61.2	SemEval 2013, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#71.44	SemEval 2013, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.4	SemEval 2013, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#62.1	SemEval 2013, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#47.52	SemEval 2013, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#52.2	SemEval 2013, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.4	SemEval 2013, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#63.05	SemEval 2013, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#67.05	SemEval 2013, F1	Adv  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#59.10	SemEval 2013, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#55.8	SemEval 2013, F1	Total  Table 2. We also present in
true	P12-2028.pdf#64.4	SemEval 2013, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#64.5	SemEval 2013, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#62.1	SemEval 2013, F1	Total  Table 2. We also present in
true	P12-2028.pdf#45.5	SemEval 2013, F1	Total  Table 2. We also present in
true	P12-2028.pdf#41.2	SemEval 2013, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#47.52	SemEval 2013, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#61.2	SemEval 2013, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#69.7	SemEval 2013, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#67.05	SemEval 2013, F1	Adv  Table 2. We also present in
true	P12-2028.pdf#70.8	SemEval 2013, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#64.5	SemEval 2013, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#39.9	SemEval 2013, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#60.5	SemEval 2013, F1	Total  Table 2. We also present in
true	P12-2028.pdf#53.5	SemEval 2013, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#48.8	SemEval 2013, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#67.97	SemEval 2013, F1	Adv  Table 2. We also present in
true	P12-2028.pdf#64.64	SemEval 2013, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#41.2	SemEval 2013, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#52.2	SemEval 2013, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#67.1	SemEval 2013, F1	Adv  Table 2. We also present in
true	P12-2028.pdf#64.4	SemEval 2013, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#43.9	SemEval 2013, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#71.45	SemEval 2013, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#45.1	SemEval 2013, F1	Total  Table 2. We also present in
true	P12-2028.pdf#70.64	SemEval 2013, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#61.5	SemEval 2013, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#59.10	SemEval 2013, F1	Total  Table 2. We also present in
true	P12-2028.pdf#57.4	SemEval 2013, F1	Total  Table 2. We also present in
true	P12-2028.pdf#71.44	SemEval 2013, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#67.1	SemEval 2013, F1	Adv  Table 2. We also present in
true	P12-2028.pdf#51.72	SemEval 2013, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#63.05	SemEval 2013, F1	Total  Table 2. We also present in
true	P12-2028.pdf#34.9	SemEval 2013, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#-59.19	SemEval 2013, F1	Semcor  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#-45.9	SemEval 2013, F1	SE07  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#-60.5	SemEval 2013, F1	SE2  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#-55.8	SemEval 2013, F1	SE3  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#-60.5	SemEval 2013, F1	SE2  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#1	SemEval 2015, F1	financial  Table 1: Three possible hypotheses of latent vectors for
true	P12-2028.pdf#2	SemEval 2015, F1	financial 20  Table 1: Three possible hypotheses of latent vectors for
true	P12-2028.pdf#3	SemEval 2015, F1	financial 20 18  Table 1: Three possible hypotheses of latent vectors for
true	P12-2028.pdf#34.9	SemEval 2015, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#67.1	SemEval 2015, F1	Adv  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.64	SemEval 2015, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#70.8	SemEval 2015, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#48.8	SemEval 2015, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.5	SemEval 2015, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#61.5	SemEval 2015, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#43.9	SemEval 2015, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#51.72	SemEval 2015, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.5	SemEval 2015, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#57.4	SemEval 2015, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#69.7	SemEval 2015, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#41.2	SemEval 2015, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#67.1	SemEval 2015, F1	Adv  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#55.8	SemEval 2015, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#67.97	SemEval 2015, F1	Adv  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#45.1	SemEval 2015, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#70.64	SemEval 2015, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#53.5	SemEval 2015, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#41.2	SemEval 2015, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#71.45	SemEval 2015, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#60.5	SemEval 2015, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#45.5	SemEval 2015, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#39.9	SemEval 2015, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#61.2	SemEval 2015, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#71.44	SemEval 2015, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.4	SemEval 2015, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#62.1	SemEval 2015, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#47.52	SemEval 2015, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#52.2	SemEval 2015, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.4	SemEval 2015, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#63.05	SemEval 2015, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#67.05	SemEval 2015, F1	Adv  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#59.10	SemEval 2015, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#55.8	SemEval 2015, F1	Total  Table 2. We also present in
true	P12-2028.pdf#64.4	SemEval 2015, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#64.5	SemEval 2015, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#62.1	SemEval 2015, F1	Total  Table 2. We also present in
true	P12-2028.pdf#45.5	SemEval 2015, F1	Total  Table 2. We also present in
true	P12-2028.pdf#41.2	SemEval 2015, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#47.52	SemEval 2015, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#61.2	SemEval 2015, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#69.7	SemEval 2015, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#67.05	SemEval 2015, F1	Adv  Table 2. We also present in
true	P12-2028.pdf#70.8	SemEval 2015, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#64.5	SemEval 2015, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#39.9	SemEval 2015, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#60.5	SemEval 2015, F1	Total  Table 2. We also present in
true	P12-2028.pdf#53.5	SemEval 2015, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#48.8	SemEval 2015, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#67.97	SemEval 2015, F1	Adv  Table 2. We also present in
true	P12-2028.pdf#64.64	SemEval 2015, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#41.2	SemEval 2015, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#52.2	SemEval 2015, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#67.1	SemEval 2015, F1	Adv  Table 2. We also present in
true	P12-2028.pdf#64.4	SemEval 2015, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#43.9	SemEval 2015, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#71.45	SemEval 2015, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#45.1	SemEval 2015, F1	Total  Table 2. We also present in
true	P12-2028.pdf#70.64	SemEval 2015, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#61.5	SemEval 2015, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#59.10	SemEval 2015, F1	Total  Table 2. We also present in
true	P12-2028.pdf#57.4	SemEval 2015, F1	Total  Table 2. We also present in
true	P12-2028.pdf#71.44	SemEval 2015, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#67.1	SemEval 2015, F1	Adv  Table 2. We also present in
true	P12-2028.pdf#51.72	SemEval 2015, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#63.05	SemEval 2015, F1	Total  Table 2. We also present in
true	P12-2028.pdf#34.9	SemEval 2015, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#-59.19	SemEval 2015, F1	Semcor  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#-45.9	SemEval 2015, F1	SE07  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#-60.5	SemEval 2015, F1	SE2  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#-55.8	SemEval 2015, F1	SE3  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#-60.5	SemEval 2015, F1	SE2  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#1	Senseval 3, F1	financial  Table 1: Three possible hypotheses of latent vectors for
true	P12-2028.pdf#2	Senseval 3, F1	financial 20  Table 1: Three possible hypotheses of latent vectors for
true	P12-2028.pdf#3	Senseval 3, F1	financial 20 18  Table 1: Three possible hypotheses of latent vectors for
true	P12-2028.pdf#34.9	Senseval 3, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#67.1	Senseval 3, F1	Adv  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.64	Senseval 3, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#70.8	Senseval 3, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#48.8	Senseval 3, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.5	Senseval 3, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#61.5	Senseval 3, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#43.9	Senseval 3, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#51.72	Senseval 3, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.5	Senseval 3, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#57.4	Senseval 3, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#69.7	Senseval 3, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#41.2	Senseval 3, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#67.1	Senseval 3, F1	Adv  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#55.8	Senseval 3, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#67.97	Senseval 3, F1	Adv  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#45.1	Senseval 3, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#70.64	Senseval 3, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#53.5	Senseval 3, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#41.2	Senseval 3, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#71.45	Senseval 3, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#60.5	Senseval 3, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#45.5	Senseval 3, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#39.9	Senseval 3, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#61.2	Senseval 3, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#71.44	Senseval 3, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.4	Senseval 3, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#62.1	Senseval 3, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#47.52	Senseval 3, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#52.2	Senseval 3, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.4	Senseval 3, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#63.05	Senseval 3, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#67.05	Senseval 3, F1	Adv  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#59.10	Senseval 3, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#55.8	Senseval 3, F1	Total  Table 2. We also present in
true	P12-2028.pdf#64.4	Senseval 3, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#64.5	Senseval 3, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#62.1	Senseval 3, F1	Total  Table 2. We also present in
true	P12-2028.pdf#45.5	Senseval 3, F1	Total  Table 2. We also present in
true	P12-2028.pdf#41.2	Senseval 3, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#47.52	Senseval 3, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#61.2	Senseval 3, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#69.7	Senseval 3, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#67.05	Senseval 3, F1	Adv  Table 2. We also present in
true	P12-2028.pdf#70.8	Senseval 3, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#64.5	Senseval 3, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#39.9	Senseval 3, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#60.5	Senseval 3, F1	Total  Table 2. We also present in
true	P12-2028.pdf#53.5	Senseval 3, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#48.8	Senseval 3, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#67.97	Senseval 3, F1	Adv  Table 2. We also present in
true	P12-2028.pdf#64.64	Senseval 3, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#41.2	Senseval 3, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#52.2	Senseval 3, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#67.1	Senseval 3, F1	Adv  Table 2. We also present in
true	P12-2028.pdf#64.4	Senseval 3, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#43.9	Senseval 3, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#71.45	Senseval 3, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#45.1	Senseval 3, F1	Total  Table 2. We also present in
true	P12-2028.pdf#70.64	Senseval 3, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#61.5	Senseval 3, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#59.10	Senseval 3, F1	Total  Table 2. We also present in
true	P12-2028.pdf#57.4	Senseval 3, F1	Total  Table 2. We also present in
true	P12-2028.pdf#71.44	Senseval 3, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#67.1	Senseval 3, F1	Adv  Table 2. We also present in
true	P12-2028.pdf#51.72	Senseval 3, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#63.05	Senseval 3, F1	Total  Table 2. We also present in
true	P12-2028.pdf#34.9	Senseval 3, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#-59.19	Senseval 3, F1	Semcor  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#-45.9	Senseval 3, F1	SE07  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#-60.5	Senseval 3, F1	SE2  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#-55.8	Senseval 3, F1	SE3  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#-60.5	Senseval 3, F1	SE2  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#1	SemEval 2007, F1	financial  Table 1: Three possible hypotheses of latent vectors for
true	P12-2028.pdf#2	SemEval 2007, F1	financial 20  Table 1: Three possible hypotheses of latent vectors for
true	P12-2028.pdf#3	SemEval 2007, F1	financial 20 18  Table 1: Three possible hypotheses of latent vectors for
true	P12-2028.pdf#34.9	SemEval 2007, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#67.1	SemEval 2007, F1	Adv  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.64	SemEval 2007, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#70.8	SemEval 2007, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#48.8	SemEval 2007, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.5	SemEval 2007, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#61.5	SemEval 2007, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#43.9	SemEval 2007, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#51.72	SemEval 2007, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.5	SemEval 2007, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#57.4	SemEval 2007, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#69.7	SemEval 2007, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#41.2	SemEval 2007, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#67.1	SemEval 2007, F1	Adv  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#55.8	SemEval 2007, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#67.97	SemEval 2007, F1	Adv  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#45.1	SemEval 2007, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#70.64	SemEval 2007, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#53.5	SemEval 2007, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#41.2	SemEval 2007, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#71.45	SemEval 2007, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#60.5	SemEval 2007, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#45.5	SemEval 2007, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#39.9	SemEval 2007, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#61.2	SemEval 2007, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#71.44	SemEval 2007, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.4	SemEval 2007, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#62.1	SemEval 2007, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#47.52	SemEval 2007, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#52.2	SemEval 2007, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.4	SemEval 2007, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#63.05	SemEval 2007, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#67.05	SemEval 2007, F1	Adv  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#59.10	SemEval 2007, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#55.8	SemEval 2007, F1	Total  Table 2. We also present in
true	P12-2028.pdf#64.4	SemEval 2007, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#64.5	SemEval 2007, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#62.1	SemEval 2007, F1	Total  Table 2. We also present in
true	P12-2028.pdf#45.5	SemEval 2007, F1	Total  Table 2. We also present in
true	P12-2028.pdf#41.2	SemEval 2007, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#47.52	SemEval 2007, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#61.2	SemEval 2007, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#69.7	SemEval 2007, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#67.05	SemEval 2007, F1	Adv  Table 2. We also present in
true	P12-2028.pdf#70.8	SemEval 2007, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#64.5	SemEval 2007, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#39.9	SemEval 2007, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#60.5	SemEval 2007, F1	Total  Table 2. We also present in
true	P12-2028.pdf#53.5	SemEval 2007, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#48.8	SemEval 2007, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#67.97	SemEval 2007, F1	Adv  Table 2. We also present in
true	P12-2028.pdf#64.64	SemEval 2007, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#41.2	SemEval 2007, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#52.2	SemEval 2007, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#67.1	SemEval 2007, F1	Adv  Table 2. We also present in
true	P12-2028.pdf#64.4	SemEval 2007, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#43.9	SemEval 2007, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#71.45	SemEval 2007, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#45.1	SemEval 2007, F1	Total  Table 2. We also present in
true	P12-2028.pdf#70.64	SemEval 2007, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#61.5	SemEval 2007, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#59.10	SemEval 2007, F1	Total  Table 2. We also present in
true	P12-2028.pdf#57.4	SemEval 2007, F1	Total  Table 2. We also present in
true	P12-2028.pdf#71.44	SemEval 2007, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#67.1	SemEval 2007, F1	Adv  Table 2. We also present in
true	P12-2028.pdf#51.72	SemEval 2007, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#63.05	SemEval 2007, F1	Total  Table 2. We also present in
true	P12-2028.pdf#34.9	SemEval 2007, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#-59.19	SemEval 2007, F1	Semcor  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#-45.9	SemEval 2007, F1	SE07  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#-60.5	SemEval 2007, F1	SE2  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#-55.8	SemEval 2007, F1	SE3  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#-60.5	SemEval 2007, F1	SE2  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#1	Senseval 2, F1	financial  Table 1: Three possible hypotheses of latent vectors for
true	P12-2028.pdf#2	Senseval 2, F1	financial 20  Table 1: Three possible hypotheses of latent vectors for
true	P12-2028.pdf#3	Senseval 2, F1	financial 20 18  Table 1: Three possible hypotheses of latent vectors for
true	P12-2028.pdf#34.9	Senseval 2, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#67.1	Senseval 2, F1	Adv  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.64	Senseval 2, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#70.8	Senseval 2, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#48.8	Senseval 2, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.5	Senseval 2, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#61.5	Senseval 2, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#43.9	Senseval 2, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#51.72	Senseval 2, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.5	Senseval 2, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#57.4	Senseval 2, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#69.7	Senseval 2, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#41.2	Senseval 2, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#67.1	Senseval 2, F1	Adv  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#55.8	Senseval 2, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#67.97	Senseval 2, F1	Adv  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#45.1	Senseval 2, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#70.64	Senseval 2, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#53.5	Senseval 2, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#41.2	Senseval 2, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#71.45	Senseval 2, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#60.5	Senseval 2, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#45.5	Senseval 2, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#39.9	Senseval 2, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#61.2	Senseval 2, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#71.44	Senseval 2, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.4	Senseval 2, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#62.1	Senseval 2, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#47.52	Senseval 2, F1	Verb  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#52.2	Senseval 2, F1	Noun  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#64.4	Senseval 2, F1	Adj  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#63.05	Senseval 2, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#67.05	Senseval 2, F1	Adv  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#59.10	Senseval 2, F1	Total  Table 2: WSD results per POS (K = 100)
true	P12-2028.pdf#55.8	Senseval 2, F1	Total  Table 2. We also present in
true	P12-2028.pdf#64.4	Senseval 2, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#64.5	Senseval 2, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#62.1	Senseval 2, F1	Total  Table 2. We also present in
true	P12-2028.pdf#45.5	Senseval 2, F1	Total  Table 2. We also present in
true	P12-2028.pdf#41.2	Senseval 2, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#47.52	Senseval 2, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#61.2	Senseval 2, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#69.7	Senseval 2, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#67.05	Senseval 2, F1	Adv  Table 2. We also present in
true	P12-2028.pdf#70.8	Senseval 2, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#64.5	Senseval 2, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#39.9	Senseval 2, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#60.5	Senseval 2, F1	Total  Table 2. We also present in
true	P12-2028.pdf#53.5	Senseval 2, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#48.8	Senseval 2, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#67.97	Senseval 2, F1	Adv  Table 2. We also present in
true	P12-2028.pdf#64.64	Senseval 2, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#41.2	Senseval 2, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#52.2	Senseval 2, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#67.1	Senseval 2, F1	Adv  Table 2. We also present in
true	P12-2028.pdf#64.4	Senseval 2, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#43.9	Senseval 2, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#71.45	Senseval 2, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#45.1	Senseval 2, F1	Total  Table 2. We also present in
true	P12-2028.pdf#70.64	Senseval 2, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#61.5	Senseval 2, F1	Noun  Table 2. We also present in
true	P12-2028.pdf#59.10	Senseval 2, F1	Total  Table 2. We also present in
true	P12-2028.pdf#57.4	Senseval 2, F1	Total  Table 2. We also present in
true	P12-2028.pdf#71.44	Senseval 2, F1	Adj  Table 2. We also present in
true	P12-2028.pdf#67.1	Senseval 2, F1	Adv  Table 2. We also present in
true	P12-2028.pdf#51.72	Senseval 2, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#63.05	Senseval 2, F1	Total  Table 2. We also present in
true	P12-2028.pdf#34.9	Senseval 2, F1	Verb  Table 2. We also present in
true	P12-2028.pdf#-59.19	Senseval 2, F1	Semcor  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#-45.9	Senseval 2, F1	SE07  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#-60.5	Senseval 2, F1	SE2  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#-55.8	Senseval 2, F1	SE3  Table 3: ldavec and wmfvec (latter) results per 
true	P12-2028.pdf#-60.5	Senseval 2, F1	SE2  Table 3: ldavec and wmfvec (latter) results per 
true	D13-1049.pdf#15.2	WMT 2014 EN-FR, BLEU	1 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#28.7	WMT 2014 EN-FR, BLEU	♠ rule  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#29.0	WMT 2014 EN-FR, BLEU	1 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#15.9	WMT 2014 EN-FR, BLEU	1 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#28.9	WMT 2014 EN-FR, BLEU	♠ 2 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#15.2	WMT 2014 EN-FR, BLEU	rule  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#22.5	WMT 2014 EN-FR, BLEU	♠ 1 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#15.1	WMT 2014 EN-FR, BLEU	lexical  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#27.2	WMT 2014 EN-FR, BLEU	1 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#27.3♠	WMT 2014 EN-FR, BLEU	2 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#28.7	WMT 2014 EN-FR, BLEU	♠ lexical  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#28.8♠♣	WMT 2014 EN-FR, BLEU	2 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#21.9	WMT 2014 EN-FR, BLEU	1 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#22.5	WMT 2014 EN-FR, BLEU	♠ rule  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#28.8	WMT 2014 EN-FR, BLEU	♠ 1 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#21.8	WMT 2014 EN-FR, BLEU	2 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#22.7	WMT 2014 EN-FR, BLEU	♠ 2 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#15.2	WMT 2014 EN-FR, BLEU	2 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#15.7	WMT 2014 EN-FR, BLEU	2 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#15.9	WMT 2014 EN-FR, BLEU	rule  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#21.9	WMT 2014 EN-FR, BLEU	rule  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#34.3	WMT 2014 EN-FR, BLEU	2 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#12.6	WMT 2014 EN-FR, BLEU	2 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#19.1♣	WMT 2014 EN-FR, BLEU	2 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#20.2	WMT 2014 EN-FR, BLEU	1 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#22.9	WMT 2014 EN-FR, BLEU	1 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#20.2	WMT 2014 EN-FR, BLEU	2 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#34.0	WMT 2014 EN-FR, BLEU	1 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#18.6♣	WMT 2014 EN-FR, BLEU	2 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#22.9	WMT 2014 EN-FR, BLEU	2 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#32.7♣	WMT 2014 EN-FR, BLEU	2 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#12.5	WMT 2014 EN-FR, BLEU	1 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#18.0♣	WMT 2014 EN-FR, BLEU	2 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#32.4♣	WMT 2014 EN-FR, BLEU	2 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#36.6	WMT 2014 EN-FR, BLEU	1 - step  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#22.7	WMT 2014 EN-FR, BLEU	2 - step  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#35.7	WMT 2014 EN-FR, BLEU	rule  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#36.5	WMT 2014 EN-FR, BLEU	rule  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#32.6	WMT 2014 EN-FR, BLEU	1 - step  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#40.1	WMT 2014 EN-FR, BLEU	rule  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#26.4	WMT 2014 EN-FR, BLEU	1 - step  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#26.4	WMT 2014 EN-FR, BLEU	2 - step  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#40.1	WMT 2014 EN-FR, BLEU	1 - step  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#32.8	WMT 2014 EN-FR, BLEU	2 - step  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#22.7	WMT 2014 EN-FR, BLEU	1 - step  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#36.7	WMT 2014 EN-FR, BLEU	2 - step  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#40.1	WMT 2014 EN-FR, BLEU	2 - step  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#15.2	WMT 2014 EN-DE, BLEU	1 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#28.7	WMT 2014 EN-DE, BLEU	♠ rule  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#29.0	WMT 2014 EN-DE, BLEU	1 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#15.9	WMT 2014 EN-DE, BLEU	1 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#28.9	WMT 2014 EN-DE, BLEU	♠ 2 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#15.2	WMT 2014 EN-DE, BLEU	rule  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#22.5	WMT 2014 EN-DE, BLEU	♠ 1 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#15.1	WMT 2014 EN-DE, BLEU	lexical  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#27.2	WMT 2014 EN-DE, BLEU	1 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#27.3♠	WMT 2014 EN-DE, BLEU	2 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#28.7	WMT 2014 EN-DE, BLEU	♠ lexical  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#28.8♠♣	WMT 2014 EN-DE, BLEU	2 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#21.9	WMT 2014 EN-DE, BLEU	1 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#22.5	WMT 2014 EN-DE, BLEU	♠ rule  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#28.8	WMT 2014 EN-DE, BLEU	♠ 1 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#21.8	WMT 2014 EN-DE, BLEU	2 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#22.7	WMT 2014 EN-DE, BLEU	♠ 2 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#15.2	WMT 2014 EN-DE, BLEU	2 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#15.7	WMT 2014 EN-DE, BLEU	2 - step  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#15.9	WMT 2014 EN-DE, BLEU	rule  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#21.9	WMT 2014 EN-DE, BLEU	rule  Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are  only included when better than the phrase-based results. The base system includes a distance distortion model; the  lexical system adds lexical reordering; rule is the rule preordering system of Genzel
true	D13-1049.pdf#34.3	WMT 2014 EN-DE, BLEU	2 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#12.6	WMT 2014 EN-DE, BLEU	2 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#19.1♣	WMT 2014 EN-DE, BLEU	2 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#20.2	WMT 2014 EN-DE, BLEU	1 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#22.9	WMT 2014 EN-DE, BLEU	1 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#20.2	WMT 2014 EN-DE, BLEU	2 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#34.0	WMT 2014 EN-DE, BLEU	1 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#18.6♣	WMT 2014 EN-DE, BLEU	2 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#22.9	WMT 2014 EN-DE, BLEU	2 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#32.7♣	WMT 2014 EN-DE, BLEU	2 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#12.5	WMT 2014 EN-DE, BLEU	1 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#18.0♣	WMT 2014 EN-DE, BLEU	2 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#32.4♣	WMT 2014 EN-DE, BLEU	2 - step  Table 3: BLEU scores for language from various lan- guage families: Arabic (ar), Welsh (cy), Irish (ga), In- donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),  and Malay (ms). Lexical reordering is not included in  any of the systems. Bolded results are significant at 99%.  ♣ is significantly better than in a human eval at 95%.
true	D13-1049.pdf#36.6	WMT 2014 EN-DE, BLEU	1 - step  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#22.7	WMT 2014 EN-DE, BLEU	2 - step  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#35.7	WMT 2014 EN-DE, BLEU	rule  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#36.5	WMT 2014 EN-DE, BLEU	rule  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#32.6	WMT 2014 EN-DE, BLEU	1 - step  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#40.1	WMT 2014 EN-DE, BLEU	rule  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#26.4	WMT 2014 EN-DE, BLEU	1 - step  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#26.4	WMT 2014 EN-DE, BLEU	2 - step  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#40.1	WMT 2014 EN-DE, BLEU	1 - step  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#32.8	WMT 2014 EN-DE, BLEU	2 - step  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#22.7	WMT 2014 EN-DE, BLEU	1 - step  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#36.7	WMT 2014 EN-DE, BLEU	2 - step  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	D13-1049.pdf#40.1	WMT 2014 EN-DE, BLEU	2 - step  Table 4: BLEU scores for translating to and from En- glish for: Hungarian (hu), Dutch (nl), and Portuguese  (pt). Lexical reordering is not used for any language pair.  Bolded results are significant at 99%.
true	P12-1033.pdf#+0.8	WMT 2014 EN-DE, BLEU	Bleu ( % ) 2008  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+6.7	WMT 2014 EN-DE, BLEU	align F1 ( % ) 2008  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+0.6	WMT 2014 EN-DE, BLEU	Bleu ( % ) 2009  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+0.4	WMT 2014 EN-DE, BLEU	Bleu ( % ) 2010  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+0.6*	WMT 2014 EN-DE, BLEU	Bleu ( % ) 2008  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+5.9	WMT 2014 EN-DE, BLEU	align F1 ( % ) 2008  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+1.4	WMT 2014 EN-DE, BLEU	Bleu ( % ) 2009  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+3.3	WMT 2014 EN-DE, BLEU	align F1 ( % ) 2008  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+1.2	WMT 2014 EN-DE, BLEU	Bleu ( % ) 2009  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+0.9	WMT 2014 EN-DE, BLEU	Bleu ( % ) 2008  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+0.4	WMT 2014 EN-DE, BLEU	Bleu ( % ) 2009  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+1.3	WMT 2014 EN-DE, BLEU	Bleu ( % ) 2008  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+5.6	WMT 2014 EN-DE, BLEU	align F1 ( % ) 2008  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#65.9	WMT 2014 EN-DE, BLEU	- α model 750  Table 2: Almost all hyperparameter settings achieve higher F-scores than the baseline IBM Model 4 and HMM model  for Arabic-English alignment (α = 0).
true	P12-1033.pdf#62.7	WMT 2014 EN-DE, BLEU	- α model 750  Table 2: Almost all hyperparameter settings achieve higher F-scores than the baseline IBM Model 4 and HMM model  for Arabic-English alignment (α = 0).
true	P12-1033.pdf#+4.9	WMT 2014 EN-DE, BLEU	word classes ? no  Table 3: Adding word classes improves the F-score in  both directions for Arabic-English alignment by a little,  for the baseline system more so than ours.
true	P12-1033.pdf#+14.9	WMT 2014 EN-DE, BLEU	word classes ? no  Table 3: Adding word classes improves the F-score in  both directions for Arabic-English alignment by a little,  for the baseline system more so than ours.
true	P12-1033.pdf#63.9	WMT 2014 EN-DE, BLEU	word classes ? no  Table 3: Adding word classes improves the F-score in  both directions for Arabic-English alignment by a little,  for the baseline system more so than ours.
true	P12-1033.pdf#+5.1	WMT 2014 EN-DE, BLEU	word classes ? yes  Table 3: Adding word classes improves the F-score in  both directions for Arabic-English alignment by a little,  for the baseline system more so than ours.
true	P12-1033.pdf#65.9	WMT 2014 EN-DE, BLEU	word classes ? yes  Table 3: Adding word classes improves the F-score in  both directions for Arabic-English alignment by a little,  for the baseline system more so than ours.
true	P12-1033.pdf#+13.8	WMT 2014 EN-DE, BLEU	word classes ? yes  Table 3: Adding word classes improves the F-score in  both directions for Arabic-English alignment by a little,  for the baseline system more so than ours.
true	P12-1033.pdf#69.2	WMT 2014 EN-DE, BLEU	word classes ? no  Table 3: Adding word classes improves the F-score in  both directions for Arabic-English alignment by a little,  for the baseline system more so than ours.
true	P12-1033.pdf#70.3	WMT 2014 EN-DE, BLEU	word classes ? yes  Table 3: Adding word classes improves the F-score in  both directions for Arabic-English alignment by a little,  for the baseline system more so than ours.
true	P12-1033.pdf#+0.8	WMT 2014 EN-FR, BLEU	Bleu ( % ) 2008  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+6.7	WMT 2014 EN-FR, BLEU	align F1 ( % ) 2008  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+0.6	WMT 2014 EN-FR, BLEU	Bleu ( % ) 2009  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+0.4	WMT 2014 EN-FR, BLEU	Bleu ( % ) 2010  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+0.6*	WMT 2014 EN-FR, BLEU	Bleu ( % ) 2008  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+5.9	WMT 2014 EN-FR, BLEU	align F1 ( % ) 2008  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+1.4	WMT 2014 EN-FR, BLEU	Bleu ( % ) 2009  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+3.3	WMT 2014 EN-FR, BLEU	align F1 ( % ) 2008  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+1.2	WMT 2014 EN-FR, BLEU	Bleu ( % ) 2009  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+0.9	WMT 2014 EN-FR, BLEU	Bleu ( % ) 2008  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+0.4	WMT 2014 EN-FR, BLEU	Bleu ( % ) 2009  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+1.3	WMT 2014 EN-FR, BLEU	Bleu ( % ) 2008  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#+5.6	WMT 2014 EN-FR, BLEU	align F1 ( % ) 2008  Table 1: Adding the 0 -norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the  lexical weighting table) is reduced. The˜φThe˜ The˜φ sing. column shows the average fertility of once-seen source words. For  Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open  MT Evaluation.  *  Half of this test set was also used for tuning feature weights.
true	P12-1033.pdf#65.9	WMT 2014 EN-FR, BLEU	- α model 750  Table 2: Almost all hyperparameter settings achieve higher F-scores than the baseline IBM Model 4 and HMM model  for Arabic-English alignment (α = 0).
true	P12-1033.pdf#62.7	WMT 2014 EN-FR, BLEU	- α model 750  Table 2: Almost all hyperparameter settings achieve higher F-scores than the baseline IBM Model 4 and HMM model  for Arabic-English alignment (α = 0).
true	P12-1033.pdf#+4.9	WMT 2014 EN-FR, BLEU	word classes ? no  Table 3: Adding word classes improves the F-score in  both directions for Arabic-English alignment by a little,  for the baseline system more so than ours.
true	P12-1033.pdf#+14.9	WMT 2014 EN-FR, BLEU	word classes ? no  Table 3: Adding word classes improves the F-score in  both directions for Arabic-English alignment by a little,  for the baseline system more so than ours.
true	P12-1033.pdf#63.9	WMT 2014 EN-FR, BLEU	word classes ? no  Table 3: Adding word classes improves the F-score in  both directions for Arabic-English alignment by a little,  for the baseline system more so than ours.
true	P12-1033.pdf#+5.1	WMT 2014 EN-FR, BLEU	word classes ? yes  Table 3: Adding word classes improves the F-score in  both directions for Arabic-English alignment by a little,  for the baseline system more so than ours.
true	P12-1033.pdf#65.9	WMT 2014 EN-FR, BLEU	word classes ? yes  Table 3: Adding word classes improves the F-score in  both directions for Arabic-English alignment by a little,  for the baseline system more so than ours.
true	P12-1033.pdf#+13.8	WMT 2014 EN-FR, BLEU	word classes ? yes  Table 3: Adding word classes improves the F-score in  both directions for Arabic-English alignment by a little,  for the baseline system more so than ours.
true	P12-1033.pdf#69.2	WMT 2014 EN-FR, BLEU	word classes ? no  Table 3: Adding word classes improves the F-score in  both directions for Arabic-English alignment by a little,  for the baseline system more so than ours.
true	P12-1033.pdf#70.3	WMT 2014 EN-FR, BLEU	word classes ? yes  Table 3: Adding word classes improves the F-score in  both directions for Arabic-English alignment by a little,  for the baseline system more so than ours.
true	D13-1062.pdf#26broad	Penn Treebank, Number of params	1 v [ +nom ] n v v c v , a , z m  Table 2: Examples of mapping between CTB and  PDD's tagset
true	D13-1062.pdf#94.74	Penn Treebank, Number of params	P  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#94.92	Penn Treebank, Number of params	F1  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#89.99	Penn Treebank, Number of params	F1  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#95.11	Penn Treebank, Number of params	R  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#89.73	Penn Treebank, Number of params	R  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#90.25	Penn Treebank, Number of params	P  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#89.41	Penn Treebank, Number of params	P  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#89.16	Penn Treebank, Number of params	R  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#88.58	Penn Treebank, Number of params	R  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#89.11	Penn Treebank, Number of params	P  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#89.13	Penn Treebank, Number of params	F1  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#88.99	Penn Treebank, Number of params	F1  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#26broad	VLSP 2013 word segmentation shared task, F1	1 v [ +nom ] n v v c v , a , z m  Table 2: Examples of mapping between CTB and  PDD's tagset
true	D13-1062.pdf#94.74	VLSP 2013 word segmentation shared task, F1	P  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#94.92	VLSP 2013 word segmentation shared task, F1	F1  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#89.99	VLSP 2013 word segmentation shared task, F1	F1  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#95.11	VLSP 2013 word segmentation shared task, F1	R  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#89.73	VLSP 2013 word segmentation shared task, F1	R  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#90.25	VLSP 2013 word segmentation shared task, F1	P  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#89.41	VLSP 2013 word segmentation shared task, F1	P  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#89.16	VLSP 2013 word segmentation shared task, F1	R  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#88.58	VLSP 2013 word segmentation shared task, F1	R  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#89.11	VLSP 2013 word segmentation shared task, F1	P  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#89.13	VLSP 2013 word segmentation shared task, F1	F1  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#88.99	VLSP 2013 word segmentation shared task, F1	F1  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#26broad	VLSP 2013 POS tagging shared task, Accuracy	1 v [ +nom ] n v v c v , a , z m  Table 2: Examples of mapping between CTB and  PDD's tagset
true	D13-1062.pdf#94.74	VLSP 2013 POS tagging shared task, Accuracy	P  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#94.92	VLSP 2013 POS tagging shared task, Accuracy	F1  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#89.99	VLSP 2013 POS tagging shared task, Accuracy	F1  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#95.11	VLSP 2013 POS tagging shared task, Accuracy	R  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#89.73	VLSP 2013 POS tagging shared task, Accuracy	R  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#90.25	VLSP 2013 POS tagging shared task, Accuracy	P  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#89.41	VLSP 2013 POS tagging shared task, Accuracy	P  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#89.16	VLSP 2013 POS tagging shared task, Accuracy	R  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#88.58	VLSP 2013 POS tagging shared task, Accuracy	R  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#89.11	VLSP 2013 POS tagging shared task, Accuracy	P  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#89.13	VLSP 2013 POS tagging shared task, Accuracy	F1  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#88.99	VLSP 2013 POS tagging shared task, Accuracy	F1  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#26broad	Penn Treebank, Test perplexity	1 v [ +nom ] n v v c v , a , z m  Table 2: Examples of mapping between CTB and  PDD's tagset
true	D13-1062.pdf#94.74	Penn Treebank, Test perplexity	P  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#94.92	Penn Treebank, Test perplexity	F1  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#89.99	Penn Treebank, Test perplexity	F1  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#95.11	Penn Treebank, Test perplexity	R  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#89.73	Penn Treebank, Test perplexity	R  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#90.25	Penn Treebank, Test perplexity	P  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#89.41	Penn Treebank, Test perplexity	P  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#89.16	Penn Treebank, Test perplexity	R  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#88.58	Penn Treebank, Test perplexity	R  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#89.11	Penn Treebank, Test perplexity	P  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#89.13	Penn Treebank, Test perplexity	F1  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#88.99	Penn Treebank, Test perplexity	F1  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#26broad	PKU, F1	1 v [ +nom ] n v v c v , a , z m  Table 2: Examples of mapping between CTB and  PDD's tagset
true	D13-1062.pdf#94.74	PKU, F1	P  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#94.92	PKU, F1	F1  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#89.99	PKU, F1	F1  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#95.11	PKU, F1	R  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#89.73	PKU, F1	R  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#90.25	PKU, F1	P  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#89.41	PKU, F1	P  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#89.16	PKU, F1	R  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#88.58	PKU, F1	R  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#89.11	PKU, F1	P  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#89.13	PKU, F1	F1  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#88.99	PKU, F1	F1  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#26broad	Penn Treebank, Accuracy	1 v [ +nom ] n v v c v , a , z m  Table 2: Examples of mapping between CTB and  PDD's tagset
true	D13-1062.pdf#94.74	Penn Treebank, Accuracy	P  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#94.92	Penn Treebank, Accuracy	F1  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#89.99	Penn Treebank, Accuracy	F1  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#95.11	Penn Treebank, Accuracy	R  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#89.73	Penn Treebank, Accuracy	R  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#90.25	Penn Treebank, Accuracy	P  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#89.41	Penn Treebank, Accuracy	P  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#89.16	Penn Treebank, Accuracy	R  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#88.58	Penn Treebank, Accuracy	R  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#89.11	Penn Treebank, Accuracy	P  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#89.13	Penn Treebank, Accuracy	F1  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#88.99	Penn Treebank, Accuracy	F1  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#26broad	Chinese Treebank 6, F1	1 v [ +nom ] n v v c v , a , z m  Table 2: Examples of mapping between CTB and  PDD's tagset
true	D13-1062.pdf#94.74	Chinese Treebank 6, F1	P  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#94.92	Chinese Treebank 6, F1	F1  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#89.99	Chinese Treebank 6, F1	F1  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#95.11	Chinese Treebank 6, F1	R  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#89.73	Chinese Treebank 6, F1	R  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#90.25	Chinese Treebank 6, F1	P  Table 5: Performances of different systems on CTB-5 and PPD.
true	D13-1062.pdf#89.41	Chinese Treebank 6, F1	P  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#89.16	Chinese Treebank 6, F1	R  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#88.58	Chinese Treebank 6, F1	R  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#89.11	Chinese Treebank 6, F1	P  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#89.13	Chinese Treebank 6, F1	F1  Table 6: Performances of different systems on CTB-S and PPD.
true	D13-1062.pdf#88.99	Chinese Treebank 6, F1	F1  Table 6: Performances of different systems on CTB-S and PPD.
true	P15-2041.pdf#0.075	CCGBank, Accuracy	) . RNN  Table 2: Multi-tagging accuracy and ambiguity comparison (supertags/word) at the default C&C β levels  on CCGBank Section 00.
true	P15-2041.pdf#0.001	CCGBank, Accuracy	) . RNN β  Table 2: Multi-tagging accuracy and ambiguity comparison (supertags/word) at the default C&C β levels  on CCGBank Section 00.
true	P15-2041.pdf#0.030	CCGBank, Accuracy	) . RNN β  Table 2: Multi-tagging accuracy and ambiguity comparison (supertags/word) at the default C&C β levels  on CCGBank Section 00.
true	P15-2041.pdf#0.010	CCGBank, Accuracy	) . RNN β  Table 2: Multi-tagging accuracy and ambiguity comparison (supertags/word) at the default C&C β levels  on CCGBank Section 00.
true	P15-2041.pdf#0.005	CCGBank, Accuracy	) . RNN β  Table 2: Multi-tagging accuracy and ambiguity comparison (supertags/word) at the default C&C β levels  on CCGBank Section 00.
true	P15-2041.pdf#94.04	CCGBank, Accuracy	100 CAT  Table 4: Parsing development results on CCGBank Section 00 (auto POS).
true	P15-2041.pdf#86.25	CCGBank, Accuracy	100 LR  Table 4: Parsing development results on CCGBank Section 00 (auto POS).
true	P15-2041.pdf#35.20	CCGBank, Accuracy	100 SENT  Table 4: Parsing development results on CCGBank Section 00 (auto POS).
true	P15-2041.pdf#99.42	CCGBank, Accuracy	100 cov .  Table 4: Parsing development results on CCGBank Section 00 (auto POS).
true	P15-2041.pdf#93.84	CCGBank, Accuracy	100 CAT  Table 4: Parsing development results on CCGBank Section 00 (auto POS).
true	P15-2041.pdf#87.00	CCGBank, Accuracy	100 LF  Table 4: Parsing development results on CCGBank Section 00 (auto POS).
true	P15-2041.pdf#99.42	CCGBank, Accuracy	100 cov .  Table 4: Parsing development results on CCGBank Section 00 (auto POS).
true	P15-2041.pdf#34.97	CCGBank, Accuracy	100 SENT  Table 4: Parsing development results on CCGBank Section 00 (auto POS).
true	P15-2041.pdf#87.77	CCGBank, Accuracy	100 LP  Table 4: Parsing development results on CCGBank Section 00 (auto POS).
true	P15-2041.pdf#87.73	CCGBank, Accuracy	100 LP  Table 4: Parsing development results on CCGBank Section 00 (auto POS).
true	P15-2041.pdf#86.25	CCGBank, Accuracy	100 LF  Table 4: Parsing development results on CCGBank Section 00 (auto POS).
true	P15-2041.pdf#84.83	CCGBank, Accuracy	100 LR  Table 4: Parsing development results on CCGBank Section 00 (auto POS).
true	P15-2041.pdf#81.78	CCGBank, Accuracy	Wikipedia LR  Table 5: Parsing test results on all three domains (auto POS). We evaluate on all sentences (100%  coverage) as well as on only those sentences that returned spanning analyses (% cov.). RNN and NN  both have 100% coverage on the Wikipedia data.
true	P15-2041.pdf#80.10	CCGBank, Accuracy	Bioinfer LP  Table 5: Parsing test results on all three domains (auto POS). We evaluate on all sentences (100%  coverage) as well as on only those sentences that returned spanning analyses (% cov.). RNN and NN  both have 100% coverage on the Wikipedia data.
true	P15-2041.pdf#86.41	CCGBank, Accuracy	100 CCGBank Section 23 LR  Table 5: Parsing test results on all three domains (auto POS). We evaluate on all sentences (100%  coverage) as well as on only those sentences that returned spanning analyses (% cov.). RNN and NN  both have 100% coverage on the Wikipedia data.
true	P15-2041.pdf#80.10	CCGBank, Accuracy	100 Bioinfer LP  Table 5: Parsing test results on all three domains (auto POS). We evaluate on all sentences (100%  coverage) as well as on only those sentences that returned spanning analyses (% cov.). RNN and NN  both have 100% coverage on the Wikipedia data.
true	P15-2041.pdf#97.80	CCGBank, Accuracy	Bioinfer cov .  Table 5: Parsing test results on all three domains (auto POS). We evaluate on all sentences (100%  coverage) as well as on only those sentences that returned spanning analyses (% cov.). RNN and NN  both have 100% coverage on the Wikipedia data.
true	P15-2041.pdf#83.22	CCGBank, Accuracy	Wikipedia LP  Table 5: Parsing test results on all three domains (auto POS). We evaluate on all sentences (100%  coverage) as well as on only those sentences that returned spanning analyses (% cov.). RNN and NN  both have 100% coverage on the Wikipedia data.
true	P15-2041.pdf#86.47	CCGBank, Accuracy	CCGBank Section 23 LR  Table 5: Parsing test results on all three domains (auto POS). We evaluate on all sentences (100%  coverage) as well as on only those sentences that returned spanning analyses (% cov.). RNN and NN  both have 100% coverage on the Wikipedia data.
true	P15-2041.pdf#79.19	CCGBank, Accuracy	Bioinfer LF  Table 5: Parsing test results on all three domains (auto POS). We evaluate on all sentences (100%  coverage) as well as on only those sentences that returned spanning analyses (% cov.). RNN and NN  both have 100% coverage on the Wikipedia data.
true	P15-2041.pdf#77.74	CCGBank, Accuracy	100 Bioinfer LF  Table 5: Parsing test results on all three domains (auto POS). We evaluate on all sentences (100%  coverage) as well as on only those sentences that returned spanning analyses (% cov.). RNN and NN  both have 100% coverage on the Wikipedia data.
true	P15-2041.pdf#87.68	CCGBank, Accuracy	CCGBank Section 23 LP  Table 5: Parsing test results on all three domains (auto POS). We evaluate on all sentences (100%  coverage) as well as on only those sentences that returned spanning analyses (% cov.). RNN and NN  both have 100% coverage on the Wikipedia data.
true	P15-2041.pdf#99.96	CCGBank, Accuracy	CCGBank Section 23 cov .  Table 5: Parsing test results on all three domains (auto POS). We evaluate on all sentences (100%  coverage) as well as on only those sentences that returned spanning analyses (% cov.). RNN and NN  both have 100% coverage on the Wikipedia data.
true	P15-2041.pdf#87.07	CCGBank, Accuracy	CCGBank Section 23 LF  Table 5: Parsing test results on all three domains (auto POS). We evaluate on all sentences (100%  coverage) as well as on only those sentences that returned spanning analyses (% cov.). RNN and NN  both have 100% coverage on the Wikipedia data.
true	P15-2041.pdf#75.52	CCGBank, Accuracy	100 Bioinfer LR  Table 5: Parsing test results on all three domains (auto POS). We evaluate on all sentences (100%  coverage) as well as on only those sentences that returned spanning analyses (% cov.). RNN and NN  both have 100% coverage on the Wikipedia data.
true	P15-2041.pdf#82.49	CCGBank, Accuracy	Wikipedia LF  Table 5: Parsing test results on all three domains (auto POS). We evaluate on all sentences (100%  coverage) as well as on only those sentences that returned spanning analyses (% cov.). RNN and NN  both have 100% coverage on the Wikipedia data.
true	P15-2041.pdf#78.62	CCGBank, Accuracy	Bioinfer LR  Table 5: Parsing test results on all three domains (auto POS). We evaluate on all sentences (100%  coverage) as well as on only those sentences that returned spanning analyses (% cov.). RNN and NN  both have 100% coverage on the Wikipedia data.
true	P15-2041.pdf#87.04	CCGBank, Accuracy	100 CCGBank Section 23 LF  Table 5: Parsing test results on all three domains (auto POS). We evaluate on all sentences (100%  coverage) as well as on only those sentences that returned spanning analyses (% cov.). RNN and NN  both have 100% coverage on the Wikipedia data.
true	P15-2041.pdf#87.68	CCGBank, Accuracy	100 CCGBank Section 23 LP  Table 5: Parsing test results on all three domains (auto POS). We evaluate on all sentences (100%  coverage) as well as on only those sentences that returned spanning analyses (% cov.). RNN and NN  both have 100% coverage on the Wikipedia data.
true	D15-1154.pdf#0.05.	Penn Treebank, Number of params	is Johansson and Nugues ( 2008 ) , and NV06 is Nivre et al . ( 2006 ) Bold indicates the best result for a Model 0 LAS UAS and LAS of non - projective versions of our parsing algorithms on 14 treebanks from MD06 is McDonald et al . ( 2006 ) , RD06 is Riedel et al . ( 2006 ) , JN08 Red values represent statistically significant improvements over two - stage baseline system  Table 1: UAS and LAS of non-projective versions of our parsing algorithms on 14 treebanks from  CoNLL shared tasks, together with three baseline systems and the best systems for each language re- ported in CoNLL shared tasks. MD06 is McDonald et al. (2006), RD06 is Riedel et al. (2006), JN08  is Johansson and Nugues (2008), and NV06 is Nivre et al. (2006) Bold indicates the best result for a  language. Red values represent statistically significant improvements over two-stage baseline system  on the corresponding metrics with p < 0.01, using McNemar's test. Blue values indicate statistically  significant improvements with p < 0.05.
true	D15-1154.pdf#0.01,usingMcNemar'stest.	Penn Treebank, Number of params	is Johansson and Nugues ( 2008 ) , and NV06 is Nivre et al . ( 2006 ) Bold indicates the best result for a Our Model LAS UAS and LAS of non - projective versions of our parsing algorithms on 14 treebanks from MD06 is McDonald et al . ( 2006 ) , RD06 is Riedel et al . ( 2006 ) , JN08  Table 1: UAS and LAS of non-projective versions of our parsing algorithms on 14 treebanks from  CoNLL shared tasks, together with three baseline systems and the best systems for each language re- ported in CoNLL shared tasks. MD06 is McDonald et al. (2006), RD06 is Riedel et al. (2006), JN08  is Johansson and Nugues (2008), and NV06 is Nivre et al. (2006) Bold indicates the best result for a  language. Red values represent statistically significant improvements over two-stage baseline system  on the corresponding metrics with p < 0.01, using McNemar's test. Blue values indicate statistically  significant improvements with p < 0.05.
true	D15-1154.pdf#92.4	Penn Treebank, Number of params	UAS  Table 2: Parsing performance on PTB. The re- sults for MaltParser, MSTParser and DNNParser  are from table 5 of Chen and Manning (2014).
true	D15-1154.pdf#89.9	Penn Treebank, Number of params	LAS  Table 2: Parsing performance on PTB. The re- sults for MaltParser, MSTParser and DNNParser  are from table 5 of Chen and Manning (2014).
true	D15-1154.pdf#0.05.	Penn Treebank, POS	is Johansson and Nugues ( 2008 ) , and NV06 is Nivre et al . ( 2006 ) Bold indicates the best result for a Model 0 LAS UAS and LAS of non - projective versions of our parsing algorithms on 14 treebanks from MD06 is McDonald et al . ( 2006 ) , RD06 is Riedel et al . ( 2006 ) , JN08 Red values represent statistically significant improvements over two - stage baseline system  Table 1: UAS and LAS of non-projective versions of our parsing algorithms on 14 treebanks from  CoNLL shared tasks, together with three baseline systems and the best systems for each language re- ported in CoNLL shared tasks. MD06 is McDonald et al. (2006), RD06 is Riedel et al. (2006), JN08  is Johansson and Nugues (2008), and NV06 is Nivre et al. (2006) Bold indicates the best result for a  language. Red values represent statistically significant improvements over two-stage baseline system  on the corresponding metrics with p < 0.01, using McNemar's test. Blue values indicate statistically  significant improvements with p < 0.05.
true	D15-1154.pdf#0.01,usingMcNemar'stest.	Penn Treebank, POS	is Johansson and Nugues ( 2008 ) , and NV06 is Nivre et al . ( 2006 ) Bold indicates the best result for a Our Model LAS UAS and LAS of non - projective versions of our parsing algorithms on 14 treebanks from MD06 is McDonald et al . ( 2006 ) , RD06 is Riedel et al . ( 2006 ) , JN08  Table 1: UAS and LAS of non-projective versions of our parsing algorithms on 14 treebanks from  CoNLL shared tasks, together with three baseline systems and the best systems for each language re- ported in CoNLL shared tasks. MD06 is McDonald et al. (2006), RD06 is Riedel et al. (2006), JN08  is Johansson and Nugues (2008), and NV06 is Nivre et al. (2006) Bold indicates the best result for a  language. Red values represent statistically significant improvements over two-stage baseline system  on the corresponding metrics with p < 0.01, using McNemar's test. Blue values indicate statistically  significant improvements with p < 0.05.
true	D15-1154.pdf#92.4	Penn Treebank, POS	UAS  Table 2: Parsing performance on PTB. The re- sults for MaltParser, MSTParser and DNNParser  are from table 5 of Chen and Manning (2014).
true	D15-1154.pdf#89.9	Penn Treebank, POS	LAS  Table 2: Parsing performance on PTB. The re- sults for MaltParser, MSTParser and DNNParser  are from table 5 of Chen and Manning (2014).
true	D15-1154.pdf#0.05.	Penn Treebank, LAS	is Johansson and Nugues ( 2008 ) , and NV06 is Nivre et al . ( 2006 ) Bold indicates the best result for a Model 0 LAS UAS and LAS of non - projective versions of our parsing algorithms on 14 treebanks from MD06 is McDonald et al . ( 2006 ) , RD06 is Riedel et al . ( 2006 ) , JN08 Red values represent statistically significant improvements over two - stage baseline system  Table 1: UAS and LAS of non-projective versions of our parsing algorithms on 14 treebanks from  CoNLL shared tasks, together with three baseline systems and the best systems for each language re- ported in CoNLL shared tasks. MD06 is McDonald et al. (2006), RD06 is Riedel et al. (2006), JN08  is Johansson and Nugues (2008), and NV06 is Nivre et al. (2006) Bold indicates the best result for a  language. Red values represent statistically significant improvements over two-stage baseline system  on the corresponding metrics with p < 0.01, using McNemar's test. Blue values indicate statistically  significant improvements with p < 0.05.
true	D15-1154.pdf#0.01,usingMcNemar'stest.	Penn Treebank, LAS	is Johansson and Nugues ( 2008 ) , and NV06 is Nivre et al . ( 2006 ) Bold indicates the best result for a Our Model LAS UAS and LAS of non - projective versions of our parsing algorithms on 14 treebanks from MD06 is McDonald et al . ( 2006 ) , RD06 is Riedel et al . ( 2006 ) , JN08  Table 1: UAS and LAS of non-projective versions of our parsing algorithms on 14 treebanks from  CoNLL shared tasks, together with three baseline systems and the best systems for each language re- ported in CoNLL shared tasks. MD06 is McDonald et al. (2006), RD06 is Riedel et al. (2006), JN08  is Johansson and Nugues (2008), and NV06 is Nivre et al. (2006) Bold indicates the best result for a  language. Red values represent statistically significant improvements over two-stage baseline system  on the corresponding metrics with p < 0.01, using McNemar's test. Blue values indicate statistically  significant improvements with p < 0.05.
true	D15-1154.pdf#92.4	Penn Treebank, LAS	UAS  Table 2: Parsing performance on PTB. The re- sults for MaltParser, MSTParser and DNNParser  are from table 5 of Chen and Manning (2014).
true	D15-1154.pdf#89.9	Penn Treebank, LAS	LAS  Table 2: Parsing performance on PTB. The re- sults for MaltParser, MSTParser and DNNParser  are from table 5 of Chen and Manning (2014).
true	D15-1154.pdf#0.05.	Penn Treebank, UAS	is Johansson and Nugues ( 2008 ) , and NV06 is Nivre et al . ( 2006 ) Bold indicates the best result for a Model 0 LAS UAS and LAS of non - projective versions of our parsing algorithms on 14 treebanks from MD06 is McDonald et al . ( 2006 ) , RD06 is Riedel et al . ( 2006 ) , JN08 Red values represent statistically significant improvements over two - stage baseline system  Table 1: UAS and LAS of non-projective versions of our parsing algorithms on 14 treebanks from  CoNLL shared tasks, together with three baseline systems and the best systems for each language re- ported in CoNLL shared tasks. MD06 is McDonald et al. (2006), RD06 is Riedel et al. (2006), JN08  is Johansson and Nugues (2008), and NV06 is Nivre et al. (2006) Bold indicates the best result for a  language. Red values represent statistically significant improvements over two-stage baseline system  on the corresponding metrics with p < 0.01, using McNemar's test. Blue values indicate statistically  significant improvements with p < 0.05.
true	D15-1154.pdf#0.01,usingMcNemar'stest.	Penn Treebank, UAS	is Johansson and Nugues ( 2008 ) , and NV06 is Nivre et al . ( 2006 ) Bold indicates the best result for a Our Model LAS UAS and LAS of non - projective versions of our parsing algorithms on 14 treebanks from MD06 is McDonald et al . ( 2006 ) , RD06 is Riedel et al . ( 2006 ) , JN08  Table 1: UAS and LAS of non-projective versions of our parsing algorithms on 14 treebanks from  CoNLL shared tasks, together with three baseline systems and the best systems for each language re- ported in CoNLL shared tasks. MD06 is McDonald et al. (2006), RD06 is Riedel et al. (2006), JN08  is Johansson and Nugues (2008), and NV06 is Nivre et al. (2006) Bold indicates the best result for a  language. Red values represent statistically significant improvements over two-stage baseline system  on the corresponding metrics with p < 0.01, using McNemar's test. Blue values indicate statistically  significant improvements with p < 0.05.
true	D15-1154.pdf#92.4	Penn Treebank, UAS	UAS  Table 2: Parsing performance on PTB. The re- sults for MaltParser, MSTParser and DNNParser  are from table 5 of Chen and Manning (2014).
true	D15-1154.pdf#89.9	Penn Treebank, UAS	LAS  Table 2: Parsing performance on PTB. The re- sults for MaltParser, MSTParser and DNNParser  are from table 5 of Chen and Manning (2014).
true	D15-1154.pdf#0.05.	benchmark Vietnamese dependency treebank VnDT, UAS	is Johansson and Nugues ( 2008 ) , and NV06 is Nivre et al . ( 2006 ) Bold indicates the best result for a Model 0 LAS UAS and LAS of non - projective versions of our parsing algorithms on 14 treebanks from MD06 is McDonald et al . ( 2006 ) , RD06 is Riedel et al . ( 2006 ) , JN08 Red values represent statistically significant improvements over two - stage baseline system  Table 1: UAS and LAS of non-projective versions of our parsing algorithms on 14 treebanks from  CoNLL shared tasks, together with three baseline systems and the best systems for each language re- ported in CoNLL shared tasks. MD06 is McDonald et al. (2006), RD06 is Riedel et al. (2006), JN08  is Johansson and Nugues (2008), and NV06 is Nivre et al. (2006) Bold indicates the best result for a  language. Red values represent statistically significant improvements over two-stage baseline system  on the corresponding metrics with p < 0.01, using McNemar's test. Blue values indicate statistically  significant improvements with p < 0.05.
true	D15-1154.pdf#0.01,usingMcNemar'stest.	benchmark Vietnamese dependency treebank VnDT, UAS	is Johansson and Nugues ( 2008 ) , and NV06 is Nivre et al . ( 2006 ) Bold indicates the best result for a Our Model LAS UAS and LAS of non - projective versions of our parsing algorithms on 14 treebanks from MD06 is McDonald et al . ( 2006 ) , RD06 is Riedel et al . ( 2006 ) , JN08  Table 1: UAS and LAS of non-projective versions of our parsing algorithms on 14 treebanks from  CoNLL shared tasks, together with three baseline systems and the best systems for each language re- ported in CoNLL shared tasks. MD06 is McDonald et al. (2006), RD06 is Riedel et al. (2006), JN08  is Johansson and Nugues (2008), and NV06 is Nivre et al. (2006) Bold indicates the best result for a  language. Red values represent statistically significant improvements over two-stage baseline system  on the corresponding metrics with p < 0.01, using McNemar's test. Blue values indicate statistically  significant improvements with p < 0.05.
true	D15-1154.pdf#92.4	benchmark Vietnamese dependency treebank VnDT, UAS	UAS  Table 2: Parsing performance on PTB. The re- sults for MaltParser, MSTParser and DNNParser  are from table 5 of Chen and Manning (2014).
true	D15-1154.pdf#89.9	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 2: Parsing performance on PTB. The re- sults for MaltParser, MSTParser and DNNParser  are from table 5 of Chen and Manning (2014).
true	D15-1154.pdf#0.05.	Penn Treebank, Test perplexity	is Johansson and Nugues ( 2008 ) , and NV06 is Nivre et al . ( 2006 ) Bold indicates the best result for a Model 0 LAS UAS and LAS of non - projective versions of our parsing algorithms on 14 treebanks from MD06 is McDonald et al . ( 2006 ) , RD06 is Riedel et al . ( 2006 ) , JN08 Red values represent statistically significant improvements over two - stage baseline system  Table 1: UAS and LAS of non-projective versions of our parsing algorithms on 14 treebanks from  CoNLL shared tasks, together with three baseline systems and the best systems for each language re- ported in CoNLL shared tasks. MD06 is McDonald et al. (2006), RD06 is Riedel et al. (2006), JN08  is Johansson and Nugues (2008), and NV06 is Nivre et al. (2006) Bold indicates the best result for a  language. Red values represent statistically significant improvements over two-stage baseline system  on the corresponding metrics with p < 0.01, using McNemar's test. Blue values indicate statistically  significant improvements with p < 0.05.
true	D15-1154.pdf#0.01,usingMcNemar'stest.	Penn Treebank, Test perplexity	is Johansson and Nugues ( 2008 ) , and NV06 is Nivre et al . ( 2006 ) Bold indicates the best result for a Our Model LAS UAS and LAS of non - projective versions of our parsing algorithms on 14 treebanks from MD06 is McDonald et al . ( 2006 ) , RD06 is Riedel et al . ( 2006 ) , JN08  Table 1: UAS and LAS of non-projective versions of our parsing algorithms on 14 treebanks from  CoNLL shared tasks, together with three baseline systems and the best systems for each language re- ported in CoNLL shared tasks. MD06 is McDonald et al. (2006), RD06 is Riedel et al. (2006), JN08  is Johansson and Nugues (2008), and NV06 is Nivre et al. (2006) Bold indicates the best result for a  language. Red values represent statistically significant improvements over two-stage baseline system  on the corresponding metrics with p < 0.01, using McNemar's test. Blue values indicate statistically  significant improvements with p < 0.05.
true	D15-1154.pdf#92.4	Penn Treebank, Test perplexity	UAS  Table 2: Parsing performance on PTB. The re- sults for MaltParser, MSTParser and DNNParser  are from table 5 of Chen and Manning (2014).
true	D15-1154.pdf#89.9	Penn Treebank, Test perplexity	LAS  Table 2: Parsing performance on PTB. The re- sults for MaltParser, MSTParser and DNNParser  are from table 5 of Chen and Manning (2014).
true	D15-1154.pdf#0.05.	Text8, Number of params	is Johansson and Nugues ( 2008 ) , and NV06 is Nivre et al . ( 2006 ) Bold indicates the best result for a Model 0 LAS UAS and LAS of non - projective versions of our parsing algorithms on 14 treebanks from MD06 is McDonald et al . ( 2006 ) , RD06 is Riedel et al . ( 2006 ) , JN08 Red values represent statistically significant improvements over two - stage baseline system  Table 1: UAS and LAS of non-projective versions of our parsing algorithms on 14 treebanks from  CoNLL shared tasks, together with three baseline systems and the best systems for each language re- ported in CoNLL shared tasks. MD06 is McDonald et al. (2006), RD06 is Riedel et al. (2006), JN08  is Johansson and Nugues (2008), and NV06 is Nivre et al. (2006) Bold indicates the best result for a  language. Red values represent statistically significant improvements over two-stage baseline system  on the corresponding metrics with p < 0.01, using McNemar's test. Blue values indicate statistically  significant improvements with p < 0.05.
true	D15-1154.pdf#0.01,usingMcNemar'stest.	Text8, Number of params	is Johansson and Nugues ( 2008 ) , and NV06 is Nivre et al . ( 2006 ) Bold indicates the best result for a Our Model LAS UAS and LAS of non - projective versions of our parsing algorithms on 14 treebanks from MD06 is McDonald et al . ( 2006 ) , RD06 is Riedel et al . ( 2006 ) , JN08  Table 1: UAS and LAS of non-projective versions of our parsing algorithms on 14 treebanks from  CoNLL shared tasks, together with three baseline systems and the best systems for each language re- ported in CoNLL shared tasks. MD06 is McDonald et al. (2006), RD06 is Riedel et al. (2006), JN08  is Johansson and Nugues (2008), and NV06 is Nivre et al. (2006) Bold indicates the best result for a  language. Red values represent statistically significant improvements over two-stage baseline system  on the corresponding metrics with p < 0.01, using McNemar's test. Blue values indicate statistically  significant improvements with p < 0.05.
true	D15-1154.pdf#92.4	Text8, Number of params	UAS  Table 2: Parsing performance on PTB. The re- sults for MaltParser, MSTParser and DNNParser  are from table 5 of Chen and Manning (2014).
true	D15-1154.pdf#89.9	Text8, Number of params	LAS  Table 2: Parsing performance on PTB. The re- sults for MaltParser, MSTParser and DNNParser  are from table 5 of Chen and Manning (2014).
true	D15-1154.pdf#0.05.	Penn Treebank, F1	is Johansson and Nugues ( 2008 ) , and NV06 is Nivre et al . ( 2006 ) Bold indicates the best result for a Model 0 LAS UAS and LAS of non - projective versions of our parsing algorithms on 14 treebanks from MD06 is McDonald et al . ( 2006 ) , RD06 is Riedel et al . ( 2006 ) , JN08 Red values represent statistically significant improvements over two - stage baseline system  Table 1: UAS and LAS of non-projective versions of our parsing algorithms on 14 treebanks from  CoNLL shared tasks, together with three baseline systems and the best systems for each language re- ported in CoNLL shared tasks. MD06 is McDonald et al. (2006), RD06 is Riedel et al. (2006), JN08  is Johansson and Nugues (2008), and NV06 is Nivre et al. (2006) Bold indicates the best result for a  language. Red values represent statistically significant improvements over two-stage baseline system  on the corresponding metrics with p < 0.01, using McNemar's test. Blue values indicate statistically  significant improvements with p < 0.05.
true	D15-1154.pdf#0.01,usingMcNemar'stest.	Penn Treebank, F1	is Johansson and Nugues ( 2008 ) , and NV06 is Nivre et al . ( 2006 ) Bold indicates the best result for a Our Model LAS UAS and LAS of non - projective versions of our parsing algorithms on 14 treebanks from MD06 is McDonald et al . ( 2006 ) , RD06 is Riedel et al . ( 2006 ) , JN08  Table 1: UAS and LAS of non-projective versions of our parsing algorithms on 14 treebanks from  CoNLL shared tasks, together with three baseline systems and the best systems for each language re- ported in CoNLL shared tasks. MD06 is McDonald et al. (2006), RD06 is Riedel et al. (2006), JN08  is Johansson and Nugues (2008), and NV06 is Nivre et al. (2006) Bold indicates the best result for a  language. Red values represent statistically significant improvements over two-stage baseline system  on the corresponding metrics with p < 0.01, using McNemar's test. Blue values indicate statistically  significant improvements with p < 0.05.
true	D15-1154.pdf#92.4	Penn Treebank, F1	UAS  Table 2: Parsing performance on PTB. The re- sults for MaltParser, MSTParser and DNNParser  are from table 5 of Chen and Manning (2014).
true	D15-1154.pdf#89.9	Penn Treebank, F1	LAS  Table 2: Parsing performance on PTB. The re- sults for MaltParser, MSTParser and DNNParser  are from table 5 of Chen and Manning (2014).
true	D14-1044.pdf#0.301	New York Times Corpus, P@10%	MAP  Table 2: Results on the NELL knowledge base.  The bolded line is significantly better than all other  results with p < 0.025.
true	D14-1044.pdf#0.900	New York Times Corpus, P@10%	MRR  Table 2: Results on the NELL knowledge base.  The bolded line is significantly better than all other  results with p < 0.025.
true	D14-1044.pdf#0.350	New York Times Corpus, P@10%	MAP  Table 3: Results on the Freebase knowledge base.  The bolded line is significantly better than all other  results with p < 0.0002.
true	D14-1044.pdf#0.670	New York Times Corpus, P@10%	MRR  Table 3: Results on the Freebase knowledge base.  The bolded line is significantly better than all other  results with p < 0.0002.
true	D14-1044.pdf#0.319	New York Times Corpus, P@10%	KB + Vector SVO  Table 4: Average precision for each relation tested on the NELL KB. The best performing method on  each relation is bolded.
true	D14-1044.pdf#0.321	New York Times Corpus, P@10%	KB + Vector SVO  Table 4: Average precision for each relation tested on the NELL KB. The best performing method on  each relation is bolded.
true	D14-1044.pdf#0.000	New York Times Corpus, P@10%	KB + Clustered SVO  Table 4: Average precision for each relation tested on the NELL KB. The best performing method on  each relation is bolded.
true	D14-1044.pdf#0.037	New York Times Corpus, P@10%	Table 4: Average precision for each relation tested on the NELL KB. The best performing method on  each relation is bolded.
true	D14-1044.pdf#0.589	New York Times Corpus, P@10%	KB + Vector SVO  Table 4: Average precision for each relation tested on the NELL KB. The best performing method on  each relation is bolded.
true	D14-1044.pdf#0.217	New York Times Corpus, P@10%	KB + SVO  Table 4: Average precision for each relation tested on the NELL KB. The best performing method on  each relation is bolded.
true	D14-1044.pdf#0.202	New York Times Corpus, P@10%	KB + Vector SVO  Table 4: Average precision for each relation tested on the NELL KB. The best performing method on  each relation is bolded.
true	D14-1044.pdf#0.347	New York Times Corpus, P@10%	KB + Vector SVO  Table 4: Average precision for each relation tested on the NELL KB. The best performing method on  each relation is bolded.
true	D14-1044.pdf#0.076	New York Times Corpus, P@10%	KB + Vector SVO  Table 4: Average precision for each relation tested on the NELL KB. The best performing method on  each relation is bolded.
true	D14-1044.pdf#0.947	New York Times Corpus, P@10%	KB + Clustered SVO  Table 4: Average precision for each relation tested on the NELL KB. The best performing method on  each relation is bolded.
true	D14-1044.pdf#0.169	New York Times Corpus, P@10%	KB  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.768	New York Times Corpus, P@10%	KB + V - SVO  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.017	New York Times Corpus, P@10%	KB + SVO  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.336	New York Times Corpus, P@10%	KB + V - SVO  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.181	New York Times Corpus, P@10%	KB + V - SVO  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.001	New York Times Corpus, P@10%	KB + V - SVO  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.295	New York Times Corpus, P@10%	KB + SVO  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.066	New York Times Corpus, P@10%	KB + V - SVO  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.013	New York Times Corpus, P@10%	Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.602	New York Times Corpus, P@10%	KB + V - SVO  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.991	New York Times Corpus, P@10%	KB + C - SVO  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.003	New York Times Corpus, P@10%	KB + SVO  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.914	New York Times Corpus, P@10%	KB  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.437	New York Times Corpus, P@10%	KB + V - SVO  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.006	New York Times Corpus, P@10%	KB + V - SVO  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.727	New York Times Corpus, P@10%	KB + V - SVO  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.177	New York Times Corpus, P@10%	KB + C - SVO  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.226	New York Times Corpus, P@10%	KB + SVO  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.376	New York Times Corpus, P@10%	KB + V - SVO  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.364	New York Times Corpus, P@10%	KB + V - SVO  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.078	New York Times Corpus, P@10%	KB + SVO  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.830	New York Times Corpus, P@10%	KB + V - SVO  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.848	New York Times Corpus, P@10%	KB + V - SVO  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	D14-1044.pdf#0.046	New York Times Corpus, P@10%	KB + C - SVO  Table 5: Average precision for each relation tested on the Freebase KB. The best performing method on  each relation is bolded. For space considerations, "Clustered SVO" is shortened to "C-SVO" and "Vector  SVO" is shortened to "V-SVO" in the table header.
true	P11-1051.pdf#0.539	Text8, Number of params	LexRank FL  Table 6: Extraction Evaluation
true	D11-1116.pdf#93.7(94.3)	Penn Treebank, UAS	Arc Accuracy  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#92.1(93.3)	Penn Treebank, UAS	Arc Accuracy  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#38.4(42.5)	Penn Treebank, UAS	Perfect Sentences  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#46.2(48.5)	Penn Treebank, UAS	Perfect Sentences  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#69.3(71.7)	Penn Treebank, UAS	Non - Proj Arcs  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#67.3(70.9)	Penn Treebank, UAS	Non - Proj Arcs Labeled  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#69.3(72.5)	Penn Treebank, UAS	Non - Proj Arcs Unlabeled  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#93.7(94.3)	SemEval 2013, F1	Arc Accuracy  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#92.1(93.3)	SemEval 2013, F1	Arc Accuracy  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#38.4(42.5)	SemEval 2013, F1	Perfect Sentences  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#46.2(48.5)	SemEval 2013, F1	Perfect Sentences  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#69.3(71.7)	SemEval 2013, F1	Non - Proj Arcs  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#67.3(70.9)	SemEval 2013, F1	Non - Proj Arcs Labeled  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#69.3(72.5)	SemEval 2013, F1	Non - Proj Arcs Unlabeled  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#93.7(94.3)	Penn Treebank, POS	Arc Accuracy  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#92.1(93.3)	Penn Treebank, POS	Arc Accuracy  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#38.4(42.5)	Penn Treebank, POS	Perfect Sentences  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#46.2(48.5)	Penn Treebank, POS	Perfect Sentences  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#69.3(71.7)	Penn Treebank, POS	Non - Proj Arcs  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#67.3(70.9)	Penn Treebank, POS	Non - Proj Arcs Labeled  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#69.3(72.5)	Penn Treebank, POS	Non - Proj Arcs Unlabeled  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#93.7(94.3)	SemEval 2015, F1	Arc Accuracy  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#92.1(93.3)	SemEval 2015, F1	Arc Accuracy  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#38.4(42.5)	SemEval 2015, F1	Perfect Sentences  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#46.2(48.5)	SemEval 2015, F1	Perfect Sentences  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#69.3(71.7)	SemEval 2015, F1	Non - Proj Arcs  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#67.3(70.9)	SemEval 2015, F1	Non - Proj Arcs Labeled  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#69.3(72.5)	SemEval 2015, F1	Non - Proj Arcs Unlabeled  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#93.7(94.3)	Penn Treebank, Accuracy	Arc Accuracy  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#92.1(93.3)	Penn Treebank, Accuracy	Arc Accuracy  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#38.4(42.5)	Penn Treebank, Accuracy	Perfect Sentences  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#46.2(48.5)	Penn Treebank, Accuracy	Perfect Sentences  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#69.3(71.7)	Penn Treebank, Accuracy	Non - Proj Arcs  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#67.3(70.9)	Penn Treebank, Accuracy	Non - Proj Arcs Labeled  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#69.3(72.5)	Penn Treebank, Accuracy	Non - Proj Arcs Unlabeled  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#93.7(94.3)	SemEval 2007, F1	Arc Accuracy  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#92.1(93.3)	SemEval 2007, F1	Arc Accuracy  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#38.4(42.5)	SemEval 2007, F1	Perfect Sentences  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#46.2(48.5)	SemEval 2007, F1	Perfect Sentences  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#69.3(71.7)	SemEval 2007, F1	Non - Proj Arcs  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#67.3(70.9)	SemEval 2007, F1	Non - Proj Arcs Labeled  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#69.3(72.5)	SemEval 2007, F1	Non - Proj Arcs Unlabeled  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#93.7(94.3)	Penn Treebank, LAS	Arc Accuracy  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#92.1(93.3)	Penn Treebank, LAS	Arc Accuracy  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#38.4(42.5)	Penn Treebank, LAS	Perfect Sentences  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#46.2(48.5)	Penn Treebank, LAS	Perfect Sentences  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#69.3(71.7)	Penn Treebank, LAS	Non - Proj Arcs  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#67.3(70.9)	Penn Treebank, LAS	Non - Proj Arcs Labeled  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	D11-1116.pdf#69.3(72.5)	Penn Treebank, LAS	Non - Proj Arcs Unlabeled  Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were  produced using gold POS tags.  † Eisner (1996) algorithm with non-projective rewriting and second order features.
true	P12-2063.pdf#43.99	MSR, F1	Small MSA  Table 4: BLEU scores for all experiments. Full MSA is  the the full MT-08 corpus, Small MSA is a 1.3M-word  subset, Lev Dial our Levantine dataset. For each of these,  the highest Lee segmenter score is in bold, with "+" if  statistically significant vs. MADA at the 95% confidence  level or higher. The highest overall score is in bold italic.
true	P12-2063.pdf#20.15+	MSR, F1	Lev Dial  Table 4: BLEU scores for all experiments. Full MSA is  the the full MT-08 corpus, Small MSA is a 1.3M-word  subset, Lev Dial our Levantine dataset. For each of these,  the highest Lee segmenter score is in bold, with "+" if  statistically significant vs. MADA at the 95% confidence  level or higher. The highest overall score is in bold italic.
true	P12-2063.pdf#43.64+	MSR, F1	Lee GS Small MSA  Table 4: BLEU scores for all experiments. Full MSA is  the the full MT-08 corpus, Small MSA is a 1.3M-word  subset, Lev Dial our Levantine dataset. For each of these,  the highest Lee segmenter score is in bold, with "+" if  statistically significant vs. MADA at the 95% confidence  level or higher. The highest overall score is in bold italic.
true	P12-2063.pdf#20.09	MSR, F1	Lee GS Lev Dial  Table 4: BLEU scores for all experiments. Full MSA is  the the full MT-08 corpus, Small MSA is a 1.3M-word  subset, Lev Dial our Levantine dataset. For each of these,  the highest Lee segmenter score is in bold, with "+" if  statistically significant vs. MADA at the 95% confidence  level or higher. The highest overall score is in bold italic.
true	P12-2063.pdf#46.51	MSR, F1	Full MSA  Table 4: BLEU scores for all experiments. Full MSA is  the the full MT-08 corpus, Small MSA is a 1.3M-word  subset, Lev Dial our Levantine dataset. For each of these,  the highest Lee segmenter score is in bold, with "+" if  statistically significant vs. MADA at the 95% confidence  level or higher. The highest overall score is in bold italic.
true	P12-2063.pdf#45.84	MSR, F1	Lee GS Full MSA  Table 4: BLEU scores for all experiments. Full MSA is  the the full MT-08 corpus, Small MSA is a 1.3M-word  subset, Lev Dial our Levantine dataset. For each of these,  the highest Lee segmenter score is in bold, with "+" if  statistically significant vs. MADA at the 95% confidence  level or higher. The highest overall score is in bold italic.
true	P12-1111.pdf#0.996	VLSP 2013 word segmentation shared task, F1	1 F  Table 6: POS tagging with deterministic constraints.
true	P12-1111.pdf#0.857	VLSP 2013 word segmentation shared task, F1	1 F  Table 6: POS tagging with deterministic constraints.
true	P12-1111.pdf#0.920	VLSP 2013 word segmentation shared task, F1	1 F  Table 6: POS tagging with deterministic constraints.
true	P12-1111.pdf#0.920	VLSP 2013 word segmentation shared task, F1	1 F  Table 7. As de- fined in Section 2.2, we use the set of all possible  POS tags for a word, e.g. {VBN, VBZ}, as its morph  feature if the word is frequent (occurring more than  5 times in training data). For a rare word, the last two  characters are used as its morph feature, e.g. −es. A  constraint is composed of w −1 , w 0 and w 1 , as well  as the morph features m −1 , m 0 and m 1 . For ex-
true	P12-1111.pdf#0.996	VLSP 2013 word segmentation shared task, F1	1 F  Table 7. As de- fined in Section 2.2, we use the set of all possible  POS tags for a word, e.g. {VBN, VBZ}, as its morph  feature if the word is frequent (occurring more than  5 times in training data). For a rare word, the last two  characters are used as its morph feature, e.g. −es. A  constraint is composed of w −1 , w 0 and w 1 , as well  as the morph features m −1 , m 0 and m 1 . For ex-
true	P12-1111.pdf#0.857	VLSP 2013 word segmentation shared task, F1	1 F  Table 7. As de- fined in Section 2.2, we use the set of all possible  POS tags for a word, e.g. {VBN, VBZ}, as its morph  feature if the word is frequent (occurring more than  5 times in training data). For a rare word, the last two  characters are used as its morph feature, e.g. −es. A  constraint is composed of w −1 , w 0 and w 1 , as well  as the morph features m −1 , m 0 and m 1 . For ex-
true	P12-1111.pdf#0.996	VLSP 2013 word segmentation shared task, F1	1 F  Table 8: Character tagging with deterministic constraints.
true	P12-1111.pdf#0.812	VLSP 2013 word segmentation shared task, F1	1 F  Table 8: Character tagging with deterministic constraints.
true	P12-1111.pdf#0.686	VLSP 2013 word segmentation shared task, F1	1 F  Table 8: Character tagging with deterministic constraints.
true	P12-1111.pdf#0.974	VLSP 2013 word segmentation shared task, F1	F - measure  Table 10: F-measure on Chinese word segmentation.  Only character-based features are used. POS-/+: percep- tron trained without/with POS.
true	P12-1111.pdf#0.996	VLSP 2013 POS tagging shared task, Accuracy	1 F  Table 6: POS tagging with deterministic constraints.
true	P12-1111.pdf#0.857	VLSP 2013 POS tagging shared task, Accuracy	1 F  Table 6: POS tagging with deterministic constraints.
true	P12-1111.pdf#0.920	VLSP 2013 POS tagging shared task, Accuracy	1 F  Table 6: POS tagging with deterministic constraints.
true	P12-1111.pdf#0.920	VLSP 2013 POS tagging shared task, Accuracy	1 F  Table 7. As de- fined in Section 2.2, we use the set of all possible  POS tags for a word, e.g. {VBN, VBZ}, as its morph  feature if the word is frequent (occurring more than  5 times in training data). For a rare word, the last two  characters are used as its morph feature, e.g. −es. A  constraint is composed of w −1 , w 0 and w 1 , as well  as the morph features m −1 , m 0 and m 1 . For ex-
true	P12-1111.pdf#0.996	VLSP 2013 POS tagging shared task, Accuracy	1 F  Table 7. As de- fined in Section 2.2, we use the set of all possible  POS tags for a word, e.g. {VBN, VBZ}, as its morph  feature if the word is frequent (occurring more than  5 times in training data). For a rare word, the last two  characters are used as its morph feature, e.g. −es. A  constraint is composed of w −1 , w 0 and w 1 , as well  as the morph features m −1 , m 0 and m 1 . For ex-
true	P12-1111.pdf#0.857	VLSP 2013 POS tagging shared task, Accuracy	1 F  Table 7. As de- fined in Section 2.2, we use the set of all possible  POS tags for a word, e.g. {VBN, VBZ}, as its morph  feature if the word is frequent (occurring more than  5 times in training data). For a rare word, the last two  characters are used as its morph feature, e.g. −es. A  constraint is composed of w −1 , w 0 and w 1 , as well  as the morph features m −1 , m 0 and m 1 . For ex-
true	P12-1111.pdf#0.996	VLSP 2013 POS tagging shared task, Accuracy	1 F  Table 8: Character tagging with deterministic constraints.
true	P12-1111.pdf#0.812	VLSP 2013 POS tagging shared task, Accuracy	1 F  Table 8: Character tagging with deterministic constraints.
true	P12-1111.pdf#0.686	VLSP 2013 POS tagging shared task, Accuracy	1 F  Table 8: Character tagging with deterministic constraints.
true	P12-1111.pdf#0.974	VLSP 2013 POS tagging shared task, Accuracy	F - measure  Table 10: F-measure on Chinese word segmentation.  Only character-based features are used. POS-/+: percep- tron trained without/with POS.
true	P12-1111.pdf#0.996	Chinese Treebank 6, F1	1 F  Table 6: POS tagging with deterministic constraints.
true	P12-1111.pdf#0.857	Chinese Treebank 6, F1	1 F  Table 6: POS tagging with deterministic constraints.
true	P12-1111.pdf#0.920	Chinese Treebank 6, F1	1 F  Table 6: POS tagging with deterministic constraints.
true	P12-1111.pdf#0.920	Chinese Treebank 6, F1	1 F  Table 7. As de- fined in Section 2.2, we use the set of all possible  POS tags for a word, e.g. {VBN, VBZ}, as its morph  feature if the word is frequent (occurring more than  5 times in training data). For a rare word, the last two  characters are used as its morph feature, e.g. −es. A  constraint is composed of w −1 , w 0 and w 1 , as well  as the morph features m −1 , m 0 and m 1 . For ex-
true	P12-1111.pdf#0.996	Chinese Treebank 6, F1	1 F  Table 7. As de- fined in Section 2.2, we use the set of all possible  POS tags for a word, e.g. {VBN, VBZ}, as its morph  feature if the word is frequent (occurring more than  5 times in training data). For a rare word, the last two  characters are used as its morph feature, e.g. −es. A  constraint is composed of w −1 , w 0 and w 1 , as well  as the morph features m −1 , m 0 and m 1 . For ex-
true	P12-1111.pdf#0.857	Chinese Treebank 6, F1	1 F  Table 7. As de- fined in Section 2.2, we use the set of all possible  POS tags for a word, e.g. {VBN, VBZ}, as its morph  feature if the word is frequent (occurring more than  5 times in training data). For a rare word, the last two  characters are used as its morph feature, e.g. −es. A  constraint is composed of w −1 , w 0 and w 1 , as well  as the morph features m −1 , m 0 and m 1 . For ex-
true	P12-1111.pdf#0.996	Chinese Treebank 6, F1	1 F  Table 8: Character tagging with deterministic constraints.
true	P12-1111.pdf#0.812	Chinese Treebank 6, F1	1 F  Table 8: Character tagging with deterministic constraints.
true	P12-1111.pdf#0.686	Chinese Treebank 6, F1	1 F  Table 8: Character tagging with deterministic constraints.
true	P12-1111.pdf#0.974	Chinese Treebank 6, F1	F - measure  Table 10: F-measure on Chinese word segmentation.  Only character-based features are used. POS-/+: percep- tron trained without/with POS.
true	D11-1091.pdf#.556	SemEval-2010 Task 8, F1	Y 10  Table 2: Clustering scores for induced classes.
true	D11-1091.pdf#.595	SemEval-2010 Task 8, F1	Y 10  Table 2: Clustering scores for induced classes.
true	D11-1091.pdf#.510	SemEval-2010 Task 8, F1	Y 10  Table 2: Clustering scores for induced classes.
true	D11-1091.pdf#.788	SemEval-2010 Task 8, F1	Y 10  Table 2: Clustering scores for induced classes.
true	D11-1091.pdf#.567	SemEval-2010 Task 8, F1	Y 10  Table 2: Clustering scores for induced classes.
true	D11-1091.pdf#.593	SemEval-2010 Task 8, F1	Y 10  Table 2: Clustering scores for induced classes.
true	D11-1091.pdf#81.2	SemEval-2010 Task 8, F1	N 25  Table 3: Accuracy on SemEval-2010 Task 7 data.
true	D11-1091.pdf#80.8	SemEval-2010 Task 8, F1	Y 50  Table 3: Accuracy on SemEval-2010 Task 7 data.
true	D11-1091.pdf#0.767	SemEval-2010 Task 8, F1	LM T H N 50  Table 4: Mean average precision (MAP) scores on the Shutova and Teufel (2009) data set. The bold items indicate the  best scores with/without induced predicates as well as using/not using a threshold-based interpretation method.
true	D11-1091.pdf#0.708	SemEval-2010 Task 8, F1	LM IND Y 50  Table 4: Mean average precision (MAP) scores on the Shutova and Teufel (2009) data set. The bold items indicate the  best scores with/without induced predicates as well as using/not using a threshold-based interpretation method.
true	D11-1091.pdf#0.455	SemEval-2010 Task 8, F1	LM IND N 50  Table 4: Mean average precision (MAP) scores on the Shutova and Teufel (2009) data set. The bold items indicate the  best scores with/without induced predicates as well as using/not using a threshold-based interpretation method.
true	D11-1091.pdf#0.468	SemEval-2010 Task 8, F1	Y 25  Table 4: Mean average precision (MAP) scores on the Shutova and Teufel (2009) data set. The bold items indicate the  best scores with/without induced predicates as well as using/not using a threshold-based interpretation method.
true	D11-1091.pdf#0.307	SemEval-2010 Task 8, F1	LM IND Y 50  Table 5: Mean average precision (MAP) scores on 100 logical metonymies manually annotated with interpretations.  The bold items indicate the best scores with/without induced predicates as well as using/not using a threshold-based  interpretation method.
true	D11-1091.pdf#0.333	SemEval-2010 Task 8, F1	LM IND N 50  Table 5: Mean average precision (MAP) scores on 100 logical metonymies manually annotated with interpretations.  The bold items indicate the best scores with/without induced predicates as well as using/not using a threshold-based  interpretation method.
true	D11-1091.pdf#0.513	SemEval-2010 Task 8, F1	LM IND Y 50  Table 5: Mean average precision (MAP) scores on 100 logical metonymies manually annotated with interpretations.  The bold items indicate the best scores with/without induced predicates as well as using/not using a threshold-based  interpretation method.
true	D11-1091.pdf#0.577	SemEval-2010 Task 8, F1	LM T H N 50  Table 5: Mean average precision (MAP) scores on 100 logical metonymies manually annotated with interpretations.  The bold items indicate the best scores with/without induced predicates as well as using/not using a threshold-based  interpretation method.
true	P10-2028.pdf#0.181	New York Times Corpus, P@10%	k  Table 1: Results on the ESL podcast dataset. For  all metrics, lower values are better.
true	P10-2028.pdf#0.239	New York Times Corpus, P@10%	k  Table 1: Results on the ESL podcast dataset. For  all metrics, lower values are better.
true	P10-2028.pdf#0.193	New York Times Corpus, P@10%	k  Table 1: Results on the ESL podcast dataset. For  all metrics, lower values are better.
true	P12-1014.pdf#68	New York Times Corpus, P@10%	7 
true	P12-1014.pdf#16	New York Times Corpus, P@10%	7 
true	P12-1014.pdf#80.2	New York Times Corpus, P@10%	%Plans  Table 4: Percentage of tasks solved successfully by our  model and the baselines. All performance differences be- tween methods are statistically significant at p ≤ .01.
true	P15-2141.pdf#.67	LDC2014T12, F1 on Full	experiments . on all the  Table 1: AMR parsing performance on develop- ment set using different syntactic parsers.
true	P15-2141.pdf#.64	LDC2014T12, F1 on Full	experiments . on all the  Table 1: AMR parsing performance on develop- ment set using different syntactic parsers.
true	P15-2141.pdf#.65	LDC2014T12, F1 on Full	experiments . on all the  Table 1: AMR parsing performance on develop- ment set using different syntactic parsers.
true	P15-2141.pdf#.69	LDC2014T12, F1 on Full	R  Table 2: AMR parsing performance on the devel- opment set.
true	P15-2141.pdf#.72	LDC2014T12, F1 on Full	P  Table 2: AMR parsing performance on the devel- opment set.
true	P15-2141.pdf#.71	LDC2014T12, F1 on Full	F 1  Table 2: AMR parsing performance on the devel- opment set.
true	P15-2141.pdf#.70	LDC2014T12, F1 on Full	Table 3: AMR parsing performance on the news  wire test set of LDC2013E117.
true	P15-2141.pdf#.71	LDC2014T12, F1 on Full	Table 3: AMR parsing performance on the news  wire test set of LDC2013E117.
true	P15-2141.pdf#.69	LDC2014T12, F1 on Full	Table 3: AMR parsing performance on the news  wire test set of LDC2013E117.
true	P15-2141.pdf#.67	LDC2014T12, F1 on Full	Table 5: AMR parsing performance on newswire  section of LDC2014T12 test set
true	P15-2141.pdf#.72	LDC2014T12, F1 on Full	Table 5: AMR parsing performance on newswire  section of LDC2014T12 test set
true	P15-2141.pdf#.70	LDC2014T12, F1 on Full	Table 5: AMR parsing performance on newswire  section of LDC2014T12 test set
true	P15-2141.pdf#.67	LDC2014T12, F1 on Newswire	experiments . on all the  Table 1: AMR parsing performance on develop- ment set using different syntactic parsers.
true	P15-2141.pdf#.64	LDC2014T12, F1 on Newswire	experiments . on all the  Table 1: AMR parsing performance on develop- ment set using different syntactic parsers.
true	P15-2141.pdf#.65	LDC2014T12, F1 on Newswire	experiments . on all the  Table 1: AMR parsing performance on develop- ment set using different syntactic parsers.
true	P15-2141.pdf#.69	LDC2014T12, F1 on Newswire	R  Table 2: AMR parsing performance on the devel- opment set.
true	P15-2141.pdf#.72	LDC2014T12, F1 on Newswire	P  Table 2: AMR parsing performance on the devel- opment set.
true	P15-2141.pdf#.71	LDC2014T12, F1 on Newswire	F 1  Table 2: AMR parsing performance on the devel- opment set.
true	P15-2141.pdf#.70	LDC2014T12, F1 on Newswire	Table 3: AMR parsing performance on the news  wire test set of LDC2013E117.
true	P15-2141.pdf#.71	LDC2014T12, F1 on Newswire	Table 3: AMR parsing performance on the news  wire test set of LDC2013E117.
true	P15-2141.pdf#.69	LDC2014T12, F1 on Newswire	Table 3: AMR parsing performance on the news  wire test set of LDC2013E117.
true	P15-2141.pdf#.67	LDC2014T12, F1 on Newswire	Table 5: AMR parsing performance on newswire  section of LDC2014T12 test set
true	P15-2141.pdf#.72	LDC2014T12, F1 on Newswire	Table 5: AMR parsing performance on newswire  section of LDC2014T12 test set
true	P15-2141.pdf#.70	LDC2014T12, F1 on Newswire	Table 5: AMR parsing performance on newswire  section of LDC2014T12 test set
true	P14-1033.pdf#0isequiv-	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- 1510 et al . , 2011 ) to correlate well with human judg - - 1470 - 1490 GK - LDA LDA  Tablet  Webcam  Cell Phone  Home Theater System  Monitor  Radar Detector Telephone Wireless Router  Computer  Keyboard  Mouse  Remote Control  TV  Xbox
true	P10-1081.pdf#44.55	New York Times Corpus, P@10%	rules Role classification F  Table 5. Overall performance on blind test data
true	P10-1081.pdf#50.28	New York Times Corpus, P@10%	rules Argument classification F  Table 5. Overall performance on blind test data
true	P10-1081.pdf#68.79	New York Times Corpus, P@10%	rules Trigger classification F  Table 5. Overall performance on blind test data
true	D15-1212.pdf#83.09	Penn Treebank, F1	Korean  Table 2: Development F-scores
true	D15-1212.pdf#88.27	Penn Treebank, F1	Hungarian  Table 2: Development F-scores
true	D15-1212.pdf#81.25	Penn Treebank, F1	Arabic  Table 2: Development F-scores
true	D15-1212.pdf#84.01	Penn Treebank, F1	Basque  Table 2: Development F-scores
true	D15-1212.pdf#84.08	Penn Treebank, F1	German  Table 2: Development F-scores
true	D15-1212.pdf#90.69	Penn Treebank, F1	Hebrew  Table 2: Development F-scores
true	D15-1212.pdf#77.87	Penn Treebank, F1	Swedish  Table 2: Development F-scores
true	D15-1212.pdf#84.77	Penn Treebank, F1	Avg  Table 2: Development F-scores
true	D15-1212.pdf#92.78	Penn Treebank, F1	Polish  Table 2: Development F-scores
true	D15-1212.pdf#81.05	Penn Treebank, F1	French  Table 2: Development F-scores
true	D15-1212.pdf#81.31	Penn Treebank, F1	Arabic  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#92.66	Penn Treebank, F1	Polish  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#80.84	Penn Treebank, F1	French  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#85.90	Penn Treebank, F1	Basque  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#79.26	Penn Treebank, F1	German  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#89.65	Penn Treebank, F1	Hebrew  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#85.42	Penn Treebank, F1	Avg  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#82.65	Penn Treebank, F1	Korean  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#83.24	Penn Treebank, F1	Swedish  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#90.14	Penn Treebank, F1	Hungarian  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#83.71	Penn Treebank, F1	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Hungarian  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#85.28	Penn Treebank, F1	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Hebrew  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#88.61	Penn Treebank, F1	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Polish  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#87.65	Penn Treebank, F1	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) German  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#91.95	Penn Treebank, F1	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) English  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#84.33	Penn Treebank, F1	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Basque  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#84.57	Penn Treebank, F1	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Arabic  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#86.68	Penn Treebank, F1	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) French  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#86.22	Penn Treebank, F1	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Swedish  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#87.22	Penn Treebank, F1	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Korean  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#83.09	Penn Treebank, UAS	Korean  Table 2: Development F-scores
true	D15-1212.pdf#88.27	Penn Treebank, UAS	Hungarian  Table 2: Development F-scores
true	D15-1212.pdf#81.25	Penn Treebank, UAS	Arabic  Table 2: Development F-scores
true	D15-1212.pdf#84.01	Penn Treebank, UAS	Basque  Table 2: Development F-scores
true	D15-1212.pdf#84.08	Penn Treebank, UAS	German  Table 2: Development F-scores
true	D15-1212.pdf#90.69	Penn Treebank, UAS	Hebrew  Table 2: Development F-scores
true	D15-1212.pdf#77.87	Penn Treebank, UAS	Swedish  Table 2: Development F-scores
true	D15-1212.pdf#84.77	Penn Treebank, UAS	Avg  Table 2: Development F-scores
true	D15-1212.pdf#92.78	Penn Treebank, UAS	Polish  Table 2: Development F-scores
true	D15-1212.pdf#81.05	Penn Treebank, UAS	French  Table 2: Development F-scores
true	D15-1212.pdf#81.31	Penn Treebank, UAS	Arabic  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#92.66	Penn Treebank, UAS	Polish  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#80.84	Penn Treebank, UAS	French  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#85.90	Penn Treebank, UAS	Basque  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#79.26	Penn Treebank, UAS	German  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#89.65	Penn Treebank, UAS	Hebrew  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#85.42	Penn Treebank, UAS	Avg  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#82.65	Penn Treebank, UAS	Korean  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#83.24	Penn Treebank, UAS	Swedish  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#90.14	Penn Treebank, UAS	Hungarian  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#83.71	Penn Treebank, UAS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Hungarian  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#85.28	Penn Treebank, UAS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Hebrew  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#88.61	Penn Treebank, UAS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Polish  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#87.65	Penn Treebank, UAS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) German  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#91.95	Penn Treebank, UAS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) English  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#84.33	Penn Treebank, UAS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Basque  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#84.57	Penn Treebank, UAS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Arabic  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#86.68	Penn Treebank, UAS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) French  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#86.22	Penn Treebank, UAS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Swedish  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#87.22	Penn Treebank, UAS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Korean  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#83.09	Penn Treebank, LAS	Korean  Table 2: Development F-scores
true	D15-1212.pdf#88.27	Penn Treebank, LAS	Hungarian  Table 2: Development F-scores
true	D15-1212.pdf#81.25	Penn Treebank, LAS	Arabic  Table 2: Development F-scores
true	D15-1212.pdf#84.01	Penn Treebank, LAS	Basque  Table 2: Development F-scores
true	D15-1212.pdf#84.08	Penn Treebank, LAS	German  Table 2: Development F-scores
true	D15-1212.pdf#90.69	Penn Treebank, LAS	Hebrew  Table 2: Development F-scores
true	D15-1212.pdf#77.87	Penn Treebank, LAS	Swedish  Table 2: Development F-scores
true	D15-1212.pdf#84.77	Penn Treebank, LAS	Avg  Table 2: Development F-scores
true	D15-1212.pdf#92.78	Penn Treebank, LAS	Polish  Table 2: Development F-scores
true	D15-1212.pdf#81.05	Penn Treebank, LAS	French  Table 2: Development F-scores
true	D15-1212.pdf#81.31	Penn Treebank, LAS	Arabic  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#92.66	Penn Treebank, LAS	Polish  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#80.84	Penn Treebank, LAS	French  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#85.90	Penn Treebank, LAS	Basque  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#79.26	Penn Treebank, LAS	German  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#89.65	Penn Treebank, LAS	Hebrew  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#85.42	Penn Treebank, LAS	Avg  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#82.65	Penn Treebank, LAS	Korean  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#83.24	Penn Treebank, LAS	Swedish  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#90.14	Penn Treebank, LAS	Hungarian  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#83.71	Penn Treebank, LAS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Hungarian  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#85.28	Penn Treebank, LAS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Hebrew  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#88.61	Penn Treebank, LAS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Polish  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#87.65	Penn Treebank, LAS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) German  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#91.95	Penn Treebank, LAS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) English  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#84.33	Penn Treebank, LAS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Basque  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#84.57	Penn Treebank, LAS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Arabic  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#86.68	Penn Treebank, LAS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) French  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#86.22	Penn Treebank, LAS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Swedish  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#87.22	Penn Treebank, LAS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Korean  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#83.09	Penn Treebank, POS	Korean  Table 2: Development F-scores
true	D15-1212.pdf#88.27	Penn Treebank, POS	Hungarian  Table 2: Development F-scores
true	D15-1212.pdf#81.25	Penn Treebank, POS	Arabic  Table 2: Development F-scores
true	D15-1212.pdf#84.01	Penn Treebank, POS	Basque  Table 2: Development F-scores
true	D15-1212.pdf#84.08	Penn Treebank, POS	German  Table 2: Development F-scores
true	D15-1212.pdf#90.69	Penn Treebank, POS	Hebrew  Table 2: Development F-scores
true	D15-1212.pdf#77.87	Penn Treebank, POS	Swedish  Table 2: Development F-scores
true	D15-1212.pdf#84.77	Penn Treebank, POS	Avg  Table 2: Development F-scores
true	D15-1212.pdf#92.78	Penn Treebank, POS	Polish  Table 2: Development F-scores
true	D15-1212.pdf#81.05	Penn Treebank, POS	French  Table 2: Development F-scores
true	D15-1212.pdf#81.31	Penn Treebank, POS	Arabic  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#92.66	Penn Treebank, POS	Polish  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#80.84	Penn Treebank, POS	French  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#85.90	Penn Treebank, POS	Basque  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#79.26	Penn Treebank, POS	German  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#89.65	Penn Treebank, POS	Hebrew  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#85.42	Penn Treebank, POS	Avg  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#82.65	Penn Treebank, POS	Korean  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#83.24	Penn Treebank, POS	Swedish  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#90.14	Penn Treebank, POS	Hungarian  Table 3: Multilingual test (F-scores, phrase structure parsing)
true	D15-1212.pdf#83.71	Penn Treebank, POS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Hungarian  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#85.28	Penn Treebank, POS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Hebrew  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#88.61	Penn Treebank, POS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Polish  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#87.65	Penn Treebank, POS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) German  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#91.95	Penn Treebank, POS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) English  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#84.33	Penn Treebank, POS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Basque  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#84.57	Penn Treebank, POS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Arabic  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#86.68	Penn Treebank, POS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) French  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#86.22	Penn Treebank, POS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Swedish  Table 4: Multilingual test (UAS, dependency parsing)
true	D15-1212.pdf#87.22	Penn Treebank, POS	Table 3 : Multilingual test ( F - scores , phrase structure parsing ) Korean  Table 4: Multilingual test (UAS, dependency parsing)
true	P13-2095.pdf#89.6%	New York Times Corpus, P@10%	Acc  Table 1: Evaluation results for the classification of  definitional sentences, in terms of Precision (P ),  Recall (R), F-Measure (F ), and Accuracy (Acc),  using 10-folds cross validation. For the WCL-3  approach and the Star Patterns see
true	P13-2095.pdf#82.7%	New York Times Corpus, P@10%	R  Table 1: Evaluation results for the classification of  definitional sentences, in terms of Precision (P ),  Recall (R), F-Measure (F ), and Accuracy (Acc),  using 10-folds cross validation. For the WCL-3  approach and the Star Patterns see
true	P13-2095.pdf#81.6%	New York Times Corpus, P@10%	F  Table 1: Evaluation results for the classification of  definitional sentences, in terms of Precision (P ),  Recall (R), F-Measure (F ), and Accuracy (Acc),  using 10-folds cross validation. For the WCL-3  approach and the Star Patterns see
true	D14-1081.pdf#91.02(+1.05)	Penn Treebank, F1	Mixture reranker with Figure 7 : Performance of the mixture reranker on LAS  Table 3: Comparison based on reranking on PTB- U-23. The numbers in the brackets are improve- ments over the MSTParser.
true	D14-1081.pdf#93.08(+1.09)	Penn Treebank, F1	Mixture reranker with Figure 7 : Performance of the mixture reranker on UAS  Table 3: Comparison based on reranking on PTB- U-23. The numbers in the brackets are improve- ments over the MSTParser.
true	D14-1081.pdf#91.02(+1.05)	Penn Treebank, UAS	Mixture reranker with Figure 7 : Performance of the mixture reranker on LAS  Table 3: Comparison based on reranking on PTB- U-23. The numbers in the brackets are improve- ments over the MSTParser.
true	D14-1081.pdf#93.08(+1.09)	Penn Treebank, UAS	Mixture reranker with Figure 7 : Performance of the mixture reranker on UAS  Table 3: Comparison based on reranking on PTB- U-23. The numbers in the brackets are improve- ments over the MSTParser.
true	D14-1081.pdf#91.02(+1.05)	benchmark Vietnamese dependency treebank VnDT, LAS	Mixture reranker with Figure 7 : Performance of the mixture reranker on LAS  Table 3: Comparison based on reranking on PTB- U-23. The numbers in the brackets are improve- ments over the MSTParser.
true	D14-1081.pdf#93.08(+1.09)	benchmark Vietnamese dependency treebank VnDT, LAS	Mixture reranker with Figure 7 : Performance of the mixture reranker on UAS  Table 3: Comparison based on reranking on PTB- U-23. The numbers in the brackets are improve- ments over the MSTParser.
true	D14-1081.pdf#91.02(+1.05)	Penn Treebank, LAS	Mixture reranker with Figure 7 : Performance of the mixture reranker on LAS  Table 3: Comparison based on reranking on PTB- U-23. The numbers in the brackets are improve- ments over the MSTParser.
true	D14-1081.pdf#93.08(+1.09)	Penn Treebank, LAS	Mixture reranker with Figure 7 : Performance of the mixture reranker on UAS  Table 3: Comparison based on reranking on PTB- U-23. The numbers in the brackets are improve- ments over the MSTParser.
true	D14-1081.pdf#91.02(+1.05)	benchmark Vietnamese dependency treebank VnDT, UAS	Mixture reranker with Figure 7 : Performance of the mixture reranker on LAS  Table 3: Comparison based on reranking on PTB- U-23. The numbers in the brackets are improve- ments over the MSTParser.
true	D14-1081.pdf#93.08(+1.09)	benchmark Vietnamese dependency treebank VnDT, UAS	Mixture reranker with Figure 7 : Performance of the mixture reranker on UAS  Table 3: Comparison based on reranking on PTB- U-23. The numbers in the brackets are improve- ments over the MSTParser.
true	D14-1081.pdf#91.02(+1.05)	Penn Treebank, POS	Mixture reranker with Figure 7 : Performance of the mixture reranker on LAS  Table 3: Comparison based on reranking on PTB- U-23. The numbers in the brackets are improve- ments over the MSTParser.
true	D14-1081.pdf#93.08(+1.09)	Penn Treebank, POS	Mixture reranker with Figure 7 : Performance of the mixture reranker on UAS  Table 3: Comparison based on reranking on PTB- U-23. The numbers in the brackets are improve- ments over the MSTParser.
true	D15-1124.pdf#8	WMT 2014 EN-DE, BLEU	≥ 8 , separately . The computed variance values are  Table 1.
true	D15-1124.pdf#8	WMT 2014 EN-DE, BLEU	≥ 8 , separately . The computed variance values are  Table 1: Variances computed using Algorithm 1
true	D15-1124.pdf#0.54	WMT 2014 EN-DE, BLEU	cs - en  Table 3: Results: System-Level Correlations on WMT-14
true	D15-1124.pdf#8	WMT 2014 EN-FR, BLEU	≥ 8 , separately . The computed variance values are  Table 1.
true	D15-1124.pdf#8	WMT 2014 EN-FR, BLEU	≥ 8 , separately . The computed variance values are  Table 1: Variances computed using Algorithm 1
true	D15-1124.pdf#0.54	WMT 2014 EN-FR, BLEU	cs - en  Table 3: Results: System-Level Correlations on WMT-14
true	N15-1035.pdf#52.3%	Senseval 3, F1	* Sports  Table 6: DS05 task results. The values inside brackets are the selected window sizes and statistically significant (p <  0.05) improvements over 'IMS + CC' are marked with '*'.
true	N15-1035.pdf#62.3%	Senseval 3, F1	Finance  Table 6: DS05 task results. The values inside brackets are the selected window sizes and statistically significant (p <  0.05) improvements over 'IMS + CC' are marked with '*'.
true	N15-1035.pdf#57.1%	Senseval 3, F1	* Total  Table 6: DS05 task results. The values inside brackets are the selected window sizes and statistically significant (p <  0.05) improvements over 'IMS + CC' are marked with '*'.
true	N15-1035.pdf#57.1%	Senseval 3, F1	* Sports  Table 6: DS05 task results. The values inside brackets are the selected window sizes and statistically significant (p <  0.05) improvements over 'IMS + CC' are marked with '*'.
true	N15-1035.pdf#68.2%	Senseval 3, F1	SE3  Table 8: SE3 all-words task results. The values inside  brackets are the selected window sizes and statistically  significant (p < 0.05) improvements over the IMS base- line are marked with '*'.
true	N15-1035.pdf#52.3%	Senseval 2, F1	* Sports  Table 6: DS05 task results. The values inside brackets are the selected window sizes and statistically significant (p <  0.05) improvements over 'IMS + CC' are marked with '*'.
true	N15-1035.pdf#62.3%	Senseval 2, F1	Finance  Table 6: DS05 task results. The values inside brackets are the selected window sizes and statistically significant (p <  0.05) improvements over 'IMS + CC' are marked with '*'.
true	N15-1035.pdf#57.1%	Senseval 2, F1	* Total  Table 6: DS05 task results. The values inside brackets are the selected window sizes and statistically significant (p <  0.05) improvements over 'IMS + CC' are marked with '*'.
true	N15-1035.pdf#57.1%	Senseval 2, F1	* Sports  Table 6: DS05 task results. The values inside brackets are the selected window sizes and statistically significant (p <  0.05) improvements over 'IMS + CC' are marked with '*'.
true	N15-1035.pdf#68.2%	Senseval 2, F1	SE3  Table 8: SE3 all-words task results. The values inside  brackets are the selected window sizes and statistically  significant (p < 0.05) improvements over the IMS base- line are marked with '*'.
true	N15-1035.pdf#52.3%	SemEval 2007, F1	* Sports  Table 6: DS05 task results. The values inside brackets are the selected window sizes and statistically significant (p <  0.05) improvements over 'IMS + CC' are marked with '*'.
true	N15-1035.pdf#62.3%	SemEval 2007, F1	Finance  Table 6: DS05 task results. The values inside brackets are the selected window sizes and statistically significant (p <  0.05) improvements over 'IMS + CC' are marked with '*'.
true	N15-1035.pdf#57.1%	SemEval 2007, F1	* Total  Table 6: DS05 task results. The values inside brackets are the selected window sizes and statistically significant (p <  0.05) improvements over 'IMS + CC' are marked with '*'.
true	N15-1035.pdf#57.1%	SemEval 2007, F1	* Sports  Table 6: DS05 task results. The values inside brackets are the selected window sizes and statistically significant (p <  0.05) improvements over 'IMS + CC' are marked with '*'.
true	N15-1035.pdf#68.2%	SemEval 2007, F1	SE3  Table 8: SE3 all-words task results. The values inside  brackets are the selected window sizes and statistically  significant (p < 0.05) improvements over the IMS base- line are marked with '*'.
true	N15-1035.pdf#52.3%	SemEval 2013, F1	* Sports  Table 6: DS05 task results. The values inside brackets are the selected window sizes and statistically significant (p <  0.05) improvements over 'IMS + CC' are marked with '*'.
true	N15-1035.pdf#62.3%	SemEval 2013, F1	Finance  Table 6: DS05 task results. The values inside brackets are the selected window sizes and statistically significant (p <  0.05) improvements over 'IMS + CC' are marked with '*'.
true	N15-1035.pdf#57.1%	SemEval 2013, F1	* Total  Table 6: DS05 task results. The values inside brackets are the selected window sizes and statistically significant (p <  0.05) improvements over 'IMS + CC' are marked with '*'.
true	N15-1035.pdf#57.1%	SemEval 2013, F1	* Sports  Table 6: DS05 task results. The values inside brackets are the selected window sizes and statistically significant (p <  0.05) improvements over 'IMS + CC' are marked with '*'.
true	N15-1035.pdf#68.2%	SemEval 2013, F1	SE3  Table 8: SE3 all-words task results. The values inside  brackets are the selected window sizes and statistically  significant (p < 0.05) improvements over the IMS base- line are marked with '*'.
true	N15-1035.pdf#52.3%	SemEval 2015, F1	* Sports  Table 6: DS05 task results. The values inside brackets are the selected window sizes and statistically significant (p <  0.05) improvements over 'IMS + CC' are marked with '*'.
true	N15-1035.pdf#62.3%	SemEval 2015, F1	Finance  Table 6: DS05 task results. The values inside brackets are the selected window sizes and statistically significant (p <  0.05) improvements over 'IMS + CC' are marked with '*'.
true	N15-1035.pdf#57.1%	SemEval 2015, F1	* Total  Table 6: DS05 task results. The values inside brackets are the selected window sizes and statistically significant (p <  0.05) improvements over 'IMS + CC' are marked with '*'.
true	N15-1035.pdf#57.1%	SemEval 2015, F1	* Sports  Table 6: DS05 task results. The values inside brackets are the selected window sizes and statistically significant (p <  0.05) improvements over 'IMS + CC' are marked with '*'.
true	N15-1035.pdf#68.2%	SemEval 2015, F1	SE3  Table 8: SE3 all-words task results. The values inside  brackets are the selected window sizes and statistically  significant (p < 0.05) improvements over the IMS base- line are marked with '*'.
true	P15-1164.pdf#0.955*	Text8, Number of params	R8 Accuracy  Table 2: Test accuracy and macro-average F1-score on four standard datasets. Bold font marks the best  performance in a column. * indicates statistical significance at p < 0.05 using micro sign test with regards  to the SVM baseline of the same column. MC corresponds to unsupervised feature selection using the  main core of each graph-of-words to extract n-gram and subgraph features. gSpan mining support values  are 1.6% (WebKB), 7% (R8), 4% (LingSpam) and 0.5% (Amazon).
true	P15-1164.pdf#0.973	Text8, Number of params	LingSpam F1 - score  Table 2: Test accuracy and macro-average F1-score on four standard datasets. Bold font marks the best  performance in a column. * indicates statistical significance at p < 0.05 using micro sign test with regards  to the SVM baseline of the same column. MC corresponds to unsupervised feature selection using the  main core of each graph-of-words to extract n-gram and subgraph features. gSpan mining support values  are 1.6% (WebKB), 7% (R8), 4% (LingSpam) and 0.5% (Amazon).
true	P15-1164.pdf#0.912*	Text8, Number of params	WebKB Accuracy  Table 2: Test accuracy and macro-average F1-score on four standard datasets. Bold font marks the best  performance in a column. * indicates statistical significance at p < 0.05 using micro sign test with regards  to the SVM baseline of the same column. MC corresponds to unsupervised feature selection using the  main core of each graph-of-words to extract n-gram and subgraph features. gSpan mining support values  are 1.6% (WebKB), 7% (R8), 4% (LingSpam) and 0.5% (Amazon).
true	P15-1164.pdf#0.991	Text8, Number of params	LingSpam Accuracy  Table 2: Test accuracy and macro-average F1-score on four standard datasets. Bold font marks the best  performance in a column. * indicates statistical significance at p < 0.05 using micro sign test with regards  to the SVM baseline of the same column. MC corresponds to unsupervised feature selection using the  main core of each graph-of-words to extract n-gram and subgraph features. gSpan mining support values  are 1.6% (WebKB), 7% (R8), 4% (LingSpam) and 0.5% (Amazon).
true	P15-1164.pdf#0.882	Text8, Number of params	WebKB F1 - score  Table 2: Test accuracy and macro-average F1-score on four standard datasets. Bold font marks the best  performance in a column. * indicates statistical significance at p < 0.05 using micro sign test with regards  to the SVM baseline of the same column. MC corresponds to unsupervised feature selection using the  main core of each graph-of-words to extract n-gram and subgraph features. gSpan mining support values  are 1.6% (WebKB), 7% (R8), 4% (LingSpam) and 0.5% (Amazon).
true	P15-1164.pdf#0.798	Text8, Number of params	Amazon F1 - score  Table 2: Test accuracy and macro-average F1-score on four standard datasets. Bold font marks the best  performance in a column. * indicates statistical significance at p < 0.05 using micro sign test with regards  to the SVM baseline of the same column. MC corresponds to unsupervised feature selection using the  main core of each graph-of-words to extract n-gram and subgraph features. gSpan mining support values  are 1.6% (WebKB), 7% (R8), 4% (LingSpam) and 0.5% (Amazon).
true	P15-1164.pdf#0.800*	Text8, Number of params	Amazon Accuracy  Table 2: Test accuracy and macro-average F1-score on four standard datasets. Bold font marks the best  performance in a column. * indicates statistical significance at p < 0.05 using micro sign test with regards  to the SVM baseline of the same column. MC corresponds to unsupervised feature selection using the  main core of each graph-of-words to extract n-gram and subgraph features. gSpan mining support values  are 1.6% (WebKB), 7% (R8), 4% (LingSpam) and 0.5% (Amazon).
true	P15-1164.pdf#0.991	Text8, Number of params	LingSpam Accuracy  Table 2: Test accuracy and macro-average F1-score on four standard datasets. Bold font marks the best  performance in a column. * indicates statistical significance at p < 0.05 using micro sign test with regards  to the SVM baseline of the same column. MC corresponds to unsupervised feature selection using the  main core of each graph-of-words to extract n-gram and subgraph features. gSpan mining support values  are 1.6% (WebKB), 7% (R8), 4% (LingSpam) and 0.5% (Amazon).
true	P15-1164.pdf#0.864	Text8, Number of params	R8 F1 - score  Table 2: Test accuracy and macro-average F1-score on four standard datasets. Bold font marks the best  performance in a column. * indicates statistical significance at p < 0.05 using micro sign test with regards  to the SVM baseline of the same column. MC corresponds to unsupervised feature selection using the  main core of each graph-of-words to extract n-gram and subgraph features. gSpan mining support values  are 1.6% (WebKB), 7% (R8), 4% (LingSpam) and 0.5% (Amazon).
true	P15-1164.pdf#0.973	Text8, Number of params	LingSpam F1 - score  Table 2: Test accuracy and macro-average F1-score on four standard datasets. Bold font marks the best  performance in a column. * indicates statistical significance at p < 0.05 using micro sign test with regards  to the SVM baseline of the same column. MC corresponds to unsupervised feature selection using the  main core of each graph-of-words to extract n-gram and subgraph features. gSpan mining support values  are 1.6% (WebKB), 7% (R8), 4% (LingSpam) and 0.5% (Amazon).
true	D15-1034.pdf#48.9	FB15K-237, H@10	H@10  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#78.8	FB15K-237, H@10	H@10  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#76.8	FB15K-237, H@10	TRANSE  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#50.7	FB15K-237, H@10	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#71.3	FB15K-237, H@10	H@10  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#17.7	FB15K-237, H@10	TRANSE  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#123.8	FB15K-237, H@10	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#16.4	FB15K-237, H@10	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#205.7	FB15K-237, H@10	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#46.8	FB15K-237, H@10	H@10  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#21.6	FB15K-237, H@10	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#71.5	FB15K-237, H@10	H@10  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#212.2	FB15K-237, H@10	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#48.9	Text8, Number of params	H@10  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#78.8	Text8, Number of params	H@10  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#76.8	Text8, Number of params	TRANSE  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#50.7	Text8, Number of params	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#71.3	Text8, Number of params	H@10  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#17.7	Text8, Number of params	TRANSE  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#123.8	Text8, Number of params	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#16.4	Text8, Number of params	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#205.7	Text8, Number of params	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#46.8	Text8, Number of params	H@10  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#21.6	Text8, Number of params	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#71.5	Text8, Number of params	H@10  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#212.2	Text8, Number of params	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#48.9	FB15K-237, H@1	H@10  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#78.8	FB15K-237, H@1	H@10  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#76.8	FB15K-237, H@1	TRANSE  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#50.7	FB15K-237, H@1	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#71.3	FB15K-237, H@1	H@10  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#17.7	FB15K-237, H@1	TRANSE  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#123.8	FB15K-237, H@1	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#16.4	FB15K-237, H@1	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#205.7	FB15K-237, H@1	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#46.8	FB15K-237, H@1	H@10  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#21.6	FB15K-237, H@1	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#71.5	FB15K-237, H@1	H@10  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#212.2	FB15K-237, H@1	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#48.9	FB15K-237, MRR	H@10  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#78.8	FB15K-237, MRR	H@10  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#76.8	FB15K-237, MRR	TRANSE  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#50.7	FB15K-237, MRR	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#71.3	FB15K-237, MRR	H@10  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#17.7	FB15K-237, MRR	TRANSE  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#123.8	FB15K-237, MRR	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#16.4	FB15K-237, MRR	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#205.7	FB15K-237, MRR	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#46.8	FB15K-237, MRR	H@10  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#21.6	FB15K-237, MRR	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#71.5	FB15K-237, MRR	H@10  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	D15-1034.pdf#212.2	FB15K-237, MRR	MR  Table 2: Detailed performances on FB15k of  TRANSE and RTRANSE. H@10 are in %. W.  COMP. indicates examples for which there exist  quadruplets in train matching their relationship.
true	P15-2046.pdf#81.7	New York Times Corpus, P@10%	F1  Table 2:
true	P15-2046.pdf#73.8	New York Times Corpus, P@10%	Penn Treebank set F1  Table 3: Comparison of complete Open IE sys- tems. The asterisks denote results reported in pre- vious work.
true	P15-2046.pdf#48.7	New York Times Corpus, P@10%	ClueWeb set F1  Table 3: Comparison of complete Open IE sys- tems. The asterisks denote results reported in pre- vious work.
true	P15-2046.pdf#57.6	New York Times Corpus, P@10%	New York Times set F1  Table 3: Comparison of complete Open IE sys- tems. The asterisks denote results reported in pre- vious work.
true	P15-1072.pdf#0.71	Senseval 3, F1	ρ  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.86	Senseval 3, F1	- r  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.76	Senseval 3, F1	r  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.77	Senseval 3, F1	r  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.77	Senseval 3, F1	ρ  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.87	Senseval 3, F1	- ρ  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.83	Senseval 3, F1	DE - FR  Table 3: Pearson correlation performance of dif- ferent similarity measures on the three cross- lingual RG-65 datasets.
true	P15-1072.pdf#0.83	Senseval 3, F1	FR - EN  Table 3: Pearson correlation performance of dif- ferent similarity measures on the three cross- lingual RG-65 datasets.
true	P15-1072.pdf#0.76	Senseval 3, F1	EN - DE  Table 3: Pearson correlation performance of dif- ferent similarity measures on the three cross- lingual RG-65 datasets.
true	P15-1072.pdf#0.83	Senseval 3, F1	FR - EN  Table 3: Pearson correlation performance of dif- ferent similarity measures on the three cross- lingual RG-65 datasets.
true	P15-1072.pdf#83.1	Senseval 3, F1	German  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#67.3	Senseval 3, F1	German SemEval - 2007  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#85.1	Senseval 3, F1	Spanish  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#84.3	Senseval 3, F1	Italian  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#72.3	Senseval 3, F1	French  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#66.0	Senseval 3, F1	French SemEval - 2013  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#87.4	Senseval 3, F1	English  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#0.71	SemEval 2015, F1	ρ  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.86	SemEval 2015, F1	- r  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.76	SemEval 2015, F1	r  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.77	SemEval 2015, F1	r  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.77	SemEval 2015, F1	ρ  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.87	SemEval 2015, F1	- ρ  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.83	SemEval 2015, F1	DE - FR  Table 3: Pearson correlation performance of dif- ferent similarity measures on the three cross- lingual RG-65 datasets.
true	P15-1072.pdf#0.83	SemEval 2015, F1	FR - EN  Table 3: Pearson correlation performance of dif- ferent similarity measures on the three cross- lingual RG-65 datasets.
true	P15-1072.pdf#0.76	SemEval 2015, F1	EN - DE  Table 3: Pearson correlation performance of dif- ferent similarity measures on the three cross- lingual RG-65 datasets.
true	P15-1072.pdf#0.83	SemEval 2015, F1	FR - EN  Table 3: Pearson correlation performance of dif- ferent similarity measures on the three cross- lingual RG-65 datasets.
true	P15-1072.pdf#83.1	SemEval 2015, F1	German  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#67.3	SemEval 2015, F1	German SemEval - 2007  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#85.1	SemEval 2015, F1	Spanish  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#84.3	SemEval 2015, F1	Italian  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#72.3	SemEval 2015, F1	French  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#66.0	SemEval 2015, F1	French SemEval - 2013  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#87.4	SemEval 2015, F1	English  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#0.71	SemEval 2007, F1	ρ  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.86	SemEval 2007, F1	- r  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.76	SemEval 2007, F1	r  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.77	SemEval 2007, F1	r  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.77	SemEval 2007, F1	ρ  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.87	SemEval 2007, F1	- ρ  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.83	SemEval 2007, F1	DE - FR  Table 3: Pearson correlation performance of dif- ferent similarity measures on the three cross- lingual RG-65 datasets.
true	P15-1072.pdf#0.83	SemEval 2007, F1	FR - EN  Table 3: Pearson correlation performance of dif- ferent similarity measures on the three cross- lingual RG-65 datasets.
true	P15-1072.pdf#0.76	SemEval 2007, F1	EN - DE  Table 3: Pearson correlation performance of dif- ferent similarity measures on the three cross- lingual RG-65 datasets.
true	P15-1072.pdf#0.83	SemEval 2007, F1	FR - EN  Table 3: Pearson correlation performance of dif- ferent similarity measures on the three cross- lingual RG-65 datasets.
true	P15-1072.pdf#83.1	SemEval 2007, F1	German  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#67.3	SemEval 2007, F1	German SemEval - 2007  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#85.1	SemEval 2007, F1	Spanish  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#84.3	SemEval 2007, F1	Italian  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#72.3	SemEval 2007, F1	French  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#66.0	SemEval 2007, F1	French SemEval - 2013  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#87.4	SemEval 2007, F1	English  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#0.71	SemEval 2013, F1	ρ  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.86	SemEval 2013, F1	- r  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.76	SemEval 2013, F1	r  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.77	SemEval 2013, F1	r  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.77	SemEval 2013, F1	ρ  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.87	SemEval 2013, F1	- ρ  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.83	SemEval 2013, F1	DE - FR  Table 3: Pearson correlation performance of dif- ferent similarity measures on the three cross- lingual RG-65 datasets.
true	P15-1072.pdf#0.83	SemEval 2013, F1	FR - EN  Table 3: Pearson correlation performance of dif- ferent similarity measures on the three cross- lingual RG-65 datasets.
true	P15-1072.pdf#0.76	SemEval 2013, F1	EN - DE  Table 3: Pearson correlation performance of dif- ferent similarity measures on the three cross- lingual RG-65 datasets.
true	P15-1072.pdf#0.83	SemEval 2013, F1	FR - EN  Table 3: Pearson correlation performance of dif- ferent similarity measures on the three cross- lingual RG-65 datasets.
true	P15-1072.pdf#83.1	SemEval 2013, F1	German  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#67.3	SemEval 2013, F1	German SemEval - 2007  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#85.1	SemEval 2013, F1	Spanish  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#84.3	SemEval 2013, F1	Italian  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#72.3	SemEval 2013, F1	French  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#66.0	SemEval 2013, F1	French SemEval - 2013  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#87.4	SemEval 2013, F1	English  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#0.71	Senseval 2, F1	ρ  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.86	Senseval 2, F1	- r  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.76	Senseval 2, F1	r  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.77	Senseval 2, F1	r  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.77	Senseval 2, F1	ρ  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.87	Senseval 2, F1	- ρ  Table 2: Spearman (ρ) and Pearson (r) correlation performance of different systems on the English,  German and French RG-65 datasets.
true	P15-1072.pdf#0.83	Senseval 2, F1	DE - FR  Table 3: Pearson correlation performance of dif- ferent similarity measures on the three cross- lingual RG-65 datasets.
true	P15-1072.pdf#0.83	Senseval 2, F1	FR - EN  Table 3: Pearson correlation performance of dif- ferent similarity measures on the three cross- lingual RG-65 datasets.
true	P15-1072.pdf#0.76	Senseval 2, F1	EN - DE  Table 3: Pearson correlation performance of dif- ferent similarity measures on the three cross- lingual RG-65 datasets.
true	P15-1072.pdf#0.83	Senseval 2, F1	FR - EN  Table 3: Pearson correlation performance of dif- ferent similarity measures on the three cross- lingual RG-65 datasets.
true	P15-1072.pdf#83.1	Senseval 2, F1	German  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#67.3	Senseval 2, F1	German SemEval - 2007  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#85.1	Senseval 2, F1	Spanish  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#84.3	Senseval 2, F1	Italian  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#72.3	Senseval 2, F1	French  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#66.0	Senseval 2, F1	French SemEval - 2013  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	P15-1072.pdf#87.4	Senseval 2, F1	English  Table 4: F1 percentage performance on the SemEval-2013 Multilingual WSD datasets using Wikipedia  as sense inventory.
true	D15-1117.pdf#94%	New York Times Corpus, P@10%	AI  Table 1: Experiment result for finance and AI do- mains. P stands for Precision, and N indicates the  number of extracted relations.
true	D15-1117.pdf#90%	New York Times Corpus, P@10%	AI  Table 1: Experiment result for finance and AI do- mains. P stands for Precision, and N indicates the  number of extracted relations.
true	D15-1117.pdf#73	New York Times Corpus, P@10%	Plant F  Table 2: Experiment results for animal and plant  domains. P stands for Precision, R Recall, and F  F-score. The unit is %.
true	D15-1117.pdf#59	New York Times Corpus, P@10%	Plant R  Table 2: Experiment results for animal and plant  domains. P stands for Precision, R Recall, and F  F-score. The unit is %.
true	D15-1117.pdf#97	New York Times Corpus, P@10%	Plant  Table 2: Experiment results for animal and plant  domains. P stands for Precision, R Recall, and F  F-score. The unit is %.
true	D15-1117.pdf#97	New York Times Corpus, P@10%	Plant P  Table 2: Experiment results for animal and plant  domains. P stands for Precision, R Recall, and F  F-score. The unit is %.
true	D15-1117.pdf#65	New York Times Corpus, P@10%	Plant R  Table 2: Experiment results for animal and plant  domains. P stands for Precision, R Recall, and F  F-score. The unit is %.
true	D15-1117.pdf#98	New York Times Corpus, P@10%	Plant  Table 2: Experiment results for animal and plant  domains. P stands for Precision, R Recall, and F  F-score. The unit is %.
true	D15-1117.pdf#78	New York Times Corpus, P@10%	Plant F  Table 2: Experiment results for animal and plant  domains. P stands for Precision, R Recall, and F  F-score. The unit is %.
true	D15-1117.pdf#97	New York Times Corpus, P@10%	Plant P  Table 2: Experiment results for animal and plant  domains. P stands for Precision, R Recall, and F  F-score. The unit is %.
true	D15-1117.pdf#65	New York Times Corpus, P@10%	Plant R  Table 2: Experiment results for animal and plant  domains. P stands for Precision, R Recall, and F  F-score. The unit is %.
true	D15-1117.pdf#97	New York Times Corpus, P@10%	Plant P  Table 2: Experiment results for animal and plant  domains. P stands for Precision, R Recall, and F  F-score. The unit is %.
true	D15-1117.pdf#69	New York Times Corpus, P@10%	Virus F  Table 3: Experiment results for vehicle and virus  domains. P stands for Precision, R Recall, and F  F-score. The unit is %.
true	D15-1117.pdf#99	New York Times Corpus, P@10%	Virus P  Table 3: Experiment results for vehicle and virus  domains. P stands for Precision, R Recall, and F  F-score. The unit is %.
true	D15-1117.pdf#73	New York Times Corpus, P@10%	Vehicle R  Table 3: Experiment results for vehicle and virus  domains. P stands for Precision, R Recall, and F  F-score. The unit is %.
true	D15-1117.pdf#83	New York Times Corpus, P@10%	Vehicle F  Table 3: Experiment results for vehicle and virus  domains. P stands for Precision, R Recall, and F  F-score. The unit is %.
true	D15-1117.pdf#54	New York Times Corpus, P@10%	Virus R  Table 3: Experiment results for vehicle and virus  domains. P stands for Precision, R Recall, and F  F-score. The unit is %.
true	D15-1117.pdf#99	New York Times Corpus, P@10%	Vehicle  Table 3: Experiment results for vehicle and virus  domains. P stands for Precision, R Recall, and F  F-score. The unit is %.
true	D15-1117.pdf#74%	New York Times Corpus, P@10%	Synonym : Animal  Table 4: Contribution of individual trustiness mea- sures and collective synonym evidence in terms of  F-measure. Imp stands for Important and Accu  stands for Accuracy
true	D15-1117.pdf#80%	New York Times Corpus, P@10%	Synonym : Vehicle  Table 4: Contribution of individual trustiness mea- sures and collective synonym evidence in terms of  F-measure. Imp stands for Important and Accu  stands for Accuracy
true	D15-1117.pdf#65%	New York Times Corpus, P@10%	Synonym : Virus  Table 4: Contribution of individual trustiness mea- sures and collective synonym evidence in terms of  F-measure. Imp stands for Important and Accu  stands for Accuracy
true	D15-1117.pdf#70%	New York Times Corpus, P@10%	Trustiness : Plant  Table 4: Contribution of individual trustiness mea- sures and collective synonym evidence in terms of  F-measure. Imp stands for Important and Accu  stands for Accuracy
true	D15-1117.pdf#74%	New York Times Corpus, P@10%	Synonym : Animal  Table 4: Contribution of individual trustiness mea- sures and collective synonym evidence in terms of  F-measure. Imp stands for Important and Accu  stands for Accuracy
true	D15-1117.pdf#80%	New York Times Corpus, P@10%	Synonym : Vehicle  Table 4: Contribution of individual trustiness mea- sures and collective synonym evidence in terms of  F-measure. Imp stands for Important and Accu  stands for Accuracy
true	D15-1117.pdf#70%	New York Times Corpus, P@10%	Plant  Table 4: Contribution of individual trustiness mea- sures and collective synonym evidence in terms of  F-measure. Imp stands for Important and Accu  stands for Accuracy
true	D15-1117.pdf#82%	New York Times Corpus, P@10%	Trustiness : Vehicle  Table 4: Contribution of individual trustiness mea- sures and collective synonym evidence in terms of  F-measure. Imp stands for Important and Accu  stands for Accuracy
true	D15-1117.pdf#70%	New York Times Corpus, P@10%	Synonym : Plant  Table 4: Contribution of individual trustiness mea- sures and collective synonym evidence in terms of  F-measure. Imp stands for Important and Accu  stands for Accuracy
true	D15-1117.pdf#70%	New York Times Corpus, P@10%	Trustiness : Plant  Table 4: Contribution of individual trustiness mea- sures and collective synonym evidence in terms of  F-measure. Imp stands for Important and Accu  stands for Accuracy
true	D15-1117.pdf#64%	New York Times Corpus, P@10%	Trustiness : Virus  Table 4: Contribution of individual trustiness mea- sures and collective synonym evidence in terms of  F-measure. Imp stands for Important and Accu  stands for Accuracy
true	D15-1117.pdf#75%	New York Times Corpus, P@10%	Trustiness : Animal  Table 4: Contribution of individual trustiness mea- sures and collective synonym evidence in terms of  F-measure. Imp stands for Important and Accu  stands for Accuracy
true	P13-1134.pdf#0.783	SemEval 2015, F1	noun  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.728	SemEval 2015, F1	all  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.784	SemEval 2015, F1	adj  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.83	SemEval 2015, F1	adj  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.677	SemEval 2015, F1	verb  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.739	SemEval 2015, F1	all  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.742	SemEval 2015, F1	adj  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.75	SemEval 2015, F1	all  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.68	SemEval 2015, F1	verb  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.683	SemEval 2015, F1	verb  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.778	SemEval 2015, F1	noun  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.775	SemEval 2015, F1	noun  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.783	SemEval 2007, F1	noun  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.728	SemEval 2007, F1	all  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.784	SemEval 2007, F1	adj  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.83	SemEval 2007, F1	adj  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.677	SemEval 2007, F1	verb  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.739	SemEval 2007, F1	all  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.742	SemEval 2007, F1	adj  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.75	SemEval 2007, F1	all  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.68	SemEval 2007, F1	verb  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.683	SemEval 2007, F1	verb  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.778	SemEval 2007, F1	noun  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.775	SemEval 2007, F1	noun  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.783	SemEval 2013, F1	noun  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.728	SemEval 2013, F1	all  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.784	SemEval 2013, F1	adj  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.83	SemEval 2013, F1	adj  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.677	SemEval 2013, F1	verb  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.739	SemEval 2013, F1	all  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.742	SemEval 2013, F1	adj  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.75	SemEval 2013, F1	all  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.68	SemEval 2013, F1	verb  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.683	SemEval 2013, F1	verb  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.778	SemEval 2013, F1	noun  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.775	SemEval 2013, F1	noun  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.783	Senseval 3, F1	noun  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.728	Senseval 3, F1	all  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.784	Senseval 3, F1	adj  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.83	Senseval 3, F1	adj  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.677	Senseval 3, F1	verb  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.739	Senseval 3, F1	all  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.742	Senseval 3, F1	adj  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.75	Senseval 3, F1	all  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.68	Senseval 3, F1	verb  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.683	Senseval 3, F1	verb  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.778	Senseval 3, F1	noun  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.775	Senseval 3, F1	noun  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.783	Senseval 2, F1	noun  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.728	Senseval 2, F1	all  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.784	Senseval 2, F1	adj  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.83	Senseval 2, F1	adj  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.677	Senseval 2, F1	verb  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.739	Senseval 2, F1	all  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.742	Senseval 2, F1	adj  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.75	Senseval 2, F1	all  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.68	Senseval 2, F1	verb  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.683	Senseval 2, F1	verb  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.778	Senseval 2, F1	noun  Table 2: Alignment performance by POS.
true	P13-1134.pdf#0.775	Senseval 2, F1	noun  Table 2: Alignment performance by POS.
true	D15-1257.pdf#0.519	Text8, Number of params	) . F - Score  Table 4: Distant supervision results (*p < 0.01).
true	D15-1257.pdf#0.745	Text8, Number of params	) . Recall  Table 4: Distant supervision results (*p < 0.01).
true	D15-1257.pdf#0.398	Text8, Number of params	) . Precision  Table 4: Distant supervision results (*p < 0.01).
true	D15-1257.pdf#0.478	Text8, Number of params	Precision  Table 6: Best self-training results (*p < 0.01).
true	D15-1257.pdf#0.635	Text8, Number of params	F - Score  Table 6: Best self-training results (*p < 0.01).
true	D15-1257.pdf#0.946	Text8, Number of params	Recall  Table 6: Best self-training results (*p < 0.01).
true	P12-1086.pdf#4.2	SemEval 2007, F1	P 10 55 ST 3 ST 4 ST 5 ST 6 − − − −  Table 1: Characteristics of each collection.
true	P12-1086.pdf#4.2	SemEval 2015, F1	P 10 55 ST 3 ST 4 ST 5 ST 6 − − − −  Table 1: Characteristics of each collection.
true	P12-1086.pdf#4.2	SemEval 2013, F1	P 10 55 ST 3 ST 4 ST 5 ST 6 − − − −  Table 1: Characteristics of each collection.
true	N10-1134.pdf#0.1	DUC 2004 Task 1, ROUGE-L	. factor true - 0 . 65 / 0 . 15  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#1.0	DUC 2004 Task 1, ROUGE-L	. factor true - 0 . 65 / 0 . 15 0 . 71 / 0 . 15 0 . 88 / 0 . 11 0 . 96 / 0 . 04 0 . 98 / 0 . 02  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#0.3	DUC 2004 Task 1, ROUGE-L	. factor true - 0 . 65 / 0 . 15 0 . 71 / 0 . 15  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#0.0	DUC 2004 Task 1, ROUGE-L	. factor true -  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#1.00	DUC 2004 Task 1, ROUGE-L	. factor true  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#0.7	DUC 2004 Task 1, ROUGE-L	. factor true - 0 . 65 / 0 . 15 0 . 71 / 0 . 15 0 . 88 / 0 . 11 0 . 96 / 0 . 04  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#1.2	DUC 2004 Task 1, ROUGE-L	. factor true - 0 . 65 / 0 . 15 0 . 71 / 0 . 15 0 . 88 / 0 . 11 0 . 96 / 0 . 04 0 . 98 / 0 . 02 0 . 98 / 0 . 02  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#0.5	DUC 2004 Task 1, ROUGE-L	. factor true - 0 . 65 / 0 . 15 0 . 71 / 0 . 15 0 . 88 / 0 . 11  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#38.39	DUC 2004 Task 1, ROUGE-L	Table 2 : ROUGE - 1 F - measure results ( % ) ROUGE - 1 score  Table 2: ROUGE-1 F-measure results (%)
true	N10-1134.pdf#0.3,λ=4)	DUC 2004 Task 1, ROUGE-L	Table 2 : ROUGE - 1 F - measure results ( % ) Method  Table 2: ROUGE-1 F-measure results (%)
true	N10-1134.pdf#0.1	DUC 2004 Task 1, ROUGE-1	. factor true - 0 . 65 / 0 . 15  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#1.0	DUC 2004 Task 1, ROUGE-1	. factor true - 0 . 65 / 0 . 15 0 . 71 / 0 . 15 0 . 88 / 0 . 11 0 . 96 / 0 . 04 0 . 98 / 0 . 02  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#0.3	DUC 2004 Task 1, ROUGE-1	. factor true - 0 . 65 / 0 . 15 0 . 71 / 0 . 15  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#0.0	DUC 2004 Task 1, ROUGE-1	. factor true -  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#1.00	DUC 2004 Task 1, ROUGE-1	. factor true  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#0.7	DUC 2004 Task 1, ROUGE-1	. factor true - 0 . 65 / 0 . 15 0 . 71 / 0 . 15 0 . 88 / 0 . 11 0 . 96 / 0 . 04  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#1.2	DUC 2004 Task 1, ROUGE-1	. factor true - 0 . 65 / 0 . 15 0 . 71 / 0 . 15 0 . 88 / 0 . 11 0 . 96 / 0 . 04 0 . 98 / 0 . 02 0 . 98 / 0 . 02  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#0.5	DUC 2004 Task 1, ROUGE-1	. factor true - 0 . 65 / 0 . 15 0 . 71 / 0 . 15 0 . 88 / 0 . 11  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#38.39	DUC 2004 Task 1, ROUGE-1	Table 2 : ROUGE - 1 F - measure results ( % ) ROUGE - 1 score  Table 2: ROUGE-1 F-measure results (%)
true	N10-1134.pdf#0.3,λ=4)	DUC 2004 Task 1, ROUGE-1	Table 2 : ROUGE - 1 F - measure results ( % ) Method  Table 2: ROUGE-1 F-measure results (%)
true	N10-1134.pdf#0.1	DUC 2004 Task 1, ROUGE-2	. factor true - 0 . 65 / 0 . 15  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#1.0	DUC 2004 Task 1, ROUGE-2	. factor true - 0 . 65 / 0 . 15 0 . 71 / 0 . 15 0 . 88 / 0 . 11 0 . 96 / 0 . 04 0 . 98 / 0 . 02  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#0.3	DUC 2004 Task 1, ROUGE-2	. factor true - 0 . 65 / 0 . 15 0 . 71 / 0 . 15  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#0.0	DUC 2004 Task 1, ROUGE-2	. factor true -  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#1.00	DUC 2004 Task 1, ROUGE-2	. factor true  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#0.7	DUC 2004 Task 1, ROUGE-2	. factor true - 0 . 65 / 0 . 15 0 . 71 / 0 . 15 0 . 88 / 0 . 11 0 . 96 / 0 . 04  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#1.2	DUC 2004 Task 1, ROUGE-2	. factor true - 0 . 65 / 0 . 15 0 . 71 / 0 . 15 0 . 88 / 0 . 11 0 . 96 / 0 . 04 0 . 98 / 0 . 02 0 . 98 / 0 . 02  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#0.5	DUC 2004 Task 1, ROUGE-2	. factor true - 0 . 65 / 0 . 15 0 . 71 / 0 . 15 0 . 88 / 0 . 11  Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The "true" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.
true	N10-1134.pdf#38.39	DUC 2004 Task 1, ROUGE-2	Table 2 : ROUGE - 1 F - measure results ( % ) ROUGE - 1 score  Table 2: ROUGE-1 F-measure results (%)
true	N10-1134.pdf#0.3,λ=4)	DUC 2004 Task 1, ROUGE-2	Table 2 : ROUGE - 1 F - measure results ( % ) Method  Table 2: ROUGE-1 F-measure results (%)
true	P13-3006.pdf#73.53	New York Times Corpus, P@10%	R  Table 1: Performance of various classifiers in identifying  causal connectives
true	P13-3006.pdf#89.13	New York Times Corpus, P@10%	P  Table 1: Performance of various classifiers in identifying  causal connectives
true	P13-3006.pdf#89.29	New York Times Corpus, P@10%	P  Table 1: Performance of various classifiers in identifying  causal connectives
true	P13-3006.pdf#72.87	New York Times Corpus, P@10%	F 1  Table 1: Performance of various classifiers in identifying  causal connectives
true	P13-3006.pdf#79.35	New York Times Corpus, P@10%	F 1  Table 1: Performance of various classifiers in identifying  causal connectives
true	P13-3006.pdf#64.04	New York Times Corpus, P@10%	R  Table 1: Performance of various classifiers in identifying  causal connectives
true	P13-3006.pdf#64.94	New York Times Corpus, P@10%	R  Table 2: Effect of feature types on the sequence labelling  task, given in percentages.
true	P13-3006.pdf#89.13	New York Times Corpus, P@10%	P  Table 2: Effect of feature types on the sequence labelling  task, given in percentages.
true	P13-3006.pdf#79.35	New York Times Corpus, P@10%	F 1  Table 2: Effect of feature types on the sequence labelling  task, given in percentages.
true	P13-3006.pdf#92.20	New York Times Corpus, P@10%	P  Table 2: Effect of feature types on the sequence labelling  task, given in percentages.
true	P13-3006.pdf#72.87	New York Times Corpus, P@10%	F 1  Table 2: Effect of feature types on the sequence labelling  task, given in percentages.
true	P13-3006.pdf#73.53	New York Times Corpus, P@10%	R  Table 2: Effect of feature types on the sequence labelling  task, given in percentages.
true	P13-3006.pdf#73.08	New York Times Corpus, P@10%	F 1  Table 3: Effect of feature types on Random Forests.
true	P13-3006.pdf#84.34	New York Times Corpus, P@10%	P  Table 3: Effect of feature types on Random Forests.
true	P13-3006.pdf#68.30	New York Times Corpus, P@10%	Table 3: Effect of feature types on Random Forests.
true	P13-3006.pdf#61.05	New York Times Corpus, P@10%	R  Table 4. As can be observed,  the best performance is obtained when combining  the lexical and semantic feature types (69.85% F- score). The combination of all features produces the  best precision, whilst the best recall is obtained by  combining lexical and semantic features.
true	P13-3006.pdf#87.70	New York Times Corpus, P@10%	P  Table 4. As can be observed,  the best performance is obtained when combining  the lexical and semantic feature types (69.85% F- score). The combination of all features produces the  best precision, whilst the best recall is obtained by  combining lexical and semantic features.
true	P13-3006.pdf#69.85	New York Times Corpus, P@10%	F 1  Table 4. As can be observed,  the best performance is obtained when combining  the lexical and semantic feature types (69.85% F- score). The combination of all features produces the  best precision, whilst the best recall is obtained by  combining lexical and semantic features.
true	P13-3006.pdf#61.05	New York Times Corpus, P@10%	R  Table 4: Effect of feature types on SVM.
true	P13-3006.pdf#87.70	New York Times Corpus, P@10%	P  Table 4: Effect of feature types on SVM.
true	P13-3006.pdf#69.85	New York Times Corpus, P@10%	F 1  Table 4: Effect of feature types on SVM.
true	P14-2014.pdf#0.83	New York Times Corpus, P@10%	N ( Sim )  Table 1: Comparison of the worst case computa- tional complexicity (ρ -the maximum branching  factor) and kernel performance on the 3 examples  from
true	P14-2014.pdf#0.83	New York Times Corpus, P@10%	N ( Sim )  Table 1: Comparison of the worst case computa- tional complexicity (ρ -the maximum branching  factor) and kernel performance on the 3 examples  from
true	P14-2014.pdf#1||N	New York Times Corpus, P@10%	Kernel  Table 1: Comparison of the worst case computa- tional complexicity (ρ -the maximum branching  factor) and kernel performance on the 3 examples  from
true	P14-2014.pdf#2|	New York Times Corpus, P@10%	Kernel  Table 1: Comparison of the worst case computa- tional complexicity (ρ -the maximum branching  factor) and kernel performance on the 3 examples  from
true	P14-2014.pdf#2|	New York Times Corpus, P@10%	Kernel  Table 1: Comparison of the worst case computa- tional complexicity (ρ -the maximum branching  factor) and kernel performance on the 3 examples  from
true	P14-2014.pdf#2|	New York Times Corpus, P@10%	Kernel  Table 1: Comparison of the worst case computa- tional complexicity (ρ -the maximum branching  factor) and kernel performance on the 3 examples  from
true	P14-2014.pdf#0.78	New York Times Corpus, P@10%	N ( Sim )  Table 1: Comparison of the worst case computa- tional complexicity (ρ -the maximum branching  factor) and kernel performance on the 3 examples  from
true	P14-2014.pdf#1||N	New York Times Corpus, P@10%	Kernel  Table 1: Comparison of the worst case computa- tional complexicity (ρ -the maximum branching  factor) and kernel performance on the 3 examples  from
true	P14-2014.pdf#2|N	New York Times Corpus, P@10%	Kernel  Table 1: Comparison of the worst case computa- tional complexicity (ρ -the maximum branching  factor) and kernel performance on the 3 examples  from
true	P14-2014.pdf#1||N	New York Times Corpus, P@10%	Kernel  Table 1: Comparison of the worst case computa- tional complexicity (ρ -the maximum branching  factor) and kernel performance on the 3 examples  from
true	P14-2014.pdf#3|N	New York Times Corpus, P@10%	Kernel  Table 1: Comparison of the worst case computa- tional complexicity (ρ -the maximum branching  factor) and kernel performance on the 3 examples  from
true	P14-2014.pdf#0.292	New York Times Corpus, P@10%	THYME F  Table 2: Comparison of tree kernel performance  for temporal relation extraction on THYME and  TempEval-2013 data.
true	P14-2014.pdf#0.732	New York Times Corpus, P@10%	THYME F  Table 2: Comparison of tree kernel performance  for temporal relation extraction on THYME and  TempEval-2013 data.
true	P14-2014.pdf#0.708	New York Times Corpus, P@10%	F  Table 2: Comparison of tree kernel performance  for temporal relation extraction on THYME and  TempEval-2013 data.
true	D13-1166.pdf#0.241	Senseval 2, F1	Ambig .  Table 1: Results for the G&S dataset.
true	D13-1166.pdf#0.262	Senseval 2, F1	Disamb .  Table 1: Results for the G&S dataset.
true	D13-1166.pdf#0.001	Senseval 2, F1	Disamb .  Table 1: Results for the G&S dataset.
true	D13-1166.pdf#0.001	Senseval 2, F1	Disamb .  Table 2: Results for the Kartsaklis et al. dataset.
true	D13-1166.pdf#0.151	Senseval 2, F1	Ambig .  Table 2: Results for the Kartsaklis et al. dataset.
true	D13-1166.pdf#0.217	Senseval 2, F1	Disamb .  Table 2: Results for the Kartsaklis et al. dataset.
true	D13-1166.pdf#0.001	Senseval 2, F1	Difference between M1 and M2 is not s . s . Disamb .  Table 3: Results for the original M&L task.
true	D13-1166.pdf#0.315	Senseval 2, F1	Ambig .  Table 3: Results for the original M&L task.
true	D13-1166.pdf#0.448	Senseval 2, F1	Disamb .  Table 3: Results for the original M&L task.
true	D13-1166.pdf#0.412	Senseval 2, F1	Disamb .  Table 4: Transitive version of M&L task.
true	D13-1166.pdf#0.404	Senseval 2, F1	.  Table 4: Transitive version of M&L task.
true	D13-1166.pdf#0.241	SemEval 2007, F1	Ambig .  Table 1: Results for the G&S dataset.
true	D13-1166.pdf#0.262	SemEval 2007, F1	Disamb .  Table 1: Results for the G&S dataset.
true	D13-1166.pdf#0.001	SemEval 2007, F1	Disamb .  Table 1: Results for the G&S dataset.
true	D13-1166.pdf#0.001	SemEval 2007, F1	Disamb .  Table 2: Results for the Kartsaklis et al. dataset.
true	D13-1166.pdf#0.151	SemEval 2007, F1	Ambig .  Table 2: Results for the Kartsaklis et al. dataset.
true	D13-1166.pdf#0.217	SemEval 2007, F1	Disamb .  Table 2: Results for the Kartsaklis et al. dataset.
true	D13-1166.pdf#0.001	SemEval 2007, F1	Difference between M1 and M2 is not s . s . Disamb .  Table 3: Results for the original M&L task.
true	D13-1166.pdf#0.315	SemEval 2007, F1	Ambig .  Table 3: Results for the original M&L task.
true	D13-1166.pdf#0.448	SemEval 2007, F1	Disamb .  Table 3: Results for the original M&L task.
true	D13-1166.pdf#0.412	SemEval 2007, F1	Disamb .  Table 4: Transitive version of M&L task.
true	D13-1166.pdf#0.404	SemEval 2007, F1	.  Table 4: Transitive version of M&L task.
true	D13-1166.pdf#0.241	SemEval 2015, F1	Ambig .  Table 1: Results for the G&S dataset.
true	D13-1166.pdf#0.262	SemEval 2015, F1	Disamb .  Table 1: Results for the G&S dataset.
true	D13-1166.pdf#0.001	SemEval 2015, F1	Disamb .  Table 1: Results for the G&S dataset.
true	D13-1166.pdf#0.001	SemEval 2015, F1	Disamb .  Table 2: Results for the Kartsaklis et al. dataset.
true	D13-1166.pdf#0.151	SemEval 2015, F1	Ambig .  Table 2: Results for the Kartsaklis et al. dataset.
true	D13-1166.pdf#0.217	SemEval 2015, F1	Disamb .  Table 2: Results for the Kartsaklis et al. dataset.
true	D13-1166.pdf#0.001	SemEval 2015, F1	Difference between M1 and M2 is not s . s . Disamb .  Table 3: Results for the original M&L task.
true	D13-1166.pdf#0.315	SemEval 2015, F1	Ambig .  Table 3: Results for the original M&L task.
true	D13-1166.pdf#0.448	SemEval 2015, F1	Disamb .  Table 3: Results for the original M&L task.
true	D13-1166.pdf#0.412	SemEval 2015, F1	Disamb .  Table 4: Transitive version of M&L task.
true	D13-1166.pdf#0.404	SemEval 2015, F1	.  Table 4: Transitive version of M&L task.
true	D13-1166.pdf#0.241	Senseval 3, F1	Ambig .  Table 1: Results for the G&S dataset.
true	D13-1166.pdf#0.262	Senseval 3, F1	Disamb .  Table 1: Results for the G&S dataset.
true	D13-1166.pdf#0.001	Senseval 3, F1	Disamb .  Table 1: Results for the G&S dataset.
true	D13-1166.pdf#0.001	Senseval 3, F1	Disamb .  Table 2: Results for the Kartsaklis et al. dataset.
true	D13-1166.pdf#0.151	Senseval 3, F1	Ambig .  Table 2: Results for the Kartsaklis et al. dataset.
true	D13-1166.pdf#0.217	Senseval 3, F1	Disamb .  Table 2: Results for the Kartsaklis et al. dataset.
true	D13-1166.pdf#0.001	Senseval 3, F1	Difference between M1 and M2 is not s . s . Disamb .  Table 3: Results for the original M&L task.
true	D13-1166.pdf#0.315	Senseval 3, F1	Ambig .  Table 3: Results for the original M&L task.
true	D13-1166.pdf#0.448	Senseval 3, F1	Disamb .  Table 3: Results for the original M&L task.
true	D13-1166.pdf#0.412	Senseval 3, F1	Disamb .  Table 4: Transitive version of M&L task.
true	D13-1166.pdf#0.404	Senseval 3, F1	.  Table 4: Transitive version of M&L task.
true	D13-1166.pdf#0.241	SemEval 2013, F1	Ambig .  Table 1: Results for the G&S dataset.
true	D13-1166.pdf#0.262	SemEval 2013, F1	Disamb .  Table 1: Results for the G&S dataset.
true	D13-1166.pdf#0.001	SemEval 2013, F1	Disamb .  Table 1: Results for the G&S dataset.
true	D13-1166.pdf#0.001	SemEval 2013, F1	Disamb .  Table 2: Results for the Kartsaklis et al. dataset.
true	D13-1166.pdf#0.151	SemEval 2013, F1	Ambig .  Table 2: Results for the Kartsaklis et al. dataset.
true	D13-1166.pdf#0.217	SemEval 2013, F1	Disamb .  Table 2: Results for the Kartsaklis et al. dataset.
true	D13-1166.pdf#0.001	SemEval 2013, F1	Difference between M1 and M2 is not s . s . Disamb .  Table 3: Results for the original M&L task.
true	D13-1166.pdf#0.315	SemEval 2013, F1	Ambig .  Table 3: Results for the original M&L task.
true	D13-1166.pdf#0.448	SemEval 2013, F1	Disamb .  Table 3: Results for the original M&L task.
true	D13-1166.pdf#0.412	SemEval 2013, F1	Disamb .  Table 4: Transitive version of M&L task.
true	D13-1166.pdf#0.404	SemEval 2013, F1	.  Table 4: Transitive version of M&L task.
true	P14-1028.pdf#93.7	PKU, F1	P  Table 3: Test results with different configurations.  NN stands for the conventional neural network.  NN+Tag Embed stands for the neural network  with tag embeddings.
true	P14-1028.pdf#64.2	PKU, F1	OOV  Table 3: Test results with different configurations.  NN stands for the conventional neural network.  NN+Tag Embed stands for the neural network  with tag embeddings.
true	P14-1028.pdf#93.5	PKU, F1	F  Table 3: Test results with different configurations.  NN stands for the conventional neural network.  NN+Tag Embed stands for the neural network  with tag embeddings.
true	P14-1028.pdf#93.4	PKU, F1	R  Table 3: Test results with different configurations.  NN stands for the conventional neural network.  NN+Tag Embed stands for the neural network  with tag embeddings.
true	P14-1028.pdf#93.7	PKU, F1	PKU P  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#93.5	PKU, F1	PKU F  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#64.2	PKU, F1	PKU OOV  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#94.4	PKU, F1	MSRA F  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#93.4	PKU, F1	PKU R  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#94.4	PKU, F1	PKU P  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#94.6	PKU, F1	MSRA P  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#94.2	PKU, F1	MSRA R  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#94.6	PKU, F1	MSRA R  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#94.9	PKU, F1	MSRA F  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#95.2	PKU, F1	MSRA P  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#93.6	PKU, F1	PKU R  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#69.0	PKU, F1	PKU OOV  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#61.4	PKU, F1	MSRA OOV  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#64.8	PKU, F1	MSRA OOV  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#94.0	PKU, F1	PKU F  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#93.7	MSR, F1	P  Table 3: Test results with different configurations.  NN stands for the conventional neural network.  NN+Tag Embed stands for the neural network  with tag embeddings.
true	P14-1028.pdf#64.2	MSR, F1	OOV  Table 3: Test results with different configurations.  NN stands for the conventional neural network.  NN+Tag Embed stands for the neural network  with tag embeddings.
true	P14-1028.pdf#93.5	MSR, F1	F  Table 3: Test results with different configurations.  NN stands for the conventional neural network.  NN+Tag Embed stands for the neural network  with tag embeddings.
true	P14-1028.pdf#93.4	MSR, F1	R  Table 3: Test results with different configurations.  NN stands for the conventional neural network.  NN+Tag Embed stands for the neural network  with tag embeddings.
true	P14-1028.pdf#93.7	MSR, F1	PKU P  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#93.5	MSR, F1	PKU F  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#64.2	MSR, F1	PKU OOV  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#94.4	MSR, F1	MSRA F  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#93.4	MSR, F1	PKU R  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#94.4	MSR, F1	PKU P  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#94.6	MSR, F1	MSRA P  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#94.2	MSR, F1	MSRA R  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#94.6	MSR, F1	MSRA R  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#94.9	MSR, F1	MSRA F  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#95.2	MSR, F1	MSRA P  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#93.6	MSR, F1	PKU R  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#69.0	MSR, F1	PKU OOV  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#61.4	MSR, F1	MSRA OOV  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#64.8	MSR, F1	MSRA OOV  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#94.0	MSR, F1	PKU F  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#93.7	Chinese Treebank 6, F1	P  Table 3: Test results with different configurations.  NN stands for the conventional neural network.  NN+Tag Embed stands for the neural network  with tag embeddings.
true	P14-1028.pdf#64.2	Chinese Treebank 6, F1	OOV  Table 3: Test results with different configurations.  NN stands for the conventional neural network.  NN+Tag Embed stands for the neural network  with tag embeddings.
true	P14-1028.pdf#93.5	Chinese Treebank 6, F1	F  Table 3: Test results with different configurations.  NN stands for the conventional neural network.  NN+Tag Embed stands for the neural network  with tag embeddings.
true	P14-1028.pdf#93.4	Chinese Treebank 6, F1	R  Table 3: Test results with different configurations.  NN stands for the conventional neural network.  NN+Tag Embed stands for the neural network  with tag embeddings.
true	P14-1028.pdf#93.7	Chinese Treebank 6, F1	PKU P  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#93.5	Chinese Treebank 6, F1	PKU F  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#64.2	Chinese Treebank 6, F1	PKU OOV  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#94.4	Chinese Treebank 6, F1	MSRA F  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#93.4	Chinese Treebank 6, F1	PKU R  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#94.4	Chinese Treebank 6, F1	PKU P  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#94.6	Chinese Treebank 6, F1	MSRA P  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#94.2	Chinese Treebank 6, F1	MSRA R  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#94.6	Chinese Treebank 6, F1	MSRA R  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#94.9	Chinese Treebank 6, F1	MSRA F  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#95.2	Chinese Treebank 6, F1	MSRA P  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#93.6	Chinese Treebank 6, F1	PKU R  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#69.0	Chinese Treebank 6, F1	PKU OOV  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#61.4	Chinese Treebank 6, F1	MSRA OOV  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#64.8	Chinese Treebank 6, F1	MSRA OOV  Table 5: Comparison with previous neural network models
true	P14-1028.pdf#94.0	Chinese Treebank 6, F1	PKU F  Table 5: Comparison with previous neural network models
true	P13-1147.pdf#2005	New York Times Corpus, P@10%	4 , 327 positive and 39 , 120 negative instances . sentence , al . , 2011 ) .  Table 1: Overview of the ACE 2005 data.
true	P13-1147.pdf#2004data.	New York Times Corpus, P@10%	model against the state of the art we use the ACE sentence , al . , 2011 ) . docs We  Table 1: Overview of the ACE 2005 data.
true	P13-1147.pdf#48.0	New York Times Corpus, P@10%	Table 4 shows the results . F1  Table 4: Brown clusters in tree kernels (cf.
true	P13-1147.pdf#48.5	New York Times Corpus, P@10%	bc SYS  Table 6: F1 per coarse relation type
true	P13-1147.pdf#42.3	New York Times Corpus, P@10%	cts SYS  Table 6: F1 per coarse relation type
true	P13-1147.pdf#74.2	New York Times Corpus, P@10%	bc SYS  Table 6: F1 per coarse relation type
true	P13-1147.pdf#48.0	New York Times Corpus, P@10%	wl SYS  Table 6: F1 per coarse relation type
true	P13-1147.pdf#26.9	New York Times Corpus, P@10%	wl SYS  Table 6: F1 per coarse relation type
true	P13-1147.pdf#37.8	New York Times Corpus, P@10%	wl SYS  Table 6: F1 per coarse relation type
true	P13-1147.pdf#33.0	New York Times Corpus, P@10%	bc SYS  Table 6: F1 per coarse relation type
true	P13-1147.pdf#62.9	New York Times Corpus, P@10%	bc SYS  Table 6: F1 per coarse relation type
true	P13-1147.pdf#28.1	New York Times Corpus, P@10%	wl SYS  Table 6: F1 per coarse relation type
true	P13-1147.pdf#43.5	New York Times Corpus, P@10%	cts SYS  Table 6: F1 per coarse relation type
true	P13-1147.pdf#43.1	New York Times Corpus, P@10%	bc SYS  Table 6: F1 per coarse relation type
true	P13-1147.pdf#59.3	New York Times Corpus, P@10%	cts BL  Table 6: F1 per coarse relation type
true	P13-1147.pdf#40.3	New York Times Corpus, P@10%	wl SYS  Table 6: F1 per coarse relation type
true	P13-1147.pdf#66.3	New York Times Corpus, P@10%	cts BL  Table 6: F1 per coarse relation type
true	P13-1147.pdf#43.6	New York Times Corpus, P@10%	cts SYS  Table 6: F1 per coarse relation type
true	P13-1147.pdf#37.9	New York Times Corpus, P@10%	bc SYS  Table 6: F1 per coarse relation type
true	P13-1147.pdf#42.0	New York Times Corpus, P@10%	wl SYS  Table 6: F1 per coarse relation type
true	P13-1147.pdf#37.6	New York Times Corpus, P@10%	bc SYS  Table 6: F1 per coarse relation type
true	P13-1147.pdf#18.6	New York Times Corpus, P@10%	cts SYS  Table 6: F1 per coarse relation type
true	P13-1147.pdf#28.7	New York Times Corpus, P@10%	cts SYS  Table 6: F1 per coarse relation type
true	P13-1147.pdf#36.3	New York Times Corpus, P@10%	wl SYS  Table 6: F1 per coarse relation type
true	P13-1147.pdf#36.3	New York Times Corpus, P@10%	wl SYS  Table 5.
true	P13-1147.pdf#33.0	New York Times Corpus, P@10%	bc SYS  Table 5.
true	P13-1147.pdf#28.7	New York Times Corpus, P@10%	cts SYS  Table 5.
true	P13-1147.pdf#42.3	New York Times Corpus, P@10%	cts SYS  Table 5.
true	P13-1147.pdf#48.5	New York Times Corpus, P@10%	bc SYS  Table 5.
true	P13-1147.pdf#37.6	New York Times Corpus, P@10%	bc SYS  Table 5.
true	P13-1147.pdf#43.5	New York Times Corpus, P@10%	cts SYS  Table 5.
true	P13-1147.pdf#37.9	New York Times Corpus, P@10%	bc SYS  Table 5.
true	P13-1147.pdf#66.3	New York Times Corpus, P@10%	cts BL  Table 5.
true	P13-1147.pdf#48.0	New York Times Corpus, P@10%	wl SYS  Table 5.
true	P13-1147.pdf#18.6	New York Times Corpus, P@10%	cts SYS  Table 5.
true	P13-1147.pdf#74.2	New York Times Corpus, P@10%	bc SYS  Table 5.
true	P13-1147.pdf#43.1	New York Times Corpus, P@10%	bc SYS  Table 5.
true	P13-1147.pdf#40.3	New York Times Corpus, P@10%	wl SYS  Table 5.
true	P13-1147.pdf#37.8	New York Times Corpus, P@10%	wl SYS  Table 5.
true	P13-1147.pdf#28.1	New York Times Corpus, P@10%	wl SYS  Table 5.
true	P13-1147.pdf#59.3	New York Times Corpus, P@10%	cts BL  Table 5.
true	P13-1147.pdf#42.0	New York Times Corpus, P@10%	wl SYS  Table 5.
true	P13-1147.pdf#26.9	New York Times Corpus, P@10%	wl SYS  Table 5.
true	P13-1147.pdf#62.9	New York Times Corpus, P@10%	bc SYS  Table 5.
true	P13-1147.pdf#43.6	New York Times Corpus, P@10%	cts SYS  Table 5.
true	D12-1042.pdf#64.8	New York Times Corpus, P@10%	P  Table 2: Results at the highest F1 point in the preci- sion/recall curve on the dataset that contains groups with  at least 10 mentions.
true	D12-1042.pdf#42.6	New York Times Corpus, P@10%	F1  Table 2: Results at the highest F1 point in the preci- sion/recall curve on the dataset that contains groups with  at least 10 mentions.
true	D12-1042.pdf#36.8	New York Times Corpus, P@10%	R  Table 2: Results at the highest F1 point in the preci- sion/recall curve on the dataset that contains groups with  at least 10 mentions.
true	P10-1110.pdf#92.1	Penn Treebank, LAS	O ( n ) ‡ word −  Table 2: Perceptron iterations with DP (left) and  non-DP (right). Early updates happen much more  often with DP due to equivalent state merging,  which leads to faster training (time in minutes).
true	P10-1110.pdf#0.04	Penn Treebank, LAS	O ( n ) ‡ time −  Table 2: Perceptron iterations with DP (left) and  non-DP (right). Early updates happen much more  often with DP due to equivalent state merging,  which leads to faster training (time in minutes).
true	P10-1110.pdf#0.04	Penn Treebank, LAS	O ( n ) ‡ time −  Table 3: Final test results on English (PTB). Our  parser (in pure Python) has the highest accuracy  among dependency parsers trained on the Tree- bank, and is also much faster than major parsers.   † converted from constituency trees. C=C/C++,  Py=Python, Ja=Java. Time is in seconds per sen- tence. Search spaces:  ‡ linear; others exponential.
true	P10-1110.pdf#92.1	Penn Treebank, LAS	O ( n ) ‡ word −  Table 3: Final test results on English (PTB). Our  parser (in pure Python) has the highest accuracy  among dependency parsers trained on the Tree- bank, and is also much faster than major parsers.   † converted from constituency trees. C=C/C++,  Py=Python, Ja=Java. Time is in seconds per sen- tence. Search spaces:  ‡ linear; others exponential.
true	P10-1110.pdf#92.1	Penn Treebank, POS	O ( n ) ‡ word −  Table 2: Perceptron iterations with DP (left) and  non-DP (right). Early updates happen much more  often with DP due to equivalent state merging,  which leads to faster training (time in minutes).
true	P10-1110.pdf#0.04	Penn Treebank, POS	O ( n ) ‡ time −  Table 2: Perceptron iterations with DP (left) and  non-DP (right). Early updates happen much more  often with DP due to equivalent state merging,  which leads to faster training (time in minutes).
true	P10-1110.pdf#0.04	Penn Treebank, POS	O ( n ) ‡ time −  Table 3: Final test results on English (PTB). Our  parser (in pure Python) has the highest accuracy  among dependency parsers trained on the Tree- bank, and is also much faster than major parsers.   † converted from constituency trees. C=C/C++,  Py=Python, Ja=Java. Time is in seconds per sen- tence. Search spaces:  ‡ linear; others exponential.
true	P10-1110.pdf#92.1	Penn Treebank, POS	O ( n ) ‡ word −  Table 3: Final test results on English (PTB). Our  parser (in pure Python) has the highest accuracy  among dependency parsers trained on the Tree- bank, and is also much faster than major parsers.   † converted from constituency trees. C=C/C++,  Py=Python, Ja=Java. Time is in seconds per sen- tence. Search spaces:  ‡ linear; others exponential.
true	P10-1110.pdf#92.1	Penn Treebank, F1	O ( n ) ‡ word −  Table 2: Perceptron iterations with DP (left) and  non-DP (right). Early updates happen much more  often with DP due to equivalent state merging,  which leads to faster training (time in minutes).
true	P10-1110.pdf#0.04	Penn Treebank, F1	O ( n ) ‡ time −  Table 2: Perceptron iterations with DP (left) and  non-DP (right). Early updates happen much more  often with DP due to equivalent state merging,  which leads to faster training (time in minutes).
true	P10-1110.pdf#0.04	Penn Treebank, F1	O ( n ) ‡ time −  Table 3: Final test results on English (PTB). Our  parser (in pure Python) has the highest accuracy  among dependency parsers trained on the Tree- bank, and is also much faster than major parsers.   † converted from constituency trees. C=C/C++,  Py=Python, Ja=Java. Time is in seconds per sen- tence. Search spaces:  ‡ linear; others exponential.
true	P10-1110.pdf#92.1	Penn Treebank, F1	O ( n ) ‡ word −  Table 3: Final test results on English (PTB). Our  parser (in pure Python) has the highest accuracy  among dependency parsers trained on the Tree- bank, and is also much faster than major parsers.   † converted from constituency trees. C=C/C++,  Py=Python, Ja=Java. Time is in seconds per sen- tence. Search spaces:  ‡ linear; others exponential.
true	P10-1110.pdf#92.1	Penn Treebank, UAS	O ( n ) ‡ word −  Table 2: Perceptron iterations with DP (left) and  non-DP (right). Early updates happen much more  often with DP due to equivalent state merging,  which leads to faster training (time in minutes).
true	P10-1110.pdf#0.04	Penn Treebank, UAS	O ( n ) ‡ time −  Table 2: Perceptron iterations with DP (left) and  non-DP (right). Early updates happen much more  often with DP due to equivalent state merging,  which leads to faster training (time in minutes).
true	P10-1110.pdf#0.04	Penn Treebank, UAS	O ( n ) ‡ time −  Table 3: Final test results on English (PTB). Our  parser (in pure Python) has the highest accuracy  among dependency parsers trained on the Tree- bank, and is also much faster than major parsers.   † converted from constituency trees. C=C/C++,  Py=Python, Ja=Java. Time is in seconds per sen- tence. Search spaces:  ‡ linear; others exponential.
true	P10-1110.pdf#92.1	Penn Treebank, UAS	O ( n ) ‡ word −  Table 3: Final test results on English (PTB). Our  parser (in pure Python) has the highest accuracy  among dependency parsers trained on the Tree- bank, and is also much faster than major parsers.   † converted from constituency trees. C=C/C++,  Py=Python, Ja=Java. Time is in seconds per sen- tence. Search spaces:  ‡ linear; others exponential.
true	D14-1110.pdf#68.9	SemEval 2013, F1	ρ × 100  Table 3: Spearman's ρ on the SCWS dataset. Our  Model-S uses one representation per word to com- pute similarities, while Our Model-M uses one  representation per sense to compute similarities.  AvgSim calculates the similarity with each sense  contributing equally, while AvgSimC weighs the  sense according to the probability of the word  choosing that sense in context c.
true	D14-1110.pdf#60.6	SemEval 2013, F1	Finance Recall  Table 4: Performance on the Sports and Finance  sections of the dataset from (
true	D14-1110.pdf#57.3	SemEval 2013, F1	Sports Recall  Table 4: Performance on the Sports and Finance  sections of the dataset from (
true	D14-1110.pdf#85.3	SemEval 2013, F1	Nouns only F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#83.2	SemEval 2013, F1	All words F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#82.5	SemEval 2013, F1	All words F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#84.1	SemEval 2013, F1	Nouns only F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#85.5	SemEval 2013, F1	Nouns only F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#82.6	SemEval 2013, F1	All words F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#81.7	SemEval 2013, F1	All words F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#−0.10	SemEval 2013, F1	Table 6: Evaluation results on the coarse-grained  all-words WSD when the similarity threshold δ  ranges from −0.1 to 0.3.
true	D14-1110.pdf#−0.05	SemEval 2013, F1	Parameter  Table 6: Evaluation results on the coarse-grained  all-words WSD when the similarity threshold δ  ranges from −0.1 to 0.3.
true	D14-1110.pdf#81.6	SemEval 2013, F1	Nouns only  Table 6: Evaluation results on the coarse-grained  all-words WSD when the similarity threshold δ  ranges from −0.1 to 0.3.
true	D14-1110.pdf#75.8	SemEval 2013, F1	All words  Table 6: Evaluation results on the coarse-grained  all-words WSD when the similarity threshold δ  ranges from −0.1 to 0.3.
true	D14-1110.pdf#75.8	SemEval 2013, F1	All words  Table 7: Evaluation results on the coarse-grained  all-words WSD when the score margin threshold  ε ranges from 0.0 to 0.3.
true	D14-1110.pdf#81.6	SemEval 2013, F1	Nouns only  Table 7: Evaluation results on the coarse-grained  all-words WSD when the score margin threshold  ε ranges from 0.0 to 0.3.
true	D14-1110.pdf#68.9	Senseval 2, F1	ρ × 100  Table 3: Spearman's ρ on the SCWS dataset. Our  Model-S uses one representation per word to com- pute similarities, while Our Model-M uses one  representation per sense to compute similarities.  AvgSim calculates the similarity with each sense  contributing equally, while AvgSimC weighs the  sense according to the probability of the word  choosing that sense in context c.
true	D14-1110.pdf#60.6	Senseval 2, F1	Finance Recall  Table 4: Performance on the Sports and Finance  sections of the dataset from (
true	D14-1110.pdf#57.3	Senseval 2, F1	Sports Recall  Table 4: Performance on the Sports and Finance  sections of the dataset from (
true	D14-1110.pdf#85.3	Senseval 2, F1	Nouns only F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#83.2	Senseval 2, F1	All words F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#82.5	Senseval 2, F1	All words F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#84.1	Senseval 2, F1	Nouns only F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#85.5	Senseval 2, F1	Nouns only F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#82.6	Senseval 2, F1	All words F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#81.7	Senseval 2, F1	All words F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#−0.10	Senseval 2, F1	Table 6: Evaluation results on the coarse-grained  all-words WSD when the similarity threshold δ  ranges from −0.1 to 0.3.
true	D14-1110.pdf#−0.05	Senseval 2, F1	Parameter  Table 6: Evaluation results on the coarse-grained  all-words WSD when the similarity threshold δ  ranges from −0.1 to 0.3.
true	D14-1110.pdf#81.6	Senseval 2, F1	Nouns only  Table 6: Evaluation results on the coarse-grained  all-words WSD when the similarity threshold δ  ranges from −0.1 to 0.3.
true	D14-1110.pdf#75.8	Senseval 2, F1	All words  Table 6: Evaluation results on the coarse-grained  all-words WSD when the similarity threshold δ  ranges from −0.1 to 0.3.
true	D14-1110.pdf#75.8	Senseval 2, F1	All words  Table 7: Evaluation results on the coarse-grained  all-words WSD when the score margin threshold  ε ranges from 0.0 to 0.3.
true	D14-1110.pdf#81.6	Senseval 2, F1	Nouns only  Table 7: Evaluation results on the coarse-grained  all-words WSD when the score margin threshold  ε ranges from 0.0 to 0.3.
true	D14-1110.pdf#68.9	SemEval 2015, F1	ρ × 100  Table 3: Spearman's ρ on the SCWS dataset. Our  Model-S uses one representation per word to com- pute similarities, while Our Model-M uses one  representation per sense to compute similarities.  AvgSim calculates the similarity with each sense  contributing equally, while AvgSimC weighs the  sense according to the probability of the word  choosing that sense in context c.
true	D14-1110.pdf#60.6	SemEval 2015, F1	Finance Recall  Table 4: Performance on the Sports and Finance  sections of the dataset from (
true	D14-1110.pdf#57.3	SemEval 2015, F1	Sports Recall  Table 4: Performance on the Sports and Finance  sections of the dataset from (
true	D14-1110.pdf#85.3	SemEval 2015, F1	Nouns only F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#83.2	SemEval 2015, F1	All words F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#82.5	SemEval 2015, F1	All words F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#84.1	SemEval 2015, F1	Nouns only F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#85.5	SemEval 2015, F1	Nouns only F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#82.6	SemEval 2015, F1	All words F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#81.7	SemEval 2015, F1	All words F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#−0.10	SemEval 2015, F1	Table 6: Evaluation results on the coarse-grained  all-words WSD when the similarity threshold δ  ranges from −0.1 to 0.3.
true	D14-1110.pdf#−0.05	SemEval 2015, F1	Parameter  Table 6: Evaluation results on the coarse-grained  all-words WSD when the similarity threshold δ  ranges from −0.1 to 0.3.
true	D14-1110.pdf#81.6	SemEval 2015, F1	Nouns only  Table 6: Evaluation results on the coarse-grained  all-words WSD when the similarity threshold δ  ranges from −0.1 to 0.3.
true	D14-1110.pdf#75.8	SemEval 2015, F1	All words  Table 6: Evaluation results on the coarse-grained  all-words WSD when the similarity threshold δ  ranges from −0.1 to 0.3.
true	D14-1110.pdf#75.8	SemEval 2015, F1	All words  Table 7: Evaluation results on the coarse-grained  all-words WSD when the score margin threshold  ε ranges from 0.0 to 0.3.
true	D14-1110.pdf#81.6	SemEval 2015, F1	Nouns only  Table 7: Evaluation results on the coarse-grained  all-words WSD when the score margin threshold  ε ranges from 0.0 to 0.3.
true	D14-1110.pdf#68.9	SemEval 2007, F1	ρ × 100  Table 3: Spearman's ρ on the SCWS dataset. Our  Model-S uses one representation per word to com- pute similarities, while Our Model-M uses one  representation per sense to compute similarities.  AvgSim calculates the similarity with each sense  contributing equally, while AvgSimC weighs the  sense according to the probability of the word  choosing that sense in context c.
true	D14-1110.pdf#60.6	SemEval 2007, F1	Finance Recall  Table 4: Performance on the Sports and Finance  sections of the dataset from (
true	D14-1110.pdf#57.3	SemEval 2007, F1	Sports Recall  Table 4: Performance on the Sports and Finance  sections of the dataset from (
true	D14-1110.pdf#85.3	SemEval 2007, F1	Nouns only F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#83.2	SemEval 2007, F1	All words F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#82.5	SemEval 2007, F1	All words F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#84.1	SemEval 2007, F1	Nouns only F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#85.5	SemEval 2007, F1	Nouns only F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#82.6	SemEval 2007, F1	All words F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#81.7	SemEval 2007, F1	All words F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#−0.10	SemEval 2007, F1	Table 6: Evaluation results on the coarse-grained  all-words WSD when the similarity threshold δ  ranges from −0.1 to 0.3.
true	D14-1110.pdf#−0.05	SemEval 2007, F1	Parameter  Table 6: Evaluation results on the coarse-grained  all-words WSD when the similarity threshold δ  ranges from −0.1 to 0.3.
true	D14-1110.pdf#81.6	SemEval 2007, F1	Nouns only  Table 6: Evaluation results on the coarse-grained  all-words WSD when the similarity threshold δ  ranges from −0.1 to 0.3.
true	D14-1110.pdf#75.8	SemEval 2007, F1	All words  Table 6: Evaluation results on the coarse-grained  all-words WSD when the similarity threshold δ  ranges from −0.1 to 0.3.
true	D14-1110.pdf#75.8	SemEval 2007, F1	All words  Table 7: Evaluation results on the coarse-grained  all-words WSD when the score margin threshold  ε ranges from 0.0 to 0.3.
true	D14-1110.pdf#81.6	SemEval 2007, F1	Nouns only  Table 7: Evaluation results on the coarse-grained  all-words WSD when the score margin threshold  ε ranges from 0.0 to 0.3.
true	D14-1110.pdf#68.9	Senseval 3, F1	ρ × 100  Table 3: Spearman's ρ on the SCWS dataset. Our  Model-S uses one representation per word to com- pute similarities, while Our Model-M uses one  representation per sense to compute similarities.  AvgSim calculates the similarity with each sense  contributing equally, while AvgSimC weighs the  sense according to the probability of the word  choosing that sense in context c.
true	D14-1110.pdf#60.6	Senseval 3, F1	Finance Recall  Table 4: Performance on the Sports and Finance  sections of the dataset from (
true	D14-1110.pdf#57.3	Senseval 3, F1	Sports Recall  Table 4: Performance on the Sports and Finance  sections of the dataset from (
true	D14-1110.pdf#85.3	Senseval 3, F1	Nouns only F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#83.2	Senseval 3, F1	All words F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#82.5	Senseval 3, F1	All words F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#84.1	Senseval 3, F1	Nouns only F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#85.5	Senseval 3, F1	Nouns only F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#82.6	Senseval 3, F1	All words F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#81.7	Senseval 3, F1	All words F 1  Table 5: Performance on Semeval-2007 coarse- grained all-words WSD. In the type column,  U, Semi and S stand for unsupervised, semi- supervised and supervised, respectively. The dif- ferences between the results in bold in each col- umn of the table are not statistically significant at  p < 0.05.
true	D14-1110.pdf#−0.10	Senseval 3, F1	Table 6: Evaluation results on the coarse-grained  all-words WSD when the similarity threshold δ  ranges from −0.1 to 0.3.
true	D14-1110.pdf#−0.05	Senseval 3, F1	Parameter  Table 6: Evaluation results on the coarse-grained  all-words WSD when the similarity threshold δ  ranges from −0.1 to 0.3.
true	D14-1110.pdf#81.6	Senseval 3, F1	Nouns only  Table 6: Evaluation results on the coarse-grained  all-words WSD when the similarity threshold δ  ranges from −0.1 to 0.3.
true	D14-1110.pdf#75.8	Senseval 3, F1	All words  Table 6: Evaluation results on the coarse-grained  all-words WSD when the similarity threshold δ  ranges from −0.1 to 0.3.
true	D14-1110.pdf#75.8	Senseval 3, F1	All words  Table 7: Evaluation results on the coarse-grained  all-words WSD when the score margin threshold  ε ranges from 0.0 to 0.3.
true	D14-1110.pdf#81.6	Senseval 3, F1	Nouns only  Table 7: Evaluation results on the coarse-grained  all-words WSD when the score margin threshold  ε ranges from 0.0 to 0.3.
true	P11-1016.pdf#68.2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	% )  Table 2. Evaluation of target extension methods.
true	P11-1016.pdf#66.2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Percentage  Table 4. Percentages of tweets having at least one relat- ed tweet according to various relation types.
true	P10-1151.pdf#45.8	Penn Treebank, UAS	German NPR  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#58.1	Penn Treebank, UAS	German NPP -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#90.82	Penn Treebank, UAS	Portuguese UAS  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#83.81	Penn Treebank, UAS	Danish LAS  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#88.52	Penn Treebank, UAS	Danish UAS -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#87.08	Penn Treebank, UAS	Portuguese LAS -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#60.7	Penn Treebank, UAS	Czech NPR  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#46.2	Penn Treebank, UAS	Portuguese NPR -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#86.50	Penn Treebank, UAS	German LAS  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#76.7	Penn Treebank, UAS	Czech NPP -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#85.70	Penn Treebank, UAS	Czech UAS -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#25.0	Penn Treebank, UAS	Danish NPR -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#79.80	Penn Treebank, UAS	Czech LAS -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#83.3	Penn Treebank, UAS	Portuguese NPP -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#66.7	Penn Treebank, UAS	Danish NPP  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#88.84	Penn Treebank, UAS	German UAS  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#45.8	benchmark Vietnamese dependency treebank VnDT, LAS	German NPR  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#58.1	benchmark Vietnamese dependency treebank VnDT, LAS	German NPP -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#90.82	benchmark Vietnamese dependency treebank VnDT, LAS	Portuguese UAS  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#83.81	benchmark Vietnamese dependency treebank VnDT, LAS	Danish LAS  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#88.52	benchmark Vietnamese dependency treebank VnDT, LAS	Danish UAS -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#87.08	benchmark Vietnamese dependency treebank VnDT, LAS	Portuguese LAS -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#60.7	benchmark Vietnamese dependency treebank VnDT, LAS	Czech NPR  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#46.2	benchmark Vietnamese dependency treebank VnDT, LAS	Portuguese NPR -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#86.50	benchmark Vietnamese dependency treebank VnDT, LAS	German LAS  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#76.7	benchmark Vietnamese dependency treebank VnDT, LAS	Czech NPP -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#85.70	benchmark Vietnamese dependency treebank VnDT, LAS	Czech UAS -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#25.0	benchmark Vietnamese dependency treebank VnDT, LAS	Danish NPR -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#79.80	benchmark Vietnamese dependency treebank VnDT, LAS	Czech LAS -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#83.3	benchmark Vietnamese dependency treebank VnDT, LAS	Portuguese NPP -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#66.7	benchmark Vietnamese dependency treebank VnDT, LAS	Danish NPP  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#88.84	benchmark Vietnamese dependency treebank VnDT, LAS	German UAS  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#45.8	benchmark Vietnamese dependency treebank VnDT, UAS	German NPR  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#58.1	benchmark Vietnamese dependency treebank VnDT, UAS	German NPP -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#90.82	benchmark Vietnamese dependency treebank VnDT, UAS	Portuguese UAS  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#83.81	benchmark Vietnamese dependency treebank VnDT, UAS	Danish LAS  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#88.52	benchmark Vietnamese dependency treebank VnDT, UAS	Danish UAS -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#87.08	benchmark Vietnamese dependency treebank VnDT, UAS	Portuguese LAS -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#60.7	benchmark Vietnamese dependency treebank VnDT, UAS	Czech NPR  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#46.2	benchmark Vietnamese dependency treebank VnDT, UAS	Portuguese NPR -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#86.50	benchmark Vietnamese dependency treebank VnDT, UAS	German LAS  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#76.7	benchmark Vietnamese dependency treebank VnDT, UAS	Czech NPP -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#85.70	benchmark Vietnamese dependency treebank VnDT, UAS	Czech UAS -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#25.0	benchmark Vietnamese dependency treebank VnDT, UAS	Danish NPR -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#79.80	benchmark Vietnamese dependency treebank VnDT, UAS	Czech LAS -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#83.3	benchmark Vietnamese dependency treebank VnDT, UAS	Portuguese NPP -  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#66.7	benchmark Vietnamese dependency treebank VnDT, UAS	Danish NPP  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P10-1151.pdf#88.84	benchmark Vietnamese dependency treebank VnDT, UAS	German UAS  Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)  pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;  NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
true	P15-1015.pdf#5.22x	benchmark Vietnamese dependency treebank VnDT, LAS	Speed 1x  Table 1: Comparison of our models using differ- ent margins m, with speeds measured relative to  the baseline. We train a model as accurate as the  baseline while tagging 3.4x tokens/sec, and in an- other model maintain > 97% accuracy while tag- ging 5.2x, and > 96% accuracy with a speedup of  10.3x.
true	P15-1015.pdf#10.36x	benchmark Vietnamese dependency treebank VnDT, LAS	Speed 1x  Table 1: Comparison of our models using differ- ent margins m, with speeds measured relative to  the baseline. We train a model as accurate as the  baseline while tagging 3.4x tokens/sec, and in an- other model maintain > 97% accuracy while tag- ging 5.2x, and > 96% accuracy with a speedup of  10.3x.
true	P15-1015.pdf#97.21	benchmark Vietnamese dependency treebank VnDT, LAS	Tok . 46  Table 1: Comparison of our models using differ- ent margins m, with speeds measured relative to  the baseline. We train a model as accurate as the  baseline while tagging 3.4x tokens/sec, and in an- other model maintain > 97% accuracy while tag- ging 5.2x, and > 96% accuracy with a speedup of  10.3x.
true	P15-1015.pdf#5.22x	benchmark Vietnamese dependency treebank VnDT, UAS	Speed 1x  Table 1: Comparison of our models using differ- ent margins m, with speeds measured relative to  the baseline. We train a model as accurate as the  baseline while tagging 3.4x tokens/sec, and in an- other model maintain > 97% accuracy while tag- ging 5.2x, and > 96% accuracy with a speedup of  10.3x.
true	P15-1015.pdf#10.36x	benchmark Vietnamese dependency treebank VnDT, UAS	Speed 1x  Table 1: Comparison of our models using differ- ent margins m, with speeds measured relative to  the baseline. We train a model as accurate as the  baseline while tagging 3.4x tokens/sec, and in an- other model maintain > 97% accuracy while tag- ging 5.2x, and > 96% accuracy with a speedup of  10.3x.
true	P15-1015.pdf#97.21	benchmark Vietnamese dependency treebank VnDT, UAS	Tok . 46  Table 1: Comparison of our models using differ- ent margins m, with speeds measured relative to  the baseline. We train a model as accurate as the  baseline while tagging 3.4x tokens/sec, and in an- other model maintain > 97% accuracy while tag- ging 5.2x, and > 96% accuracy with a speedup of  10.3x.
true	P15-1015.pdf#5.22x	Penn Treebank, UAS	Speed 1x  Table 1: Comparison of our models using differ- ent margins m, with speeds measured relative to  the baseline. We train a model as accurate as the  baseline while tagging 3.4x tokens/sec, and in an- other model maintain > 97% accuracy while tag- ging 5.2x, and > 96% accuracy with a speedup of  10.3x.
true	P15-1015.pdf#10.36x	Penn Treebank, UAS	Speed 1x  Table 1: Comparison of our models using differ- ent margins m, with speeds measured relative to  the baseline. We train a model as accurate as the  baseline while tagging 3.4x tokens/sec, and in an- other model maintain > 97% accuracy while tag- ging 5.2x, and > 96% accuracy with a speedup of  10.3x.
true	P15-1015.pdf#97.21	Penn Treebank, UAS	Tok . 46  Table 1: Comparison of our models using differ- ent margins m, with speeds measured relative to  the baseline. We train a model as accurate as the  baseline while tagging 3.4x tokens/sec, and in an- other model maintain > 97% accuracy while tag- ging 5.2x, and > 96% accuracy with a speedup of  10.3x.
true	D15-1007.pdf#10	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Negative : Military Disputes N N  Table 5: Top: The number of NATO/EU nation  pairs with automatic sentiment labels. Bottom:  The number of pairs with military disputes (MID  dataset) and automatic sentiment labels.
true	D15-1007.pdf#22	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	N  Table 5: Top: The number of NATO/EU nation  pairs with automatic sentiment labels. Bottom:  The number of pairs with military disputes (MID  dataset) and automatic sentiment labels.
true	D15-1279.pdf#51.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Le and Mikolov ( 2014 ) 5 - class accuracy -  Table 2: Accuracy of sentiment prediction (in percentage). For 2-class prediction, " †" remarks indicate  that the network is transferred directly from that of 5-class.
true	D15-1279.pdf#88.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Blunsom et al . ( 2014 ) 2 - class accuracy  Table 2: Accuracy of sentiment prediction (in percentage). For 2-class prediction, " †" remarks indicate  that the network is transferred directly from that of 5-class.
true	D15-1279.pdf#20	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Model  Table 4: Accuracies of different pooling methods,  averaged over 5 random initializations. We chose  sensible hyperparameters manually in advance to  make a fair comparison. This leads to performance  degradation (1-2%) vis-a-vis Table 2.
true	D15-1279.pdf#10	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Model  Table 4: Accuracies of different pooling methods,  averaged over 5 random initializations. We chose  sensible hyperparameters manually in advance to  make a fair comparison. This leads to performance  degradation (1-2%) vis-a-vis Table 2.
true	D15-1279.pdf#30	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Model  Table 4: Accuracies of different pooling methods,  averaged over 5 random initializations. We chose  sensible hyperparameters manually in advance to  make a fair comparison. This leads to performance  degradation (1-2%) vis-a-vis Table 2.
true	D15-1279.pdf#50	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Model  Table 4: Accuracies of different pooling methods,  averaged over 5 random initializations. We chose  sensible hyperparameters manually in advance to  make a fair comparison. This leads to performance  degradation (1-2%) vis-a-vis Table 2.
true	D15-1279.pdf#40	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Model  Table 4: Accuracies of different pooling methods,  averaged over 5 random initializations. We chose  sensible hyperparameters manually in advance to  make a fair comparison. This leads to performance  degradation (1-2%) vis-a-vis Table 2.
true	D15-1304.pdf#1-(positivescore+negativescore).	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	WordNet , respectively , while the lower table represents a Figure 3 : The linking between SentiWordNet and AraMorph  Table 2: Examples of SLSA entries; Obj. = Objective. All  scores are rounded for readability.
true	D15-1304.pdf#1-(positivescore+negativescore).	SUBJ, Accuracy	WordNet , respectively , while the lower table represents a Figure 3 : The linking between SentiWordNet and AraMorph  Table 2: Examples of SLSA entries; Obj. = Objective. All  scores are rounded for readability.
true	D15-1174.pdf#0.01)	WN18RR, H@1	KB and text Model MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.25)	WN18RR, H@1	KB and text Model MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.25)	WN18RR, H@1	KB and text Overall MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.01)	WN18RR, H@1	KB and text Model MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.01)	FB15K-237, H@10	KB and text Model MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.25)	FB15K-237, H@10	KB and text Model MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.25)	FB15K-237, H@10	KB and text Overall MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.01)	FB15K-237, H@10	KB and text Model MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.01)	FB15K-237, H@1	KB and text Model MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.25)	FB15K-237, H@1	KB and text Model MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.25)	FB15K-237, H@1	KB and text Overall MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.01)	FB15K-237, H@1	KB and text Model MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.01)	WN18RR, MRR	KB and text Model MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.25)	WN18RR, MRR	KB and text Model MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.25)	WN18RR, MRR	KB and text Overall MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.01)	WN18RR, MRR	KB and text Model MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.01)	WN18RR, H@10	KB and text Model MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.25)	WN18RR, H@10	KB and text Model MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.25)	WN18RR, H@10	KB and text Overall MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.01)	WN18RR, H@10	KB and text Model MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.01)	FB15K-237, MRR	KB and text Model MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.25)	FB15K-237, MRR	KB and text Model MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.25)	FB15K-237, MRR	KB and text Overall MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	D15-1174.pdf#0.01)	FB15K-237, MRR	KB and text Model MRR  Table 3: Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro- posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the
true	P15-1069.pdf#27.2*	DBpedia, Error	18 , 7 METEOR  Table 2: Automatic translation evaluation on the  evaluation dataset of the ICD ontology (Size =  amount of selected sentences from the generic par- allel corpus. bold results = best performance; *sta- tistically significant compared to baseline)
true	P15-1069.pdf#8.9*	DBpedia, Error	18 , 7 BLEU - 4  Table 2: Automatic translation evaluation on the  evaluation dataset of the ICD ontology (Size =  amount of selected sentences from the generic par- allel corpus. bold results = best performance; *sta- tistically significant compared to baseline)
true	P15-1069.pdf#20.1	DBpedia, Error	18 , 7 BLEU - 2  Table 2: Automatic translation evaluation on the  evaluation dataset of the ICD ontology (Size =  amount of selected sentences from the generic par- allel corpus. bold results = best performance; *sta- tistically significant compared to baseline)
true	D14-1126.pdf#0.863	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 6 : Top sentiment words for aspect " room " with different sentiment labels Top sentiment words old , dirty , worn , older , dark , stained , broken , dated , outdated , bad small , tiny , little , noisy , single , double , uncomfortable , smaller , larger , narrow large , double , big , mini , hard , main , huge , twin , single , jacuzzi nice , comfortable , modern , clean , new , good , great , flat , big , comfy large , huge , great , beautiful , big , lovely , separate , spacious , wonderful , excellent ρ hotel  Table 6: Top sentiment words for aspect "room" with different sentiment labels
true	D14-1126.pdf#0.814	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 6 : Top sentiment words for aspect " room " with different sentiment labels Top sentiment words old , dirty , worn , older , dark , stained , broken , dated , outdated , bad small , tiny , little , noisy , single , double , uncomfortable , smaller , larger , narrow large , double , big , mini , hard , main , huge , twin , single , jacuzzi nice , comfortable , modern , clean , new , good , great , flat , big , comfy large , huge , great , beautiful , big , lovely , separate , spacious , wonderful , excellent P@10  Table 6: Top sentiment words for aspect "room" with different sentiment labels
true	D14-1126.pdf#0.384	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 6 : Top sentiment words for aspect " room " with different sentiment labels Sentiment label old , dirty , worn , older , dark , stained , broken , dated , outdated , bad small , tiny , little , noisy , single , double , uncomfortable , smaller , larger , narrow large , double , big , mini , hard , main , huge , twin , single , jacuzzi nice , comfortable , modern , clean , new , good , great , flat , big , comfy large , huge , great , beautiful , big , lovely , separate , spacious , wonderful , excellent RMSE  Table 6: Top sentiment words for aspect "room" with different sentiment labels
true	D14-1126.pdf#0.384	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 6 : Top sentiment words for aspect " room " with different sentiment labels RMSE  Table 4: Experimental results except LRR
true	D14-1126.pdf#0.863	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 6 : Top sentiment words for aspect " room " with different sentiment labels ρ hotel  Table 4: Experimental results except LRR
true	D14-1126.pdf#0.814	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 6 : Top sentiment words for aspect " room " with different sentiment labels P@10  Table 4: Experimental results except LRR
true	D14-1126.pdf#0.829	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 4 : Experimental results except LRR P@10  Table 5: Experimental comparison with LRR
true	D14-1126.pdf#0.373	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 4 : Experimental results except LRR RMSE  Table 5: Experimental comparison with LRR
true	D14-1126.pdf#0.849	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 4 : Experimental results except LRR ρ hotel  Table 5: Experimental comparison with LRR
true	D14-1126.pdf#0.606	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	[ 1 , 2 )  Table 7: RMSE on hotels with different overall  rating ranges
true	D14-1126.pdf#0.394	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	[ 3 , 4 )  Table 7: RMSE on hotels with different overall  rating ranges
true	D14-1126.pdf#0.494	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	[ 2 , 3 )  Table 7: RMSE on hotels with different overall  rating ranges
true	D14-1126.pdf#0.320	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	[ 4 - 5 ]  Table 7: RMSE on hotels with different overall  rating ranges
true	D12-1001.pdf#0.1andadoubledaggerindicates	Penn Treebank, UAS	catenation of non - target - language treebanks ( the BANKS setting ) . Values reported are UAS for sentences of all lengths Past work TMU12 * * X - lingual ∆ No clusters 2 . 0 * * 1 . 8 * * 3 . 5 * * 2 . 7 * * 4 . 2 * * 1 . 5 * * 4 . 2 * * 1 . 5 * * 2 . 7 * * Daggers indicate statistical  Table 1: Evaluation of features derived from AUTOMATIC and MANUAL bilingual lexicons when trained on a con- catenation of non-target-language treebanks (the BANKS setting). Values reported are UAS for sentences of all lengths  in the standard CoNLL test sets, with punctuation removed from training and test sets. Daggers indicate statistical  significance computed using bootstrap resampling; a single dagger indicates p < 0.1 and a double dagger indicates  p < 0.05. We also include the baseline results of McDonald et al. (2011) and Täckström et al.
true	D12-1001.pdf#0.05.WealsoincludethebaselineresultsofMcDonaldetal.(2011)andTäckström	Penn Treebank, UAS	catenation of non - target - language treebanks ( the BANKS setting ) . Values reported are UAS for sentences of all lengths This work MPH11 * ∆ Multi - dir 36 . 7 * * - 0 . 1 * 59 . 5 * * 60 . 2 * * 64 . 6 * * 52 . 8 * * 66 . 8 * * 55 . 4 * * 55 . 6 * * in the standard CoNLL test sets , with punctuation removed from training and test sets .  Table 1: Evaluation of features derived from AUTOMATIC and MANUAL bilingual lexicons when trained on a con- catenation of non-target-language treebanks (the BANKS setting). Values reported are UAS for sentences of all lengths  in the standard CoNLL test sets, with punctuation removed from training and test sets. Daggers indicate statistical  significance computed using bootstrap resampling; a single dagger indicates p < 0.1 and a double dagger indicates  p < 0.05. We also include the baseline results of McDonald et al. (2011) and Täckström et al.
true	D14-1164.pdf#37.7	New York Times Corpus, P@10%	Learning Criterion Sample JS F 1 -  Table 1: A summary of results on the end-to-end KBP 2013 evaluation for various experiments. The  first column denotes the algorithm used: either traditional distant supervision (Mintz++), MIML-RE, or  a supervised classifier. In the case of MIML-RE, the model may be initialized either using Mintz++, or  the corresponding supervised classifier (the "Not Used" column is initialized with the "All" supervised  classifier). One of five active learning scenarios are evaluated: no annotated examples provided, the three  active learning criteria, and all available examples used. The entry in blue denotes the basic MIML-RE  model; entries in gray perform worse than this model. The bold items denote the best performance  among selection criteria.
true	D14-1164.pdf#35.5	New York Times Corpus, P@10%	Learning Criterion Uniform P -  Table 1: A summary of results on the end-to-end KBP 2013 evaluation for various experiments. The  first column denotes the algorithm used: either traditional distant supervision (Mintz++), MIML-RE, or  a supervised classifier. In the case of MIML-RE, the model may be initialized either using Mintz++, or  the corresponding supervised classifier (the "Not Used" column is initialized with the "All" supervised  classifier). One of five active learning scenarios are evaluated: no annotated examples provided, the three  active learning criteria, and all available examples used. The entry in blue denotes the basic MIML-RE  model; entries in gray perform worse than this model. The bold items denote the best performance  among selection criteria.
true	D14-1164.pdf#37.1	New York Times Corpus, P@10%	Learning Criterion All Available R -  Table 1: A summary of results on the end-to-end KBP 2013 evaluation for various experiments. The  first column denotes the algorithm used: either traditional distant supervision (Mintz++), MIML-RE, or  a supervised classifier. In the case of MIML-RE, the model may be initialized either using Mintz++, or  the corresponding supervised classifier (the "Not Used" column is initialized with the "All" supervised  classifier). One of five active learning scenarios are evaluated: no annotated examples provided, the three  active learning criteria, and all available examples used. The entry in blue denotes the basic MIML-RE  model; entries in gray perform worse than this model. The bold items denote the best performance  among selection criteria.
true	D14-1164.pdf#34.2	New York Times Corpus, P@10%	Learning Criterion Sample JS F 1 -  Table 1: A summary of results on the end-to-end KBP 2013 evaluation for various experiments. The  first column denotes the algorithm used: either traditional distant supervision (Mintz++), MIML-RE, or  a supervised classifier. In the case of MIML-RE, the model may be initialized either using Mintz++, or  the corresponding supervised classifier (the "Not Used" column is initialized with the "All" supervised  classifier). One of five active learning scenarios are evaluated: no annotated examples provided, the three  active learning criteria, and all available examples used. The entry in blue denotes the basic MIML-RE  model; entries in gray perform worse than this model. The bold items denote the best performance  among selection criteria.
true	D14-1164.pdf#46.2	New York Times Corpus, P@10%	Learning Criterion High JS P -  Table 1: A summary of results on the end-to-end KBP 2013 evaluation for various experiments. The  first column denotes the algorithm used: either traditional distant supervision (Mintz++), MIML-RE, or  a supervised classifier. In the case of MIML-RE, the model may be initialized either using Mintz++, or  the corresponding supervised classifier (the "Not Used" column is initialized with the "All" supervised  classifier). One of five active learning scenarios are evaluated: no annotated examples provided, the three  active learning criteria, and all available examples used. The entry in blue denotes the basic MIML-RE  model; entries in gray perform worse than this model. The bold items denote the best performance  among selection criteria.
true	D14-1164.pdf#35.0	New York Times Corpus, P@10%	Learning Criterion Sample JS R -  Table 1: A summary of results on the end-to-end KBP 2013 evaluation for various experiments. The  first column denotes the algorithm used: either traditional distant supervision (Mintz++), MIML-RE, or  a supervised classifier. In the case of MIML-RE, the model may be initialized either using Mintz++, or  the corresponding supervised classifier (the "Not Used" column is initialized with the "All" supervised  classifier). One of five active learning scenarios are evaluated: no annotated examples provided, the three  active learning criteria, and all available examples used. The entry in blue denotes the basic MIML-RE  model; entries in gray perform worse than this model. The bold items denote the best performance  among selection criteria.
true	D14-1164.pdf#37.7	New York Times Corpus, P@10%	F 1  Table 2: A summary of improvements to MIML- RE on the end-to-end slotfilling task, copied from
true	D14-1164.pdf#39.4	New York Times Corpus, P@10%	P  Table 2: A summary of improvements to MIML- RE on the end-to-end slotfilling task, copied from
true	D14-1164.pdf#36.2	New York Times Corpus, P@10%	R  Table 2: A summary of improvements to MIML- RE on the end-to-end slotfilling task, copied from
true	D14-1164.pdf#37.7	New York Times Corpus, P@10%	F 1  Table 1. Mintz++ is the traditional distantly su- pervised model. The second row corresponds to  the unmodified MIML-RE model. The third row  corresponds to MIML-RE initialized with a su- pervised classifier (trained on all examples). The  fourth row is MIML-RE with annotated exam- ples incorporated during training (but not initial- ization). The last row shows the best results ob- tained by our model.
true	D14-1164.pdf#39.4	New York Times Corpus, P@10%	P  Table 1. Mintz++ is the traditional distantly su- pervised model. The second row corresponds to  the unmodified MIML-RE model. The third row  corresponds to MIML-RE initialized with a su- pervised classifier (trained on all examples). The  fourth row is MIML-RE with annotated exam- ples incorporated during training (but not initial- ization). The last row shows the best results ob- tained by our model.
true	D14-1164.pdf#36.2	New York Times Corpus, P@10%	R  Table 1. Mintz++ is the traditional distantly su- pervised model. The second row corresponds to  the unmodified MIML-RE model. The third row  corresponds to MIML-RE initialized with a su- pervised classifier (trained on all examples). The  fourth row is MIML-RE with annotated exam- ples incorporated during training (but not initial- ization). The last row shows the best results ob- tained by our model.
true	D14-1164.pdf#37.7	New York Times Corpus, P@10%	F 1  Table 3: A summary of the performance of each  example selection criterion. In each case, the  model was initialized with a supervised classifier.  The first row corresponds to the MIML-RE model  initialized with a supervised classifier. The middle  three rows show performance for the three selec- tion criteria, used both for initialization and during  training. The last row shows results if all available  annotations are used, independent of their source.
true	D14-1164.pdf#37.1	New York Times Corpus, P@10%	R  Table 3: A summary of the performance of each  example selection criterion. In each case, the  model was initialized with a supervised classifier.  The first row corresponds to the MIML-RE model  initialized with a supervised classifier. The middle  three rows show performance for the three selec- tion criteria, used both for initialization and during  training. The last row shows results if all available  annotations are used, independent of their source.
true	D14-1164.pdf#46.2	New York Times Corpus, P@10%	P  Table 3: A summary of the performance of each  example selection criterion. In each case, the  model was initialized with a supervised classifier.  The first row corresponds to the MIML-RE model  initialized with a supervised classifier. The middle  three rows show performance for the three selec- tion criteria, used both for initialization and during  training. The last row shows results if all available  annotations are used, independent of their source.
true	D14-1164.pdf#35.0	New York Times Corpus, P@10%	R  Table 4: A comparison of the best performing su- pervised classifier with other systems. The top  section compares the supervised classifier with  prior work. The lower section highlights the im- provements gained from initializing MIML-RE  with a supervised classifier.
true	D14-1164.pdf#34.2	New York Times Corpus, P@10%	F 1  Table 4: A comparison of the best performing su- pervised classifier with other systems. The top  section compares the supervised classifier with  prior work. The lower section highlights the im- provements gained from initializing MIML-RE  with a supervised classifier.
true	D14-1164.pdf#41.3	New York Times Corpus, P@10%	Table 4: A comparison of the best performing su- pervised classifier with other systems. The top  section compares the supervised classifier with  prior work. The lower section highlights the im- provements gained from initializing MIML-RE  with a supervised classifier.
true	D14-1052.pdf#4.89	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	EMO . LABELS TED talks MSE  Table 3: MAE and MSE (× 100) on sentiment  and emotion prediction with 5-fold c.-v. Scores  on TED talks are averaged over the 12 emotions.  The scores of the best method are in bold and the  second best ones are underlined. Significant im- provements (paired t-test, p < 0.05) are in italics.
true	D14-1052.pdf#15.02	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	EMO . LABELS TED talks MAE  Table 3: MAE and MSE (× 100) on sentiment  and emotion prediction with 5-fold c.-v. Scores  on TED talks are averaged over the 12 emotions.  The scores of the best method are in bold and the  second best ones are underlined. Significant im- provements (paired t-test, p < 0.05) are in italics.
true	D14-1052.pdf#+6.0	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	EMO . LABELS TED talks MAE  Table 3: MAE and MSE (× 100) on sentiment  and emotion prediction with 5-fold c.-v. Scores  on TED talks are averaged over the 12 emotions.  The scores of the best method are in bold and the  second best ones are underlined. Significant im- provements (paired t-test, p < 0.05) are in italics.
true	D14-1052.pdf#+11.5	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SENT . LABELS TED comm . MSE  Table 3: MAE and MSE (× 100) on sentiment  and emotion prediction with 5-fold c.-v. Scores  on TED talks are averaged over the 12 emotions.  The scores of the best method are in bold and the  second best ones are underlined. Significant im- provements (paired t-test, p < 0.05) are in italics.
true	D14-1052.pdf#+6.8	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SENT . LABELS TED comm . MAE  Table 3: MAE and MSE (× 100) on sentiment  and emotion prediction with 5-fold c.-v. Scores  on TED talks are averaged over the 12 emotions.  The scores of the best method are in bold and the  second best ones are underlined. Significant im- provements (paired t-test, p < 0.05) are in italics.
true	D14-1052.pdf#+5.7	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SENT . LABELS TED comm . MAE  Table 3: MAE and MSE (× 100) on sentiment  and emotion prediction with 5-fold c.-v. Scores  on TED talks are averaged over the 12 emotions.  The scores of the best method are in bold and the  second best ones are underlined. Significant im- provements (paired t-test, p < 0.05) are in italics.
true	D14-1052.pdf#+5.7	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SENT . LABELS TED comm . MAE  Table 3: MAE and MSE (× 100) on sentiment  and emotion prediction with 5-fold c.-v. Scores  on TED talks are averaged over the 12 emotions.  The scores of the best method are in bold and the  second best ones are underlined. Significant im- provements (paired t-test, p < 0.05) are in italics.
true	D14-1052.pdf#15.91	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SENT . LABELS TED comm . MAE  Table 3: MAE and MSE (× 100) on sentiment  and emotion prediction with 5-fold c.-v. Scores  on TED talks are averaged over the 12 emotions.  The scores of the best method are in bold and the  second best ones are underlined. Significant im- provements (paired t-test, p < 0.05) are in italics.
true	D14-1052.pdf#3.95	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SENT . LABELS TED comm . MSE  Table 3: MAE and MSE (× 100) on sentiment  and emotion prediction with 5-fold c.-v. Scores  on TED talks are averaged over the 12 emotions.  The scores of the best method are in bold and the  second best ones are underlined. Significant im- provements (paired t-test, p < 0.05) are in italics.
true	D14-1052.pdf#+2.9	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	EMO . LABELS TED talks MSE  Table 3: MAE and MSE (× 100) on sentiment  and emotion prediction with 5-fold c.-v. Scores  on TED talks are averaged over the 12 emotions.  The scores of the best method are in bold and the  second best ones are underlined. Significant im- provements (paired t-test, p < 0.05) are in italics.
true	P15-1110.pdf#86.6	Penn Treebank, F1	F 1  Table 5: Comparison with the state-of-the-art sys- tems on Chinese test set. * marks neural network  based systems.  ‡ marks shift-reduce parsing sys- tems.
true	P15-1110.pdf#92.3	Penn Treebank, F1	F 1  Table 6: Comparing with the state-of-the-art sys- tems on English test set. * marks neural network  based systems.  ‡ marks shift-reduce parsing sys- tems.
true	D14-1108.pdf#76.1	Penn Treebank, POS	1 ( % ) Table 3 and more variants of the first - order model . TEST - FOSTER  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#82.0	Penn Treebank, POS	1 ( % ) Table 3 and more variants of the first - order model . TEST - NEW  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#82.0	Penn Treebank, POS	1 ( % ) Table 3 and more variants of the first - order model . TEST - NEW  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#83.2	Penn Treebank, POS	1 ( % ) Table 3 and more variants of the first - order model . TEST - NEW  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#76.2	Penn Treebank, POS	1 ( % ) Table 3 and more variants of the first - order model . TEST - FOSTER  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#82.3	Penn Treebank, POS	1 ( % ) Table 3 and more variants of the first - order model . TEST - FOSTER  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#82.8	Penn Treebank, POS	1 ( % ) Table 3 and more variants of the first - order model . TEST - FOSTER  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#81.2	Penn Treebank, POS	Single ablations : Table 3 and more variants of the first - order model . TEST - NEW  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#76.1	Penn Treebank, LAS	1 ( % ) Table 3 and more variants of the first - order model . TEST - FOSTER  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#82.0	Penn Treebank, LAS	1 ( % ) Table 3 and more variants of the first - order model . TEST - NEW  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#82.0	Penn Treebank, LAS	1 ( % ) Table 3 and more variants of the first - order model . TEST - NEW  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#83.2	Penn Treebank, LAS	1 ( % ) Table 3 and more variants of the first - order model . TEST - NEW  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#76.2	Penn Treebank, LAS	1 ( % ) Table 3 and more variants of the first - order model . TEST - FOSTER  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#82.3	Penn Treebank, LAS	1 ( % ) Table 3 and more variants of the first - order model . TEST - FOSTER  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#82.8	Penn Treebank, LAS	1 ( % ) Table 3 and more variants of the first - order model . TEST - FOSTER  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#81.2	Penn Treebank, LAS	Single ablations : Table 3 and more variants of the first - order model . TEST - NEW  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#76.1	Penn Treebank, UAS	1 ( % ) Table 3 and more variants of the first - order model . TEST - FOSTER  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#82.0	Penn Treebank, UAS	1 ( % ) Table 3 and more variants of the first - order model . TEST - NEW  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#82.0	Penn Treebank, UAS	1 ( % ) Table 3 and more variants of the first - order model . TEST - NEW  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#83.2	Penn Treebank, UAS	1 ( % ) Table 3 and more variants of the first - order model . TEST - NEW  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#76.2	Penn Treebank, UAS	1 ( % ) Table 3 and more variants of the first - order model . TEST - FOSTER  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#82.3	Penn Treebank, UAS	1 ( % ) Table 3 and more variants of the first - order model . TEST - FOSTER  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#82.8	Penn Treebank, UAS	1 ( % ) Table 3 and more variants of the first - order model . TEST - FOSTER  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	D14-1108.pdf#81.2	Penn Treebank, UAS	Single ablations : Table 3 and more variants of the first - order model . TEST - NEW  Table 3: Effects of gold-standard POS tagging and token  selection (TS;  §5.5) and of feature ablation ( §5.6). The "base- lines" are TurboParser without the parsing adaptations in  §4.2  and without Penn Treebank or Brown features. The best result  in each column is bolded. See also
true	P11-1032.pdf#95.0	New York Times Corpus, P@10%	TRUTHFUL R  Table 2: Performance of three human judges and two meta-judges on a subset of 160 opinions, corresponding to the  first fold of our cross-validation experiments in Section 5. Boldface indicates the largest value for each column.
true	P11-1032.pdf#61.9%	New York Times Corpus, P@10%	TRUTHFUL Accuracy  Table 2: Performance of three human judges and two meta-judges on a subset of 160 opinions, corresponding to the  first fold of our cross-validation experiments in Section 5. Boldface indicates the largest value for each column.
true	P11-1032.pdf#78.9	New York Times Corpus, P@10%	DECEPTIVE P  Table 2: Performance of three human judges and two meta-judges on a subset of 160 opinions, corresponding to the  first fold of our cross-validation experiments in Section 5. Boldface indicates the largest value for each column.
true	P11-1032.pdf#60.8	New York Times Corpus, P@10%	TRUTHFUL P  Table 2: Performance of three human judges and two meta-judges on a subset of 160 opinions, corresponding to the  first fold of our cross-validation experiments in Section 5. Boldface indicates the largest value for each column.
true	P11-1032.pdf#69.7	New York Times Corpus, P@10%	TRUTHFUL F  Table 2: Performance of three human judges and two meta-judges on a subset of 160 opinions, corresponding to the  first fold of our cross-validation experiments in Section 5. Boldface indicates the largest value for each column.
true	P11-1032.pdf#60.9	New York Times Corpus, P@10%	DECEPTIVE F  Table 2: Performance of three human judges and two meta-judges on a subset of 160 opinions, corresponding to the  first fold of our cross-validation experiments in Section 5. Boldface indicates the largest value for each column.
true	P11-1032.pdf#61.3	New York Times Corpus, P@10%	DECEPTIVE R  Table 2: Performance of three human judges and two meta-judges on a subset of 160 opinions, corresponding to the  first fold of our cross-validation experiments in Section 5. Boldface indicates the largest value for each column.
true	P11-1032.pdf#89.8	New York Times Corpus, P@10%	SVM TRUTHFUL F  Table 3: Automated classifier performance for three approaches based on nested 5-fold cross-validation experiments.  Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, false  positive and false negative rates, as suggested by
true	P11-1032.pdf#92.5	New York Times Corpus, P@10%	SVM TRUTHFUL P  Table 3: Automated classifier performance for three approaches based on nested 5-fold cross-validation experiments.  Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, false  positive and false negative rates, as suggested by
true	P11-1032.pdf#89.8	New York Times Corpus, P@10%	SVM TRUTHFUL R  Table 3: Automated classifier performance for three approaches based on nested 5-fold cross-validation experiments.  Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, false  positive and false negative rates, as suggested by
true	P11-1032.pdf#69.7	New York Times Corpus, P@10%	NB TRUTHFUL F  Table 3: Automated classifier performance for three approaches based on nested 5-fold cross-validation experiments.  Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, false  positive and false negative rates, as suggested by
true	P11-1032.pdf#89.8	New York Times Corpus, P@10%	SVM DECEPTIVE F  Table 3: Automated classifier performance for three approaches based on nested 5-fold cross-validation experiments.  Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, false  positive and false negative rates, as suggested by
true	P11-1032.pdf#61.9%	New York Times Corpus, P@10%	NB TRUTHFUL Accuracy  Table 3: Automated classifier performance for three approaches based on nested 5-fold cross-validation experiments.  Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, false  positive and false negative rates, as suggested by
true	P11-1032.pdf#93.3	New York Times Corpus, P@10%	SVM DECEPTIVE R  Table 3: Automated classifier performance for three approaches based on nested 5-fold cross-validation experiments.  Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, false  positive and false negative rates, as suggested by
true	P11-1032.pdf#61.3	New York Times Corpus, P@10%	NB DECEPTIVE R  Table 3: Automated classifier performance for three approaches based on nested 5-fold cross-validation experiments.  Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, false  positive and false negative rates, as suggested by
true	P11-1032.pdf#60.9	New York Times Corpus, P@10%	NB DECEPTIVE F  Table 3: Automated classifier performance for three approaches based on nested 5-fold cross-validation experiments.  Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, false  positive and false negative rates, as suggested by
true	P11-1032.pdf#89.8	New York Times Corpus, P@10%	SVM DECEPTIVE P  Table 3: Automated classifier performance for three approaches based on nested 5-fold cross-validation experiments.  Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, false  positive and false negative rates, as suggested by
true	P11-1032.pdf#95.0	New York Times Corpus, P@10%	NB TRUTHFUL R  Table 3: Automated classifier performance for three approaches based on nested 5-fold cross-validation experiments.  Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, false  positive and false negative rates, as suggested by
true	P11-1032.pdf#89.8%	New York Times Corpus, P@10%	SVM TRUTHFUL Accuracy  Table 3: Automated classifier performance for three approaches based on nested 5-fold cross-validation experiments.  Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, false  positive and false negative rates, as suggested by
true	P11-1032.pdf#60.8	New York Times Corpus, P@10%	NB TRUTHFUL P  Table 3: Automated classifier performance for three approaches based on nested 5-fold cross-validation experiments.  Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, false  positive and false negative rates, as suggested by
true	P11-1032.pdf#78.9	New York Times Corpus, P@10%	NB DECEPTIVE P  Table 3: Automated classifier performance for three approaches based on nested 5-fold cross-validation experiments.  Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, false  positive and false negative rates, as suggested by
true	P11-1032.pdf#-0.041	New York Times Corpus, P@10%	TRUTHFUL / INFORMATIVE Weight  Table 4: Average feature weights learned by POS SVM . Based on work by Rayson et al.
true	P11-1032.pdf#0.026	New York Times Corpus, P@10%	NOUNS DECEPTIVE / IMAGINATIVE Weight  Table 4: Average feature weights learned by POS SVM . Based on work by Rayson et al.
true	P11-1032.pdf#-0.094	New York Times Corpus, P@10%	PRONOUNS TRUTHFUL / INFORMATIVE Weight  Table 4: Average feature weights learned by POS SVM . Based on work by Rayson et al.
true	P11-1032.pdf#0.017	New York Times Corpus, P@10%	PRONOUNS DECEPTIVE / IMAGINATIVE Weight  Table 4: Average feature weights learned by POS SVM . Based on work by Rayson et al.
true	P11-1032.pdf#0.041	New York Times Corpus, P@10%	DECEPTIVE / IMAGINATIVE Weight  Table 4: Average feature weights learned by POS SVM . Based on work by Rayson et al.
true	P11-1032.pdf#0.001	New York Times Corpus, P@10%	singular , present DECEPTIVE / IMAGINATIVE Weight  Table 4: Average feature weights learned by POS SVM . Based on work by Rayson et al.
true	P11-1032.pdf#-0.164	New York Times Corpus, P@10%	singular , present TRUTHFUL / INFORMATIVE Weight  Table 4: Average feature weights learned by POS SVM . Based on work by Rayson et al.
true	P15-1114.pdf#93.72	benchmark Vietnamese dependency treebank VnDT, UAS	9 mins UAS  Table 2: Comparison between our system and the  state-of-art systems on English dataset. LM is  short for Linear Model, hrs, mins are short for  hours and minutes respectively
true	P15-1114.pdf#87.86	benchmark Vietnamese dependency treebank VnDT, UAS	Official Best  Table 4: Experimental Results on CoNLL 2009  non-English datasets.
true	P15-1114.pdf#87.82	benchmark Vietnamese dependency treebank VnDT, UAS	Ours  Table 4: Experimental Results on CoNLL 2009  non-English datasets.
true	P15-1114.pdf#80.51	benchmark Vietnamese dependency treebank VnDT, UAS	Ours  Table 4: Experimental Results on CoNLL 2009  non-English datasets.
true	P15-1114.pdf#79.17	benchmark Vietnamese dependency treebank VnDT, UAS	Table 4: Experimental Results on CoNLL 2009  non-English datasets.
true	P15-1114.pdf#92.68	benchmark Vietnamese dependency treebank VnDT, UAS	Ours  Table 4: Experimental Results on CoNLL 2009  non-English datasets.
true	P15-1114.pdf#87.48	benchmark Vietnamese dependency treebank VnDT, UAS	Official Best  Table 4: Experimental Results on CoNLL 2009  non-English datasets.
true	P15-1114.pdf#93.72	Penn Treebank, UAS	9 mins UAS  Table 2: Comparison between our system and the  state-of-art systems on English dataset. LM is  short for Linear Model, hrs, mins are short for  hours and minutes respectively
true	P15-1114.pdf#87.86	Penn Treebank, UAS	Official Best  Table 4: Experimental Results on CoNLL 2009  non-English datasets.
true	P15-1114.pdf#87.82	Penn Treebank, UAS	Ours  Table 4: Experimental Results on CoNLL 2009  non-English datasets.
true	P15-1114.pdf#80.51	Penn Treebank, UAS	Ours  Table 4: Experimental Results on CoNLL 2009  non-English datasets.
true	P15-1114.pdf#79.17	Penn Treebank, UAS	Table 4: Experimental Results on CoNLL 2009  non-English datasets.
true	P15-1114.pdf#92.68	Penn Treebank, UAS	Ours  Table 4: Experimental Results on CoNLL 2009  non-English datasets.
true	P15-1114.pdf#87.48	Penn Treebank, UAS	Official Best  Table 4: Experimental Results on CoNLL 2009  non-English datasets.
true	P14-1019.pdf#88.38	Penn Treebank, UAS	Testing Set UAS  Table 3: Results for parsing and corrective tagging  on the CATiB dataset. The upper part shows UAS  of our model with gold/predicted information or  POS correction. Bottom part shows UAS of the  best systems in the SPMRL shared task. IMS- Single (Björkelund et al., 2013) is the best single  parsing system, while IMS-Ensemble (Björkelund  et al., 2013) is the best ensemble parsing system.  We also show results for CADIM (Marton et al.,  2013), the second best system, because we use  their predicted features.
true	P14-1019.pdf#88.38	benchmark Vietnamese dependency treebank VnDT, LAS	Testing Set UAS  Table 3: Results for parsing and corrective tagging  on the CATiB dataset. The upper part shows UAS  of our model with gold/predicted information or  POS correction. Bottom part shows UAS of the  best systems in the SPMRL shared task. IMS- Single (Björkelund et al., 2013) is the best single  parsing system, while IMS-Ensemble (Björkelund  et al., 2013) is the best ensemble parsing system.  We also show results for CADIM (Marton et al.,  2013), the second best system, because we use  their predicted features.
true	P14-1019.pdf#88.38	benchmark Vietnamese dependency treebank VnDT, UAS	Testing Set UAS  Table 3: Results for parsing and corrective tagging  on the CATiB dataset. The upper part shows UAS  of our model with gold/predicted information or  POS correction. Bottom part shows UAS of the  best systems in the SPMRL shared task. IMS- Single (Björkelund et al., 2013) is the best single  parsing system, while IMS-Ensemble (Björkelund  et al., 2013) is the best ensemble parsing system.  We also show results for CADIM (Marton et al.,  2013), the second best system, because we use  their predicted features.
true	D12-1046.pdf#0≤i≤j≤l−	Chinese Treebank 6, F1	decoding . get the best POS tags of words left / right to b  Table 4: Complexity Analysis of Algorithm 1.
true	D12-1046.pdf#1do	Chinese Treebank 6, F1	unary ( x , i , j , w get the best POS tags of words left / right to b score bottom ( x , i , j , w Keep B best score bottom . score top ( x , i , j , w +θ unary · f  Table 4: Complexity Analysis of Algorithm 1.
true	D12-1046.pdf#1	Chinese Treebank 6, F1	decoding . get the best POS tags of words left / right to b  Table 4: Complexity Analysis of Algorithm 1.
true	D12-1046.pdf#1do	Chinese Treebank 6, F1	unary ( x , i , j , w get the best POS tags of words left / right to b i , j , t , N ) = score seg⊕pos ( x , i , j , t ) + θ bottom · f Keep B best score bottom . i , j , t , N ) = max N {score bottom ( x , i , j , w i , j , t , N → N )  Table 4: Complexity Analysis of Algorithm 1.
true	D12-1046.pdf#84.13	Chinese Treebank 6, F1	F  Table 9: Parsing results using gold standard word seg- mentation.
true	D12-1046.pdf#82.85	Chinese Treebank 6, F1	F  Table 10: Results for the joint segmentation, tagging, and  parsing task using pipeline and joint models.
true	D11-1109.pdf#94.17	benchmark Vietnamese dependency treebank VnDT, UAS	ing threshold λ t on the development set . 100 2 k  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#30.62	benchmark Vietnamese dependency treebank VnDT, UAS	ing threshold λ t on the development set . 100 1 number of candidate POS tags compl .  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#77.38	benchmark Vietnamese dependency treebank VnDT, UAS	ing threshold λ t on the development set . 100 5 number of candidate POS tags root  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#81.83	benchmark Vietnamese dependency treebank VnDT, UAS	ing threshold λ t on the development set . 100 3 number of candidate POS tags word  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#33.72	benchmark Vietnamese dependency treebank VnDT, UAS	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#75.84	benchmark Vietnamese dependency treebank VnDT, UAS	Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#29.11	benchmark Vietnamese dependency treebank VnDT, UAS	Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#79.29	benchmark Vietnamese dependency treebank VnDT, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#29.06	benchmark Vietnamese dependency treebank VnDT, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#27.24	benchmark Vietnamese dependency treebank VnDT, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#78.58	benchmark Vietnamese dependency treebank VnDT, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#92.96	benchmark Vietnamese dependency treebank VnDT, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#86.18	benchmark Vietnamese dependency treebank VnDT, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#75.90	benchmark Vietnamese dependency treebank VnDT, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#34.02	benchmark Vietnamese dependency treebank VnDT, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#78.32	benchmark Vietnamese dependency treebank VnDT, UAS	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#86.00	benchmark Vietnamese dependency treebank VnDT, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#34.07	benchmark Vietnamese dependency treebank VnDT, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#74.70	benchmark Vietnamese dependency treebank VnDT, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#85.77	benchmark Vietnamese dependency treebank VnDT, UAS	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#93.20	benchmark Vietnamese dependency treebank VnDT, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#80.74	benchmark Vietnamese dependency treebank VnDT, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#80.79	benchmark Vietnamese dependency treebank VnDT, UAS	Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#34.41	benchmark Vietnamese dependency treebank VnDT, UAS	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#93.51	benchmark Vietnamese dependency treebank VnDT, UAS	Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#76.75	benchmark Vietnamese dependency treebank VnDT, UAS	root  Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#82.11	benchmark Vietnamese dependency treebank VnDT, UAS	word  Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#30.62	benchmark Vietnamese dependency treebank VnDT, UAS	compl .  Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#+1.8	benchmark Vietnamese dependency treebank VnDT, UAS	pattern joint error ( % )  Table 5: Comparison of parsing error rates on different  POS tag patterns between the pipelined and joint models.  Given a pattern "X → Y", "prop" means its proportion in  all occurrence of 'X' ( Count(X→Y )  Count(X) ), and "error" refers  to its parsing error rate ( Count(wrongly headed X→Y )
true	D11-1109.pdf#+2.1	benchmark Vietnamese dependency treebank VnDT, UAS	pattern joint error ( % )  Table 5: Comparison of parsing error rates on different  POS tag patterns between the pipelined and joint models.  Given a pattern "X → Y", "prop" means its proportion in  all occurrence of 'X' ( Count(X→Y )  Count(X) ), and "error" refers  to its parsing error rate ( Count(wrongly headed X→Y )
true	D11-1109.pdf#94.17	Penn Treebank, F1	ing threshold λ t on the development set . 100 2 k  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#30.62	Penn Treebank, F1	ing threshold λ t on the development set . 100 1 number of candidate POS tags compl .  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#77.38	Penn Treebank, F1	ing threshold λ t on the development set . 100 5 number of candidate POS tags root  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#81.83	Penn Treebank, F1	ing threshold λ t on the development set . 100 3 number of candidate POS tags word  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#33.72	Penn Treebank, F1	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#75.84	Penn Treebank, F1	Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#29.11	Penn Treebank, F1	Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#79.29	Penn Treebank, F1	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#29.06	Penn Treebank, F1	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#27.24	Penn Treebank, F1	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#78.58	Penn Treebank, F1	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#92.96	Penn Treebank, F1	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#86.18	Penn Treebank, F1	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#75.90	Penn Treebank, F1	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#34.02	Penn Treebank, F1	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#78.32	Penn Treebank, F1	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#86.00	Penn Treebank, F1	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#34.07	Penn Treebank, F1	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#74.70	Penn Treebank, F1	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#85.77	Penn Treebank, F1	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#93.20	Penn Treebank, F1	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#80.74	Penn Treebank, F1	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#80.79	Penn Treebank, F1	Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#34.41	Penn Treebank, F1	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#93.51	Penn Treebank, F1	Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#76.75	Penn Treebank, F1	root  Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#82.11	Penn Treebank, F1	word  Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#30.62	Penn Treebank, F1	compl .  Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#+1.8	Penn Treebank, F1	pattern joint error ( % )  Table 5: Comparison of parsing error rates on different  POS tag patterns between the pipelined and joint models.  Given a pattern "X → Y", "prop" means its proportion in  all occurrence of 'X' ( Count(X→Y )  Count(X) ), and "error" refers  to its parsing error rate ( Count(wrongly headed X→Y )
true	D11-1109.pdf#+2.1	Penn Treebank, F1	pattern joint error ( % )  Table 5: Comparison of parsing error rates on different  POS tag patterns between the pipelined and joint models.  Given a pattern "X → Y", "prop" means its proportion in  all occurrence of 'X' ( Count(X→Y )  Count(X) ), and "error" refers  to its parsing error rate ( Count(wrongly headed X→Y )
true	D11-1109.pdf#94.17	Penn Treebank, POS	ing threshold λ t on the development set . 100 2 k  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#30.62	Penn Treebank, POS	ing threshold λ t on the development set . 100 1 number of candidate POS tags compl .  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#77.38	Penn Treebank, POS	ing threshold λ t on the development set . 100 5 number of candidate POS tags root  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#81.83	Penn Treebank, POS	ing threshold λ t on the development set . 100 3 number of candidate POS tags word  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#33.72	Penn Treebank, POS	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#75.84	Penn Treebank, POS	Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#29.11	Penn Treebank, POS	Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#79.29	Penn Treebank, POS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#29.06	Penn Treebank, POS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#27.24	Penn Treebank, POS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#78.58	Penn Treebank, POS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#92.96	Penn Treebank, POS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#86.18	Penn Treebank, POS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#75.90	Penn Treebank, POS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#34.02	Penn Treebank, POS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#78.32	Penn Treebank, POS	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#86.00	Penn Treebank, POS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#34.07	Penn Treebank, POS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#74.70	Penn Treebank, POS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#85.77	Penn Treebank, POS	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#93.20	Penn Treebank, POS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#80.74	Penn Treebank, POS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#80.79	Penn Treebank, POS	Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#34.41	Penn Treebank, POS	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#93.51	Penn Treebank, POS	Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#76.75	Penn Treebank, POS	root  Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#82.11	Penn Treebank, POS	word  Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#30.62	Penn Treebank, POS	compl .  Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#+1.8	Penn Treebank, POS	pattern joint error ( % )  Table 5: Comparison of parsing error rates on different  POS tag patterns between the pipelined and joint models.  Given a pattern "X → Y", "prop" means its proportion in  all occurrence of 'X' ( Count(X→Y )  Count(X) ), and "error" refers  to its parsing error rate ( Count(wrongly headed X→Y )
true	D11-1109.pdf#+2.1	Penn Treebank, POS	pattern joint error ( % )  Table 5: Comparison of parsing error rates on different  POS tag patterns between the pipelined and joint models.  Given a pattern "X → Y", "prop" means its proportion in  all occurrence of 'X' ( Count(X→Y )  Count(X) ), and "error" refers  to its parsing error rate ( Count(wrongly headed X→Y )
true	D11-1109.pdf#94.17	benchmark Vietnamese dependency treebank VnDT, LAS	ing threshold λ t on the development set . 100 2 k  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#30.62	benchmark Vietnamese dependency treebank VnDT, LAS	ing threshold λ t on the development set . 100 1 number of candidate POS tags compl .  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#77.38	benchmark Vietnamese dependency treebank VnDT, LAS	ing threshold λ t on the development set . 100 5 number of candidate POS tags root  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#81.83	benchmark Vietnamese dependency treebank VnDT, LAS	ing threshold λ t on the development set . 100 3 number of candidate POS tags word  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#33.72	benchmark Vietnamese dependency treebank VnDT, LAS	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#75.84	benchmark Vietnamese dependency treebank VnDT, LAS	Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#29.11	benchmark Vietnamese dependency treebank VnDT, LAS	Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#79.29	benchmark Vietnamese dependency treebank VnDT, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#29.06	benchmark Vietnamese dependency treebank VnDT, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#27.24	benchmark Vietnamese dependency treebank VnDT, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#78.58	benchmark Vietnamese dependency treebank VnDT, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#92.96	benchmark Vietnamese dependency treebank VnDT, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#86.18	benchmark Vietnamese dependency treebank VnDT, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#75.90	benchmark Vietnamese dependency treebank VnDT, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#34.02	benchmark Vietnamese dependency treebank VnDT, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#78.32	benchmark Vietnamese dependency treebank VnDT, LAS	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#86.00	benchmark Vietnamese dependency treebank VnDT, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#34.07	benchmark Vietnamese dependency treebank VnDT, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#74.70	benchmark Vietnamese dependency treebank VnDT, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#85.77	benchmark Vietnamese dependency treebank VnDT, LAS	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#93.20	benchmark Vietnamese dependency treebank VnDT, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#80.74	benchmark Vietnamese dependency treebank VnDT, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#80.79	benchmark Vietnamese dependency treebank VnDT, LAS	Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#34.41	benchmark Vietnamese dependency treebank VnDT, LAS	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#93.51	benchmark Vietnamese dependency treebank VnDT, LAS	Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#76.75	benchmark Vietnamese dependency treebank VnDT, LAS	root  Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#82.11	benchmark Vietnamese dependency treebank VnDT, LAS	word  Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#30.62	benchmark Vietnamese dependency treebank VnDT, LAS	compl .  Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#+1.8	benchmark Vietnamese dependency treebank VnDT, LAS	pattern joint error ( % )  Table 5: Comparison of parsing error rates on different  POS tag patterns between the pipelined and joint models.  Given a pattern "X → Y", "prop" means its proportion in  all occurrence of 'X' ( Count(X→Y )  Count(X) ), and "error" refers  to its parsing error rate ( Count(wrongly headed X→Y )
true	D11-1109.pdf#+2.1	benchmark Vietnamese dependency treebank VnDT, LAS	pattern joint error ( % )  Table 5: Comparison of parsing error rates on different  POS tag patterns between the pipelined and joint models.  Given a pattern "X → Y", "prop" means its proportion in  all occurrence of 'X' ( Count(X→Y )  Count(X) ), and "error" refers  to its parsing error rate ( Count(wrongly headed X→Y )
true	D11-1109.pdf#94.17	Penn Treebank, UAS	ing threshold λ t on the development set . 100 2 k  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#30.62	Penn Treebank, UAS	ing threshold λ t on the development set . 100 1 number of candidate POS tags compl .  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#77.38	Penn Treebank, UAS	ing threshold λ t on the development set . 100 5 number of candidate POS tags root  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#81.83	Penn Treebank, UAS	ing threshold λ t on the development set . 100 3 number of candidate POS tags word  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#33.72	Penn Treebank, UAS	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#75.84	Penn Treebank, UAS	Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#29.11	Penn Treebank, UAS	Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#79.29	Penn Treebank, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#29.06	Penn Treebank, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#27.24	Penn Treebank, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#78.58	Penn Treebank, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#92.96	Penn Treebank, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#86.18	Penn Treebank, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#75.90	Penn Treebank, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#34.02	Penn Treebank, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#78.32	Penn Treebank, UAS	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#86.00	Penn Treebank, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#34.07	Penn Treebank, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#74.70	Penn Treebank, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#85.77	Penn Treebank, UAS	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#93.20	Penn Treebank, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#80.74	Penn Treebank, UAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#80.79	Penn Treebank, UAS	Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#34.41	Penn Treebank, UAS	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#93.51	Penn Treebank, UAS	Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#76.75	Penn Treebank, UAS	root  Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#82.11	Penn Treebank, UAS	word  Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#30.62	Penn Treebank, UAS	compl .  Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#+1.8	Penn Treebank, UAS	pattern joint error ( % )  Table 5: Comparison of parsing error rates on different  POS tag patterns between the pipelined and joint models.  Given a pattern "X → Y", "prop" means its proportion in  all occurrence of 'X' ( Count(X→Y )  Count(X) ), and "error" refers  to its parsing error rate ( Count(wrongly headed X→Y )
true	D11-1109.pdf#+2.1	Penn Treebank, UAS	pattern joint error ( % )  Table 5: Comparison of parsing error rates on different  POS tag patterns between the pipelined and joint models.  Given a pattern "X → Y", "prop" means its proportion in  all occurrence of 'X' ( Count(X→Y )  Count(X) ), and "error" refers  to its parsing error rate ( Count(wrongly headed X→Y )
true	D11-1109.pdf#94.17	Penn Treebank, LAS	ing threshold λ t on the development set . 100 2 k  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#30.62	Penn Treebank, LAS	ing threshold λ t on the development set . 100 1 number of candidate POS tags compl .  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#77.38	Penn Treebank, LAS	ing threshold λ t on the development set . 100 5 number of candidate POS tags root  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#81.83	Penn Treebank, LAS	ing threshold λ t on the development set . 100 3 number of candidate POS tags word  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#33.72	Penn Treebank, LAS	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#75.84	Penn Treebank, LAS	Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#29.11	Penn Treebank, LAS	Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#79.29	Penn Treebank, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#29.06	Penn Treebank, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#27.24	Penn Treebank, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#78.58	Penn Treebank, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#92.96	Penn Treebank, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#86.18	Penn Treebank, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#75.90	Penn Treebank, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#34.02	Penn Treebank, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#78.32	Penn Treebank, LAS	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#86.00	Penn Treebank, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#34.07	Penn Treebank, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#74.70	Penn Treebank, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#85.77	Penn Treebank, LAS	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#93.20	Penn Treebank, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#80.74	Penn Treebank, LAS	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#80.79	Penn Treebank, LAS	Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#34.41	Penn Treebank, LAS	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#93.51	Penn Treebank, LAS	Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#76.75	Penn Treebank, LAS	root  Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#82.11	Penn Treebank, LAS	word  Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#30.62	Penn Treebank, LAS	compl .  Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#+1.8	Penn Treebank, LAS	pattern joint error ( % )  Table 5: Comparison of parsing error rates on different  POS tag patterns between the pipelined and joint models.  Given a pattern "X → Y", "prop" means its proportion in  all occurrence of 'X' ( Count(X→Y )  Count(X) ), and "error" refers  to its parsing error rate ( Count(wrongly headed X→Y )
true	D11-1109.pdf#+2.1	Penn Treebank, LAS	pattern joint error ( % )  Table 5: Comparison of parsing error rates on different  POS tag patterns between the pipelined and joint models.  Given a pattern "X → Y", "prop" means its proportion in  all occurrence of 'X' ( Count(X→Y )  Count(X) ), and "error" refers  to its parsing error rate ( Count(wrongly headed X→Y )
true	D11-1109.pdf#94.17	Penn Treebank, Accuracy	ing threshold λ t on the development set . 100 2 k  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#30.62	Penn Treebank, Accuracy	ing threshold λ t on the development set . 100 1 number of candidate POS tags compl .  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#77.38	Penn Treebank, Accuracy	ing threshold λ t on the development set . 100 5 number of candidate POS tags root  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#81.83	Penn Treebank, Accuracy	ing threshold λ t on the development set . 100 3 number of candidate POS tags word  Table 1: Performance of the second-order joint model of  version 1 with different pruning threshold λ t (top k = 5)  on the development set. "Acc." means the tagging accu- racy. "Speed" refers to the parsing speed (the number of  sentences processed per second).
true	D11-1109.pdf#33.72	Penn Treebank, Accuracy	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#75.84	Penn Treebank, Accuracy	Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#29.11	Penn Treebank, Accuracy	Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#79.29	Penn Treebank, Accuracy	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#29.06	Penn Treebank, Accuracy	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#27.24	Penn Treebank, Accuracy	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#78.58	Penn Treebank, Accuracy	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#92.96	Penn Treebank, Accuracy	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#86.18	Penn Treebank, Accuracy	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#75.90	Penn Treebank, Accuracy	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#34.02	Penn Treebank, Accuracy	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#78.32	Penn Treebank, Accuracy	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#86.00	Penn Treebank, Accuracy	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#34.07	Penn Treebank, Accuracy	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#74.70	Penn Treebank, Accuracy	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#85.77	Penn Treebank, Accuracy	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#93.20	Penn Treebank, Accuracy	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#80.74	Penn Treebank, Accuracy	Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#80.79	Penn Treebank, Accuracy	Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#34.41	Penn Treebank, Accuracy	- Sec  Table 3: Final results on the test set. "Gold POS" means that gold POS tags are used as input by the pipelined parsing  models; while "Auto POS" means that the POS tags are generated by the baseline POS tagging model.
true	D11-1109.pdf#93.51	Penn Treebank, Accuracy	Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#76.75	Penn Treebank, Accuracy	root  Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#82.11	Penn Treebank, Accuracy	word  Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#30.62	Penn Treebank, Accuracy	compl .  Table 2: Performance of the second-order joint model of  version 1 with different top k (λ t = 0.01) on the devel- opment set.
true	D11-1109.pdf#+1.8	Penn Treebank, Accuracy	pattern joint error ( % )  Table 5: Comparison of parsing error rates on different  POS tag patterns between the pipelined and joint models.  Given a pattern "X → Y", "prop" means its proportion in  all occurrence of 'X' ( Count(X→Y )  Count(X) ), and "error" refers  to its parsing error rate ( Count(wrongly headed X→Y )
true	D11-1109.pdf#+2.1	Penn Treebank, Accuracy	pattern joint error ( % )  Table 5: Comparison of parsing error rates on different  POS tag patterns between the pipelined and joint models.  Given a pattern "X → Y", "prop" means its proportion in  all occurrence of 'X' ( Count(X→Y )  Count(X) ), and "error" refers  to its parsing error rate ( Count(wrongly headed X→Y )
true	P13-2102.pdf#2.52	Senseval 2, F1	CG5  Table 1: Comparison of different methods for  document selection by measuring the Cumulative  Gain (CG) of top 5, 10 and 20 results.
true	P13-2102.pdf#6.01	Senseval 2, F1	CG20  Table 1: Comparison of different methods for  document selection by measuring the Cumulative  Gain (CG) of top 5, 10 and 20 results.
true	P13-2102.pdf#3.91	Senseval 2, F1	CG10  Table 1: Comparison of different methods for  document selection by measuring the Cumulative  Gain (CG) of top 5, 10 and 20 results.
true	P13-2102.pdf#0.81*	Senseval 2, F1	LR  Table 4: Results of pyramid evaluation for each  of the three methods and the random baseline on  each topic.
true	P13-2102.pdf#0.34*	Senseval 2, F1	LR  Table 5. The parameter α is the RU  penalty for including a redundant sentence sub- sumed by an earlier sentence. If the summary  chooses a sentence s i with score w orig that is sub- sumed by an earlier summary sentence, the score  is reduced as w subsumed = (α  *  w orig ). We ap- proximate subsumption by marking a sentence s j  as being subsumed by s i if F j ⊂ F i , where F i and  F j are sets of factoids covered in each sentence.
true	P13-2102.pdf#0.34*	Senseval 2, F1	LR  Table 5: Results of Unnormalized Relative Utility  evaluation for the three methods and random base- line using α = 0.5.
true	P13-2102.pdf#2.52	SemEval 2013, F1	CG5  Table 1: Comparison of different methods for  document selection by measuring the Cumulative  Gain (CG) of top 5, 10 and 20 results.
true	P13-2102.pdf#6.01	SemEval 2013, F1	CG20  Table 1: Comparison of different methods for  document selection by measuring the Cumulative  Gain (CG) of top 5, 10 and 20 results.
true	P13-2102.pdf#3.91	SemEval 2013, F1	CG10  Table 1: Comparison of different methods for  document selection by measuring the Cumulative  Gain (CG) of top 5, 10 and 20 results.
true	P13-2102.pdf#0.81*	SemEval 2013, F1	LR  Table 4: Results of pyramid evaluation for each  of the three methods and the random baseline on  each topic.
true	P13-2102.pdf#0.34*	SemEval 2013, F1	LR  Table 5. The parameter α is the RU  penalty for including a redundant sentence sub- sumed by an earlier sentence. If the summary  chooses a sentence s i with score w orig that is sub- sumed by an earlier summary sentence, the score  is reduced as w subsumed = (α  *  w orig ). We ap- proximate subsumption by marking a sentence s j  as being subsumed by s i if F j ⊂ F i , where F i and  F j are sets of factoids covered in each sentence.
true	P13-2102.pdf#0.34*	SemEval 2013, F1	LR  Table 5: Results of Unnormalized Relative Utility  evaluation for the three methods and random base- line using α = 0.5.
true	P13-2102.pdf#2.52	Senseval 3, F1	CG5  Table 1: Comparison of different methods for  document selection by measuring the Cumulative  Gain (CG) of top 5, 10 and 20 results.
true	P13-2102.pdf#6.01	Senseval 3, F1	CG20  Table 1: Comparison of different methods for  document selection by measuring the Cumulative  Gain (CG) of top 5, 10 and 20 results.
true	P13-2102.pdf#3.91	Senseval 3, F1	CG10  Table 1: Comparison of different methods for  document selection by measuring the Cumulative  Gain (CG) of top 5, 10 and 20 results.
true	P13-2102.pdf#0.81*	Senseval 3, F1	LR  Table 4: Results of pyramid evaluation for each  of the three methods and the random baseline on  each topic.
true	P13-2102.pdf#0.34*	Senseval 3, F1	LR  Table 5. The parameter α is the RU  penalty for including a redundant sentence sub- sumed by an earlier sentence. If the summary  chooses a sentence s i with score w orig that is sub- sumed by an earlier summary sentence, the score  is reduced as w subsumed = (α  *  w orig ). We ap- proximate subsumption by marking a sentence s j  as being subsumed by s i if F j ⊂ F i , where F i and  F j are sets of factoids covered in each sentence.
true	P13-2102.pdf#0.34*	Senseval 3, F1	LR  Table 5: Results of Unnormalized Relative Utility  evaluation for the three methods and random base- line using α = 0.5.
true	P13-2102.pdf#2.52	SemEval 2007, F1	CG5  Table 1: Comparison of different methods for  document selection by measuring the Cumulative  Gain (CG) of top 5, 10 and 20 results.
true	P13-2102.pdf#6.01	SemEval 2007, F1	CG20  Table 1: Comparison of different methods for  document selection by measuring the Cumulative  Gain (CG) of top 5, 10 and 20 results.
true	P13-2102.pdf#3.91	SemEval 2007, F1	CG10  Table 1: Comparison of different methods for  document selection by measuring the Cumulative  Gain (CG) of top 5, 10 and 20 results.
true	P13-2102.pdf#0.81*	SemEval 2007, F1	LR  Table 4: Results of pyramid evaluation for each  of the three methods and the random baseline on  each topic.
true	P13-2102.pdf#0.34*	SemEval 2007, F1	LR  Table 5. The parameter α is the RU  penalty for including a redundant sentence sub- sumed by an earlier sentence. If the summary  chooses a sentence s i with score w orig that is sub- sumed by an earlier summary sentence, the score  is reduced as w subsumed = (α  *  w orig ). We ap- proximate subsumption by marking a sentence s j  as being subsumed by s i if F j ⊂ F i , where F i and  F j are sets of factoids covered in each sentence.
true	P13-2102.pdf#0.34*	SemEval 2007, F1	LR  Table 5: Results of Unnormalized Relative Utility  evaluation for the three methods and random base- line using α = 0.5.
true	P10-1063.pdf#1.7	WMT 2014 EN-DE, BLEU	TE vBLEU∆ [ 4 ] Q1 − 4  Table 3: Detailed performance using all features (English-Spanish).
true	P10-1063.pdf#35%	WMT 2014 EN-DE, BLEU	Ranking Accuracy BLEU Q2  Table 3: Detailed performance using all features (English-Spanish).
true	P10-1063.pdf#8.5	WMT 2014 EN-DE, BLEU	MAE vBLEU∆ [ 4 ] Q1 − 4  Table 3: Detailed performance using all features (English-Spanish).
true	P10-1063.pdf#56%	WMT 2014 EN-DE, BLEU	Ranking Accuracy BLEU Q4  Table 3: Detailed performance using all features (English-Spanish).
true	P10-1063.pdf#+5.9	WMT 2014 EN-DE, BLEU	Translation Accuracy vBLEU∆ [ 4 ] Q1 − 4  Table 3: Detailed performance using all features (English-Spanish).
true	P10-1063.pdf#45%	WMT 2014 EN-DE, BLEU	Ranking Accuracy BLEU rAcc  Table 3: Detailed performance using all features (English-Spanish).
true	P10-1063.pdf#37%	WMT 2014 EN-DE, BLEU	Ranking Accuracy BLEU Q3  Table 3: Detailed performance using all features (English-Spanish).
true	P10-1063.pdf#52%	WMT 2014 EN-DE, BLEU	Ranking Accuracy BLEU Q1  Table 3: Detailed performance using all features (English-Spanish).
true	P10-1063.pdf#1.6	WMT 2014 EN-DE, BLEU	" Time - constrained " condition Table 5 TE  Table 5:  "Time-constrained" performance  (English-Spanish).
true	P10-1063.pdf#+5.8	WMT 2014 EN-DE, BLEU	" Time - constrained " condition Table 5 vBLEU∆ [ 4 ]  Table 5:  "Time-constrained" performance  (English-Spanish).
true	P10-1063.pdf#8.6	WMT 2014 EN-DE, BLEU	" Time - constrained " condition Table 5 MAE  Table 5:  "Time-constrained" performance  (English-Spanish).
true	P10-1063.pdf#1.7	WMT 2014 EN-FR, BLEU	TE vBLEU∆ [ 4 ] Q1 − 4  Table 3: Detailed performance using all features (English-Spanish).
true	P10-1063.pdf#35%	WMT 2014 EN-FR, BLEU	Ranking Accuracy BLEU Q2  Table 3: Detailed performance using all features (English-Spanish).
true	P10-1063.pdf#8.5	WMT 2014 EN-FR, BLEU	MAE vBLEU∆ [ 4 ] Q1 − 4  Table 3: Detailed performance using all features (English-Spanish).
true	P10-1063.pdf#56%	WMT 2014 EN-FR, BLEU	Ranking Accuracy BLEU Q4  Table 3: Detailed performance using all features (English-Spanish).
true	P10-1063.pdf#+5.9	WMT 2014 EN-FR, BLEU	Translation Accuracy vBLEU∆ [ 4 ] Q1 − 4  Table 3: Detailed performance using all features (English-Spanish).
true	P10-1063.pdf#45%	WMT 2014 EN-FR, BLEU	Ranking Accuracy BLEU rAcc  Table 3: Detailed performance using all features (English-Spanish).
true	P10-1063.pdf#37%	WMT 2014 EN-FR, BLEU	Ranking Accuracy BLEU Q3  Table 3: Detailed performance using all features (English-Spanish).
true	P10-1063.pdf#52%	WMT 2014 EN-FR, BLEU	Ranking Accuracy BLEU Q1  Table 3: Detailed performance using all features (English-Spanish).
true	P10-1063.pdf#1.6	WMT 2014 EN-FR, BLEU	" Time - constrained " condition Table 5 TE  Table 5:  "Time-constrained" performance  (English-Spanish).
true	P10-1063.pdf#+5.8	WMT 2014 EN-FR, BLEU	" Time - constrained " condition Table 5 vBLEU∆ [ 4 ]  Table 5:  "Time-constrained" performance  (English-Spanish).
true	P10-1063.pdf#8.6	WMT 2014 EN-FR, BLEU	" Time - constrained " condition Table 5 MAE  Table 5:  "Time-constrained" performance  (English-Spanish).
true	P12-1027.pdf#94.8	Chinese Treebank 6, F1	CWS F - score  Table 2: Incremental evaluations, by incrementally adding new features (word features and high dimensional edge  features), new word detection, and ADF training (replacing SGD training with ADF training). Number of passes is  decided by empirical convergence of the training methods.
true	P12-1027.pdf#95.4	Chinese Treebank 6, F1	CWS F - score  Table 2: Incremental evaluations, by incrementally adding new features (word features and high dimensional edge  features), new word detection, and ADF training (replacing SGD training with ADF training). Number of passes is  decided by empirical convergence of the training methods.
true	P12-1027.pdf#97.4	Chinese Treebank 6, F1	CWS F - score  Table 2: Incremental evaluations, by incrementally adding new features (word features and high dimensional edge  features), new word detection, and ADF training (replacing SGD training with ADF training). Number of passes is  decided by empirical convergence of the training methods.
true	P12-1027.pdf#3	Chinese Treebank 6, F1	
true	P12-1027.pdf#6	Chinese Treebank 6, F1	
true	P12-1027.pdf#3	Chinese Treebank 6, F1	
true	P12-1027.pdf#6	Chinese Treebank 6, F1	
true	P12-1027.pdf#6	Chinese Treebank 6, F1	
true	P12-1027.pdf#4	Chinese Treebank 6, F1	
true	P12-1027.pdf#4	Chinese Treebank 6, F1	
true	P12-1027.pdf#4	Chinese Treebank 6, F1	
true	P12-1027.pdf#6	Chinese Treebank 6, F1	
true	P12-1027.pdf#3	Chinese Treebank 6, F1	
true	P12-1027.pdf#6	Chinese Treebank 6, F1	
true	P12-1027.pdf#6	Chinese Treebank 6, F1	
true	P12-1027.pdf#95.4	Chinese Treebank 6, F1	F - score  Table 3: Comparing our method with the state-of-the-art CWS systems.
true	P12-1027.pdf#97.4	Chinese Treebank 6, F1	F - score  Table 3: Comparing our method with the state-of-the-art CWS systems.
true	N12-1023.pdf#49.8(0.3)	Text8, Number of params	AR→EN avg MT08 NW  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#55.6(0.1)	Text8, Number of params	AR→EN avg MT05  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#24.6(0.0)	Text8, Number of params	UR→EN Method MT09  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#49.8	Text8, Number of params	AR→EN avg MT08 NW  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#24.6	Text8, Number of params	UR→EN Method MT09  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#34.7	Text8, Number of params	ZH→EN Method MT05  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#32.6(0.2)	Text8, Number of params	AR→EN avg MT08 WB  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#36.8	Text8, Number of params	AR→EN avg  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#24.5(0.1)	Text8, Number of params	UR→EN Method MT08 *  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#36.4	Text8, Number of params	ZH→EN Method MT02  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#24.5	Text8, Number of params	UR→EN Method MT08 *  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#40.9	Text8, Number of params	Method ZH→EN Tune  Table 2: %BLEU with large feature sets.
true	N12-1023.pdf#34.3	Text8, Number of params	Method ZH→EN MT05  Table 2: %BLEU with large feature sets.
true	N12-1023.pdf#29.4	Text8, Number of params	Method UR→EN Tune  Table 2: %BLEU with large feature sets.
true	N12-1023.pdf#36.2	Text8, Number of params	Method ZH→EN MT02  Table 2: %BLEU with large feature sets.
true	N12-1023.pdf#24.6	Text8, Number of params	Method UR→EN MT09  Table 2: %BLEU with large feature sets.
true	N12-1023.pdf#24.2	Text8, Number of params	Method UR→EN MT08 *  Table 2: %BLEU with large feature sets.
true	N12-1023.pdf#49.8(0.3)	Text8, Bit per Character (BPC)	AR→EN avg MT08 NW  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#55.6(0.1)	Text8, Bit per Character (BPC)	AR→EN avg MT05  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#24.6(0.0)	Text8, Bit per Character (BPC)	UR→EN Method MT09  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#49.8	Text8, Bit per Character (BPC)	AR→EN avg MT08 NW  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#24.6	Text8, Bit per Character (BPC)	UR→EN Method MT09  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#34.7	Text8, Bit per Character (BPC)	ZH→EN Method MT05  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#32.6(0.2)	Text8, Bit per Character (BPC)	AR→EN avg MT08 WB  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#36.8	Text8, Bit per Character (BPC)	AR→EN avg  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#24.5(0.1)	Text8, Bit per Character (BPC)	UR→EN Method MT08 *  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#36.4	Text8, Bit per Character (BPC)	ZH→EN Method MT02  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#24.5	Text8, Bit per Character (BPC)	UR→EN Method MT08 *  Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization  (MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by  standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).  The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT  and PRO and 7 for RAMPION.
true	N12-1023.pdf#40.9	Text8, Bit per Character (BPC)	Method ZH→EN Tune  Table 2: %BLEU with large feature sets.
true	N12-1023.pdf#34.3	Text8, Bit per Character (BPC)	Method ZH→EN MT05  Table 2: %BLEU with large feature sets.
true	N12-1023.pdf#29.4	Text8, Bit per Character (BPC)	Method UR→EN Tune  Table 2: %BLEU with large feature sets.
true	N12-1023.pdf#36.2	Text8, Bit per Character (BPC)	Method ZH→EN MT02  Table 2: %BLEU with large feature sets.
true	N12-1023.pdf#24.6	Text8, Bit per Character (BPC)	Method UR→EN MT09  Table 2: %BLEU with large feature sets.
true	N12-1023.pdf#24.2	Text8, Bit per Character (BPC)	Method UR→EN MT08 *  Table 2: %BLEU with large feature sets.
true	P11-1156.pdf#91.44	benchmark Vietnamese dependency treebank VnDT, UAS	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#92.89	benchmark Vietnamese dependency treebank VnDT, UAS	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#92.47	benchmark Vietnamese dependency treebank VnDT, UAS	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#91.39	benchmark Vietnamese dependency treebank VnDT, UAS	+V1 - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#90.94	benchmark Vietnamese dependency treebank VnDT, UAS	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#92.38	benchmark Vietnamese dependency treebank VnDT, UAS	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#91.46	benchmark Vietnamese dependency treebank VnDT, UAS	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#91.60	benchmark Vietnamese dependency treebank VnDT, UAS	+V1  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#91.60	benchmark Vietnamese dependency treebank VnDT, UAS	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#92.64	benchmark Vietnamese dependency treebank VnDT, UAS	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#90.98	benchmark Vietnamese dependency treebank VnDT, UAS	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#91.83	benchmark Vietnamese dependency treebank VnDT, UAS	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#92.16	benchmark Vietnamese dependency treebank VnDT, UAS	+V1  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#90.69	benchmark Vietnamese dependency treebank VnDT, UAS	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#92.16	benchmark Vietnamese dependency treebank VnDT, UAS	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#90.03	benchmark Vietnamese dependency treebank VnDT, UAS	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#90.15	benchmark Vietnamese dependency treebank VnDT, UAS	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#90.91	benchmark Vietnamese dependency treebank VnDT, UAS	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#91.39	benchmark Vietnamese dependency treebank VnDT, UAS	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#92.87	benchmark Vietnamese dependency treebank VnDT, UAS	- are mildly significant using the Z - test of UAS  Table 4: Comparison of our final results with other best- performing systems on the whole Section 23. Type  D, C and S denote discriminative, combined and semi- supervised systems, respectively.  † These papers were  not directly reported the results on this data set, we im- plemented the experiments in this paper.
true	P11-1156.pdf#93.5	benchmark Vietnamese dependency treebank VnDT, UAS	D are mildly significant using the Z - test of UAS  Table 4: Comparison of our final results with other best- performing systems on the whole Section 23. Type  D, C and S denote discriminative, combined and semi- supervised systems, respectively.  † These papers were  not directly reported the results on this data set, we im- plemented the experiments in this paper.
true	P11-1156.pdf#93.79	benchmark Vietnamese dependency treebank VnDT, UAS	- are mildly significant using the Z - test of UAS  Table 4: Comparison of our final results with other best- performing systems on the whole Section 23. Type  D, C and S denote discriminative, combined and semi- supervised systems, respectively.  † These papers were  not directly reported the results on this data set, we im- plemented the experiments in this paper.
true	P11-1156.pdf#92.64	benchmark Vietnamese dependency treebank VnDT, UAS	- are mildly significant using the Z - test of UAS  Table 4: Comparison of our final results with other best- performing systems on the whole Section 23. Type  D, C and S denote discriminative, combined and semi- supervised systems, respectively.  † These papers were  not directly reported the results on this data set, we im- plemented the experiments in this paper.
true	P11-1156.pdf#3.4B	benchmark Vietnamese dependency treebank VnDT, UAS	
true	P11-1156.pdf#3.7B	benchmark Vietnamese dependency treebank VnDT, UAS	
true	P11-1156.pdf#3.2B	benchmark Vietnamese dependency treebank VnDT, UAS	
true	P11-1156.pdf#91.44	Penn Treebank, UAS	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#92.89	Penn Treebank, UAS	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#92.47	Penn Treebank, UAS	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#91.39	Penn Treebank, UAS	+V1 - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#90.94	Penn Treebank, UAS	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#92.38	Penn Treebank, UAS	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#91.46	Penn Treebank, UAS	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#91.60	Penn Treebank, UAS	+V1  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#91.60	Penn Treebank, UAS	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#92.64	Penn Treebank, UAS	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#90.98	Penn Treebank, UAS	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#91.83	Penn Treebank, UAS	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#92.16	Penn Treebank, UAS	+V1  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#90.69	Penn Treebank, UAS	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#92.16	Penn Treebank, UAS	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#90.03	Penn Treebank, UAS	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#90.15	Penn Treebank, UAS	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#90.91	Penn Treebank, UAS	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#91.39	Penn Treebank, UAS	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#92.87	Penn Treebank, UAS	- are mildly significant using the Z - test of UAS  Table 4: Comparison of our final results with other best- performing systems on the whole Section 23. Type  D, C and S denote discriminative, combined and semi- supervised systems, respectively.  † These papers were  not directly reported the results on this data set, we im- plemented the experiments in this paper.
true	P11-1156.pdf#93.5	Penn Treebank, UAS	D are mildly significant using the Z - test of UAS  Table 4: Comparison of our final results with other best- performing systems on the whole Section 23. Type  D, C and S denote discriminative, combined and semi- supervised systems, respectively.  † These papers were  not directly reported the results on this data set, we im- plemented the experiments in this paper.
true	P11-1156.pdf#93.79	Penn Treebank, UAS	- are mildly significant using the Z - test of UAS  Table 4: Comparison of our final results with other best- performing systems on the whole Section 23. Type  D, C and S denote discriminative, combined and semi- supervised systems, respectively.  † These papers were  not directly reported the results on this data set, we im- plemented the experiments in this paper.
true	P11-1156.pdf#92.64	Penn Treebank, UAS	- are mildly significant using the Z - test of UAS  Table 4: Comparison of our final results with other best- performing systems on the whole Section 23. Type  D, C and S denote discriminative, combined and semi- supervised systems, respectively.  † These papers were  not directly reported the results on this data set, we im- plemented the experiments in this paper.
true	P11-1156.pdf#3.4B	Penn Treebank, UAS	
true	P11-1156.pdf#3.7B	Penn Treebank, UAS	
true	P11-1156.pdf#3.2B	Penn Treebank, UAS	
true	P11-1156.pdf#91.44	New York Times Corpus, P@10%	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#92.89	New York Times Corpus, P@10%	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#92.47	New York Times Corpus, P@10%	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#91.39	New York Times Corpus, P@10%	+V1 - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#90.94	New York Times Corpus, P@10%	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#92.38	New York Times Corpus, P@10%	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#91.46	New York Times Corpus, P@10%	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#91.60	New York Times Corpus, P@10%	+V1  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#91.60	New York Times Corpus, P@10%	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#92.64	New York Times Corpus, P@10%	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#90.98	New York Times Corpus, P@10%	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#91.83	New York Times Corpus, P@10%	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#92.16	New York Times Corpus, P@10%	+V1  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#90.69	New York Times Corpus, P@10%	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#92.16	New York Times Corpus, P@10%	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#90.03	New York Times Corpus, P@10%	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#90.15	New York Times Corpus, P@10%	+hits  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#90.91	New York Times Corpus, P@10%	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#91.39	New York Times Corpus, P@10%	+hits - L  Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:  dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from  the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are  scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
true	P11-1156.pdf#92.87	New York Times Corpus, P@10%	- are mildly significant using the Z - test of UAS  Table 4: Comparison of our final results with other best- performing systems on the whole Section 23. Type  D, C and S denote discriminative, combined and semi- supervised systems, respectively.  † These papers were  not directly reported the results on this data set, we im- plemented the experiments in this paper.
true	P11-1156.pdf#93.5	New York Times Corpus, P@10%	D are mildly significant using the Z - test of UAS  Table 4: Comparison of our final results with other best- performing systems on the whole Section 23. Type  D, C and S denote discriminative, combined and semi- supervised systems, respectively.  † These papers were  not directly reported the results on this data set, we im- plemented the experiments in this paper.
true	P11-1156.pdf#93.79	New York Times Corpus, P@10%	- are mildly significant using the Z - test of UAS  Table 4: Comparison of our final results with other best- performing systems on the whole Section 23. Type  D, C and S denote discriminative, combined and semi- supervised systems, respectively.  † These papers were  not directly reported the results on this data set, we im- plemented the experiments in this paper.
true	P11-1156.pdf#92.64	New York Times Corpus, P@10%	- are mildly significant using the Z - test of UAS  Table 4: Comparison of our final results with other best- performing systems on the whole Section 23. Type  D, C and S denote discriminative, combined and semi- supervised systems, respectively.  † These papers were  not directly reported the results on this data set, we im- plemented the experiments in this paper.
true	P11-1156.pdf#3.4B	New York Times Corpus, P@10%	
true	P11-1156.pdf#3.7B	New York Times Corpus, P@10%	
true	P11-1156.pdf#3.2B	New York Times Corpus, P@10%	
true	D14-1079.pdf#0.456	SemEval 2015, F1	Frobenius outer ( 0 . 350 ) , NWE  Table 3: Spearman ρ correlations of models with  human judgements for the word sense disam- biguation task. The best result (NWE Copy ob- ject) outperforms the nearest co-occurrence-based  competitor (KS14 Frobenius outer) with a statisti- cally significant difference (p < 0.05, t-test).
true	D14-1079.pdf#0.732	SemEval 2015, F1	KS14  Table 4: Results for sentence similarity. There  is no statistically significant difference between  KS14 addition and NWE addition (the second best  result).
true	D14-1079.pdf#0.53	SemEval 2015, F1	Neural word embeddings Unlemmatized F - Score  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.66	SemEval 2015, F1	Co - occurrence KS14 Accuracy  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.80	SemEval 2015, F1	Co - occurrence KS14 F - Score  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.73	SemEval 2015, F1	Neural word embeddings Unlemmatized Accuracy  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.82	SemEval 2015, F1	Neural word embeddings Unlemmatized F - Score  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.60	SemEval 2015, F1	Neural word embeddings Unlemmatized F - Score  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.58	SemEval 2015, F1	Neural word embeddings Unlemmatized Accuracy  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.63	SemEval 2015, F1	Neural word embeddings Unlemmatized Accuracy  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.456	SemEval 2013, F1	Frobenius outer ( 0 . 350 ) , NWE  Table 3: Spearman ρ correlations of models with  human judgements for the word sense disam- biguation task. The best result (NWE Copy ob- ject) outperforms the nearest co-occurrence-based  competitor (KS14 Frobenius outer) with a statisti- cally significant difference (p < 0.05, t-test).
true	D14-1079.pdf#0.732	SemEval 2013, F1	KS14  Table 4: Results for sentence similarity. There  is no statistically significant difference between  KS14 addition and NWE addition (the second best  result).
true	D14-1079.pdf#0.53	SemEval 2013, F1	Neural word embeddings Unlemmatized F - Score  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.66	SemEval 2013, F1	Co - occurrence KS14 Accuracy  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.80	SemEval 2013, F1	Co - occurrence KS14 F - Score  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.73	SemEval 2013, F1	Neural word embeddings Unlemmatized Accuracy  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.82	SemEval 2013, F1	Neural word embeddings Unlemmatized F - Score  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.60	SemEval 2013, F1	Neural word embeddings Unlemmatized F - Score  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.58	SemEval 2013, F1	Neural word embeddings Unlemmatized Accuracy  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.63	SemEval 2013, F1	Neural word embeddings Unlemmatized Accuracy  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.456	SemEval 2007, F1	Frobenius outer ( 0 . 350 ) , NWE  Table 3: Spearman ρ correlations of models with  human judgements for the word sense disam- biguation task. The best result (NWE Copy ob- ject) outperforms the nearest co-occurrence-based  competitor (KS14 Frobenius outer) with a statisti- cally significant difference (p < 0.05, t-test).
true	D14-1079.pdf#0.732	SemEval 2007, F1	KS14  Table 4: Results for sentence similarity. There  is no statistically significant difference between  KS14 addition and NWE addition (the second best  result).
true	D14-1079.pdf#0.53	SemEval 2007, F1	Neural word embeddings Unlemmatized F - Score  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.66	SemEval 2007, F1	Co - occurrence KS14 Accuracy  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.80	SemEval 2007, F1	Co - occurrence KS14 F - Score  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.73	SemEval 2007, F1	Neural word embeddings Unlemmatized Accuracy  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.82	SemEval 2007, F1	Neural word embeddings Unlemmatized F - Score  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.60	SemEval 2007, F1	Neural word embeddings Unlemmatized F - Score  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.58	SemEval 2007, F1	Neural word embeddings Unlemmatized Accuracy  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	D14-1079.pdf#0.63	SemEval 2007, F1	Neural word embeddings Unlemmatized Accuracy  Table 5: Results for paraphrase detection (MSR) and dialog act tagging (SWDA) tasks. All top results  significantly outperform corresponding nearest competitors (for accuracy): p < 0.05, χ 2 test.
true	P14-1080.pdf#32.63*	WMT 2014 EN-DE, BLEU	24 . 58 * * CWMT08 ' s NIST05 Dev . 34 . 58 * *  Table 3. BLEU scores of the testing sets with different integrating strategies
true	P13-2137.pdf#68.9	VLSP 2013 POS tagging shared task, Accuracy	Accuracy ( % ) A  Table 4: Results on synonym choice task
true	P13-2137.pdf#70.0	VLSP 2013 POS tagging shared task, Accuracy	Accuracy ( % ) N  Table 4: Results on synonym choice task
true	P13-2137.pdf#63.2	VLSP 2013 POS tagging shared task, Accuracy	Accuracy ( % ) V  Table 4: Results on synonym choice task
true	D10-1003.pdf#90.43	Penn Treebank, F1	development set - length ≤ 40 LR test set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#41.25	Penn Treebank, F1	development set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#67.55	Penn Treebank, F1	development set - all sentences 0CB test set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#0.85	Penn Treebank, F1	development set - all sentences CB test set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#73.45	Penn Treebank, F1	development set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#91.25	Penn Treebank, F1	development set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#90.64	Penn Treebank, F1	development set - length ≤ 40 LR test set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#90.62	Penn Treebank, F1	development set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#0.74	Penn Treebank, F1	development set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#90.08	Penn Treebank, F1	development set - all sentences F1 test set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#0.72	Penn Treebank, F1	development set - length ≤ 40 CB test set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#90.20	Penn Treebank, F1	development set - all sentences LP test set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#89.97	Penn Treebank, F1	development set - all sentences LR test set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#70.47	Penn Treebank, F1	development set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#90.47	Penn Treebank, F1	development set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#38.88	Penn Treebank, F1	development set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#39.02	Penn Treebank, F1	development set - length ≤ 40 Exact test set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#90.38	Penn Treebank, F1	development set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#69.84	Penn Treebank, F1	development set - length ≤ 40 0CB test set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#90.78	Penn Treebank, F1	development set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#90.98	Penn Treebank, F1	development set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#0.74	Penn Treebank, F1	development set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#0.61	Penn Treebank, F1	development set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#90.54	Penn Treebank, F1	development set - length ≤ 40 F1 test set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#36.84	Penn Treebank, F1	development set - all sentences Exact test set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#0.72	Penn Treebank, F1	CB test set - length ≤ 40  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#90.43	Penn Treebank, F1	LR test set - length ≤ 40  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#90.54	Penn Treebank, F1	F1 test set - length ≤ 40  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#90.20	Penn Treebank, F1	LP test set - all sentences  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#69.84	Penn Treebank, F1	0CB test set - length ≤ 40  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#89.97	Penn Treebank, F1	LR test set - all sentences  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#39.02	Penn Treebank, F1	Exact test set - length ≤ 40  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#90.08	Penn Treebank, F1	F1 test set - all sentences  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#0.85	Penn Treebank, F1	CB test set - all sentences  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#36.84	Penn Treebank, F1	Exact test set - all sentences  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#90.64	Penn Treebank, F1	LR test set - length ≤ 40  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#67.55	Penn Treebank, F1	0CB test set - all sentences  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#0.4(3.8%)	Penn Treebank, F1	Imp . ( rel . )  Table 8: A comparison of parsers specialized to exploit  intra-or extra-sentential syntactic parallelism on section  23 in terms of the generative baseline they compare them- selves against, the best F1 their non-baseline models  achieve, and the absolute and relative improvements.
true	D10-1003.pdf#89.9	Penn Treebank, F1	Best  Table 8: A comparison of parsers specialized to exploit  intra-or extra-sentential syntactic parallelism on section  23 in terms of the generative baseline they compare them- selves against, the best F1 their non-baseline models  achieve, and the absolute and relative improvements.
true	D10-1003.pdf#89.5	Penn Treebank, F1	Baseline  Table 8: A comparison of parsers specialized to exploit  intra-or extra-sentential syntactic parallelism on section  23 in terms of the generative baseline they compare them- selves against, the best F1 their non-baseline models  achieve, and the absolute and relative improvements.
true	D10-1003.pdf#90.43	Penn Treebank, Number of params	development set - length ≤ 40 LR test set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#41.25	Penn Treebank, Number of params	development set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#67.55	Penn Treebank, Number of params	development set - all sentences 0CB test set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#0.85	Penn Treebank, Number of params	development set - all sentences CB test set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#73.45	Penn Treebank, Number of params	development set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#91.25	Penn Treebank, Number of params	development set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#90.64	Penn Treebank, Number of params	development set - length ≤ 40 LR test set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#90.62	Penn Treebank, Number of params	development set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#0.74	Penn Treebank, Number of params	development set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#90.08	Penn Treebank, Number of params	development set - all sentences F1 test set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#0.72	Penn Treebank, Number of params	development set - length ≤ 40 CB test set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#90.20	Penn Treebank, Number of params	development set - all sentences LP test set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#89.97	Penn Treebank, Number of params	development set - all sentences LR test set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#70.47	Penn Treebank, Number of params	development set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#90.47	Penn Treebank, Number of params	development set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#38.88	Penn Treebank, Number of params	development set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#39.02	Penn Treebank, Number of params	development set - length ≤ 40 Exact test set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#90.38	Penn Treebank, Number of params	development set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#69.84	Penn Treebank, Number of params	development set - length ≤ 40 0CB test set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#90.78	Penn Treebank, Number of params	development set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#90.98	Penn Treebank, Number of params	development set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#0.74	Penn Treebank, Number of params	development set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#0.61	Penn Treebank, Number of params	development set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#90.54	Penn Treebank, Number of params	development set - length ≤ 40 F1 test set - length ≤ 40  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#36.84	Penn Treebank, Number of params	development set - all sentences Exact test set - all sentences  Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative  is the generative baseline of
true	D10-1003.pdf#0.72	Penn Treebank, Number of params	CB test set - length ≤ 40  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#90.43	Penn Treebank, Number of params	LR test set - length ≤ 40  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#90.54	Penn Treebank, Number of params	F1 test set - length ≤ 40  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#90.20	Penn Treebank, Number of params	LP test set - all sentences  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#69.84	Penn Treebank, Number of params	0CB test set - length ≤ 40  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#89.97	Penn Treebank, Number of params	LR test set - all sentences  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#39.02	Penn Treebank, Number of params	Exact test set - length ≤ 40  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#90.08	Penn Treebank, Number of params	F1 test set - all sentences  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#0.85	Penn Treebank, Number of params	CB test set - all sentences  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#36.84	Penn Treebank, Number of params	Exact test set - all sentences  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#90.64	Penn Treebank, Number of params	LR test set - length ≤ 40  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#67.55	Penn Treebank, Number of params	0CB test set - all sentences  Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
true	D10-1003.pdf#0.4(3.8%)	Penn Treebank, Number of params	Imp . ( rel . )  Table 8: A comparison of parsers specialized to exploit  intra-or extra-sentential syntactic parallelism on section  23 in terms of the generative baseline they compare them- selves against, the best F1 their non-baseline models  achieve, and the absolute and relative improvements.
true	D10-1003.pdf#89.9	Penn Treebank, Number of params	Best  Table 8: A comparison of parsers specialized to exploit  intra-or extra-sentential syntactic parallelism on section  23 in terms of the generative baseline they compare them- selves against, the best F1 their non-baseline models  achieve, and the absolute and relative improvements.
true	D10-1003.pdf#89.5	Penn Treebank, Number of params	Baseline  Table 8: A comparison of parsers specialized to exploit  intra-or extra-sentential syntactic parallelism on section  23 in terms of the generative baseline they compare them- selves against, the best F1 their non-baseline models  achieve, and the absolute and relative improvements.
true	D12-1092.pdf#63.5	Chinese Treebank 6, F1	without filtering P on the development Trigger Identification R ( % )  Table 8. Contribution to Chinese triggers identification  (incremental)
true	D12-1092.pdf#79.3	Chinese Treebank 6, F1	without filtering P Figure 3 . Effect of threshold θ Trigger Identification P ( % )  Table 8. Contribution to Chinese triggers identification  (incremental)
true	D12-1092.pdf#70.5	Chinese Treebank 6, F1	without filtering P on the development Trigger Identification F  Table 8. Contribution to Chinese triggers identification  (incremental)
true	D12-1092.pdf#79.3	Chinese Treebank 6, F1	F  Table 9: Overall contribution to Chinese event extraction
true	D12-1092.pdf#56.9	Chinese Treebank 6, F1	F  Table 9: Overall contribution to Chinese event extraction
true	D12-1092.pdf#61.6	Chinese Treebank 6, F1	F  Table 9: Overall contribution to Chinese event extraction
true	D12-1092.pdf#45.8	Chinese Treebank 6, F1	F  Table 9: Overall contribution to Chinese event extraction
true	D12-1092.pdf#66.9	Chinese Treebank 6, F1	F  Table 9: Overall contribution to Chinese event extraction
true	D12-1092.pdf#50.8	Chinese Treebank 6, F1	F  Table 9: Overall contribution to Chinese event extraction
true	D12-1092.pdf#50.2	Chinese Treebank 6, F1	F  Table 9: Overall contribution to Chinese event extraction
true	D12-1092.pdf#60.2	Chinese Treebank 6, F1	F  Table 9: Overall contribution to Chinese event extraction
true	D12-1092.pdf#63.5	Chinese Treebank 6, F1	F  Table 9: Overall contribution to Chinese event extraction
true	D12-1092.pdf#55.3	Chinese Treebank 6, F1	F  Table 9: Overall contribution to Chinese event extraction
true	D12-1092.pdf#75.2	Chinese Treebank 6, F1	F  Table 9: Overall contribution to Chinese event extraction
true	D12-1092.pdf#70.5	Chinese Treebank 6, F1	F  Table 9: Overall contribution to Chinese event extraction
true	D11-1036.pdf#0.8782	Penn Treebank, LAS	Functional Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9492†	Penn Treebank, LAS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8826	Penn Treebank, LAS	Lexical Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9476	Penn Treebank, LAS	Functional Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9493	Penn Treebank, LAS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8952	Penn Treebank, LAS	Old LTH Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8709	Penn Treebank, LAS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8491	Penn Treebank, LAS	Lexical Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9226	Penn Treebank, LAS	Functional Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8471	Penn Treebank, LAS	Old LTH Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9173	Penn Treebank, LAS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9513	Penn Treebank, LAS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9550	Penn Treebank, LAS	Old LTH Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9208	Penn Treebank, LAS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8833	Penn Treebank, LAS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9258	Penn Treebank, LAS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8978	Penn Treebank, LAS	Functional Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8991	Penn Treebank, LAS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9483	Penn Treebank, LAS	Lexical Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9479	Penn Treebank, LAS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9281	Penn Treebank, LAS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9479	Penn Treebank, LAS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9289	Penn Treebank, LAS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9533	Penn Treebank, LAS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9474†	Penn Treebank, LAS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9298	Penn Treebank, LAS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8709	Penn Treebank, LAS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9249	Penn Treebank, LAS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8991	Penn Treebank, LAS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9224	Penn Treebank, LAS	Old LTH Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9208	Penn Treebank, LAS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9533	Penn Treebank, LAS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9483	Penn Treebank, LAS	Lexical Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8826	Penn Treebank, LAS	Lexical Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8991	Penn Treebank, LAS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9474†	Penn Treebank, LAS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9208	Penn Treebank, LAS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9550	Penn Treebank, LAS	Old LTH Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8978	Penn Treebank, LAS	Functional Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8471	Penn Treebank, LAS	Old LTH Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9479	Penn Treebank, LAS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9281	Penn Treebank, LAS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9298	Penn Treebank, LAS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9289	Penn Treebank, LAS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9249	Penn Treebank, LAS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9493	Penn Treebank, LAS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8952	Penn Treebank, LAS	Old LTH Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9208	Penn Treebank, LAS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8782	Penn Treebank, LAS	Functional Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9476	Penn Treebank, LAS	Functional Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9173	Penn Treebank, LAS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8709	Penn Treebank, LAS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9258	Penn Treebank, LAS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8833	Penn Treebank, LAS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9513	Penn Treebank, LAS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8991	Penn Treebank, LAS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8491	Penn Treebank, LAS	Lexical Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9492†	Penn Treebank, LAS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9224	Penn Treebank, LAS	Old LTH Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9226	Penn Treebank, LAS	Functional Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8709	Penn Treebank, LAS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9479	Penn Treebank, LAS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8782	Penn Treebank, POS	Functional Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9492†	Penn Treebank, POS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8826	Penn Treebank, POS	Lexical Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9476	Penn Treebank, POS	Functional Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9493	Penn Treebank, POS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8952	Penn Treebank, POS	Old LTH Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8709	Penn Treebank, POS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8491	Penn Treebank, POS	Lexical Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9226	Penn Treebank, POS	Functional Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8471	Penn Treebank, POS	Old LTH Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9173	Penn Treebank, POS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9513	Penn Treebank, POS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9550	Penn Treebank, POS	Old LTH Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9208	Penn Treebank, POS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8833	Penn Treebank, POS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9258	Penn Treebank, POS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8978	Penn Treebank, POS	Functional Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8991	Penn Treebank, POS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9483	Penn Treebank, POS	Lexical Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9479	Penn Treebank, POS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9281	Penn Treebank, POS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9479	Penn Treebank, POS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9289	Penn Treebank, POS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9533	Penn Treebank, POS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9474†	Penn Treebank, POS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9298	Penn Treebank, POS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8709	Penn Treebank, POS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9249	Penn Treebank, POS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8991	Penn Treebank, POS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9224	Penn Treebank, POS	Old LTH Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9208	Penn Treebank, POS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9533	Penn Treebank, POS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9483	Penn Treebank, POS	Lexical Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8826	Penn Treebank, POS	Lexical Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8991	Penn Treebank, POS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9474†	Penn Treebank, POS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9208	Penn Treebank, POS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9550	Penn Treebank, POS	Old LTH Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8978	Penn Treebank, POS	Functional Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8471	Penn Treebank, POS	Old LTH Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9479	Penn Treebank, POS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9281	Penn Treebank, POS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9298	Penn Treebank, POS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9289	Penn Treebank, POS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9249	Penn Treebank, POS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9493	Penn Treebank, POS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8952	Penn Treebank, POS	Old LTH Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9208	Penn Treebank, POS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8782	Penn Treebank, POS	Functional Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9476	Penn Treebank, POS	Functional Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9173	Penn Treebank, POS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8709	Penn Treebank, POS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9258	Penn Treebank, POS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8833	Penn Treebank, POS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9513	Penn Treebank, POS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8991	Penn Treebank, POS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8491	Penn Treebank, POS	Lexical Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9492†	Penn Treebank, POS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9224	Penn Treebank, POS	Old LTH Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9226	Penn Treebank, POS	Functional Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8709	Penn Treebank, POS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9479	Penn Treebank, POS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8782	Penn Treebank, UAS	Functional Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9492†	Penn Treebank, UAS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8826	Penn Treebank, UAS	Lexical Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9476	Penn Treebank, UAS	Functional Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9493	Penn Treebank, UAS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8952	Penn Treebank, UAS	Old LTH Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8709	Penn Treebank, UAS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8491	Penn Treebank, UAS	Lexical Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9226	Penn Treebank, UAS	Functional Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8471	Penn Treebank, UAS	Old LTH Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9173	Penn Treebank, UAS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9513	Penn Treebank, UAS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9550	Penn Treebank, UAS	Old LTH Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9208	Penn Treebank, UAS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8833	Penn Treebank, UAS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9258	Penn Treebank, UAS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8978	Penn Treebank, UAS	Functional Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8991	Penn Treebank, UAS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9483	Penn Treebank, UAS	Lexical Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9479	Penn Treebank, UAS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9281	Penn Treebank, UAS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9479	Penn Treebank, UAS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9289	Penn Treebank, UAS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9533	Penn Treebank, UAS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9474†	Penn Treebank, UAS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9298	Penn Treebank, UAS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8709	Penn Treebank, UAS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9249	Penn Treebank, UAS	Default Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8991	Penn Treebank, UAS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9224	Penn Treebank, UAS	Old LTH Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9208	Penn Treebank, UAS	CoNLL07 Gold  Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes. We report stan- dard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results reported  in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9533	Penn Treebank, UAS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9483	Penn Treebank, UAS	Lexical Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8826	Penn Treebank, UAS	Lexical Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8991	Penn Treebank, UAS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9474†	Penn Treebank, UAS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9208	Penn Treebank, UAS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9550	Penn Treebank, UAS	Old LTH Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8978	Penn Treebank, UAS	Functional Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8471	Penn Treebank, UAS	Old LTH Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9479	Penn Treebank, UAS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9281	Penn Treebank, UAS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9298	Penn Treebank, UAS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9289	Penn Treebank, UAS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9249	Penn Treebank, UAS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9493	Penn Treebank, UAS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8952	Penn Treebank, UAS	Old LTH Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9208	Penn Treebank, UAS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8782	Penn Treebank, UAS	Functional Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9476	Penn Treebank, UAS	Functional Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9173	Penn Treebank, UAS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8709	Penn Treebank, UAS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9258	Penn Treebank, UAS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8833	Penn Treebank, UAS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9513	Penn Treebank, UAS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8991	Penn Treebank, UAS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8491	Penn Treebank, UAS	Lexical Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9492†	Penn Treebank, UAS	Default Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9224	Penn Treebank, UAS	Old LTH Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9226	Penn Treebank, UAS	Functional Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.8709	Penn Treebank, UAS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	D11-1036.pdf#0.9479	Penn Treebank, UAS	CoNLL07 Gold  Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes. We  report standard LAS scores and TEDEVAL global average metrics. Boldface results outperform the rest of the results  reported in the same row. The  † sign marks pairwise results where the difference is not statistically significant.
true	P15-1043.pdf#0.66	Senseval 2, F1	Lexrank  Table 5: Pyramid scores obtained by different content models for each topic along with average scores  for each model across all topics. For each topic as well as the average, the best performing method has  been highlighted with a  *  .
true	P15-1043.pdf#0.76*	Senseval 2, F1	HITSUM  Table 5: Pyramid scores obtained by different content models for each topic along with average scores  for each model across all topics. For each topic as well as the average, the best performing method has  been highlighted with a  *  .
true	P15-1043.pdf#0.69	Senseval 2, F1	TOPICSUM  Table 5: Pyramid scores obtained by different content models for each topic along with average scores  for each model across all topics. For each topic as well as the average, the best performing method has  been highlighted with a  *  .
true	P15-1043.pdf#0.72	Senseval 2, F1	C - Lexrank  Table 5: Pyramid scores obtained by different content models for each topic along with average scores  for each model across all topics. For each topic as well as the average, the best performing method has  been highlighted with a  *  .
true	P15-1043.pdf#0.66	SemEval 2013, F1	Lexrank  Table 5: Pyramid scores obtained by different content models for each topic along with average scores  for each model across all topics. For each topic as well as the average, the best performing method has  been highlighted with a  *  .
true	P15-1043.pdf#0.76*	SemEval 2013, F1	HITSUM  Table 5: Pyramid scores obtained by different content models for each topic along with average scores  for each model across all topics. For each topic as well as the average, the best performing method has  been highlighted with a  *  .
true	P15-1043.pdf#0.69	SemEval 2013, F1	TOPICSUM  Table 5: Pyramid scores obtained by different content models for each topic along with average scores  for each model across all topics. For each topic as well as the average, the best performing method has  been highlighted with a  *  .
true	P15-1043.pdf#0.72	SemEval 2013, F1	C - Lexrank  Table 5: Pyramid scores obtained by different content models for each topic along with average scores  for each model across all topics. For each topic as well as the average, the best performing method has  been highlighted with a  *  .
true	P15-1043.pdf#0.66	SemEval 2007, F1	Lexrank  Table 5: Pyramid scores obtained by different content models for each topic along with average scores  for each model across all topics. For each topic as well as the average, the best performing method has  been highlighted with a  *  .
true	P15-1043.pdf#0.76*	SemEval 2007, F1	HITSUM  Table 5: Pyramid scores obtained by different content models for each topic along with average scores  for each model across all topics. For each topic as well as the average, the best performing method has  been highlighted with a  *  .
true	P15-1043.pdf#0.69	SemEval 2007, F1	TOPICSUM  Table 5: Pyramid scores obtained by different content models for each topic along with average scores  for each model across all topics. For each topic as well as the average, the best performing method has  been highlighted with a  *  .
true	P15-1043.pdf#0.72	SemEval 2007, F1	C - Lexrank  Table 5: Pyramid scores obtained by different content models for each topic along with average scores  for each model across all topics. For each topic as well as the average, the best performing method has  been highlighted with a  *  .
true	P15-1043.pdf#0.66	SemEval 2015, F1	Lexrank  Table 5: Pyramid scores obtained by different content models for each topic along with average scores  for each model across all topics. For each topic as well as the average, the best performing method has  been highlighted with a  *  .
true	P15-1043.pdf#0.76*	SemEval 2015, F1	HITSUM  Table 5: Pyramid scores obtained by different content models for each topic along with average scores  for each model across all topics. For each topic as well as the average, the best performing method has  been highlighted with a  *  .
true	P15-1043.pdf#0.69	SemEval 2015, F1	TOPICSUM  Table 5: Pyramid scores obtained by different content models for each topic along with average scores  for each model across all topics. For each topic as well as the average, the best performing method has  been highlighted with a  *  .
true	P15-1043.pdf#0.72	SemEval 2015, F1	C - Lexrank  Table 5: Pyramid scores obtained by different content models for each topic along with average scores  for each model across all topics. For each topic as well as the average, the best performing method has  been highlighted with a  *  .
true	P15-1043.pdf#0.66	Senseval 3, F1	Lexrank  Table 5: Pyramid scores obtained by different content models for each topic along with average scores  for each model across all topics. For each topic as well as the average, the best performing method has  been highlighted with a  *  .
true	P15-1043.pdf#0.76*	Senseval 3, F1	HITSUM  Table 5: Pyramid scores obtained by different content models for each topic along with average scores  for each model across all topics. For each topic as well as the average, the best performing method has  been highlighted with a  *  .
true	P15-1043.pdf#0.69	Senseval 3, F1	TOPICSUM  Table 5: Pyramid scores obtained by different content models for each topic along with average scores  for each model across all topics. For each topic as well as the average, the best performing method has  been highlighted with a  *  .
true	P15-1043.pdf#0.72	Senseval 3, F1	C - Lexrank  Table 5: Pyramid scores obtained by different content models for each topic along with average scores  for each model across all topics. For each topic as well as the average, the best performing method has  been highlighted with a  *  .
true	D15-1298.pdf#62.40*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 1 : Results of ABSA P  Table 1: Results of ABSA
true	D15-1298.pdf#66.20	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 1 : Results of ABSA A  Table 1: Results of ABSA
true	D15-1298.pdf#66.20	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 1 : Results of ABSA R  Table 1: Results of ABSA
true	D15-1298.pdf#62.21*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 1 : Results of ABSA F  Table 1: Results of ABSA
true	D15-1298.pdf#62.21*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 1 : Results of ABSA F  Table 2: The Number of Correctly Identified As- pects in Subsets S1, S2 and S3
true	D15-1298.pdf#66.20	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 1 : Results of ABSA R  Table 2: The Number of Correctly Identified As- pects in Subsets S1, S2 and S3
true	D15-1298.pdf#62.40*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 1 : Results of ABSA P  Table 2: The Number of Correctly Identified As- pects in Subsets S1, S2 and S3
true	D15-1298.pdf#66.20	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 1 : Results of ABSA A  Table 2: The Number of Correctly Identified As- pects in Subsets S1, S2 and S3
true	D15-1298.pdf#62.40*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Table 1 : Results of ABSA P  Table 1: Results of ABSA
true	D15-1298.pdf#66.20	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Table 1 : Results of ABSA A  Table 1: Results of ABSA
true	D15-1298.pdf#66.20	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Table 1 : Results of ABSA R  Table 1: Results of ABSA
true	D15-1298.pdf#62.21*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Table 1 : Results of ABSA F  Table 1: Results of ABSA
true	D15-1298.pdf#62.21*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Table 1 : Results of ABSA F  Table 2: The Number of Correctly Identified As- pects in Subsets S1, S2 and S3
true	D15-1298.pdf#66.20	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Table 1 : Results of ABSA R  Table 2: The Number of Correctly Identified As- pects in Subsets S1, S2 and S3
true	D15-1298.pdf#62.40*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Table 1 : Results of ABSA P  Table 2: The Number of Correctly Identified As- pects in Subsets S1, S2 and S3
true	D15-1298.pdf#66.20	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Table 1 : Results of ABSA A  Table 2: The Number of Correctly Identified As- pects in Subsets S1, S2 and S3
true	P14-1065.pdf#.253	WMT 2014 EN-DE, BLEU	Tuned +DR +DR - LEX -  Table 4: Results on WMT12 at the segment- level: tuning with cross-validation on WMT12.  Kendall's Tau with human judgments.
true	P14-1065.pdf#+.061	WMT 2014 EN-DE, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.220	WMT 2014 EN-DE, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.232	WMT 2014 EN-DE, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.303	WMT 2014 EN-DE, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.207	WMT 2014 EN-DE, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.218	WMT 2014 EN-DE, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.296	WMT 2014 EN-DE, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.192	WMT 2014 EN-DE, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.242	WMT 2014 EN-DE, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.200	WMT 2014 EN-DE, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#+.037	WMT 2014 EN-DE, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.051	WMT 2014 EN-DE, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.302	WMT 2014 EN-DE, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.349	WMT 2014 EN-DE, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.274	WMT 2014 EN-DE, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.260	WMT 2014 EN-DE, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.239	WMT 2014 EN-DE, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.260	WMT 2014 EN-DE, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.226	WMT 2014 EN-DE, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.253	WMT 2014 EN-FR, BLEU	Tuned +DR +DR - LEX -  Table 4: Results on WMT12 at the segment- level: tuning with cross-validation on WMT12.  Kendall's Tau with human judgments.
true	P14-1065.pdf#+.061	WMT 2014 EN-FR, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.220	WMT 2014 EN-FR, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.232	WMT 2014 EN-FR, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.303	WMT 2014 EN-FR, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.207	WMT 2014 EN-FR, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.218	WMT 2014 EN-FR, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.296	WMT 2014 EN-FR, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.192	WMT 2014 EN-FR, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.242	WMT 2014 EN-FR, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.200	WMT 2014 EN-FR, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#+.037	WMT 2014 EN-FR, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.051	WMT 2014 EN-FR, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.302	WMT 2014 EN-FR, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.349	WMT 2014 EN-FR, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.274	WMT 2014 EN-FR, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.260	WMT 2014 EN-FR, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.239	WMT 2014 EN-FR, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.260	WMT 2014 EN-FR, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	P14-1065.pdf#.226	WMT 2014 EN-FR, BLEU	Tuned +DR +DR - LEX - -  Table 5: Results on WMT11 at the segment-level:  tuning on the entire WMT12. Kendall's Tau with  human judgments.
true	N13-1065.pdf#10	Text8, Number of params	20 , 000 Brown RCV1 Web - Sent Maximum word type size V Furthermore , the advantage of BJAC over 80 , 000 40 , 000  Table 2: Maximum word type size V max for each  partitioning method, for each corpus. For the larger  corpora, BJAC reduces V max by over 50% compared  to the baseline, and by 23% compared to Z&I.
true	N13-1065.pdf#80	Text8, Number of params	20 , 000 4369 24923 34754 max for each Furthermore , the advantage of BJAC over 80 , 000 40 , 000  Table 2: Maximum word type size V max for each  partitioning method, for each corpus. For the larger  corpora, BJAC reduces V max by over 50% compared  to the baseline, and by 23% compared to Z&I.
true	N13-1065.pdf#20	Text8, Number of params	20 , 000 Brown RCV1 Web - Sent Maximum word type size V Furthermore , the advantage of BJAC over 80 , 000 40 , 000  Table 2: Maximum word type size V max for each  partitioning method, for each corpus. For the larger  corpora, BJAC reduces V max by over 50% compared  to the baseline, and by 23% compared to Z&I.
true	N13-1065.pdf#30	Text8, Number of params	20 , 000 6368 49344 72626 Maximum word type size V Furthermore , the advantage of BJAC over 80 , 000 40 , 000  Table 2: Maximum word type size V max for each  partitioning method, for each corpus. For the larger  corpora, BJAC reduces V max by over 50% compared  to the baseline, and by 23% compared to Z&I.
true	N13-1065.pdf#50	Text8, Number of params	20 , 000 5714 32136 45989 Maximum word type size V Furthermore , the advantage of BJAC over 80 , 000 40 , 000  Table 2: Maximum word type size V max for each  partitioning method, for each corpus. For the larger  corpora, BJAC reduces V max by over 50% compared  to the baseline, and by 23% compared to Z&I.
true	N13-1065.pdf#90	Text8, Number of params	20 , 000 4369 24923 34754 max for each Furthermore , the advantage of BJAC over 80 , 000 40 , 000  Table 2: Maximum word type size V max for each  partitioning method, for each corpus. For the larger  corpora, BJAC reduces V max by over 50% compared  to the baseline, and by 23% compared to Z&I.
true	N13-1065.pdf#60	Text8, Number of params	20 , 000 5714 32136 45989 Maximum word type size V Furthermore , the advantage of BJAC over 80 , 000 40 , 000  Table 2: Maximum word type size V max for each  partitioning method, for each corpus. For the larger  corpora, BJAC reduces V max by over 50% compared  to the baseline, and by 23% compared to Z&I.
true	N13-1065.pdf#70	Text8, Number of params	20 , 000 5714 32136 45989 Maximum word type size V Furthermore , the advantage of BJAC over 80 , 000 40 , 000  Table 2: Maximum word type size V max for each  partitioning method, for each corpus. For the larger  corpora, BJAC reduces V max by over 50% compared  to the baseline, and by 23% compared to Z&I.
true	N13-1065.pdf#40	Text8, Number of params	20 , 000 6368 49344 72626 Maximum word type size V Furthermore , the advantage of BJAC over 80 , 000 40 , 000  Table 2: Maximum word type size V max for each  partitioning method, for each corpus. For the larger  corpora, BJAC reduces V max by over 50% compared  to the baseline, and by 23% compared to Z&I.
true	P15-1025.pdf#0.455‡	SearchQA, Unigram Acc	Yahoo data R - Prec  Table 2: Evaluation results on Yahoo data and Baidu data, where dim denotes the dimension of the  word embeddings. The bold formate indicates the best results for question retrieval.  † indicates that  the difference between the results of our proposed approach (Skip-gram + FV, M-NET + FV) and other  methods are mildly significant with p < 0.08 under a t-test;  ‡ indicates the comparisons are statistically  significant with p < 0.05.
true	P15-1025.pdf#0.475‡	SearchQA, Unigram Acc	MAP  Table 2: Evaluation results on Yahoo data and Baidu data, where dim denotes the dimension of the  word embeddings. The bold formate indicates the best results for question retrieval.  † indicates that  the difference between the results of our proposed approach (Skip-gram + FV, M-NET + FV) and other  methods are mildly significant with p < 0.08 under a t-test;  ‡ indicates the comparisons are statistically  significant with p < 0.05.
true	P15-1025.pdf#0.477‡	SearchQA, Unigram Acc	MRR  Table 2: Evaluation results on Yahoo data and Baidu data, where dim denotes the dimension of the  word embeddings. The bold formate indicates the best results for question retrieval.  † indicates that  the difference between the results of our proposed approach (Skip-gram + FV, M-NET + FV) and other  methods are mildly significant with p < 0.08 under a t-test;  ‡ indicates the comparisons are statistically  significant with p < 0.05.
true	P15-1025.pdf#0.643‡	SearchQA, Unigram Acc	MRR  Table 2: Evaluation results on Yahoo data and Baidu data, where dim denotes the dimension of the  word embeddings. The bold formate indicates the best results for question retrieval.  † indicates that  the difference between the results of our proposed approach (Skip-gram + FV, M-NET + FV) and other  methods are mildly significant with p < 0.08 under a t-test;  ‡ indicates the comparisons are statistically  significant with p < 0.05.
true	P15-1025.pdf#0.374‡	SearchQA, Unigram Acc	P@5  Table 2: Evaluation results on Yahoo data and Baidu data, where dim denotes the dimension of the  word embeddings. The bold formate indicates the best results for question retrieval.  † indicates that  the difference between the results of our proposed approach (Skip-gram + FV, M-NET + FV) and other  methods are mildly significant with p < 0.08 under a t-test;  ‡ indicates the comparisons are statistically  significant with p < 0.05.
true	P15-1025.pdf#0.571‡	SearchQA, Unigram Acc	MAP  Table 2: Evaluation results on Yahoo data and Baidu data, where dim denotes the dimension of the  word embeddings. The bold formate indicates the best results for question retrieval.  † indicates that  the difference between the results of our proposed approach (Skip-gram + FV, M-NET + FV) and other  methods are mildly significant with p < 0.08 under a t-test;  ‡ indicates the comparisons are statistically  significant with p < 0.05.
true	P15-1025.pdf#0.385‡	SearchQA, Unigram Acc	R - Prec  Table 2: Evaluation results on Yahoo data and Baidu data, where dim denotes the dimension of the  word embeddings. The bold formate indicates the best results for question retrieval.  † indicates that  the difference between the results of our proposed approach (Skip-gram + FV, M-NET + FV) and other  methods are mildly significant with p < 0.08 under a t-test;  ‡ indicates the comparisons are statistically  significant with p < 0.05.
true	P15-1025.pdf#0.283‡	SearchQA, Unigram Acc	P@5  Table 2: Evaluation results on Yahoo data and Baidu data, where dim denotes the dimension of the  word embeddings. The bold formate indicates the best results for question retrieval.  † indicates that  the difference between the results of our proposed approach (Skip-gram + FV, M-NET + FV) and other  methods are mildly significant with p < 0.08 under a t-test;  ‡ indicates the comparisons are statistically  significant with p < 0.05.
true	P15-2059.pdf#14.31	New York Times Corpus, P@10%	F1  Table 1: Results on the SemEval-2015 task
true	P15-2059.pdf#26.42	New York Times Corpus, P@10%	P  Table 1: Results on the SemEval-2015 task
true	P15-2059.pdf#12.09	New York Times Corpus, P@10%	R  Table 1: Results on the SemEval-2015 task
true	D12-1083.pdf#73.6	New York Times Corpus, P@10%	Instructional 10 - fold DCRF  Table 2: Parsing results using manual segmentation.
true	D12-1083.pdf#77.1	New York Times Corpus, P@10%	Instructional Test set DCRF  Table 2: Parsing results using manual segmentation.
true	D12-1083.pdf#86.9	New York Times Corpus, P@10%	Instructional Test set DCRF  Table 2: Parsing results using manual segmentation.
true	D12-1083.pdf#87.2	New York Times Corpus, P@10%	Instructional 10 - fold DCRF  Table 2: Parsing results using manual segmentation.
true	D12-1083.pdf#94.6	New York Times Corpus, P@10%	Instructional Test set DCRF  Table 2: Parsing results using manual segmentation.
true	D12-1083.pdf#97.7	New York Times Corpus, P@10%	Instructional 10 - fold DCRF  Table 2: Parsing results using manual segmentation.
true	D12-1083.pdf#90.5	New York Times Corpus, P@10%	Test Set F&R  Table 4: Segmentation results of different models.
true	D12-1083.pdf#92.3	New York Times Corpus, P@10%	10 - fold LR  Table 4: Segmentation results of different models.
true	D12-1083.pdf#88.7	New York Times Corpus, P@10%	10 - fold LR  Table 4: Segmentation results of different models.
true	D12-1083.pdf#91.3	New York Times Corpus, P@10%	Test Set F&R  Table 4: Segmentation results of different models.
true	D12-1083.pdf#89.7	New York Times Corpus, P@10%	10 - fold LR  Table 4: Segmentation results of different models.
true	D12-1083.pdf#89.9	New York Times Corpus, P@10%	10 - fold LR  Table 4: Segmentation results of different models.
true	D12-1083.pdf#87.5	New York Times Corpus, P@10%	10 - fold LR  Table 4: Segmentation results of different models.
true	D12-1083.pdf#80.9	New York Times Corpus, P@10%	10 - fold LR  Table 4: Segmentation results of different models.
true	D12-1083.pdf#73.9	New York Times Corpus, P@10%	10 - fold LR  Table 4: Segmentation results of different models.
true	D12-1083.pdf#73.6	New York Times Corpus, P@10%	Test set DCRF  Table 5: Parsing results using automatic segmentation.
true	D12-1083.pdf#80.3	New York Times Corpus, P@10%	Test set DCRF  Table 5: Parsing results using automatic segmentation.
true	D12-1083.pdf#65.4	New York Times Corpus, P@10%	Test set DCRF  Table 5: Parsing results using automatic segmentation.
true	P11-2038.pdf#93.0	Penn Treebank, Bit per Character (BPC)	BLLIP  Table 2: Classification accuracy.
true	P11-2038.pdf#100.0	Penn Treebank, Bit per Character (BPC)	Treebank  Table 2: Classification accuracy.
true	P11-2038.pdf#93.0	Penn Treebank, Test perplexity	BLLIP  Table 2: Classification accuracy.
true	P11-2038.pdf#100.0	Penn Treebank, Test perplexity	Treebank  Table 2: Classification accuracy.
true	P11-2038.pdf#93.0	Penn Treebank, Number of params	BLLIP  Table 2: Classification accuracy.
true	P11-2038.pdf#100.0	Penn Treebank, Number of params	Treebank  Table 2: Classification accuracy.
true	D10-1067.pdf#57.47	SUBJ, Accuracy	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#1.76	SUBJ, Accuracy	75% WSJ20 30%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#57.90	SUBJ, Accuracy	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#3.35	SUBJ, Accuracy	75% WSJ40 30%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#58.75	SUBJ, Accuracy	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+0.77	SUBJ, Accuracy	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#1.13	SUBJ, Accuracy	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#2.80	SUBJ, Accuracy	50% WSJ40 20%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+1.30	SUBJ, Accuracy	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#0.61	SUBJ, Accuracy	75% WSJ10 30%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#2.98	SUBJ, Accuracy	25% WSJ20 10%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#65.75	SUBJ, Accuracy	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#2.60	SUBJ, Accuracy	25% WSJ40 10%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+0.78	SUBJ, Accuracy	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+0.80	SUBJ, Accuracy	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#58.32	SUBJ, Accuracy	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+0.38	SUBJ, Accuracy	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+0.93	SUBJ, Accuracy	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#65.66	SUBJ, Accuracy	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#57.73	SUBJ, Accuracy	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#76.80	SUBJ, Accuracy	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#2.44	SUBJ, Accuracy	25% WSJ 10%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#66.14	SUBJ, Accuracy	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#3.13	SUBJ, Accuracy	50% WSJ20 20%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+1.21	SUBJ, Accuracy	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#3.23	SUBJ, Accuracy	75% WSJ 30%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+0.14	SUBJ, Accuracy	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#2.50	SUBJ, Accuracy	50% WSJ 20%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#76.38	SUBJ, Accuracy	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#1.32	SUBJ, Accuracy	25% WSJ10 10%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+1.02	SUBJ, Accuracy	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+0.82	SUBJ, Accuracy	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#58.56	SUBJ, Accuracy	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+1.20	SUBJ, Accuracy	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#0.95	SUBJ, Accuracy	50% WSJ10 20%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#76.14	SUBJ, Accuracy	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#3.30	SUBJ, Accuracy	LT 50% LT 50%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#4.60	SUBJ, Accuracy	HT 50% HT 50%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#2.74	SUBJ, Accuracy	LT 50% LT 50%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#1.42	SUBJ, Accuracy	LT 10%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#1.31	SUBJ, Accuracy	LT 70%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#5.49	SUBJ, Accuracy	HT 70% HT 70%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#2.08	SUBJ, Accuracy	LT 30% LT 30%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.61	SUBJ, Accuracy	LT 10% LT 10%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.76	SUBJ, Accuracy	HT 30%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.66	SUBJ, Accuracy	LT 70%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#1.20	SUBJ, Accuracy	LT 30%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.44	SUBJ, Accuracy	HT 10%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#4.29	SUBJ, Accuracy	HT 30% HT 30%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#1.70	SUBJ, Accuracy	LT 30% LT 30%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.30	SUBJ, Accuracy	HT 70%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#3.04	SUBJ, Accuracy	HT 10% HT 10%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#4.47	SUBJ, Accuracy	LT 70% LT 70%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.28	SUBJ, Accuracy	LT 10% LT 10%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.64	SUBJ, Accuracy	LT 50%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.80	SUBJ, Accuracy	LT 30%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#1.11	SUBJ, Accuracy	LT 10%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.42	SUBJ, Accuracy	HT 50%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#1.15	SUBJ, Accuracy	LT 50%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#4.09	SUBJ, Accuracy	LT 70% LT 70%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#57.47	Penn Treebank, Bit per Character (BPC)	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#1.76	Penn Treebank, Bit per Character (BPC)	75% WSJ20 30%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#57.90	Penn Treebank, Bit per Character (BPC)	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#3.35	Penn Treebank, Bit per Character (BPC)	75% WSJ40 30%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#58.75	Penn Treebank, Bit per Character (BPC)	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+0.77	Penn Treebank, Bit per Character (BPC)	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#1.13	Penn Treebank, Bit per Character (BPC)	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#2.80	Penn Treebank, Bit per Character (BPC)	50% WSJ40 20%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+1.30	Penn Treebank, Bit per Character (BPC)	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#0.61	Penn Treebank, Bit per Character (BPC)	75% WSJ10 30%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#2.98	Penn Treebank, Bit per Character (BPC)	25% WSJ20 10%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#65.75	Penn Treebank, Bit per Character (BPC)	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#2.60	Penn Treebank, Bit per Character (BPC)	25% WSJ40 10%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+0.78	Penn Treebank, Bit per Character (BPC)	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+0.80	Penn Treebank, Bit per Character (BPC)	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#58.32	Penn Treebank, Bit per Character (BPC)	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+0.38	Penn Treebank, Bit per Character (BPC)	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+0.93	Penn Treebank, Bit per Character (BPC)	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#65.66	Penn Treebank, Bit per Character (BPC)	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#57.73	Penn Treebank, Bit per Character (BPC)	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#76.80	Penn Treebank, Bit per Character (BPC)	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#2.44	Penn Treebank, Bit per Character (BPC)	25% WSJ 10%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#66.14	Penn Treebank, Bit per Character (BPC)	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#3.13	Penn Treebank, Bit per Character (BPC)	50% WSJ20 20%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+1.21	Penn Treebank, Bit per Character (BPC)	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#3.23	Penn Treebank, Bit per Character (BPC)	75% WSJ 30%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+0.14	Penn Treebank, Bit per Character (BPC)	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#2.50	Penn Treebank, Bit per Character (BPC)	50% WSJ 20%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#76.38	Penn Treebank, Bit per Character (BPC)	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#1.32	Penn Treebank, Bit per Character (BPC)	25% WSJ10 10%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+1.02	Penn Treebank, Bit per Character (BPC)	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+0.82	Penn Treebank, Bit per Character (BPC)	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#58.56	Penn Treebank, Bit per Character (BPC)	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+1.20	Penn Treebank, Bit per Character (BPC)	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#0.95	Penn Treebank, Bit per Character (BPC)	50% WSJ10 20%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#76.14	Penn Treebank, Bit per Character (BPC)	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#3.30	Penn Treebank, Bit per Character (BPC)	LT 50% LT 50%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#4.60	Penn Treebank, Bit per Character (BPC)	HT 50% HT 50%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#2.74	Penn Treebank, Bit per Character (BPC)	LT 50% LT 50%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#1.42	Penn Treebank, Bit per Character (BPC)	LT 10%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#1.31	Penn Treebank, Bit per Character (BPC)	LT 70%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#5.49	Penn Treebank, Bit per Character (BPC)	HT 70% HT 70%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#2.08	Penn Treebank, Bit per Character (BPC)	LT 30% LT 30%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.61	Penn Treebank, Bit per Character (BPC)	LT 10% LT 10%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.76	Penn Treebank, Bit per Character (BPC)	HT 30%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.66	Penn Treebank, Bit per Character (BPC)	LT 70%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#1.20	Penn Treebank, Bit per Character (BPC)	LT 30%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.44	Penn Treebank, Bit per Character (BPC)	HT 10%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#4.29	Penn Treebank, Bit per Character (BPC)	HT 30% HT 30%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#1.70	Penn Treebank, Bit per Character (BPC)	LT 30% LT 30%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.30	Penn Treebank, Bit per Character (BPC)	HT 70%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#3.04	Penn Treebank, Bit per Character (BPC)	HT 10% HT 10%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#4.47	Penn Treebank, Bit per Character (BPC)	LT 70% LT 70%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.28	Penn Treebank, Bit per Character (BPC)	LT 10% LT 10%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.64	Penn Treebank, Bit per Character (BPC)	LT 50%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.80	Penn Treebank, Bit per Character (BPC)	LT 30%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#1.11	Penn Treebank, Bit per Character (BPC)	LT 10%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.42	Penn Treebank, Bit per Character (BPC)	HT 50%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#1.15	Penn Treebank, Bit per Character (BPC)	LT 50%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#4.09	Penn Treebank, Bit per Character (BPC)	LT 70% LT 70%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#57.47	Penn Treebank, F1	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#1.76	Penn Treebank, F1	75% WSJ20 30%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#57.90	Penn Treebank, F1	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#3.35	Penn Treebank, F1	75% WSJ40 30%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#58.75	Penn Treebank, F1	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+0.77	Penn Treebank, F1	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#1.13	Penn Treebank, F1	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#2.80	Penn Treebank, F1	50% WSJ40 20%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+1.30	Penn Treebank, F1	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#0.61	Penn Treebank, F1	75% WSJ10 30%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#2.98	Penn Treebank, F1	25% WSJ20 10%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#65.75	Penn Treebank, F1	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#2.60	Penn Treebank, F1	25% WSJ40 10%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+0.78	Penn Treebank, F1	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+0.80	Penn Treebank, F1	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#58.32	Penn Treebank, F1	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+0.38	Penn Treebank, F1	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+0.93	Penn Treebank, F1	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#65.66	Penn Treebank, F1	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#57.73	Penn Treebank, F1	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#76.80	Penn Treebank, F1	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#2.44	Penn Treebank, F1	25% WSJ 10%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#66.14	Penn Treebank, F1	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#3.13	Penn Treebank, F1	50% WSJ20 20%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+1.21	Penn Treebank, F1	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#3.23	Penn Treebank, F1	75% WSJ 30%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+0.14	Penn Treebank, F1	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#2.50	Penn Treebank, F1	50% WSJ 20%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#76.38	Penn Treebank, F1	25%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#1.32	Penn Treebank, F1	25% WSJ10 10%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+1.02	Penn Treebank, F1	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+0.82	Penn Treebank, F1	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#58.56	Penn Treebank, F1	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#+1.20	Penn Treebank, F1	50%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#0.95	Penn Treebank, F1	50% WSJ10 20%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#76.14	Penn Treebank, F1	75%  Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA  are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top  table: Results for various values of N H (the number of sentences in the high quality training subset). Evaluation  is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and  the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL  algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from  the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific N H  value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the  improvements for the LT 's of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports  results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
true	D10-1067.pdf#3.30	Penn Treebank, F1	LT 50% LT 50%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#4.60	Penn Treebank, F1	HT 50% HT 50%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#2.74	Penn Treebank, F1	LT 50% LT 50%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#1.42	Penn Treebank, F1	LT 10%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#1.31	Penn Treebank, F1	LT 70%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#5.49	Penn Treebank, F1	HT 70% HT 70%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#2.08	Penn Treebank, F1	LT 30% LT 30%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.61	Penn Treebank, F1	LT 10% LT 10%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.76	Penn Treebank, F1	HT 30%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.66	Penn Treebank, F1	LT 70%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#1.20	Penn Treebank, F1	LT 30%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.44	Penn Treebank, F1	HT 10%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#4.29	Penn Treebank, F1	HT 30% HT 30%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#1.70	Penn Treebank, F1	LT 30% LT 30%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.30	Penn Treebank, F1	HT 70%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#3.04	Penn Treebank, F1	HT 10% HT 10%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#4.47	Penn Treebank, F1	LT 70% LT 70%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.28	Penn Treebank, F1	LT 10% LT 10%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.64	Penn Treebank, F1	LT 50%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.80	Penn Treebank, F1	LT 30%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#1.11	Penn Treebank, F1	LT 10%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#0.42	Penn Treebank, F1	HT 50%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#1.15	Penn Treebank, F1	LT 50%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	D10-1067.pdf#4.09	Penn Treebank, F1	LT 70% LT 70%  Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire  corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset  (right column section, HT ) of each corpus, as a function of the high quality training set size (N H ). Since the tables  present entire corpus results, the training and test subsets are identical.
true	P15-1168.pdf#95.7	PKU, F1	models PKU R  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.3	PKU, F1	models MSRA P  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.4	PKU, F1	+bigram PKU F PKU F  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.7	PKU, F1	+Pre - train+bigram CTB6 R CTB6 R -  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.9	PKU, F1	models PKU F  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.8	PKU, F1	+Pre - train+bigram CTB6 F CTB6 F -  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.6	PKU, F1	+bigram PKU P PKU P  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.4	PKU, F1	+Pre - train+bigram PKU F PKU F -  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.4	PKU, F1	models CTB6 P  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.3	PKU, F1	+Pre - train+bigram PKU R PKU R -  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.2	PKU, F1	models CTB6 R  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.8	PKU, F1	+bigram CTB6 F CTB6 F  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.9	PKU, F1	+bigram CTB6 P CTB6 P  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.2	PKU, F1	models MSRA F  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.1	PKU, F1	models MSRA R  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.0	PKU, F1	models PKU P  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.7	PKU, F1	+bigram CTB6 R CTB6 R  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#97.6	PKU, F1	+Pre - train+bigram MSRA F MSRA F -  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#97.5	PKU, F1	+bigram MSRA P MSRA P  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#97.8	PKU, F1	+Pre - train+bigram MSRA R MSRA R -  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.3	PKU, F1	models CTB6 F  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.4	PKU, F1	+bigram PKU F  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#96.3	PKU, F1	+Pre - train+bigram PKU R -  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#95.7	PKU, F1	+Pre - train+bigram CTB6 R -  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#95.7	PKU, F1	+bigram CTB6 R  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#95.8	PKU, F1	+bigram CTB6 F  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#97.6	PKU, F1	+Pre - train+bigram MSRA F -  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#97.5	PKU, F1	+bigram MSRA P  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#95.9	PKU, F1	+bigram CTB6 P  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#97.8	PKU, F1	+Pre - train+bigram MSRA R -  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#95.8	PKU, F1	+Pre - train+bigram CTB6 F -  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#96.4	PKU, F1	+Pre - train+bigram PKU F -  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#96.6	PKU, F1	+bigram PKU P  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#97.6	PKU, F1	- MSRA  Table 5: Comparison of GRNN with the state-of- the-art methods on PKU, MSRA and CTB6 test  sets.
true	P15-1168.pdf#96.4	PKU, F1	- PKU  Table 5: Comparison of GRNN with the state-of- the-art methods on PKU, MSRA and CTB6 test  sets.
true	P15-1168.pdf#95.8	PKU, F1	- CTB6  Table 5: Comparison of GRNN with the state-of- the-art methods on PKU, MSRA and CTB6 test  sets.
true	P15-1168.pdf#95.7	MSR, F1	models PKU R  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.3	MSR, F1	models MSRA P  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.4	MSR, F1	+bigram PKU F PKU F  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.7	MSR, F1	+Pre - train+bigram CTB6 R CTB6 R -  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.9	MSR, F1	models PKU F  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.8	MSR, F1	+Pre - train+bigram CTB6 F CTB6 F -  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.6	MSR, F1	+bigram PKU P PKU P  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.4	MSR, F1	+Pre - train+bigram PKU F PKU F -  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.4	MSR, F1	models CTB6 P  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.3	MSR, F1	+Pre - train+bigram PKU R PKU R -  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.2	MSR, F1	models CTB6 R  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.8	MSR, F1	+bigram CTB6 F CTB6 F  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.9	MSR, F1	+bigram CTB6 P CTB6 P  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.2	MSR, F1	models MSRA F  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.1	MSR, F1	models MSRA R  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.0	MSR, F1	models PKU P  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.7	MSR, F1	+bigram CTB6 R CTB6 R  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#97.6	MSR, F1	+Pre - train+bigram MSRA F MSRA F -  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#97.5	MSR, F1	+bigram MSRA P MSRA P  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#97.8	MSR, F1	+Pre - train+bigram MSRA R MSRA R -  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.3	MSR, F1	models CTB6 F  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.4	MSR, F1	+bigram PKU F  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#96.3	MSR, F1	+Pre - train+bigram PKU R -  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#95.7	MSR, F1	+Pre - train+bigram CTB6 R -  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#95.7	MSR, F1	+bigram CTB6 R  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#95.8	MSR, F1	+bigram CTB6 F  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#97.6	MSR, F1	+Pre - train+bigram MSRA F -  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#97.5	MSR, F1	+bigram MSRA P  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#95.9	MSR, F1	+bigram CTB6 P  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#97.8	MSR, F1	+Pre - train+bigram MSRA R -  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#95.8	MSR, F1	+Pre - train+bigram CTB6 F -  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#96.4	MSR, F1	+Pre - train+bigram PKU F -  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#96.6	MSR, F1	+bigram PKU P  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#97.6	MSR, F1	- MSRA  Table 5: Comparison of GRNN with the state-of- the-art methods on PKU, MSRA and CTB6 test  sets.
true	P15-1168.pdf#96.4	MSR, F1	- PKU  Table 5: Comparison of GRNN with the state-of- the-art methods on PKU, MSRA and CTB6 test  sets.
true	P15-1168.pdf#95.8	MSR, F1	- CTB6  Table 5: Comparison of GRNN with the state-of- the-art methods on PKU, MSRA and CTB6 test  sets.
true	P15-1168.pdf#95.7	Chinese Treebank 6, F1	models PKU R  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.3	Chinese Treebank 6, F1	models MSRA P  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.4	Chinese Treebank 6, F1	+bigram PKU F PKU F  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.7	Chinese Treebank 6, F1	+Pre - train+bigram CTB6 R CTB6 R -  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.9	Chinese Treebank 6, F1	models PKU F  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.8	Chinese Treebank 6, F1	+Pre - train+bigram CTB6 F CTB6 F -  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.6	Chinese Treebank 6, F1	+bigram PKU P PKU P  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.4	Chinese Treebank 6, F1	+Pre - train+bigram PKU F PKU F -  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.4	Chinese Treebank 6, F1	models CTB6 P  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.3	Chinese Treebank 6, F1	+Pre - train+bigram PKU R PKU R -  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.2	Chinese Treebank 6, F1	models CTB6 R  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.8	Chinese Treebank 6, F1	+bigram CTB6 F CTB6 F  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.9	Chinese Treebank 6, F1	+bigram CTB6 P CTB6 P  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.2	Chinese Treebank 6, F1	models MSRA F  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.1	Chinese Treebank 6, F1	models MSRA R  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.0	Chinese Treebank 6, F1	models PKU P  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.7	Chinese Treebank 6, F1	+bigram CTB6 R CTB6 R  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#97.6	Chinese Treebank 6, F1	+Pre - train+bigram MSRA F MSRA F -  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#97.5	Chinese Treebank 6, F1	+bigram MSRA P MSRA P  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#97.8	Chinese Treebank 6, F1	+Pre - train+bigram MSRA R MSRA R -  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#95.3	Chinese Treebank 6, F1	models CTB6 F  Table 3: Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings.
true	P15-1168.pdf#96.4	Chinese Treebank 6, F1	+bigram PKU F  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#96.3	Chinese Treebank 6, F1	+Pre - train+bigram PKU R -  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#95.7	Chinese Treebank 6, F1	+Pre - train+bigram CTB6 R -  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#95.7	Chinese Treebank 6, F1	+bigram CTB6 R  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#95.8	Chinese Treebank 6, F1	+bigram CTB6 F  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#97.6	Chinese Treebank 6, F1	+Pre - train+bigram MSRA F -  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#97.5	Chinese Treebank 6, F1	+bigram MSRA P  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#95.9	Chinese Treebank 6, F1	+bigram CTB6 P  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#97.8	Chinese Treebank 6, F1	+Pre - train+bigram MSRA R -  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#95.8	Chinese Treebank 6, F1	+Pre - train+bigram CTB6 F -  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#96.4	Chinese Treebank 6, F1	+Pre - train+bigram PKU F -  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#96.6	Chinese Treebank 6, F1	+bigram PKU P  Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em- beddings.
true	P15-1168.pdf#97.6	Chinese Treebank 6, F1	- MSRA  Table 5: Comparison of GRNN with the state-of- the-art methods on PKU, MSRA and CTB6 test  sets.
true	P15-1168.pdf#96.4	Chinese Treebank 6, F1	- PKU  Table 5: Comparison of GRNN with the state-of- the-art methods on PKU, MSRA and CTB6 test  sets.
true	P15-1168.pdf#95.8	Chinese Treebank 6, F1	- CTB6  Table 5: Comparison of GRNN with the state-of- the-art methods on PKU, MSRA and CTB6 test  sets.
true	N12-1068.pdf#0.535	Penn Treebank, LAS	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Chinese 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD Fix  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.351	Penn Treebank, LAS	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Chinese 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) Delta +Delta  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.084	Penn Treebank, LAS	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Chinese 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD Fix  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.724	Penn Treebank, LAS	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Swedish 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD - Fix +Delta  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.348	Penn Treebank, LAS	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Swedish 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD - Fix +Delta  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.729	Penn Treebank, LAS	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Chinese 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD Fix  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.547	Penn Treebank, LAS	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Swedish 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD - Fix +Delta  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#14datasetsandover	Penn Treebank, LAS	we see that it achieves the highest average - precision KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#(0.729vs.0.724),	Penn Treebank, LAS	row of the table , we see that it has Precision very KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#14datasets.Itimprovesaverage-	Penn Treebank, LAS	we see that it achieves the highest average - precision KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#12of	Penn Treebank, LAS	we see that it achieves the highest average - precision KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#(0.351vs.	Penn Treebank, LAS	row of the table , we see that it has Precision very KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#14datasets.	Penn Treebank, LAS	we see that it achieves the highest average - precision KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#0.535	Penn Treebank, POS	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Chinese 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD Fix  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.351	Penn Treebank, POS	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Chinese 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) Delta +Delta  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.084	Penn Treebank, POS	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Chinese 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD Fix  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.724	Penn Treebank, POS	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Swedish 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD - Fix +Delta  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.348	Penn Treebank, POS	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Swedish 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD - Fix +Delta  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.729	Penn Treebank, POS	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Chinese 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD Fix  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.547	Penn Treebank, POS	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Swedish 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD - Fix +Delta  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#14datasetsandover	Penn Treebank, POS	we see that it achieves the highest average - precision KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#(0.729vs.0.724),	Penn Treebank, POS	row of the table , we see that it has Precision very KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#14datasets.Itimprovesaverage-	Penn Treebank, POS	we see that it achieves the highest average - precision KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#12of	Penn Treebank, POS	we see that it achieves the highest average - precision KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#(0.351vs.	Penn Treebank, POS	row of the table , we see that it has Precision very KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#14datasets.	Penn Treebank, POS	we see that it achieves the highest average - precision KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#0.535	Penn Treebank, F1	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Chinese 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD Fix  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.351	Penn Treebank, F1	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Chinese 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) Delta +Delta  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.084	Penn Treebank, F1	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Chinese 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD Fix  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.724	Penn Treebank, F1	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Swedish 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD - Fix +Delta  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.348	Penn Treebank, F1	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Swedish 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD - Fix +Delta  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.729	Penn Treebank, F1	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Chinese 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD Fix  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.547	Penn Treebank, F1	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Swedish 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD - Fix +Delta  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#14datasetsandover	Penn Treebank, F1	we see that it achieves the highest average - precision KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#(0.729vs.0.724),	Penn Treebank, F1	row of the table , we see that it has Precision very KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#14datasets.Itimprovesaverage-	Penn Treebank, F1	we see that it achieves the highest average - precision KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#12of	Penn Treebank, F1	we see that it achieves the highest average - precision KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#(0.351vs.	Penn Treebank, F1	row of the table , we see that it has Precision very KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#14datasets.	Penn Treebank, F1	we see that it achieves the highest average - precision KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#0.535	Penn Treebank, UAS	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Chinese 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD Fix  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.351	Penn Treebank, UAS	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Chinese 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) Delta +Delta  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.084	Penn Treebank, UAS	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Chinese 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD Fix  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.724	Penn Treebank, UAS	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Swedish 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD - Fix +Delta  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.348	Penn Treebank, UAS	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Swedish 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD - Fix +Delta  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.729	Penn Treebank, UAS	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Chinese 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD Fix  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#0.547	Penn Treebank, UAS	performance is obtained for curves close to the line y=x ( black line ) . Delta method is omitted as its output is not in the range [ 0 , 1 ] . Swedish 1 KD − Fix WKB 0 Expected Accuracy ( bin center ) KD - Fix +Delta  Table 1: Row 1: Average precision in ranking all edges ac-
true	N12-1068.pdf#14datasetsandover	Penn Treebank, UAS	we see that it achieves the highest average - precision KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#(0.729vs.0.724),	Penn Treebank, UAS	row of the table , we see that it has Precision very KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#14datasets.Itimprovesaverage-	Penn Treebank, UAS	we see that it achieves the highest average - precision KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#12of	Penn Treebank, UAS	we see that it achieves the highest average - precision KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#(0.351vs.	Penn Treebank, UAS	row of the table , we see that it has Precision very KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	N12-1068.pdf#14datasets.	Penn Treebank, UAS	we see that it achieves the highest average - precision KD − Fix KD − Fix+Delta WKB Recall as Percent of Incorect Edges Not surpassingly , we name this method This new method enjoys the bene -  Table 2: Number of incorrect edges detected, and the corre-
true	D15-1072.pdf#3·1/3)/	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	for the domains represented by the given web review corpora . Corpus domain 1 / 3 1 / 3 1 / 3  Table 1. Sentences, tokens, and annotated local sentiments  for the domains represented by the given web review corpora.
true	D15-1072.pdf#0	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Corpus domain  Table 1. Sentences, tokens, and annotated local sentiments  for the domains represented by the given web review corpora.
true	D15-1072.pdf#0+	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	for the domains represented by the given web review corpora . Corpus domain 1 / 3 1 / 3 1 / 3  Table 1. Sentences, tokens, and annotated local sentiments  for the domains represented by the given web review corpora.
true	D15-1072.pdf#2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Corpus domain  Table 1. Sentences, tokens, and annotated local sentiments  for the domains represented by the given web review corpora.
true	D15-1072.pdf#v1.0(PangandLee,2005)thatrefertotwo	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Lebanon , 2007 ) compiles 450 Rotten Tomatoes re - Local sentiment Tokens neutral negative per sent . Table 1 . Sentences , tokens , and annotated local sentiments for the domains represented by the given web review corpora . anced among five categories : books ( 59 reviews ) , games ( 60 ) . We use the first three for training and the others for testing . Under the authors ' mapping negative reviews each . In each review , every sen - neutral , To match the other corpora , we merge the three latter into one neutral class . cor -  Table 1. Sentences, tokens, and annotated local sentiments  for the domains represented by the given web review corpora.
true	D15-1072.pdf#1(length4)	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 1. Sentences, tokens, and annotated local sentiments  for the domains represented by the given web review corpora.
true	D15-1072.pdf#4=1/4.	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	for the domains represented by the given web review corpora . Corpus domain 1 / 3 1 / 3  Table 1. Sentences, tokens, and annotated local sentiments  for the domains represented by the given web review corpora.
true	D15-1072.pdf#1(TäckströmandMcDonald,	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	dicate clear domain differences . Corpus domain 1 / 3 1 / 3 1 / 3 Figure 4 . Computation of the normalized edit distance of two sible edit operations , i . e . , substitutions , insertions , and deletions of single local sentiments . We map by a function d for any two values s and s : tence is classified as positive , If s mixed , or irrelevant . If s is inserted or deleted after s . Hotel testing . tion , and their overall ratings .  Table 1. Sentences, tokens, and annotated local sentiments  for the domains represented by the given web review corpora.
true	D15-1072.pdf#0	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Corpus  Table 1. Sentences, tokens, and annotated local sentiments  for the domains represented by the given web review corpora.
true	D15-1072.pdf#75.6	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Weighted precision Product Hotel Movie  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#81.0	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Weighted precision Product Hotel Movie  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#84.5	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Weighted precision Product Hotel Movie  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#85.3	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Aggregate recall Product Hotel Movie  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#92.9	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Aggregate recall Product Hotel Movie  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#85.7	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Aggregate recall Product Hotel Movie  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#80.0	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Weighted precision Product Hotel Movie  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#0.23	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	W ' d Hellinger distance Product Hotel  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#0.24	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	W ' d Hellinger distance Product Hotel  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#80.8	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Weighted precision Product Hotel Movie  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#0.33	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	W ' d Hellinger distance Product Hotel  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#91.7	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Aggregate recall Product Hotel Movie  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#81.6	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Weighted precision Product Hotel Movie  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#0.28	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	W ' d Hellinger distance Movie  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#0.17	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	W ' d Hellinger distance Product Hotel  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#76.7	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Weighted precision Product Hotel Movie  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#85.8	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Aggregate recall Product Hotel Movie  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#0.21	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	W ' d Hellinger distance Product Hotel  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#76.9	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Aggregate recall Product Hotel Movie  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#0.14	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	W ' d Hellinger distance Product Hotel  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#0.28	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	W ' d Hellinger distance Movie  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#0.22	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	W ' d Hellinger distance Movie  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#85.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Weighted precision Product Hotel Movie  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#73.0	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Aggregate recall Product Hotel Movie  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#72.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Weighted precision Product Hotel Movie  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#85.7	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Aggregate recall Product Hotel Movie  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#81.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Aggregate recall Product Hotel Movie  Table 2. Results on the generality of sentiment flow for all evaluated model variants on ground-truth data for each combination  of training and test domain. The most general variants in terms of both aggregated recall and weighted precision are marked in  bold. For illustration, 
true	D15-1072.pdf#54.7	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Product Hotel Movie  Table 4. Accuracy of predicting 3-class global sentiment for  each combination of training and test domain using the base- lines and/or a selection of the 16 evaluated model variants.
true	D15-1072.pdf#51.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Product Hotel Movie  Table 4. Accuracy of predicting 3-class global sentiment for  each combination of training and test domain using the base- lines and/or a selection of the 16 evaluated model variants.
true	D15-1072.pdf#54.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Product Hotel Movie -  Table 4. Accuracy of predicting 3-class global sentiment for  each combination of training and test domain using the base- lines and/or a selection of the 16 evaluated model variants.
true	D15-1072.pdf#60.0	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Product Hotel Movie  Table 4. Accuracy of predicting 3-class global sentiment for  each combination of training and test domain using the base- lines and/or a selection of the 16 evaluated model variants.
true	D15-1072.pdf#57.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Product Hotel Movie  Table 4. Accuracy of predicting 3-class global sentiment for  each combination of training and test domain using the base- lines and/or a selection of the 16 evaluated model variants.
true	D15-1072.pdf#54.2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Product Hotel Movie  Table 4. Accuracy of predicting 3-class global sentiment for  each combination of training and test domain using the base- lines and/or a selection of the 16 evaluated model variants.
true	D15-1072.pdf#71.8	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Product Hotel Movie -  Table 4. Accuracy of predicting 3-class global sentiment for  each combination of training and test domain using the base- lines and/or a selection of the 16 evaluated model variants.
true	D15-1072.pdf#79.6	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Product Hotel Movie  Table 4. Accuracy of predicting 3-class global sentiment for  each combination of training and test domain using the base- lines and/or a selection of the 16 evaluated model variants.
true	D15-1072.pdf#49.7	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Product Hotel Movie -  Table 4. Accuracy of predicting 3-class global sentiment for  each combination of training and test domain using the base- lines and/or a selection of the 16 evaluated model variants.
true	D10-1039.pdf#0.060	New York Times Corpus, P@10%	Table 1: F-scores for RSTDT relations, using a training set containing 
true	D10-1039.pdf#0.110	New York Times Corpus, P@10%	Table 1: F-scores for RSTDT relations, using a training set containing 
true	D10-1039.pdf#0.118	New York Times Corpus, P@10%	Table 1: F-scores for RSTDT relations, using a training set containing 
true	D10-1039.pdf#0.146	New York Times Corpus, P@10%	Table 1: F-scores for RSTDT relations, using a training set containing 
true	D10-1039.pdf#0.134	New York Times Corpus, P@10%	Table 1: F-scores for RSTDT relations, using a training set containing 
true	D10-1039.pdf#0.204	New York Times Corpus, P@10%	Table 1: F-scores for RSTDT relations, using a training set containing 
true	D10-1039.pdf#0.105	New York Times Corpus, P@10%	Table 1: F-scores for RSTDT relations, using a training set containing 
true	D10-1039.pdf#0.213	New York Times Corpus, P@10%	Table 1: F-scores for RSTDT relations, using a training set containing 
true	D10-1039.pdf#0.069	New York Times Corpus, P@10%	Table 1: F-scores for RSTDT relations, using a training set containing 
true	D10-1039.pdf#0.222	New York Times Corpus, P@10%	Table 1: F-scores for RSTDT relations, using a training set containing 
true	D10-1039.pdf#0.101	New York Times Corpus, P@10%	Table 1: F-scores for RSTDT relations, using a training set containing 
true	D10-1039.pdf#0.195	New York Times Corpus, P@10%	Temporal . Asynchronous [ 2 ] [ 1 ]  Table 2: F-scores for PDTB relations.
true	D10-1039.pdf#0.072	New York Times Corpus, P@10%	Temporal . Asynchronous [ 2 ] [ 1 ]  Table 2: F-scores for PDTB relations.
true	D10-1039.pdf#0.015	New York Times Corpus, P@10%	Temporal . Asynchronous [ 2 ] [ 1 ]  Table 2: F-scores for PDTB relations.
true	D10-1039.pdf#0.204	New York Times Corpus, P@10%	Temporal . Asynchronous [ 2 ] [ 1 ]  Table 2: F-scores for PDTB relations.
true	D10-1039.pdf#0.015	New York Times Corpus, P@10%	Temporal . Asynchronous [ 2 ] [ 1 ]  Table 2: F-scores for PDTB relations.
true	D10-1039.pdf#0.195	New York Times Corpus, P@10%	Temporal . Asynchronous [ 2 ] [ 1 ]  Table 2: F-scores for PDTB relations.
true	D10-1039.pdf#0.195	New York Times Corpus, P@10%	Temporal . Asynchronous [ 2 ] [ 1 ]  Table 2: F-scores for PDTB relations.
true	D10-1039.pdf#0.045	New York Times Corpus, P@10%	Temporal . Asynchronous [ 2 ] [ 1 ]  Table 2: F-scores for PDTB relations.
true	D10-1039.pdf#0.015	New York Times Corpus, P@10%	Temporal . Asynchronous [ 2 ] [ 1 ]  Table 2: F-scores for PDTB relations.
true	D10-1039.pdf#0.202	New York Times Corpus, P@10%	Temporal . Asynchronous [ 2 ] [ 1 ]  Table 2: F-scores for PDTB relations.
true	D10-1039.pdf#0.174	New York Times Corpus, P@10%	Temporal . Asynchronous [ 2 ] [ 1 ]  Table 2: F-scores for PDTB relations.
true	D10-1039.pdf#0.032	New York Times Corpus, P@10%	Temporal . Asynchronous [ 2 ] [ 1 ]  Table 2: F-scores for PDTB relations.
true	D10-1039.pdf#0.032	New York Times Corpus, P@10%	Temporal . Asynchronous [ 2 ] [ 1 ]  Table 2: F-scores for PDTB relations.
true	D10-1039.pdf#0.200	New York Times Corpus, P@10%	Temporal . Asynchronous [ 2 ] [ 1 ]  Table 2: F-scores for PDTB relations.
true	D11-1106.pdf#0.075	Penn Treebank, LAS	42 , 13 Length Method  Table 2: Development tests using various levels of lexical  categories and timeouts, after one training iteration.
true	D11-1106.pdf#0.0001	Penn Treebank, LAS	42 , 13 Length Method  Table 2: Development tests using various levels of lexical  categories and timeouts, after one training iteration.
true	D11-1106.pdf#0.075	Penn Treebank, LAS	BLEU  Table 5: Final accuracies.
true	D11-1106.pdf#0.0001	Penn Treebank, LAS	BLEU  Table 5: Final accuracies.
true	N15-1081.pdf#0.571	Penn Treebank, F1	q Baseline + extra data q  Table 4. The results show that the  extra data extracted from the Gigaword Corpus is particu- larly helpful for minority classes such as Comparison vs.  Others and Temporal vs Others, where our current sys- tem significantly outperforms that of Rutherford and Xue  (2014). Interestingly, the Expansion vs. Others classifier
true	N15-1081.pdf#0.54	Penn Treebank, F1	q Baseline features All q q  Table 4. The results show that the  extra data extracted from the Gigaword Corpus is particu- larly helpful for minority classes such as Comparison vs.  Others and Temporal vs Others, where our current sys- tem significantly outperforms that of Rutherford and Xue  (2014). Interestingly, the Expansion vs. Others classifier
true	N15-1081.pdf#0.55	Penn Treebank, F1	q Baseline features All q q  Table 4. The results show that the  extra data extracted from the Gigaword Corpus is particu- larly helpful for minority classes such as Comparison vs.  Others and Temporal vs Others, where our current sys- tem significantly outperforms that of Rutherford and Xue  (2014). Interestingly, the Expansion vs. Others classifier
true	N15-1081.pdf#0.405	Penn Treebank, F1	q Baseline + extra data q  Table 4. The results show that the  extra data extracted from the Gigaword Corpus is particu- larly helpful for minority classes such as Comparison vs.  Others and Temporal vs Others, where our current sys- tem significantly outperforms that of Rutherford and Xue  (2014). Interestingly, the Expansion vs. Others classifier
true	N15-1081.pdf#0.410	Penn Treebank, F1	Baseline  Table 3: Our current 4-way classification system outper- forms the baseline overall. The difference in accuracy is  statistically significant (p < 0.05; bootstrap test).
true	N15-1081.pdf#0.333	Penn Treebank, F1	Baseline + extra data  Table 3: Our current 4-way classification system outper- forms the baseline overall. The difference in accuracy is  statistically significant (p < 0.05; bootstrap test).
true	N15-1081.pdf#0.410	Penn Treebank, F1	Baseline  Table 4: The performance of our approach on the binary  classification task formulation.
true	N15-1081.pdf#0.333	Penn Treebank, F1	Baseline + extra data  Table 4: The performance of our approach on the binary  classification task formulation.
true	N15-1081.pdf#0.449	Penn Treebank, F1	Gigaword only  Table 5: The accuracy rates for the freely omissible class  are higher than the ones for the other classes both when  using the Gigaword data alone and when using it in con- junction with the implicit relations in the PDTB.
true	N15-1081.pdf#0.571	Penn Treebank, F1	Gigaword  Table 5: The accuracy rates for the freely omissible class  are higher than the ones for the other classes both when  using the Gigaword data alone and when using it in con- junction with the implicit relations in the PDTB.
true	N15-1081.pdf#0.554	Penn Treebank, F1	Gigaword + Implicit PDTB  Table 5: The accuracy rates for the freely omissible class  are higher than the ones for the other classes both when  using the Gigaword data alone and when using it in con- junction with the implicit relations in the PDTB.
true	N15-1081.pdf#0.399	Penn Treebank, F1	Gigaword only  Table 5: The accuracy rates for the freely omissible class  are higher than the ones for the other classes both when  using the Gigaword data alone and when using it in con- junction with the implicit relations in the PDTB.
true	N15-1081.pdf#0.313	Penn Treebank, F1	Gigaword only  Table 5: The accuracy rates for the freely omissible class  are higher than the ones for the other classes both when  using the Gigaword data alone and when using it in con- junction with the implicit relations in the PDTB.
true	N15-1081.pdf#0.547	Penn Treebank, F1	Gigaword + Implicit PDTB  Table 5: The accuracy rates for the freely omissible class  are higher than the ones for the other classes both when  using the Gigaword data alone and when using it in con- junction with the implicit relations in the PDTB.
true	N15-1081.pdf#0.505	Penn Treebank, F1	Gigaword  Table 5: The accuracy rates for the freely omissible class  are higher than the ones for the other classes both when  using the Gigaword data alone and when using it in con- junction with the implicit relations in the PDTB.
true	N15-1081.pdf#0.527	Penn Treebank, F1	Gigaword + Implicit PDTB  Table 5: The accuracy rates for the freely omissible class  are higher than the ones for the other classes both when  using the Gigaword data alone and when using it in con- junction with the implicit relations in the PDTB.
true	N15-1081.pdf#0.490	Penn Treebank, F1	Gigaword only  Table 5: The accuracy rates for the freely omissible class  are higher than the ones for the other classes both when  using the Gigaword data alone and when using it in con- junction with the implicit relations in the PDTB.
true	N15-1081.pdf#0.546	Penn Treebank, F1	Gigaword + Implicit PDTB  Table 5: The accuracy rates for the freely omissible class  are higher than the ones for the other classes both when  using the Gigaword data alone and when using it in con- junction with the implicit relations in the PDTB.
true	N10-1120.pdf#0.826*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2 : Statistical Information of Corpora Japanese NTC - J  Table 2: Statistical Information of Corpora
true	N10-1120.pdf#0.05)	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2 : Statistical Information of Corpora English MPQA  Table 2: Statistical Information of Corpora
true	N10-1120.pdf#0.804	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2 : Statistical Information of Corpora English NTC - E  Table 2: Statistical Information of Corpora
true	N10-1120.pdf#0.847*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2 : Statistical Information of Corpora Japanese KNB  Table 2: Statistical Information of Corpora
true	N10-1120.pdf#0.861*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2 : Statistical Information of Corpora English MPQA  Table 2: Statistical Information of Corpora
true	N10-1120.pdf#0.814	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2 : Statistical Information of Corpora English CR  Table 2: Statistical Information of Corpora
true	N10-1120.pdf#0.846*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2 : Statistical Information of Corpora Japanese ACP  Table 2: Statistical Information of Corpora
true	N10-1120.pdf#0.814	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2 : Statistical Information of Corpora English CR  Table 2: Statistical Information of Corpora
true	N10-1120.pdf#0.841*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2 : Statistical Information of Corpora Japanese 50 Topics  Table 2: Statistical Information of Corpora
true	N10-1120.pdf#0.773*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2 : Statistical Information of Corpora English MR  Table 2: Statistical Information of Corpora
true	N10-1120.pdf#0.847*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2 : Statistical Information of Corpora Japanese KNB  Table 3: Accuracy of Sentiment Classification
true	N10-1120.pdf#0.814	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2 : Statistical Information of Corpora English CR  Table 3: Accuracy of Sentiment Classification
true	N10-1120.pdf#0.861*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2 : Statistical Information of Corpora English MPQA  Table 3: Accuracy of Sentiment Classification
true	N10-1120.pdf#0.841*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2 : Statistical Information of Corpora Japanese 50 Topics  Table 3: Accuracy of Sentiment Classification
true	N10-1120.pdf#0.804	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2 : Statistical Information of Corpora English NTC - E  Table 3: Accuracy of Sentiment Classification
true	N10-1120.pdf#0.814	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2 : Statistical Information of Corpora English CR  Table 3: Accuracy of Sentiment Classification
true	N10-1120.pdf#0.846*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2 : Statistical Information of Corpora Japanese ACP  Table 3: Accuracy of Sentiment Classification
true	N10-1120.pdf#0.05)	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2 : Statistical Information of Corpora English MPQA  Table 3: Accuracy of Sentiment Classification
true	N10-1120.pdf#0.826*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2 : Statistical Information of Corpora Japanese NTC - J  Table 3: Accuracy of Sentiment Classification
true	N10-1120.pdf#0.773*	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2 : Statistical Information of Corpora English MR  Table 3: Accuracy of Sentiment Classification
true	D13-1052.pdf#13.3	Text8, Number of params	System Avg . ( T - B ) / 2  Table 2: All results of single and joint decoding systems.
true	D13-1052.pdf#13.6	Text8, Number of params	System MT08 web ( T - B ) / 2  Table 2: All results of single and joint decoding systems.
true	D13-1052.pdf#12.6	Text8, Number of params	System Avg . ( T - B ) / 2  Table 2: All results of single and joint decoding systems.
true	D13-1052.pdf#10.2	Text8, Number of params	System GALE - web ( T - B ) / 2  Table 2: All results of single and joint decoding systems.
true	D13-1052.pdf#22.2	Text8, Number of params	System P1R6 - web ( T - B ) / 2  Table 2: All results of single and joint decoding systems.
true	D13-1052.pdf#5.8	Text8, Number of params	System MT08 news ( T - B ) / 2  Table 2: All results of single and joint decoding systems.
true	D13-1052.pdf#9.4	Text8, Number of params	Joint GALE - web ( T - B ) / 2  Table 2: All results of single and joint decoding systems.
true	D13-1052.pdf#12.9	Text8, Number of params	System MT08 web ( T - B ) / 2  Table 2: All results of single and joint decoding systems.
true	D13-1052.pdf#12.9	Text8, Number of params	Joint MT08 web ( T - B ) / 2  Table 2: All results of single and joint decoding systems.
true	D13-1052.pdf#9.4	Text8, Number of params	System GALE - web ( T - B ) / 2  Table 2: All results of single and joint decoding systems.
true	D13-1052.pdf#23.1	Text8, Number of params	System P1R6 - web ( T - B ) / 2  Table 2: All results of single and joint decoding systems.
true	D13-1052.pdf#6.3	Text8, Number of params	System MT08 news ( T - B ) / 2  Table 2: All results of single and joint decoding systems.
true	D13-1052.pdf#6.3	Text8, Number of params	System MT08 news ( T - B ) / 2  Table 2: All results of single and joint decoding systems.
true	N10-1091.pdf#90.51	benchmark Vietnamese dependency treebank VnDT, UAS	Unweighted POS of modifier UAS  Table 2: Scores of unsupervised combination models using different voting strategies. The combined trees are assembled using a
true	N10-1091.pdf#87.17	benchmark Vietnamese dependency treebank VnDT, UAS	Weighted by POS of modifier LAS  Table 2: Scores of unsupervised combination models using different voting strategies. The combined trees are assembled using a
true	N10-1091.pdf#90.51	Penn Treebank, UAS	Unweighted POS of modifier UAS  Table 2: Scores of unsupervised combination models using different voting strategies. The combined trees are assembled using a
true	N10-1091.pdf#87.17	Penn Treebank, UAS	Weighted by POS of modifier LAS  Table 2: Scores of unsupervised combination models using different voting strategies. The combined trees are assembled using a
true	P11-1013.pdf#94.98	IMDb, Accuracy	MDS MR Book  Table 2: Supervised sentiment classification accuracy.
true	P11-1013.pdf#89.95	IMDb, Accuracy	MDS MR Book  Table 2: Supervised sentiment classification accuracy.
true	P11-1013.pdf#88.25	IMDb, Accuracy	MDS MR Elec .  Table 2: Supervised sentiment classification accuracy.
true	P11-1013.pdf#91.7	IMDb, Accuracy	MDS MR DVD  Table 2: Supervised sentiment classification accuracy.
true	P11-1013.pdf#89.85	IMDb, Accuracy	MDS MR Kitch .  Table 2: Supervised sentiment classification accuracy.
true	P11-1013.pdf#5.2	IMDb, Accuracy	JST - IG  Table 3: Adaptation loss with respect to the in-domain  gold standard. The last row shows the average loss over  all the four domains.
true	P11-1013.pdf#4.4	IMDb, Accuracy	JST - IG  Table 3: Adaptation loss with respect to the in-domain  gold standard. The last row shows the average loss over  all the four domains.
true	P11-1013.pdf#2.9	IMDb, Accuracy	JST - IG  Table 3: Adaptation loss with respect to the in-domain  gold standard. The last row shows the average loss over  all the four domains.
true	P11-1013.pdf#4.1	IMDb, Accuracy	JST - IG  Table 3: Adaptation loss with respect to the in-domain  gold standard. The last row shows the average loss over  all the four domains.
true	P11-1013.pdf#3.9	IMDb, Accuracy	JST - IG  Table 3: Adaptation loss with respect to the in-domain  gold standard. The last row shows the average loss over  all the four domains.
true	P15-1117.pdf#93.28	Penn Treebank, POS	beam size UAS  Table 5: Results on WSJ. Speed: sentences per  second.  †: semi-supervised learning.  ‡: joint  POS-tagging and dependency parsing models.
true	P15-1117.pdf#92.35	Penn Treebank, POS	beam size LAS  Table 5: Results on WSJ. Speed: sentences per  second.  †: semi-supervised learning.  ‡: joint  POS-tagging and dependency parsing models.
true	P15-1117.pdf#93.28	Penn Treebank, LAS	beam size UAS  Table 5: Results on WSJ. Speed: sentences per  second.  †: semi-supervised learning.  ‡: joint  POS-tagging and dependency parsing models.
true	P15-1117.pdf#92.35	Penn Treebank, LAS	beam size LAS  Table 5: Results on WSJ. Speed: sentences per  second.  †: semi-supervised learning.  ‡: joint  POS-tagging and dependency parsing models.
true	P15-1117.pdf#93.28	Penn Treebank, UAS	beam size UAS  Table 5: Results on WSJ. Speed: sentences per  second.  †: semi-supervised learning.  ‡: joint  POS-tagging and dependency parsing models.
true	P15-1117.pdf#92.35	Penn Treebank, UAS	beam size LAS  Table 5: Results on WSJ. Speed: sentences per  second.  †: semi-supervised learning.  ‡: joint  POS-tagging and dependency parsing models.
true	P15-1117.pdf#93.28	Penn Treebank, Accuracy	beam size UAS  Table 5: Results on WSJ. Speed: sentences per  second.  †: semi-supervised learning.  ‡: joint  POS-tagging and dependency parsing models.
true	P15-1117.pdf#92.35	Penn Treebank, Accuracy	beam size LAS  Table 5: Results on WSJ. Speed: sentences per  second.  †: semi-supervised learning.  ‡: joint  POS-tagging and dependency parsing models.
true	N15-1076.pdf#41.5%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Percentage with Positive Sentiment  Table 1: Statistics for the datasets employed in the experiments.
true	N15-1076.pdf#52.2%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 1: Statistics for the datasets employed in the experiments.
true	N15-1076.pdf#27.7%	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Percentage with Positive Sentiment  Table 1: Statistics for the datasets employed in the experiments.
true	N15-1076.pdf#0.09	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	YELP Dataset 886 Classification Preprocessing Generating ¯ Q / S TRIPADVISOR LDAC inference Classification method q ANCHOR q q q q q q  Table 2: Runtime statistics (in seconds) for the AMAZON and TRIPADVISOR datasets. Blank cells indicate a timing  which does not apply to a particular model. SUP ANCHOR is significantly faster than conventional methods.
true	N15-1076.pdf#80	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	YELP Measure 886 Classification Preprocessing Generating ¯ Q / S Training LDAC inference Classification SUP . ANCHOR q TRIPADVISOR q q  Table 2: Runtime statistics (in seconds) for the AMAZON and TRIPADVISOR datasets. Blank cells indicate a timing  which does not apply to a particular model. SUP ANCHOR is significantly faster than conventional methods.
true	N15-1076.pdf#0.045	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Dataset 886 Classification Preprocessing Generating ¯ Q / S TRIPADVISOR LDAC inference Classification method q ANCHOR  Table 2: Runtime statistics (in seconds) for the AMAZON and TRIPADVISOR datasets. Blank cells indicate a timing  which does not apply to a particular model. SUP ANCHOR is significantly faster than conventional methods.
true	N15-1076.pdf#0.05	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	AMAZON Dataset 886 Classification Preprocessing Generating ¯ Q / S TRIPADVISOR LDAC inference Classification method q ANCHOR q q q q q q  Table 2: Runtime statistics (in seconds) for the AMAZON and TRIPADVISOR datasets. Blank cells indicate a timing  which does not apply to a particular model. SUP ANCHOR is significantly faster than conventional methods.
true	N15-1076.pdf#0.11	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	AMAZON Dataset 886 Classification Preprocessing Generating ¯ Q / S TRIPADVISOR LDAC inference Classification method q ANCHOR q q q q q q  Table 2: Runtime statistics (in seconds) for the AMAZON and TRIPADVISOR datasets. Blank cells indicate a timing  which does not apply to a particular model. SUP ANCHOR is significantly faster than conventional methods.
true	N15-1076.pdf#0.050	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Dataset 886 Classification Preprocessing Generating ¯ Q / S TRIPADVISOR LDAC inference Classification method q ANCHOR  Table 2: Runtime statistics (in seconds) for the AMAZON and TRIPADVISOR datasets. Blank cells indicate a timing  which does not apply to a particular model. SUP ANCHOR is significantly faster than conventional methods.
true	N15-1076.pdf#0.055	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Dataset 886 Classification Preprocessing Generating ¯ Q / S TRIPADVISOR LDAC inference Classification method q ANCHOR  Table 2: Runtime statistics (in seconds) for the AMAZON and TRIPADVISOR datasets. Blank cells indicate a timing  which does not apply to a particular model. SUP ANCHOR is significantly faster than conventional methods.
true	N15-1076.pdf#0.06	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	AMAZON Dataset 886 Classification Preprocessing Generating ¯ Q / S TRIPADVISOR LDAC inference Classification method q ANCHOR q q  Table 2: Runtime statistics (in seconds) for the AMAZON and TRIPADVISOR datasets. Blank cells indicate a timing  which does not apply to a particular model. SUP ANCHOR is significantly faster than conventional methods.
true	N15-1076.pdf#60	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	YELP Measure 886 Classification Preprocessing Generating ¯ Q / S Training LDAC inference Classification SLDA q q TRIPADVISOR q q q q  Table 2: Runtime statistics (in seconds) for the AMAZON and TRIPADVISOR datasets. Blank cells indicate a timing  which does not apply to a particular model. SUP ANCHOR is significantly faster than conventional methods.
true	N15-1076.pdf#0.13	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	AMAZON Dataset 886 Classification Preprocessing Generating ¯ Q / S TRIPADVISOR LDAC inference Classification method q ANCHOR q q q q q q  Table 2: Runtime statistics (in seconds) for the AMAZON and TRIPADVISOR datasets. Blank cells indicate a timing  which does not apply to a particular model. SUP ANCHOR is significantly faster than conventional methods.
true	N15-1076.pdf#40	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	YELP Dataset 886 Classification Preprocessing Generating ¯ Q / S TRIPADVISOR LDAC inference Classification method q ANCHOR q q q q q q q q  Table 2: Runtime statistics (in seconds) for the AMAZON and TRIPADVISOR datasets. Blank cells indicate a timing  which does not apply to a particular model. SUP ANCHOR is significantly faster than conventional methods.
true	N15-1076.pdf#20	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	YELP Dataset 886 Classification Preprocessing Generating ¯ Q / S TRIPADVISOR LDAC inference Classification method q ANCHOR q q q q q q q q  Table 2: Runtime statistics (in seconds) for the AMAZON and TRIPADVISOR datasets. Blank cells indicate a timing  which does not apply to a particular model. SUP ANCHOR is significantly faster than conventional methods.
true	P15-1038.pdf#94.29	benchmark Vietnamese dependency treebank VnDT, UAS	Without Punctuation Overall LS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#94.67	benchmark Vietnamese dependency treebank VnDT, UAS	n / a Without Punctuation Overall LS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#56.44	benchmark Vietnamese dependency treebank VnDT, UAS	n / a With Punctuation Exact Match UAS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#94.67	benchmark Vietnamese dependency treebank VnDT, UAS	n / a Without Punctuation Overall LS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#58.36	benchmark Vietnamese dependency treebank VnDT, UAS	n / a Without Punctuation Exact Match UAS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#61.31	benchmark Vietnamese dependency treebank VnDT, UAS	Without Punctuation Exact Match LS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#49.12	benchmark Vietnamese dependency treebank VnDT, UAS	Without Punctuation Exact Match LAS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#89.19	benchmark Vietnamese dependency treebank VnDT, UAS	With Punctuation Overall LAS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#61.17	benchmark Vietnamese dependency treebank VnDT, UAS	With Punctuation Exact Match LS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#92.50	benchmark Vietnamese dependency treebank VnDT, UAS	n / a Without Punctuation Overall UAS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#90.70	benchmark Vietnamese dependency treebank VnDT, UAS	n / a Without Punctuation Overall LAS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#63.36	benchmark Vietnamese dependency treebank VnDT, UAS	n / a With Punctuation Exact Match LS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#90.03	benchmark Vietnamese dependency treebank VnDT, UAS	n / a With Punctuation Overall LAS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#51.02	benchmark Vietnamese dependency treebank VnDT, UAS	n / a Without Punctuation Exact Match LAS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#55.01	benchmark Vietnamese dependency treebank VnDT, UAS	Without Punctuation Exact Match UAS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#91.62	benchmark Vietnamese dependency treebank VnDT, UAS	n / a With Punctuation Overall UAS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#50.07	benchmark Vietnamese dependency treebank VnDT, UAS	n / a With Punctuation Exact Match LAS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#53.00	benchmark Vietnamese dependency treebank VnDT, UAS	With Punctuation Exact Match UAS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#94.94	benchmark Vietnamese dependency treebank VnDT, UAS	With Punctuation Overall LS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#91.72	benchmark Vietnamese dependency treebank VnDT, UAS	Without Punctuation Overall UAS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#63.42	benchmark Vietnamese dependency treebank VnDT, UAS	n / a Without Punctuation Exact Match LS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#90.63	benchmark Vietnamese dependency treebank VnDT, UAS	With Punctuation Overall UAS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#90.09	benchmark Vietnamese dependency treebank VnDT, UAS	Without Punctuation Overall LAS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#47.65	benchmark Vietnamese dependency treebank VnDT, UAS	With Punctuation Exact Match LAS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#95.29	benchmark Vietnamese dependency treebank VnDT, UAS	n / a With Punctuation Overall LS  Table 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and  non-greedy parsers, respectively.
true	P15-1038.pdf#188	benchmark Vietnamese dependency treebank VnDT, UAS	Java Sent / Sec 10 , 271 95 4 , 287 8 , 602 13 , 963 9 , 838 72  Table 5: Overall parsing speed.
true	P15-1038.pdf#755	benchmark Vietnamese dependency treebank VnDT, UAS	Sent / Sec 10 , 271 95 4 , 287  Table 5: Overall parsing speed.
true	P15-1038.pdf#87.40	benchmark Vietnamese dependency treebank VnDT, UAS	Non - proj . only UAS  Table 6: Accuracy for proj. and non-proj. trees.
true	P15-1038.pdf#91.91	benchmark Vietnamese dependency treebank VnDT, UAS	Projective only UAS  Table 6: Accuracy for proj. and non-proj. trees.
true	P15-1038.pdf#85.51	benchmark Vietnamese dependency treebank VnDT, UAS	Non - proj . only LAS  Table 6: Accuracy for proj. and non-proj. trees.
true	P15-1038.pdf#90.34	benchmark Vietnamese dependency treebank VnDT, UAS	Projective only LAS  Table 6: Accuracy for proj. and non-proj. trees.
true	P15-1038.pdf#90	benchmark Vietnamese dependency treebank VnDT, UAS	Table 9: Differential parsing accuracies.
true	P15-1038.pdf#23	benchmark Vietnamese dependency treebank VnDT, UAS	= 100  Table 9: Differential parsing accuracies.
true	P15-1038.pdf#90	benchmark Vietnamese dependency treebank VnDT, UAS	Table 9: Differential parsing accuracies.
true	P15-1038.pdf#90	benchmark Vietnamese dependency treebank VnDT, UAS	Table 9: Differential parsing accuracies.
true	P15-1038.pdf#75	benchmark Vietnamese dependency treebank VnDT, UAS	= 100  Table 9: Differential parsing accuracies.
true	P15-1062.pdf#53.4	New York Times Corpus, P@10%	F1  Table 1: Performance on the bc dev set for PET. Best com-
true	P15-1062.pdf#46.4	New York Times Corpus, P@10%	cts F1 :  Table 2: In-domain (first column) and out-of-domain performance (columns two to four) on ACE 2005. Systems of the rows
true	P15-1062.pdf#40.5	New York Times Corpus, P@10%	wl F1 :  Table 2: In-domain (first column) and out-of-domain performance (columns two to four) on ACE 2005. Systems of the rows
true	P15-1062.pdf#40.5	New York Times Corpus, P@10%	wl F1 :  Table 2: In-domain (first column) and out-of-domain performance (columns two to four) on ACE 2005. Systems of the rows
true	P15-1062.pdf#52.3	New York Times Corpus, P@10%	bc F1 :  Table 2: In-domain (first column) and out-of-domain performance (columns two to four) on ACE 2005. Systems of the rows
true	P15-1062.pdf#46.4	New York Times Corpus, P@10%	cts F1 :  Table 3: Tree kernel-based in Plank and Moschitti (2013) vs feature-based in
true	P15-1062.pdf#39.6	New York Times Corpus, P@10%	wl F1 :  Table 3: Tree kernel-based in Plank and Moschitti (2013) vs feature-based in
true	P15-1062.pdf#53.1	New York Times Corpus, P@10%	nw+bn ( in - dom . ) F1 :  Table 3: Tree kernel-based in Plank and Moschitti (2013) vs feature-based in
true	P15-1062.pdf#51.5	New York Times Corpus, P@10%	bc F1 :  Table 3: Tree kernel-based in Plank and Moschitti (2013) vs feature-based in
true	P15-1062.pdf#52.3	New York Times Corpus, P@10%	F1  Table 4: Performance of the feature-based method (dev).
true	P15-1062.pdf#54.0	New York Times Corpus, P@10%	F1  Table 4: Performance of the feature-based method (dev).
true	P15-1062.pdf#53.1	New York Times Corpus, P@10%	F1  Table 4: Performance of the feature-based method (dev).
true	P14-1126.pdf#72.2	Penn Treebank, UAS	PTP  Table 7: UAS on sentences of length 10 or less without punctuation from CoNLL shared task treebanks.  "USR †" is the weakly supervised system of
true	P14-1126.pdf#74.3	Penn Treebank, UAS	- U  Table 7: UAS on sentences of length 10 or less without punctuation from CoNLL shared task treebanks.  "USR †" is the weakly supervised system of
true	P14-1126.pdf#83.3	Penn Treebank, UAS	- U  Table 7: UAS on sentences of length 10 or less without punctuation from CoNLL shared task treebanks.  "USR †" is the weakly supervised system of
true	P14-1126.pdf#71.1	Penn Treebank, UAS	- U  Table 7: UAS on sentences of length 10 or less without punctuation from CoNLL shared task treebanks.  "USR †" is the weakly supervised system of
true	P14-1126.pdf#60.1	Penn Treebank, UAS	- U  Table 7: UAS on sentences of length 10 or less without punctuation from CoNLL shared task treebanks.  "USR †" is the weakly supervised system of
true	P14-1126.pdf#76.7	Penn Treebank, UAS	NMG  Table 7: UAS on sentences of length 10 or less without punctuation from CoNLL shared task treebanks.  "USR †" is the weakly supervised system of
true	P14-1126.pdf#67.5	Penn Treebank, UAS	- U  Table 7: UAS on sentences of length 10 or less without punctuation from CoNLL shared task treebanks.  "USR †" is the weakly supervised system of
true	P14-1126.pdf#75.1	Penn Treebank, UAS	- U  Table 7: UAS on sentences of length 10 or less without punctuation from CoNLL shared task treebanks.  "USR †" is the weakly supervised system of
true	P14-1126.pdf#73.6	Penn Treebank, UAS	- U  Table 7: UAS on sentences of length 10 or less without punctuation from CoNLL shared task treebanks.  "USR †" is the weakly supervised system of
true	P14-1126.pdf#72.2	benchmark Vietnamese dependency treebank VnDT, UAS	PTP  Table 7: UAS on sentences of length 10 or less without punctuation from CoNLL shared task treebanks.  "USR †" is the weakly supervised system of
true	P14-1126.pdf#74.3	benchmark Vietnamese dependency treebank VnDT, UAS	- U  Table 7: UAS on sentences of length 10 or less without punctuation from CoNLL shared task treebanks.  "USR †" is the weakly supervised system of
true	P14-1126.pdf#83.3	benchmark Vietnamese dependency treebank VnDT, UAS	- U  Table 7: UAS on sentences of length 10 or less without punctuation from CoNLL shared task treebanks.  "USR †" is the weakly supervised system of
true	P14-1126.pdf#71.1	benchmark Vietnamese dependency treebank VnDT, UAS	- U  Table 7: UAS on sentences of length 10 or less without punctuation from CoNLL shared task treebanks.  "USR †" is the weakly supervised system of
true	P14-1126.pdf#60.1	benchmark Vietnamese dependency treebank VnDT, UAS	- U  Table 7: UAS on sentences of length 10 or less without punctuation from CoNLL shared task treebanks.  "USR †" is the weakly supervised system of
true	P14-1126.pdf#76.7	benchmark Vietnamese dependency treebank VnDT, UAS	NMG  Table 7: UAS on sentences of length 10 or less without punctuation from CoNLL shared task treebanks.  "USR †" is the weakly supervised system of
true	P14-1126.pdf#67.5	benchmark Vietnamese dependency treebank VnDT, UAS	- U  Table 7: UAS on sentences of length 10 or less without punctuation from CoNLL shared task treebanks.  "USR †" is the weakly supervised system of
true	P14-1126.pdf#75.1	benchmark Vietnamese dependency treebank VnDT, UAS	- U  Table 7: UAS on sentences of length 10 or less without punctuation from CoNLL shared task treebanks.  "USR †" is the weakly supervised system of
true	P14-1126.pdf#73.6	benchmark Vietnamese dependency treebank VnDT, UAS	- U  Table 7: UAS on sentences of length 10 or less without punctuation from CoNLL shared task treebanks.  "USR †" is the weakly supervised system of
true	P15-1030.pdf#92.04	Penn Treebank, F1	F1 len ≤ 40  Table 1: Results of our sparse CRF, neural CRF,  and combined parsing models on section 22 of  the Penn Treebank.
true	P15-1030.pdf#91.34	Penn Treebank, F1	F1 all  Table 1: Results of our sparse CRF, neural CRF,  and combined parsing models on section 22 of  the Penn Treebank.
true	P15-1030.pdf#−2.16	Penn Treebank, F1	- -  Table 2: Exploration of other implementation  choices in the feedforward neural network on sen- tences of length ≤ 40 from section 22 of the Penn  Treebank. Rectified linear units perform better  than tanh or cubic units, a network with one hid- den layer performs best, and embedding the output  feature vector gives worse performance.
true	P15-1030.pdf#−1.03	Penn Treebank, F1	- -  Table 2: Exploration of other implementation  choices in the feedforward neural network on sen- tences of length ≤ 40 from section 22 of the Penn  Treebank. Rectified linear units perform better  than tanh or cubic units, a network with one hid- den layer performs best, and embedding the output  feature vector gives worse performance.
true	P15-1030.pdf#−0.23	Penn Treebank, F1	-  Table 2: Exploration of other implementation  choices in the feedforward neural network on sen- tences of length ≤ 40 from section 22 of the Penn  Treebank. Rectified linear units perform better  than tanh or cubic units, a network with one hid- den layer performs best, and embedding the output  feature vector gives worse performance.
true	P15-1030.pdf#−0.39	Penn Treebank, F1	- -  Table 2: Exploration of other implementation  choices in the feedforward neural network on sen- tences of length ≤ 40 from section 22 of the Penn  Treebank. Rectified linear units perform better  than tanh or cubic units, a network with one hid- den layer performs best, and embedding the output  feature vector gives worse performance.
true	P15-1030.pdf#−0.43	Penn Treebank, F1	- -  Table 2: Exploration of other implementation  choices in the feedforward neural network on sen- tences of length ≤ 40 from section 22 of the Penn  Treebank. Rectified linear units perform better  than tanh or cubic units, a network with one hid- den layer performs best, and embedding the output  feature vector gives worse performance.
true	P15-1030.pdf#80.95	Penn Treebank, F1	Test , all lengths German  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#84.37	Penn Treebank, F1	Dev , all lengths Basque  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#81.32	Penn Treebank, F1	Reranked ensemble Arabic  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#82.23	Penn Treebank, F1	Test , all lengths Korean  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#80.68	Penn Treebank, F1	Dev , all lengths Arabic  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#89.37	Penn Treebank, F1	Dev , all lengths Hebrew  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#90.66	Penn Treebank, F1	Test , all lengths Hungarian  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#92.97	Penn Treebank, F1	Test , all lengths Polish  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#84.68	Penn Treebank, F1	Dev , all lengths Avg  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#80.24	Penn Treebank, F1	Test , all lengths Arabic  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#77.93	Penn Treebank, F1	Dev , all lengths Swedish  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#82.35	Penn Treebank, F1	Dev , all lengths Korean  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#85.41	Penn Treebank, F1	Test , all lengths Basque  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#92.10	Penn Treebank, F1	Dev , all lengths Polish  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#81.66	Penn Treebank, F1	Reranked ensemble German  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#86.12	Penn Treebank, F1	Reranked ensemble Avg  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#85.50	Penn Treebank, F1	Reranked ensemble Swedish  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#88.61	Penn Treebank, F1	Test , all lengths Hebrew  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#89.80	Penn Treebank, F1	Reranked ensemble Hebrew  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#91.72	Penn Treebank, F1	Reranked ensemble Hungarian  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#90.50	Penn Treebank, F1	Reranked ensemble Polish  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#81.25	Penn Treebank, F1	Test , all lengths French  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#85.08	Penn Treebank, F1	Test , all lengths Avg  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#88.24	Penn Treebank, F1	Reranked ensemble Basque  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#83.45	Penn Treebank, F1	Test , all lengths Swedish  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#83.81	Penn Treebank, F1	Reranked ensemble Korean  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#82.53	Penn Treebank, F1	Reranked ensemble French  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#85.25	Penn Treebank, F1	Dev , all lengths German  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#89.46	Penn Treebank, F1	Dev , all lengths Hungarian  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	P15-1030.pdf#80.65	Penn Treebank, F1	Dev , all lengths French  Table 3: Results for the nine treebanks in the SPMRL 2013/2014 Shared Tasks; all values are F-scores  for sentences of all lengths using the version of evalb distributed with the shared task. Our parser  substantially outperforms the strongest single parser results on this dataset (Hall et al., 2014; Crabbé and  Seddah, 2014). Berkeley-Tags is an improved version of the Berkeley parser designed for the shared task  (Seddah et al., 2013). 2014 Best is a reranked ensemble of modified Berkeley parsers and constitutes the  best published numbers on this dataset (
true	D14-1173.pdf#0.915	Chinese Treebank 6, F1	F 1  Table 6: Performance of WS
true	D14-1173.pdf#0.922	Chinese Treebank 6, F1	Prec .  Table 6: Performance of WS
true	D14-1173.pdf#0.908	Chinese Treebank 6, F1	Recall  Table 6: Performance of WS
true	D14-1173.pdf#0.547	Chinese Treebank 6, F1	Table 6 : Performance of WS Recall  Table 7: Performance of unsupervised WA using  different WS strategies
true	D14-1173.pdf#0.396	Chinese Treebank 6, F1	Table 6 : Performance of WS AER  Table 7: Performance of unsupervised WA using  different WS strategies
true	D14-1173.pdf#0.674	Chinese Treebank 6, F1	Table 6 : Performance of WS Prec .  Table 7: Performance of unsupervised WA using  different WS strategies
true	D14-1173.pdf#0.915	WMT 2014 EN-DE, BLEU	F 1  Table 6: Performance of WS
true	D14-1173.pdf#0.922	WMT 2014 EN-DE, BLEU	Prec .  Table 6: Performance of WS
true	D14-1173.pdf#0.908	WMT 2014 EN-DE, BLEU	Recall  Table 6: Performance of WS
true	D14-1173.pdf#0.547	WMT 2014 EN-DE, BLEU	Table 6 : Performance of WS Recall  Table 7: Performance of unsupervised WA using  different WS strategies
true	D14-1173.pdf#0.396	WMT 2014 EN-DE, BLEU	Table 6 : Performance of WS AER  Table 7: Performance of unsupervised WA using  different WS strategies
true	D14-1173.pdf#0.674	WMT 2014 EN-DE, BLEU	Table 6 : Performance of WS Prec .  Table 7: Performance of unsupervised WA using  different WS strategies
true	D13-1148.pdf#75.25	SemEval-2010 Task 8, F1	Table 2: Performance of the model with various parameters: supervised recall on the trial data. The best value from  each row is bold-faced. The scores are averaged over 100 runs.
true	D13-1148.pdf#74.76	SemEval-2010 Task 8, F1	Table 2: Performance of the model with various parameters: supervised recall on the trial data. The best value from  each row is bold-faced. The scores are averaged over 100 runs.
true	D13-1148.pdf#75.61	SemEval-2010 Task 8, F1	Table 2: Performance of the model with various parameters: supervised recall on the trial data. The best value from  each row is bold-faced. The scores are averaged over 100 runs.
true	D13-1148.pdf#76.02	SemEval-2010 Task 8, F1	Table 2: Performance of the model with various parameters: supervised recall on the trial data. The best value from  each row is bold-faced. The scores are averaged over 100 runs.
true	D13-1148.pdf#15.6	SemEval-2010 Task 8, F1	verbs  Table 3: Unsupervised evaluation: V-Measure
true	D13-1148.pdf#18.0	SemEval-2010 Task 8, F1	all  Table 3: Unsupervised evaluation: V-Measure
true	D13-1148.pdf#23.7	SemEval-2010 Task 8, F1	nouns  Table 3: Unsupervised evaluation: V-Measure
true	D13-1148.pdf#57.0	SemEval-2010 Task 8, F1	nouns  Table 4: Unsupervised evaluation: paired F-Score
true	D13-1148.pdf#72.7	SemEval-2010 Task 8, F1	verbs  Table 4: Unsupervised evaluation: paired F-Score
true	D13-1148.pdf#63.5	SemEval-2010 Task 8, F1	all  Table 4: Unsupervised evaluation: paired F-Score
true	D13-1148.pdf#65.4	SemEval-2010 Task 8, F1	classes mapped from clusters ) . 2 for details of super - all  Table 5: Supervised evaluation: supervised recall, 80%  mapping and 20% evaluation
true	D13-1148.pdf#62.6	SemEval-2010 Task 8, F1	classes mapped from clusters ) . 2 for details of super - nouns  Table 5: Supervised evaluation: supervised recall, 80%  mapping and 20% evaluation
true	D13-1148.pdf#70.2	SemEval-2010 Task 8, F1	classes mapped from clusters ) . 2 for details of super - verbs  Table 5: Supervised evaluation: supervised recall, 80%  mapping and 20% evaluation
true	P13-2109.pdf#93.07	Penn Treebank, UAS	722 UAS  Table 2: Results for the projective English dataset.  We report unlabeled attachment scores (UAS) ig- noring punctuation, and parsing speeds in tokens  per second. Our speeds include the time necessary  for pruning, evaluating features, and decoding, as  measured on a Intel Core i7 processor @3.4 GHz.  The others are speeds reported in the cited papers;  those marked with  † were converted from times per  sentence.
true	P13-2109.pdf#92.82	Penn Treebank, UAS	66 † UAS  Table 2: Results for the projective English dataset.  We report unlabeled attachment scores (UAS) ig- noring punctuation, and parsing speeds in tokens  per second. Our speeds include the time necessary  for pruning, evaluating features, and decoding, as  measured on a Intel Core i7 processor @3.4 GHz.  The others are speeds reported in the cited papers;  those marked with  † were converted from times per  sentence.
true	P15-1047.pdf#52.14†	New York Times Corpus, P@10%	w / o  Table 2: The MAP of predicted slots (%);  † indicates that the result is significantly better than the Logistic  Regression (row (b)) with p < 0.05 in t-test.
true	P15-1047.pdf#40.46†	New York Times Corpus, P@10%	w / o  Table 2: The MAP of predicted slots (%);  † indicates that the result is significantly better than the Logistic  Regression (row (b)) with p < 0.05 in t-test.
true	P15-1047.pdf#43.51†	New York Times Corpus, P@10%	w / Explicit  Table 2: The MAP of predicted slots (%);  † indicates that the result is significantly better than the Logistic  Regression (row (b)) with p < 0.05 in t-test.
true	P15-1047.pdf#53.40†	New York Times Corpus, P@10%	w / Explicit  Table 2: The MAP of predicted slots (%);  † indicates that the result is significantly better than the Logistic  Regression (row (b)) with p < 0.05 in t-test.
true	N10-1054.pdf#80.2%	SUBJ, Accuracy	different feature settings Accuracy of lexical substitution with different B+gold senses  Table 1: Accuracy of lexical substitution with different  different feature settings
true	N10-1054.pdf#77.9%	SUBJ, Accuracy	different feature settings Accuracy of lexical substitution with different B+gold subj  Table 1: Accuracy of lexical substitution with different  different feature settings
true	N10-1054.pdf#70.1%	SUBJ, Accuracy	different feature settings Accuracy of lexical substitution with different B+auto senses  Table 1: Accuracy of lexical substitution with different  different feature settings
true	N10-1054.pdf#70.7%	SUBJ, Accuracy	different feature settings Accuracy of lexical substitution with different B+auto subj  Table 1: Accuracy of lexical substitution with different  different feature settings
true	N10-1054.pdf#57.4%	SUBJ, Accuracy	different feature settings Accuracy of lexical substitution with different Baseline ( B )  Table 1: Accuracy of lexical substitution with different  different feature settings
true	N10-1054.pdf#68.5%	SUBJ, Accuracy	different feature settings Accuracy of lexical substitution with different Basic ( B )  Table 1: Accuracy of lexical substitution with different  different feature settings
true	D10-1109.pdf#0.754	SearchQA, Unigram Acc	gram features . Although MSS is inferior to MSB on The results are shown in Tab 4 . From the results we Easy data  Table 4: Experimental results on Chinese cQA data
true	D10-1109.pdf#0.717	SearchQA, Unigram Acc	gram features . Although MSS is inferior to MSB on The results are shown in Tab 4 . From the results we All data  Table 4: Experimental results on Chinese cQA data
true	D13-1205.pdf#88.24*	Chinese Treebank 6, F1	PR  Table 1: POS tagging results. BASE represents the best  model of Täckström et al. (2013). PR is a system with  the same features but with relaxed constraints. BASE+  and PR+ add additional model features (see  §5.2.3).  *  in- dicates improvements over the previous state of the art  (BASE), and bold values indicate the best score for a lan- guage. "Avg" indicates averaged results for all 17 lan- guages, while "-zh-ar" shows averaged results without  Chinese and Arabic.
true	D13-1205.pdf#90.63*	Chinese Treebank 6, F1	PR  Table 1: POS tagging results. BASE represents the best  model of Täckström et al. (2013). PR is a system with  the same features but with relaxed constraints. BASE+  and PR+ add additional model features (see  §5.2.3).  *  in- dicates improvements over the previous state of the art  (BASE), and bold values indicate the best score for a lan- guage. "Avg" indicates averaged results for all 17 lan- guages, while "-zh-ar" shows averaged results without  Chinese and Arabic.
true	D13-1205.pdf#50.10*	Chinese Treebank 6, F1	PR+  Table 1: POS tagging results. BASE represents the best  model of Täckström et al. (2013). PR is a system with  the same features but with relaxed constraints. BASE+  and PR+ add additional model features (see  §5.2.3).  *  in- dicates improvements over the previous state of the art  (BASE), and bold values indicate the best score for a lan- guage. "Avg" indicates averaged results for all 17 lan- guages, while "-zh-ar" shows averaged results without  Chinese and Arabic.
true	D13-1205.pdf#90.41*	Chinese Treebank 6, F1	PR  Table 1: POS tagging results. BASE represents the best  model of Täckström et al. (2013). PR is a system with  the same features but with relaxed constraints. BASE+  and PR+ add additional model features (see  §5.2.3).  *  in- dicates improvements over the previous state of the art  (BASE), and bold values indicate the best score for a lan- guage. "Avg" indicates averaged results for all 17 lan- guages, while "-zh-ar" shows averaged results without  Chinese and Arabic.
true	D13-1205.pdf#89.89*	Chinese Treebank 6, F1	BASE+  Table 1: POS tagging results. BASE represents the best  model of Täckström et al. (2013). PR is a system with  the same features but with relaxed constraints. BASE+  and PR+ add additional model features (see  §5.2.3).  *  in- dicates improvements over the previous state of the art  (BASE), and bold values indicate the best score for a lan- guage. "Avg" indicates averaged results for all 17 lan- guages, while "-zh-ar" shows averaged results without  Chinese and Arabic.
true	D13-1205.pdf#80.68*	Chinese Treebank 6, F1	PR+  Table 1: POS tagging results. BASE represents the best  model of Täckström et al. (2013). PR is a system with  the same features but with relaxed constraints. BASE+  and PR+ add additional model features (see  §5.2.3).  *  in- dicates improvements over the previous state of the art  (BASE), and bold values indicate the best score for a lan- guage. "Avg" indicates averaged results for all 17 lan- guages, while "-zh-ar" shows averaged results without  Chinese and Arabic.
true	D13-1205.pdf#91.60*	Chinese Treebank 6, F1	BASE+  Table 1: POS tagging results. BASE represents the best  model of Täckström et al. (2013). PR is a system with  the same features but with relaxed constraints. BASE+  and PR+ add additional model features (see  §5.2.3).  *  in- dicates improvements over the previous state of the art  (BASE), and bold values indicate the best score for a lan- guage. "Avg" indicates averaged results for all 17 lan- guages, while "-zh-ar" shows averaged results without  Chinese and Arabic.
true	D13-1205.pdf#80.63	Chinese Treebank 6, F1	BASE  Table 1: POS tagging results. BASE represents the best  model of Täckström et al. (2013). PR is a system with  the same features but with relaxed constraints. BASE+  and PR+ add additional model features (see  §5.2.3).  *  in- dicates improvements over the previous state of the art  (BASE), and bold values indicate the best score for a lan- guage. "Avg" indicates averaged results for all 17 lan- guages, while "-zh-ar" shows averaged results without  Chinese and Arabic.
true	D13-1205.pdf#87.53*	Chinese Treebank 6, F1	BASE+  Table 1: POS tagging results. BASE represents the best  model of Täckström et al. (2013). PR is a system with  the same features but with relaxed constraints. BASE+  and PR+ add additional model features (see  §5.2.3).  *  in- dicates improvements over the previous state of the art  (BASE), and bold values indicate the best score for a lan- guage. "Avg" indicates averaged results for all 17 lan- guages, while "-zh-ar" shows averaged results without  Chinese and Arabic.
true	D13-1205.pdf#66.47*	Chinese Treebank 6, F1	PR+  Table 1: POS tagging results. BASE represents the best  model of Täckström et al. (2013). PR is a system with  the same features but with relaxed constraints. BASE+  and PR+ add additional model features (see  §5.2.3).  *  in- dicates improvements over the previous state of the art  (BASE), and bold values indicate the best score for a lan- guage. "Avg" indicates averaged results for all 17 lan- guages, while "-zh-ar" shows averaged results without  Chinese and Arabic.
true	D13-1205.pdf#83.13*	Chinese Treebank 6, F1	PR+  Table 1: POS tagging results. BASE represents the best  model of Täckström et al. (2013). PR is a system with  the same features but with relaxed constraints. BASE+  and PR+ add additional model features (see  §5.2.3).  *  in- dicates improvements over the previous state of the art  (BASE), and bold values indicate the best score for a lan- guage. "Avg" indicates averaged results for all 17 lan- guages, while "-zh-ar" shows averaged results without  Chinese and Arabic.
true	D13-1205.pdf#88.42*	Chinese Treebank 6, F1	PR+  Table 1: POS tagging results. BASE represents the best  model of Täckström et al. (2013). PR is a system with  the same features but with relaxed constraints. BASE+  and PR+ add additional model features (see  §5.2.3).  *  in- dicates improvements over the previous state of the art  (BASE), and bold values indicate the best score for a lan- guage. "Avg" indicates averaged results for all 17 lan- guages, while "-zh-ar" shows averaged results without  Chinese and Arabic.
true	D13-1205.pdf#74.51*	Chinese Treebank 6, F1	PR  Table 1: POS tagging results. BASE represents the best  model of Täckström et al. (2013). PR is a system with  the same features but with relaxed constraints. BASE+  and PR+ add additional model features (see  §5.2.3).  *  in- dicates improvements over the previous state of the art  (BASE), and bold values indicate the best score for a lan- guage. "Avg" indicates averaged results for all 17 lan- guages, while "-zh-ar" shows averaged results without  Chinese and Arabic.
true	D13-1205.pdf#83.49*	Chinese Treebank 6, F1	PR+  Table 1: POS tagging results. BASE represents the best  model of Täckström et al. (2013). PR is a system with  the same features but with relaxed constraints. BASE+  and PR+ add additional model features (see  §5.2.3).  *  in- dicates improvements over the previous state of the art  (BASE), and bold values indicate the best score for a lan- guage. "Avg" indicates averaged results for all 17 lan- guages, while "-zh-ar" shows averaged results without  Chinese and Arabic.
true	D13-1205.pdf#89.25*	Chinese Treebank 6, F1	PR  Table 1: POS tagging results. BASE represents the best  model of Täckström et al. (2013). PR is a system with  the same features but with relaxed constraints. BASE+  and PR+ add additional model features (see  §5.2.3).  *  in- dicates improvements over the previous state of the art  (BASE), and bold values indicate the best score for a lan- guage. "Avg" indicates averaged results for all 17 lan- guages, while "-zh-ar" shows averaged results without  Chinese and Arabic.
true	D13-1205.pdf#87.21*	Chinese Treebank 6, F1	PR+  Table 1: POS tagging results. BASE represents the best  model of Täckström et al. (2013). PR is a system with  the same features but with relaxed constraints. BASE+  and PR+ add additional model features (see  §5.2.3).  *  in- dicates improvements over the previous state of the art  (BASE), and bold values indicate the best score for a lan- guage. "Avg" indicates averaged results for all 17 lan- guages, while "-zh-ar" shows averaged results without  Chinese and Arabic.
true	D13-1205.pdf#82.40*	Chinese Treebank 6, F1	PR  Table 1: POS tagging results. BASE represents the best  model of Täckström et al. (2013). PR is a system with  the same features but with relaxed constraints. BASE+  and PR+ add additional model features (see  §5.2.3).  *  in- dicates improvements over the previous state of the art  (BASE), and bold values indicate the best score for a lan- guage. "Avg" indicates averaged results for all 17 lan- guages, while "-zh-ar" shows averaged results without  Chinese and Arabic.
true	D13-1205.pdf#85.98*	Chinese Treebank 6, F1	PR+  Table 1: POS tagging results. BASE represents the best  model of Täckström et al. (2013). PR is a system with  the same features but with relaxed constraints. BASE+  and PR+ add additional model features (see  §5.2.3).  *  in- dicates improvements over the previous state of the art  (BASE), and bold values indicate the best score for a lan- guage. "Avg" indicates averaged results for all 17 lan- guages, while "-zh-ar" shows averaged results without  Chinese and Arabic.
true	D13-1205.pdf#85.40*	Chinese Treebank 6, F1	PR+  Table 1: POS tagging results. BASE represents the best  model of Täckström et al. (2013). PR is a system with  the same features but with relaxed constraints. BASE+  and PR+ add additional model features (see  §5.2.3).  *  in- dicates improvements over the previous state of the art  (BASE), and bold values indicate the best score for a lan- guage. "Avg" indicates averaged results for all 17 lan- guages, while "-zh-ar" shows averaged results without  Chinese and Arabic.
true	D15-1168.pdf#75.00	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	LSTM - RNN + Feat . Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#72.20	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#80.37	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Elman - RNN Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#81.37	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	LSTM - RNN + Feat . Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#79.89	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	LSTM - RNN + Feat . Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#81.06	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Bi - LSTM - RNN Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#79.79	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	LSTM - RNN Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#73.42	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Jordan - RNN Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#74.43	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Elman - RNN Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#73.93	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Bi - Elman - RNN Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#81.66	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Elman - RNN + Feat . Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#80.10	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Elman - RNN + Feat . Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#74.55	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Bi - LSTM - RNN + Feat . Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#80.36	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#79.89	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Jordan - RNN Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#74.25	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Elman - RNN + Feat . Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#84.01	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	SemEval - 14 top systems Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#74.02	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Bi - LSTM - RNN Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#82.06	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Bi - Elman - RNN + Feat . Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#73.52	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	LSTM - RNN Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#74.03	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Bi - LSTM - RNN Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#74.57	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Bi - Elman - RNN + Feat . Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#75.00	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	LSTM - RNN + Feat . Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#72.20	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#80.37	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Elman - RNN Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#81.37	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	LSTM - RNN + Feat . Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#79.89	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	LSTM - RNN + Feat . Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#81.06	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Bi - LSTM - RNN Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#79.79	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	LSTM - RNN Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#73.42	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Jordan - RNN Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#74.43	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Elman - RNN Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#73.93	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Bi - Elman - RNN Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#81.66	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Elman - RNN + Feat . Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#80.10	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Elman - RNN + Feat . Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#74.55	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Bi - LSTM - RNN + Feat . Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#80.36	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#79.89	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Jordan - RNN Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#74.25	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Elman - RNN + Feat . Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#84.01	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	SemEval - 14 top systems Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#74.02	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Bi - LSTM - RNN Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#82.06	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Bi - Elman - RNN + Feat . Restaurant  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#73.52	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	LSTM - RNN Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#74.03	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Bi - LSTM - RNN Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D15-1168.pdf#74.57	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Bi - Elman - RNN + Feat . Laptop  Table 3: F 1 -score performance for CRF baselines,  RNNs and SemEval'14 best systems on the stan- dard Laptop and Restaturant testsets. |h l | and |h r |  columns show the number of hidden units.
true	D12-1051.pdf#91.39	Chinese Treebank 6, F1	Precision  Table 2: Experimental results on Chinese NP  chunking.
true	D12-1051.pdf#91.16	Chinese Treebank 6, F1	F1  Table 2: Experimental results on Chinese NP  chunking.
true	D12-1051.pdf#90.93	Chinese Treebank 6, F1	Recall  Table 2: Experimental results on Chinese NP  chunking.
true	D12-1051.pdf#92.11	Chinese Treebank 6, F1	specifically , learning with F1 loss provides much better results for ADJP , ADVP , DVP , NP and VP , respectively . comparable results to 0 - 1 loss in other categories . More F1 loss F1  Table 3: Experimental results on Chinese text chunking.
true	D12-1051.pdf#91.68	Chinese Treebank 6, F1	two different systems with different loss functions for learning . Observing the results in Table 3 , we can see that learning with F1 loss can improve the 0 - 1 recognition . F1 loss recall  Table 3: Experimental results on Chinese text chunking.
true	D12-1051.pdf#92.54	Chinese Treebank 6, F1	two different systems with different loss functions for learning . Observing the results in Table 3 , we can see that learning with F1 loss can improve the learning NP F1 loss precision  Table 3: Experimental results on Chinese text chunking.
true	D12-1051.pdf#92.11	Chinese Treebank 6, F1	F1  Table 4: Comparisons of chunking performance for  Chinese NP chunking and text chunking.
true	D12-1051.pdf#91.50	Chinese Treebank 6, F1	F1  Table 4: Comparisons of chunking performance for  Chinese NP chunking and text chunking.
true	D12-1051.pdf#94.22	Chinese Treebank 6, F1	) . F1  Table 6: Performance on English corpus.
true	D12-1051.pdf#94.72	Chinese Treebank 6, F1	) . F1  Table 6: Performance on English corpus.
true	D15-1196.pdf#57.16	Text8, Number of params	- RG - 65  Table 1: Spearman coefficient results (%) on word  similarity computation.
true	D15-1196.pdf#57.60	Text8, Number of params	MEN  Table 1: Spearman coefficient results (%) on word  similarity computation.
true	D15-1196.pdf#71.74	Text8, Number of params	- WS - 203  Table 1: Spearman coefficient results (%) on word  similarity computation.
true	D15-1196.pdf#94.80	Text8, Number of params	Precision  Table 2: Experiment results (%) on word intrusion  detection.
true	D15-1196.pdf#57.16	Text8, Bit per Character (BPC)	- RG - 65  Table 1: Spearman coefficient results (%) on word  similarity computation.
true	D15-1196.pdf#57.60	Text8, Bit per Character (BPC)	MEN  Table 1: Spearman coefficient results (%) on word  similarity computation.
true	D15-1196.pdf#71.74	Text8, Bit per Character (BPC)	- WS - 203  Table 1: Spearman coefficient results (%) on word  similarity computation.
true	D15-1196.pdf#94.80	Text8, Bit per Character (BPC)	Precision  Table 2: Experiment results (%) on word intrusion  detection.
true	P12-1032.pdf#44.73	Text8, Number of params	baseline , but not as well as G - Re - Rank . ) . Using Struct - LP , the performance with devset9  Table 3. Consensus-based re-ranking and decoding  for IWSLT data set. The results in bold type are  significantly better than the baseline.
true	P12-1032.pdf#48.79	Text8, Number of params	baseline , but not as well as G - Re - Rank . ) . Using Struct - LP , the performance improved , devset8+dialog  Table 3. Consensus-based re-ranking and decoding  for IWSLT data set. The results in bold type are  significantly better than the baseline.
true	P12-1032.pdf#28.51	Text8, Number of params	NIST ' 08  Table 4. Consensus-based re-ranking and decoding  for NIST data set. The results in bold type are  significantly better than the baseline.
true	P12-1032.pdf#39.17	Text8, Number of params	NIST ' 05  Table 4. Consensus-based re-ranking and decoding  for NIST data set. The results in bold type are  significantly better than the baseline.
true	P12-1032.pdf#28.18	Text8, Number of params	NIST ' 08  Table 4. Consensus-based re-ranking and decoding  for NIST data set. The results in bold type are  significantly better than the baseline.
true	P12-1032.pdf#39.62	Text8, Number of params	NIST ' 03  Table 4. Consensus-based re-ranking and decoding  for NIST data set. The results in bold type are  significantly better than the baseline.
true	P12-1032.pdf#39.21	Text8, Number of params	NIST ' 03  Table 4. Consensus-based re-ranking and decoding  for NIST data set. The results in bold type are  significantly better than the baseline.
true	P12-1032.pdf#39.02	Text8, Number of params	NIST ' 05  Table 4. Consensus-based re-ranking and decoding  for NIST data set. The results in bold type are  significantly better than the baseline.
true	P12-1032.pdf#39.42	Text8, Number of params	NIST ' 03  Table 4. Consensus-based re-ranking and decoding  for NIST data set. The results in bold type are  significantly better than the baseline.
true	P12-1032.pdf#28.20	Text8, Number of params	NIST ' 08  Table 4. Consensus-based re-ranking and decoding  for NIST data set. The results in bold type are  significantly better than the baseline.
true	P12-1032.pdf#28.76	Text8, Number of params	NIST ' 08  Table 4. Consensus-based re-ranking and decoding  for NIST data set. The results in bold type are  significantly better than the baseline.
true	P12-1032.pdf#28.21	Text8, Number of params	NIST ' 08  Table 4. Consensus-based re-ranking and decoding  for NIST data set. The results in bold type are  significantly better than the baseline.
true	P12-1032.pdf#38.93	Text8, Number of params	NIST ' 05  Table 4. Consensus-based re-ranking and decoding  for NIST data set. The results in bold type are  significantly better than the baseline.
true	D13-1061.pdf#64.24	Chinese Treebank 6, F1	− Roov  Table 3: Comparison of the F-scores on the Penn Chinese  Treebank
true	D13-1061.pdf#64.24	Penn Treebank, Accuracy	− Roov  Table 3: Comparison of the F-scores on the Penn Chinese  Treebank
true	D14-1186.pdf#0.545	PKU, F1	KE - 10  Table 1: Comparison of WS, TE and KE Perfor- mance (averaged on the 10 domains).
true	D14-1186.pdf#0.374	PKU, F1	OOV - R  Table 1: Comparison of WS, TE and KE Perfor- mance (averaged on the 10 domains).
true	D14-1186.pdf#0.640	PKU, F1	TE - 100  Table 1: Comparison of WS, TE and KE Perfor- mance (averaged on the 10 domains).
true	D14-1186.pdf#0.571	PKU, F1	KE - 5  Table 1: Comparison of WS, TE and KE Perfor- mance (averaged on the 10 domains).
true	D14-1186.pdf#0.704	PKU, F1	TE - 50  Table 1: Comparison of WS, TE and KE Perfor- mance (averaged on the 10 domains).
true	D14-1186.pdf#0.821	PKU, F1	F1  Table 1: Comparison of WS, TE and KE Perfor- mance (averaged on the 10 domains).
true	D14-1186.pdf#0.545	Chinese Treebank 6, F1	KE - 10  Table 1: Comparison of WS, TE and KE Perfor- mance (averaged on the 10 domains).
true	D14-1186.pdf#0.374	Chinese Treebank 6, F1	OOV - R  Table 1: Comparison of WS, TE and KE Perfor- mance (averaged on the 10 domains).
true	D14-1186.pdf#0.640	Chinese Treebank 6, F1	TE - 100  Table 1: Comparison of WS, TE and KE Perfor- mance (averaged on the 10 domains).
true	D14-1186.pdf#0.571	Chinese Treebank 6, F1	KE - 5  Table 1: Comparison of WS, TE and KE Perfor- mance (averaged on the 10 domains).
true	D14-1186.pdf#0.704	Chinese Treebank 6, F1	TE - 50  Table 1: Comparison of WS, TE and KE Perfor- mance (averaged on the 10 domains).
true	D14-1186.pdf#0.821	Chinese Treebank 6, F1	F1  Table 1: Comparison of WS, TE and KE Perfor- mance (averaged on the 10 domains).
true	N12-1059.pdf#56.65**	Text8, Number of params	MT06 nw BLEU  Table 1: Results on Arabic-English MT.  *  = Significant  improvement at 95% confidence, as defined by
true	N12-1059.pdf#48.00**	Text8, Number of params	MT08 eval BLEU  Table 1: Results on Arabic-English MT.  *  = Significant  improvement at 95% confidence, as defined by
true	P13-2050.pdf#0.55	New York Times Corpus, P@10%	Test KL F  Table 1: Sensitivity of Data Source
true	P13-2050.pdf#0.60	New York Times Corpus, P@10%	Test CP F  Table 1: Sensitivity of Data Source
true	P13-2050.pdf#0.62	New York Times Corpus, P@10%	Test CS F  Table 1: Sensitivity of Data Source
true	P13-2050.pdf#0.62	New York Times Corpus, P@10%	( Shown in Tab . 2 ) . F  Table 2: Existing Methods Comparison
true	P15-2014.pdf#48.30	SemEval-2010 Task 8, F1	( a ) No boundary information . . . short NPs only  Table 3: CoNLL metric scores on DIRNDL for  different prosodic features (no singletons, signifi- cant results in boldface)
true	P15-2014.pdf#48.88	SemEval-2010 Task 8, F1	( n1 vs . n2 vs . pn vs . none ) ( a ) No boundary information . . . all NPs . . . all NPs  Table 3: CoNLL metric scores on DIRNDL for  different prosodic features (no singletons, signifi- cant results in boldface)
true	P15-2014.pdf#48.45	SemEval-2010 Task 8, F1	( n1 / n2 vs . pn vs . none ) ( a ) No boundary information . . . all NPs . . . all NPs  Table 3: CoNLL metric scores on DIRNDL for  different prosodic features (no singletons, signifi- cant results in boldface)
true	P15-2014.pdf#48.76	SemEval-2010 Task 8, F1	( n1 / n2 vs . pn vs . none ) ( a ) No boundary information . . . short NPs only . . . short NPs only  Table 3: CoNLL metric scores on DIRNDL for  different prosodic features (no singletons, signifi- cant results in boldface)
true	P15-2014.pdf#48.55	SemEval-2010 Task 8, F1	( b ) Including boundary information ( a ) No boundary information . . . short NPs only . . . short NPs only  Table 3: CoNLL metric scores on DIRNDL for  different prosodic features (no singletons, signifi- cant results in boldface)
true	P12-1060.pdf#71.52	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	MPQA - EN NTCIR - CH N / A N / A  Table 2: Classification Accuracy Using Only  English Labeled Data
true	P12-1060.pdf#70.96	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	NTCIR - EN NTCIR - CH N / A N / A  Table 2: Classification Accuracy Using Only  English Labeled Data
true	P12-1060.pdf#71.52	SearchQA, Unigram Acc	MPQA - EN NTCIR - CH N / A N / A  Table 2: Classification Accuracy Using Only  English Labeled Data
true	P12-1060.pdf#70.96	SearchQA, Unigram Acc	NTCIR - EN NTCIR - CH N / A N / A  Table 2: Classification Accuracy Using Only  English Labeled Data
true	N15-1159.pdf#59.20(1)	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	NRC ML lex .  Table 4: Semeval-2014. Numbers in parenthesis is the  absolute rank of a system on a given test set. Bold scores  compares using our ML lexicon on top of the NRC sys- tem. Results marked with  † are statistically significant at  p > 0.05 (via the paired t-test).
true	N15-1159.pdf#71.32†(2)	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	NRC ML lex .  Table 4: Semeval-2014. Numbers in parenthesis is the  absolute rank of a system on a given test set. Bold scores  compares using our ML lexicon on top of the NRC sys- tem. Results marked with  † are statistically significant at  p > 0.05 (via the paired t-test).
true	N15-1159.pdf#70.51†(2)	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	NRC ML lex .  Table 4: Semeval-2014. Numbers in parenthesis is the  absolute rank of a system on a given test set. Bold scores  compares using our ML lexicon on top of the NRC sys- tem. Results marked with  † are statistically significant at  p > 0.05 (via the paired t-test).
true	N15-1159.pdf#76.54†(1)	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	NRC ML lex .  Table 4: Semeval-2014. Numbers in parenthesis is the  absolute rank of a system on a given test set. Bold scores  compares using our ML lexicon on top of the NRC sys- tem. Results marked with  † are statistically significant at  p > 0.05 (via the paired t-test).
true	N15-1159.pdf#67.20	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	NRC ML lex .  Table 4: Semeval-2014. Numbers in parenthesis is the  absolute rank of a system on a given test set. Bold scores  compares using our ML lexicon on top of the NRC sys- tem. Results marked with  † are statistically significant at  p > 0.05 (via the paired t-test).
true	P15-1149.pdf#92.83	Penn Treebank, UAS	UR  Table 3: Comparing the state-of-art with our mod- els on test set.
true	P15-1149.pdf#38.14	Penn Treebank, UAS	UEM  Table 3: Comparing the state-of-art with our mod- els on test set.
true	P15-1149.pdf#93.71	Penn Treebank, UAS	UP  Table 3: Comparing the state-of-art with our mod- els on test set.
true	P15-1149.pdf#93.23	Penn Treebank, UAS	UF  Table 3: Comparing the state-of-art with our mod- els on test set.
true	P15-1149.pdf#92.83	Penn Treebank, POS	UR  Table 3: Comparing the state-of-art with our mod- els on test set.
true	P15-1149.pdf#38.14	Penn Treebank, POS	UEM  Table 3: Comparing the state-of-art with our mod- els on test set.
true	P15-1149.pdf#93.71	Penn Treebank, POS	UP  Table 3: Comparing the state-of-art with our mod- els on test set.
true	P15-1149.pdf#93.23	Penn Treebank, POS	UF  Table 3: Comparing the state-of-art with our mod- els on test set.
true	P15-1149.pdf#92.83	Penn Treebank, LAS	UR  Table 3: Comparing the state-of-art with our mod- els on test set.
true	P15-1149.pdf#38.14	Penn Treebank, LAS	UEM  Table 3: Comparing the state-of-art with our mod- els on test set.
true	P15-1149.pdf#93.71	Penn Treebank, LAS	UP  Table 3: Comparing the state-of-art with our mod- els on test set.
true	P15-1149.pdf#93.23	Penn Treebank, LAS	UF  Table 3: Comparing the state-of-art with our mod- els on test set.
true	P15-1149.pdf#92.83	benchmark Vietnamese dependency treebank VnDT, UAS	UR  Table 3: Comparing the state-of-art with our mod- els on test set.
true	P15-1149.pdf#38.14	benchmark Vietnamese dependency treebank VnDT, UAS	UEM  Table 3: Comparing the state-of-art with our mod- els on test set.
true	P15-1149.pdf#93.71	benchmark Vietnamese dependency treebank VnDT, UAS	UP  Table 3: Comparing the state-of-art with our mod- els on test set.
true	P15-1149.pdf#93.23	benchmark Vietnamese dependency treebank VnDT, UAS	UF  Table 3: Comparing the state-of-art with our mod- els on test set.
true	P14-2032.pdf#95.9	Chinese Treebank 6, F1	Academia Sinica R  Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.
true	P14-2032.pdf#95.0	Chinese Treebank 6, F1	Academia Sinica P  Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.
true	P14-2032.pdf#78.7	Chinese Treebank 6, F1	Peking Univ . Roov  Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.
true	P14-2032.pdf#95.4	Chinese Treebank 6, F1	Academia Sinica F1  Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.
true	P14-2032.pdf#95.4	Chinese Treebank 6, F1	Academia Sinica F1  Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.
true	P14-2032.pdf#94.8	Chinese Treebank 6, F1	Peking Univ . R  Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.
true	P14-2032.pdf#95.0	Chinese Treebank 6, F1	Academia Sinica R City Univ . of Hong Kong R  Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.
true	P14-2032.pdf#94.7	Chinese Treebank 6, F1	Academia Sinica F1 City Univ . of Hong Kong F1  Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.
true	P14-2032.pdf#97.4	Chinese Treebank 6, F1	Peking Univ . P Microsoft Research P  Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.
true	P14-2032.pdf#0.055	Chinese Treebank 6, F1	Peking Univ . C Microsoft Research C  Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.
true	P14-2032.pdf#0.055	Chinese Treebank 6, F1	Academia Sinica C  Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.
true	P14-2032.pdf#0.062	Chinese Treebank 6, F1	Academia Sinica C City Univ . of Hong Kong C  Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.
true	P14-2032.pdf#97.4	Chinese Treebank 6, F1	Peking Univ . F1 Microsoft Research F1  Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.
true	P14-2032.pdf#0.086	Chinese Treebank 6, F1	Peking Univ . C  Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.
true	P14-2032.pdf#76.1	Chinese Treebank 6, F1	Academia Sinica Roov City Univ . of Hong Kong  Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.
true	P14-2032.pdf#94.4	Chinese Treebank 6, F1	Academia Sinica P City Univ . of Hong Kong P  Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.
true	P14-2032.pdf#97.3	Chinese Treebank 6, F1	Peking Univ . R Microsoft Research R  Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.
true	P14-2032.pdf#69.5	Chinese Treebank 6, F1	Academia Sinica Roov  Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.
true	P14-2032.pdf#95.7	Chinese Treebank 6, F1	Peking Univ . P  Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.
true	P14-2032.pdf#76.0	Chinese Treebank 6, F1	Peking Univ . Roov Microsoft Research Roov  Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.
true	P14-2032.pdf#95.3	Chinese Treebank 6, F1	Peking Univ . F1  Table 1: Results on SIGHAN 2005 datasets. R oov denotes OOV recall, and C denotes segmentation  consistency. Best number in each column is highlighted in bold.
true	P14-2032.pdf#95.6	Chinese Treebank 6, F1	SIGHAN 2005 CU  Table 2: Performance of dual decomposition in  comparison to past published results on SIGHAN  2003 and 2005 datasets. Best reported F 1 score  for each dataset is highlighted in bold. Z&C 07  refers to
true	P14-2032.pdf#97.4	Chinese Treebank 6, F1	SIGHAN 2005 MSR  Table 2: Performance of dual decomposition in  comparison to past published results on SIGHAN  2003 and 2005 datasets. Best reported F 1 score  for each dataset is highlighted in bold. Z&C 07  refers to
true	P14-2032.pdf#97.1	Chinese Treebank 6, F1	SIGHAN 2003 SIGHAN 2005 AS  Table 2: Performance of dual decomposition in  comparison to past published results on SIGHAN  2003 and 2005 datasets. Best reported F 1 score  for each dataset is highlighted in bold. Z&C 07  refers to
true	P14-2032.pdf#95.3	Chinese Treebank 6, F1	SIGHAN 2005 PU  Table 2: Performance of dual decomposition in  comparison to past published results on SIGHAN  2003 and 2005 datasets. Best reported F 1 score  for each dataset is highlighted in bold. Z&C 07  refers to
true	P14-2032.pdf#94.9	Chinese Treebank 6, F1	SIGHAN 2003 SIGHAN 2005 CU  Table 2: Performance of dual decomposition in  comparison to past published results on SIGHAN  2003 and 2005 datasets. Best reported F 1 score  for each dataset is highlighted in bold. Z&C 07  refers to
true	P14-2032.pdf#95.4	Chinese Treebank 6, F1	SIGHAN 2003 SIGHAN 2005 PU  Table 2: Performance of dual decomposition in  comparison to past published results on SIGHAN  2003 and 2005 datasets. Best reported F 1 score  for each dataset is highlighted in bold. Z&C 07  refers to
true	P14-2032.pdf#95.4	Chinese Treebank 6, F1	SIGHAN 2005 AS  Table 2: Performance of dual decomposition in  comparison to past published results on SIGHAN  2003 and 2005 datasets. Best reported F 1 score  for each dataset is highlighted in bold. Z&C 07  refers to
