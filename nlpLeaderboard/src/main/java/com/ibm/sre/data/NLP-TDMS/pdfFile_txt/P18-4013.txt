section: title
NCRF++: An Open-source Neural Sequence Labeling Toolkit
section: abstract
This paper describes NCRF++, a toolkit for neural sequence labeling. NCRF++ is designed for quick implementation of different neural sequence labeling models with a CRF inference layer. It provides users with an inference for building the custom model structure through configuration file with flexible neural feature design and utilization. Built on PyTorch 1 , the core operations are calculated in batch, making the toolkit efficient with the acceleration of GPU. It also includes the implementations of most state-of-the-art neural sequence labeling models such as LSTM-CRF, facilitating reproducing and refinement on those methods.
section: Introduction
Sequence labeling is one of the most fundamental NLP models, which is used for many tasks such as named entity recognition (NER), chunking, word segmentation and part-of-speech (POS) tagging. It has been traditionally investigated using statistical approaches (, where conditional random fields (CRF) () has been proven as an effective framework, by taking discrete features as the representation of input sequence.
With the advances of deep learning, neural sequence labeling models have achieved state-ofthe-art for many tasks (. Features are extracted automatically through network structures including long short-term memory (LSTM)) and convolution neural network (CNN) (),  with distributed word representations. Similar to discrete models, a CRF layer is used in many state-of-the-art neural sequence labeling models for capturing label dependencies.
There exist several open-source statistical CRF sequence labeling toolkits, such as CRF++ 2 , CRFSuite ( and), which provide users with flexible means of feature extraction, various training settings and decoding formats, facilitating quick implementation and extension on state-of-the-art models. On the other hand, there is limited choice for neural sequence labeling toolkits. Although many authors released their code along with their sequence labeling papers (, the implementations are mostly focused on specific model structures and specific tasks. Modifying or extending can need enormous coding.
In this paper, we present Neural CRF++ (NCRF++) 3 , a neural sequence labeling toolkit based on PyTorch, which is designed for solving general sequence labeling tasks with effective and efficient neural models. It can be regarded as the neural version of CRF++, with both take the CoNLL data format as input and can add hand-  crafted features to CRF framework conveniently. We take the layerwise implementation, which includes character sequence layer, word sequence layer and inference layer. NCRF++ is:
• Fully configurable: users can design their neural models only through a configuration file without any code work. shows a segment of the configuration file. It builds a LSTM-CRF framework with CNN to encode character sequence (the same structure as), plus POS and Cap features, within 10 lines. This demonstrates the convenience of designing neural models using NCRF++.
• Flexible with features: human-defined features have been proved useful in neural sequence labeling. Similar to the statistical toolkits, NCRF++ supports user-defined features but using distributed representations through lookup tables, which can be initialized randomly or from external pretrained embeddings (embedding directory: emb dir in). In addition, NCRF++ integrates several state-of-the-art automatic feature extractors, such as CNN and LSTM for character sequences, leading easy reproduction of many recent work (.
• Effective and efficient: we reimplement several state-of-the-art neural models () using NCRF++. Experiments show models builtin NCRF++ give comparable performance with reported results in the literature. Besides, NCRF++ is implemented using batch calculation, which can be accelerated using GPU. Our experiments demonstrate that NCRF++ as an effective and efficient toolkit.
• Function enriched: NCRF++ extends the Viterbi algorithm to enable decoding n best sequence labels with their probabilities. Taking NER, Chunking and POS tagging as typical examples, we investigate the performance of models builtin NCRF++, the influence of humandefined and automatic features, the performance of nbest decoding and the running speed with the batch size. Detail results are shown in Section 3.
section: NCRF++ Architecture
The framework of NCRF++ is shown in. NCRF++ is designed with three layers: a character sequence layer; a word sequence layer and inference layer. For each input word sequence, words are represented with word embeddings. The character sequence layer can be used to automatically extract word level features by encoding the character sequence within the word. Arbitrary handcrafted features such as capitalization, POS tag, prefixes and suffixes are also supported by NCRF++. Word representations are the concatenation of word embeddings (red circles), character sequence encoding hidden vector (yellow circles) and handcrafted neural features (grey circles). Then the word sequence layer takes the word representations as input and extracts the sentence level features, which are fed into the inference layer to assign a label to each word. When building the network, users only need to edit the configuration file to configure the model structure, training settings and hyperparameters. We use layer-wised encapsulation in our implementation. Users can extend NCRF++ by defining their own structure in any layer and integrate it into NCRF++ easily.
section: Layer Units
section: Character Sequence Layer
The character sequence layer integrates several typical neural encoders for character sequence information, such as RNN and CNN. It is easy to select our existing encoder through the configuration file (by setting char seq feature in). Characters are represented by character embeddings (green circles in), which serve as the input of character sequence layer.
• Character RNN and its variants Gated Recurrent Unit (GRU) and LSTM are supported by NCRF++. The character sequence layer uses bidirectional RNN to capture the left-to-right and right-to-left sequence information, and concatenates the final hidden states of two RNNs as the encoder of the input character sequence.
• Character CNN takes a sliding window to capture local features, and then uses a max-pooling for aggregated encoding of the character sequence.
section: Word Sequence Layer
Similar to the character sequence layer, NCRF++ supports both RNN and CNN as the word sequence feature extractor. The selection can be configurated through word seq feature in. The input of the word sequence layer is a word representation, which may include word embeddings, character sequence representations and handcrafted neural features (the combination depends on the configuration file). The word sequence layer can be stacked, building a deeper feature extractor.
• Word RNN together with GRU and LSTM are available in NCRF++, which are popular structures in the recent literature (. Bidirectional RNNs are supported to capture the left and right contexted information of each word. The hidden vectors for both directions on each word are concatenated to represent the corresponding word.
• Word CNN utilizes the same sliding window as character CNN, while a nonlinear function () is attached with the extracted features. Batch normalization) and dropout () are also supported to follow the features.
section: Inference Layer
The inference layer takes the extracted word sequence representations as features and assigns labels to the word sequence. NCRF++ supports both softmax and CRF as the output layer. A linear layer firstly maps the input sequence representations to label vocabulary size scores, which are used to either model the label probabilities of each word through simple softmax or calculate the label score of the whole sequence.
• Softmax maps the label scores into a probability space. Due to the support of parallel decoding, softmax is much more efficient than CRF and works well on some sequence labeling tasks (). In the training process, various loss functions such as negative likelihood loss, cross entropy loss are supported.
• CRF captures label dependencies by adding transition scores between neighboring labels. NCRF++ supports CRF trained with the sentencelevel maximum log-likelihood loss. During the decoding process, the Viterbi algorithm is used to search the label sequence with the highest probability. In addition, NCRF++ extends the decoding algorithm with the support of nbest output.
section: User Interface
NCRF++ provides users with abundant network configuration interfaces, including the network structure, input and output directory setting, training settings and hyperparameters. By editing a configuration file, users can build most state-ofthe-art neural sequence labeling models. On the other hand, all the layers above are designed as "plug-in" modules, where user-defined layer can be integrated seamlessly.
section: Configuration
• Networks can be configurated in the three layers as described in Section 2.1. It controls the choice of neural structures in character and word levels with char seq feature and word seq feature, respectively. The inference layer is set by use crf. It also defines the usage of handcrafted features and their properties in feature.
• I/O is the input and output file directory configuration.
It includes training dir, 90.94 -97.51 91. 91.20 94.66 97.55 90.87 95.00 -: Results on three benchmarks.
dev dir, test dir, raw dir, pretrained character or word embedding (char emb dim or word emb dim), and decode file directory (decode dir).
• Training includes the loss function (loss function), optimizer (optimizer) shuffle training instances train shuffle and average batch loss ave batch loss.
• Hyperparameter includes most of the parameters in the networks and training such as learning rate (lr) and its decay (lr decay), hidden layer size of word and character (hidden dim and char hidden dim), nbest size (nbest), batch size (batch size), dropout (dropout), etc. Note that the embedding size of each handcrafted feature is configured in the networks configuration (feature= emb dir=None emb size=10 in).
section: Extension
Users can write their own custom modules on all three layers, and user-defined layers can be integrated into the system easily. For example, if a user wants to define a custom character sequence layer with a specific neural structure, he/she only needs to implement the part between input character sequence indexes to sequence representations. All the other networks structures can be used and controlled through the configuration file. A README file is given on this.
section: Evaluation
section: Settings
To evaluate the performance of our toolkit, we conduct the experiments on several datasets. For NER task, CoNLL 2003 data (Tjong Kim Sang Currently NCRF++ supports five optimizers: SGD/AdaGrad/AdaDelta/RMSProp/Adam.) with the standard split is used. For the chunking task, we perform experiments on CoNLL 2000 shared task), data split is following. For POS tagging, we use the same data and split with. We test different combinations of character representations and word sequence representations on these three benchmarks. Hyperparameters are mostly following and almost keep the same in all these experiments . Standard SGD with a decaying learning rate is used as the optimizer. shows the results of six CRF-based models with different character sequence and word sequence representations on three benchmarks. State-of-the-art results are also listed. In this table, "Nochar" suggests a model without character sequence information. "CLSTM" and "CCNN" represent models using LSTM and CNN to encode character sequence, respectively. Similarly, "WL-STM" and "WCNN" indicate that the model uses LSTM and CNN to represent word sequence, respectively.
section: Results
As shown in, "WCNN" based models consistently underperform the "WLSTM" based models, showing the advantages of LSTM on capturing global features.
Character information can improve model performance significantly, while using LSTM or CNN give similar improvement. Most of state-of-the-art models utilize the framework of word LSTM-CRF with character LSTM or CNN features (correspond to "CLSTM+WLSTM+CRF" and "CCNN+WLSTM+CRF" of our models) (. Our implementations can achieve comparable results, with better NER and   chunking performances and slightly lower POS tagging accuracy. Note that we use almost the same hyperparameters across all the experiments to achieve the results, which demonstrates the robustness of our implementation. The full experimental results and analysis are published in.
section: Influence of Features
We also investigate the influence of different features on system performance. shows the results on the NER task. POS tag and capital indicator are two common features on NER tasks). In our implementation, each POS tag or capital indicator feature is mapped as 10-dimension feature embeddings through randomly initialized feature lookup table . The feature embeddings are concatenated with the word embeddings as the representation of the corresponding word. Results show that both human features and can contribute the NER system, this is consistent with previous observations. By utilizing LSTM or CNN to encode character sequence automatically, the system can achieve better performance on NER task.
section: N best Decoding
We investigate nbest Viterbi decoding on NER dataset through the best model "CCNN+WLSTM+CRF". rises significantly with the increasement of nbest size, reaching 97.47% at n = 10 from the baseline of 91.35%. The token level accuracy increases from 98.00% to 99.39% in 10-best. Results show that the nbest outputs include the gold entities and labels in a large coverage, which greatly enlarges the performance of successor tasks.
section: Speed with Batch Size
As NCRF++ is implemented on batched calculation, it can be greatly accelerated through parallel computing through GPU. We test the system speeds on both training and decoding process on NER dataset using a Nvidia GTX 1080 GPU. As shown in, both the training and the decoding speed can be significantly accelerated through a large batch size. The decoding speed reaches saturation at batch size 100, while the training speed keeps growing. The decoding speed and training speed of NCRF++ are over 2000 sentences/second and 1000 sentences/second, respectively, demonstrating the efficiency of our implementation.
section: Conclusion
We presented NCRF++, an open-source neural sequence labeling toolkit, which has a CRF architecture with configurable neural representation layers. Users can design custom neural models through the configuration file. NCRF++ supports flexible feature utilization, including handcrafted features and automatically extracted features. It can also generate nbest label sequences rather than the best one. We conduct a series of experiments and the results show models built on NCRF++ can achieve state-of-the-art results with an efficient running speed.
