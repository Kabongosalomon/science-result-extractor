section: title
Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification
section: abstract
Relation classification is an important semantic processing task in the field of natural language processing (NLP). State-of-the-art systems still rely on lexical resources such as WordNet or NLP systems like dependency parser and named entity recognizers (NER) to get high-level features. Another challenge is that important information can appear at any position in the sentence. To tackle these problems, we propose Attention-Based Bidirectional Long Short-Term Memory Networks(Att-BLSTM) to capture the most important semantic information in a sentence. The experimental results on the SemEval-2010 relation classification task show that our method outperforms most of the existing methods, with only word vectors.
section: Introduction
Relation classification is the task of finding semantic relations between pairs of nominals, which is useful for many NLP applications, such as information extraction (, question answering (). For instance, the following sentence contains an example of the Entity-Destination relation between the nominals Flowers and chapel. ⟨e 1 ⟩ Flowers ⟨/e 1 ⟩ are carried into the ⟨e 2 ⟩ chapel ⟨/e 2 ⟩. ⟨e 1 ⟩, ⟨/e 1 ⟩, ⟨e 2 ⟩, ⟨/e 2 ⟩ are four position indicators which specify the starting and ending of the nominals (.
Traditional relation classification methods that employ handcrafted features from lexical resources, are usually based on pattern matching, and have achieved high performance (Bunescu * Correspondence author: zhenyu.qi@ia.ac.cn and. One downside of these methods is that many traditional NLP systems are utilized to extract high-level features, such as part of speech tags, shortest dependency path and named entities, which consequently results in the increase of computational cost and additional propagation errors. Another downside is that designing features manually is time-consuming, and performing poor on generalization due to the low coverage of different training datasets.
Recently, deep learning methods provide an effective way of reducing the number of handcrafted features). However, these approaches still use lexical resources such as WordNet or NLP systems like dependency parsers and NER to get high-level features.
This paper proposes a novel neural network Att-BLSTM for relation classification. Our model utilizes neural attention mechanism with Bidirectional Long Short-Term Memory Networks(BLSTM) to capture the most important semantic information in a sentence. This model doesn't utilize any features derived from lexical resources or NLP systems.
The contribution of this paper is using BLST-M with attention mechanism, which can automatically focus on the words that have decisive effect on classification, to capture the most important semantic information in a sentence, without using extra knowledge and NLP systems. We conduct experiments on the SemEval-2010 Task 8 dataset, and achieve an F 1-score of 84.0%, higher than most of the existing methods in the literature.
The remainder of the paper is structured as follows. In Section 2, we review related work about relation classification. Section 3 presents our Att-BLSTM model in detail. In Section 4, we describe details about the setup of experimental evaluation and the experimental results. Finally, we have our conclusion in Section 5.
section: Related Work
Over the years, various methods have been proposed for relation classification. Most of them are based on pattern matching and apply extra NLP systems to derive lexical features. One related work is proposed by, which utilizes many features derived from external corpora fora Support Vector Machine(SVM) classifier.
Recently, deep neural networks can learn underlying features automatically and have been used in the literature. Most representative progress was made by, who utilized convolutional neural networks(CNN) for relation classification. While CNN is not suitable for learning long-distance semantic information, so our approach builds on Recurrent Neural Network(RNN) ().
One related work was proposed by, which employed bidirectional RN-N to learn patterns of relations from raw text data. Although bidirectional RNN has access to both past and future context information, the range of context is limited due to the vanishing gradient problem. To overcome this problem, Long short-Term memory(LSTM) units are introduced by.
Another related work is SDP-LSTM model proposed by. This model leverages the shortest dependency path(SDP) between two nominals, then it picks up heterogeneous information along the SDP with LSTM units. While our method regards the raw text as a sequence.
Finally, our work is related to BLSTM model proposed by . This model utilizing NLP tools and lexical resources to get word, position, POS, NER, dependency parse and hypernym features, together with LSTM units, achieved a comparable result to the state-ofthe-art. However, comparing to the complicated features that employed by , our method regards the four position indicators ⟨e1⟩, ⟨/e1⟩, ⟨e2⟩, ⟨/e2⟩ as single words, and transforms all words to word vectors, forming a simple but competing model.
section: Model
In this section we propose Att-BLSTM model in detail. As shown in, the model proposed in this paper contains five components:
(1) Input layer: input sentence to this model; (2) Embedding layer: map each word into a low dimension vector; (3) LSTM layer: utilize BLSTM to get high level features from step; (4) Attention layer: produce a weight vector, and merge word-level features from each time step into a sentence-level feature vector, by multiplying the weight vector;
(5) Output layer: the sentence-level feature vector is finally used for relation classification.
These components will be presented in detail in this section.
section: Word Embeddings
Given a sentence consisting of T words S = {x 1 , x 2 , . . . , x T }, every word xi is converted into a real-valued vector e i . For each word in S, we first lookup the embedding matrix W wrd ∈ Rd w |V | , where V is a fixed-sized vocabulary, and d w is the size of word embedding. The matrix W wrd is a parameter to be learned, and d w is a hyper-parameter to be chosen by user. We transform a word xi into its word embedding e i by using the matrix-vector product:
where vi is a vector of size |V | which has value 1 at index e i and 0 in all other positions. Then the sentence is feed into the next layer as a real-valued vectors emb s = {e 1 , e 2 , . . . , e T } .
section: Bidirectional Network
LSTM units are firstly proposed by to overcome gradient vanishing problem. The main idea is to introduce an adaptive gating mechanism, which decides the degree to which LSTM units keep the previous state and memorize the extracted features of the current data input. Then lots of LSTM variants have been proposed. We adopt a variant introduced by , which adds weighted peephole connections from the Constant Error Carousel (CEC) to the gates of the same memory block. By directly employing the current cell state to generate the gate degrees, the peephole connections allow all gates to inspect into the cell (i.e.   , all of those gates are set to generate some degrees, using current input xi , the state h i−1 that previous step generated , and current state of this cell c i−1 (peephole), for the decisions whether to take the inputs, forget the memory stored before, and output the state generated later. Just as these following equations demonstrate:
Hence, current cell state ct will be generated by calculating the weighted sum using both previous cell state and current information generated by the cell.
For many sequence modelling tasks, it is beneficial to have access to future as well as past context. However, standard LSTM networks process sequences in temporal order, they ignore future context. Bidirectional LSTM networks extend the unidirectional LSTM networks by introducing a second layer, where the hidden to hidden connections flow in opposite temporal order. The model is therefore able to exploit information both from the past and the future.
In this paper, we use BLSTM. As also shown in, the network contains two sub-networks for the left and right sequence context, which are forward and backward pass respectively. The output of the i th word is shown in the following equation:
Here, we use element-wise sum to combine the forward and backward pass outputs.
section: Attention
Attentive neural networks have recently demonstrated success in a wide range of tasks ranging from question answering, machine translations, speech recognition, to image captioning (Hermann et al., 2015;. In this section, we propose the attention mechanism for relation classification tasks. Let H be a matrix consisting of output vectors[h 1 , h 2 , . . . , h T ] that the LSTM layer produced, where T is the sentence length. The representation r of the sentence is formed by a weighted sum of these output vectors:
section: Model
Feature Set F1 SVM POS, prefixes, morphological, WordNet, dependency parse, () Levin classed, ProBank, FramNet, NomLex-Plus, 82.2 Google n-gram, paraphrases, TextRunner CNN WV () (dim=50) 69.7 ( + PF + WordNet 82.7 RNN WV () (dim=50) + PI 80.0 ( WV () (dim=300) + PI 82.5 SDP-LSTM WV (pretrained by word2vec) (dim=200), syntactic parse 82.4 + POS + WordNet + grammar relation embeddings 83.7 BLSTM WV () (dim=100) 82.7 (  where H ∈ Rd w ×T , d w is the dimension of the word vectors, w is a trained parameter vector and w T is a transpose. The dimension of w, α, r is d w , T, d w separately. We obtain the final sentence-pair representation used for classification from:
section: Classifying
In this setting, we use a softmax classifier to predict labeî y from a discrete set of classes Y fora sentence S. The classifier takes the hidden state h * as input:
The cost function is the negative log-likelihood of the true class labelsˆylabelsˆ labelsˆy:
where t ∈ ℜ m is the one-hot represented ground truth and y ∈ ℜ m is the estimated probability for each class by softmax (m is the number of target classes), and λ is an L2 regularization hyperparameter. In this paper, we combine dropout with L2 regularization to alleviate overfitting.
section: Regularization
Dropout, proposed by (Hinton et al., 2012), prevents co-adaptation of hidden units by randomly omitting feature detectors from the network during forward propagation. We employ dropout on the embedding layer, LSTM layer and the penultimate layer. We additionally constrain L2-norms of the weight vectors by rescaling w to have ∥w∥ = s, whenever ∥w∥ > s after a gradient descent step, as shown in equation 15. Training details are further introduced in Section 4.1.
section: Experiments
section: Dataset and Experimental Setup
Experiments are conducted on). This dataset contains 9 relationships (with two directions) and an undirected Other class. There are 10,717 annotated examples, including 8,000 sentences for training, and 2,717 for testing. We adopt the official evaluation metric to evaluate our systems, which is based on macro-averaged F1-score for the nine actual relations (excluding the Other relation) and takes the directionality into consideration.
In order to compare with the work by, we use the same word vectors proposed by to initialize the embedding layer. Additionally, to compare with the work by , we also use the 100-dimensional word vectors pretrained by.
Since there is no official development dataset, we randomly select 800 sentence for validation. The hyper-parameters for our model were tuned on the development set for each task. Our model was trained using AdaDelta) with a learning rate of 1.0 and a minibatch size 10. The model parameters were regularized with a perminibatch L2 regularization strength of 10 −5 . We evaluate the effect of dropout embedding layer, dropout LSTM layer and dropout the penultimate layer, the model has a better performance, when the dropout rate is set as 0.3, 0.3, 0.5 respectively. Other parameters in our model are initialized randomly. compares our Att-BLSTM with other state-of-the-art methods of relation classification.
section: Experimental Results
SVM: This is the top performed system in SemEval-2010. leveraged a variety of handcrafted features, and use SVM as the classifier. They achieved an F 1 -score of 82.2%.
CNN: treated a sentences as a sequential data and exploited the convolutional neural network to learn sentence-level features; they also used a special position vector to represent each word. Then the sentence-level and lexical features were concatenated into a single vector and fed into a softmax classifier for prediction. This model achieves an F 1 -score of 82.7%.
RNN: Zhang and Wang (2015) employed bidirectional RNN networks with two different dimension word vectors for relation classification. They achieved an F 1 -score of 82.8% using 300-dimensional word vectors pre-trained by, and an F 1 -score of 80.0% using 50-dimensional word vectors pre-trained by. Our model with the same 50-dimensional word vectors achieves an F 1 -score of 82.5%, about 2.5 percent more than theirs. SDP-LSTM: utilized four different channels to pickup heterogeneous along the SDP, and they achieved an F 1 -score of 83.7%. Comparing with their model, our model regarding the raw text as a sequence is simpler.
BLSTM:  employed many features derived from NLP tools and lexical resources with bidirectional LSTM networks to learn the sentence level features, and they achieved state-of-the-art performance on the SemEval-2010 Task 8 dataset. Our model with the same word vectors achieves a very similar result (84.0%), and our model is more simple.
Our proposed Att-BLSTM model yields an F 1 -score of 84.0%. It outperforms most of the existing competing approaches, without using lexical resources such as WordNet or NLP systems like dependency parser and NER to get high-level features.
section: Conclusion
In this paper, we propose a novel neural network model, named Att-BLSTM, for relation classification. This model does not rely on NLP tools or lexical resources to get, it uses raw text with position indicators as input. The effectiveness of Att-BLSTM is demonstrated by evaluating the model on SemEval-2010 relation classification task.
