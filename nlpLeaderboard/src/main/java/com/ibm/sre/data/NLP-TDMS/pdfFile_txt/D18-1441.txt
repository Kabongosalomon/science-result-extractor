section: title
Improving Neural Abstractive Document Summarization with Structural Regularization *
section: abstract
Recent neural sequence-to-sequence models have shown significant progress on short text summarization. However, for document sum-marization, they fail to capture the long-term structure of both documents and multi-sentence summaries, resulting in information loss and repetitions. In this paper, we propose to leverage the structural information of both documents and multi-sentence summaries to improve the document summariza-tion performance. Specifically, we import both structural-compression and structural-coverage regularization into the summariza-tion process in order to capture the information compression and information coverage properties, which are the two most important structural properties of document sum-marization. Experimental results demonstrate that the structural regularization improves the document summarization performance significantly , which enables our model to generate more informative and concise summaries, and thus significantly outperforms state-of-the-art neural abstractive methods.
section: Introduction
Document summarization is the task of generating a fluent and condensed summary fora document while retaining the gist information. Recent success of neural sequence-to-sequence (seq2seq) architecture on text generation tasks like machine translation () and image caption (, has attracted growing attention to abstractive summarization research. Huge success has been witnessed in abstractive sentence summarization, which builds onesentence summaries from one or two-sentence in- * This work was done while the first author was doing internship at Baidu Inc. on a news article. (a) is the heatmap for the gold reference summary, (b) is for the Seq2seq-baseline system, (c) is for the Point-gen-cov (See et al., 2017) system, (d) is for the Hierarchical-baseline system and (e) is for our system. Ii and Oi indicate the i-th sentence of the input and output, respectively. Obviously, the seq2seq models, including the Seq2seq-baseline model and the Point-gen-cov model, lose much salient information of the input document and focus on the same set of sentences repeatedly. The Hierarchical-baseline model fails to detect several specific sentences that are salient and relevant for each summary sentence and focuses on the same set of sentences repeatedly. On the contrary, our method with structural regularizations focuses on different sets of source sentences when generating different summary sentences and discovers more salient information from the document.
put. However, the extension of sentence abstractive methods to document summarization task is not straightforward.
As long-distance dependencies are difficult to be captured in the recurrent framework, the seq2seq models are not yet able to achieve convincing performance in encoding and decoding fora long sequence of multiple sentences (: the family of conjoined twin sisters who died 19 days after they were born have been left mortified after they arrived at their gravesite to find cemetery staff had cleared the baby section of all mementos and tossed them in the rubbish . faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition known as disrosopus . they died in hospital less than a month after they were born and their parents , simon howie and renee young , laid them to rest at pinegrove memorial park in sydney 's west . scroll down for video . faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition known as disrosopus . family members have visited the grave every week to leave mementos and flowers for faith and hope , but when mr howie and ms young arrived on thursday they found the site completely bare . ' we took renee's aunts to seethe girls for the first time and we found everything had been stripped away , ' mr howie told daily mail australia . ' we were devastated and mortified . we 've had a little shrine setup and we 've been adding to it since the funeral . ' it 's heartbreaking to know we 've set this up and it has been treated like rubbish . ' faith and hope were buried in a pink coffin and their family and friends released doves and pink and white balloons at their funeral . their family and friends had built up a small memorial with pink and white statues , flowers , pebbles and toys over the past 11 months . when they arrived on thursday , everything had been removed apart from a bunch of flowers . the twins were buried at pinegrove memorial park in western sydney after they died after just 19 days . their family and friends had built a small shrine at their gravesite , which they have added to since the funeral . family members have visited the grave every week to leave mementos and flowers for faith and hope , but when parents simon howie and renee young arrived on thursday they found the site completely bare . Gold Reference: faith and hope howie were born with one body and two faces on may 8. they tragically died in hospital just 19 days after they were born . parents simon howie and renee young visit their grave at pinegrove in western sydney fortnightly . they arrived on thursday to find the grave bare of all the girls ' mementos . staff had cleared entire baby section and thrown belongings in rubbish . Seq2seq-baseline: faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition known as disrosopus . faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition known as disrosopus . faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition known as disrosopus .: faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition known as disrosopus . they died in hospital less than a month after they were born and their parents , simon howie and renee young , laid them to rest at pinegrove memorial park in sydney 's west. Hierarchical-baseline: faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition . they died in hospital less than a month after they died in hospital less than a month after they were born and laid them to rest at pinegrove memorial park in sydney 's west . family members have visited the grave every week to leave mementos and flowers for faith and hope , but when they were born on thursday they found the site completely bare . family members have visited the grave every week to leave mementos and flowers for faith and hope , but when they found the site completely bare . Our Method: faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition . they died in hospital less than a month after they were born and their parents laid them to rest at pinegrove memorial park in sydney 's west . family members have visited the grave every week to leave mementos and flowers for faith and hope , but when mr howie and ms young arrived on thursday they found the site completely bare .: Comparison of the generated summaries of four abstractive summarization models and the gold reference summary on a news article. The summaries generated by the seq2seq models, both the Seq2seq-baseline model and the Point-cov model, lose some salient information. The Seq2seq-baseline model even contains serious information repetitions. The Hierarchicalbaseline model not only contains serious repetitions, but also makes non-grammatical or non-coherent sentences. On the contrary, the summary generated by our model contains more salient information and is more concise. Our model also shows the ability to generate a summary sentence by compressing several source sentences, such as shortening along sentence. 2017). In document summarization, it is also difficult for the seq2seq models to discover important information from too much input content of a document (. The summary generated by the seq2seq models usually loses salient information of the original document or even contains repetitions (see).
In fact, both document and summary naturally have document-sentence hierarchical structure, instead of being a flat sequence of words. It is widely aware that the hierarchical structure is necessary and useful for neural document modeling. Hierarchical neural models have already been successfully used in document-level language modeling () and document classification (. However, few work makes use of the hierarchical structure of document and multi-sentence summary in document summarization. The basic hierarchical encoderdecoder model ( ) is also not yet able to capture the structural properties of both document and summary (see ), resulting in 1 To simulate the sentence-level attention mechanism on the gold reference summary, we compute the words-matching similarities (based on TF-IDF cosine similarity) between a reference-summary sentence and the corresponding source document sentences and normalize them into attention distributions. The sentence-level attention distributions of the Seq2seq-baseline model and the Point-gen-cov model are computed by summing the attention weights of all words in each sentence and then normalized across sentences. more serious repetitions and even nonsensical sentences (see).
In document summarization, information compression and information coverage are the two most important structural properties. Based on the hierarchical structure of document and summary, they can be realized at the sentencelevel as: (1) Structural-compression: each summary sentence is generated by compressing several specific source sentences; (2) Structuralcoverage: different summary sentences usually focus on different sets of source sentences to cover more salient information of the original document.(a) intuitively shows the two properties in human-written gold reference summaries. We import both structural-compression and structural-coverage regularizations into the document summarization process based on a hierarchical encoder-decoder with hybrid sentenceword attention model. Typically, we design an effective learning and inference algorithm to explicitly model the structural-compression and structural-coverage properties of document summarization process, so as to generate more informative and concise summaries (see).
We conduct our experiments on benchmark datasets and the results demonstrate that properly modeling the structural-compression and structural-coverage properties based on the hier-
Figure 2: Our hierarchical encoder-decoder model with structural regularization for abstractive document summarization.
archical structure of document and summary, improves document summarization performance significantly. Our model is able to generate more informative and concise summaries by enhancing sentences compression and coverage, and significantly outperforms state-of-the-art seq2seq-based abstractive methods, especially on summarizing long documents with long summaries.
section: Hierarchical Encoder-Decoder Model
In this section, we introduce our baseline hierarchical encoder-decoder model which consists of two parts: a hierarchical encoder and a hierarchical decoder, as shown in. Similar to ( , both the encoder and decoder consists of two levels: a sentence level and a word level.
The main distinction is that we design a hybrid sentence-word attention mechanism on the hierarchical decoder to help organize summary content and realize summary sentences.
section: Hierarchical Encoder
The goal of the encoder is to map the input document to a hidden vector representation. We consider a source document X as a sequence of sentences: X = {s i }, and each sentence s i as a sequence of words: s i = {w ij }. The wordlevel encoder encodes the words of a sentence into a sentence representation, and the sentencelevel encoder encodes the sentences of a document into the document representation. In this work, both the word-level encoder and sentencelevel encoder use the bidirectional Gated Recurrent Unit (BiGRU) (). The word-level encoder sequentially updates its hidden state upon each received word, as
where h i,j and e i,j denote the hidden state and the embedding of word w i,j , respectively. The concatenation of the forward and backward final hidden states in the word-level encoder is indicated as the vector representation r i of sentence s i , which is used as input to the sentencelevel encoder. The sentence-level encoder updates its hidden state after receiving each sentence representation, ash i = BiGRU (h i−1 , r i ) where hi denotes the hidden state of sentence s i . The concatenation of the forward and backward final states in the sentence-level encoder is used as the vector representation of document d.
In the hierarchical encoder architecture, long dependency problem will be largely reduced at both the sentence level and the word level, so it can better capture the structural information of the input document.
section: Hierarchical Decoder with Hybrid
Sentence-Word Attention
where h ′ t,k denotes the hidden state of word w ′ t,k in sentence s ′ t and e t,k−1 denotes the embedding of previously generated word
In this work, we design a hybrid sentenceword attention mechanism based on the hierarchical encoder-decoder architecture, which contains both sentence-level attention and word-level attention, to better exploit both the sentence-level information and word-level information from the input document and the output summary.
section: Sentence-level Attention
The sentence-level attention mechanism is designed on the sentence-level encoder and decoder, which is used to help our model to detect important and relevant source sentences in each sentence generation step. α it indicates how much the t-th summary sentence attends to the i-th source sentence, which is computed by
where f is the function modeling the relation between hi and h ′ t . We use the function f (a,
where v, W a , W bare all learnable parameters. Then the sentence level context vector c st when generating the tth sentence s ′ t can be computed as: c st = ∑ i α it hi , which is incorporated into the sentence-level decoding process.
section: Word-level Attention with
Sentence-level Normalization The word-level attention is designed on the wordlevel encoder and decoder, which is used to help our model to realize the summary sentence by locating relevant words in the selected source sentences in each word generation step. Let β i,j t,k denotes how much the j-th word in source sentence s i contributes to generating the k-th word in summary sentence s ′ t , which is computed by
Since the word-level attention above is within each source sentence, we normalize it by sentencelevel attentions to get word attention overall source words, as γ i t,k = β i t,k α it . Then the wordlevel context vector when generating word w ′ t,k can be computed as:
, which is also incorporated into the word-level decoding process.
At each word generation step, the vocabulary distribution is calculated from the context vector cw t,k and the decoder state h ′ t,k by:
where W v , W c , b c and b v are learned parameters. We also incorporate the copy mechanism (See et al., 2017) based on the normalized wordlevel attention to help generate out-of-vocabulary (OOV) words during the sentence realization process.
section: Structural Regularization
Although the above hierarchical encoder-decoder model is designed based on the documentsentence hierarchical structure, it can't capture the basic structural properties of document summarization (see(d) and). However, the hierarchical architecture makes it possible for importing structural regularization to capture the sentence-level characteristics of document summarization process. In this work, we propose to model the structural-compression and structural-coverage properties based on the hierarchical encoder-decoder model by adding structural regularization during both the model learning phase and inference phase.
section: Structural Compression
Compression is a basic property of document summarization, which has been widely explored in traditional document summarization research, such as sentence compression-based methods which shorten sentences by removing non-salient parts () and sentence fusion-based methods which merge information from several different source sentences (). As shown in, each summary sentence in the human-written reference summary is also created by compressing several specific source sentences.
In this paper, we propose to model the structural-compression property of document summarization based on sentence-level attention distributions by:
where α t denotes the sentence-level attention distribution when generating the tth summary sentence and N denotes the length of distribution α t . The right part in the above formula is actually the entropy of the distribution α t . As the attention distribution becomes sparser, the entropy of the distribution becomes lower, so the value of strCom(α t ) defined above will become larger. Sparse sentence-level attentions help the model compress and generalize several specific source sentences which are salient and relevant in the sentence generation process. Note that, 0 ≤ strCom(α t ) ≤ 1.
section: Structural Coverage
A good summary should have the ability to cover most of the important information of an input document. As shown in, the humanwritten reference summary covers the information of many source sentences. Coverage has been used as a measure in many traditional document summarization research, such as the submodularbased methods which optimize the information coverage of the summary with similarity-based coverage metrics (. In this work, we simply model the structuralcoverage property of summary based on the hierarchical architecture by encouraging different summary sentences to focus on different sets of source sentences so that the summary can cover more salient sentences of the input document. We measure the structural-coverage of summary based on the sentence-level attention distributions:
which is used to encourage different summary sentences to focus on different sets of source sentences during the summary generation process. As the sentence-level attention distributions of different summary sentences become more diversified, the summary will cover more source sentences, which is effective to improve the informativeness and conciseness of summaries. Note that, 0≤strCov(α t ) ≤ 1.
section: Model Learning
Experimental results reveal that the properties of structural-compression and structural-coverage are hard to be captured by both the seq2seq models and the hierarchical encoder-decoder baseline model, which largely restricts their performance (Section 4). In this work, we model them explicitly by regulating the sentence-level attention distributions based on the hierarchical encoderdecoder framework. The loss function L of the model is the mix of negative log-likelihood of generating summaries over training set T , the structural-compression loss and the structuralcoverage loss:
where λ 1 and λ 2 are hyper-parameters tuned on the validation set. We use Adagrad (Duchi et al.,
2011) with learning rate 0.1 and an initial accumulator value 0.1 to optimize the model parameters θ.
section: Hierarchical Decoding Algorithm
The traditional beam search algorithm that widely used for text generation can only help generate fluent sentence, and is not easy to extend to the sentence level. The reason is that the K-best sentences generated by a word decoder will mostly be similar to each other (). We propose a hierarchical beam search algorithm with structural-compression and structural-coverage regularization. The hierarchical decoding algorithm has two levels: K-best word-level beam search and N -best sentence-level beam search. At the word-level, the vanilla beam search algorithm is used to maximize the accumulated scorê P (s ′ t ) of generating current summary sentence s ′ t . At the sentencelevel, N -best beam search is realized by maximizing the accumulated score score t of all the sentences generated, including the sentences generation score, structural-compression score and structural-coverage score, which are defined as:
where ζ 1 and ζ 2 are factors introduced to control the influence of structural regularization during the decoding process.
section: Experiments
section: Experimental Settings
We conduct our experiments on the CNN/Daily Mail dataset (, which has been widely used for exploration on summarizing documents with multi-sentence summaries  use 512-dimensional hidden states. The dimension of word embeddings is 128, which is learned from scratch during training. We use a vocabulary of 50k words for both the encoder and decoder. We trained our model on a single Tesla K40m GPU with a batch size of 16 and an epoch is set containing 10,000 randomly sampled documents. Convergence is reached within 300 epochs. After tuning on the validation set, parameters λ 1 , λ 2 , ζ 1 and ζ 2 , are set as -0.5, -1.0, 1.2 and 1.4, respectively. At the test time, we use the hierarchical decoding algorithm with sentence-level beam size 4 and word-level beam size 8.
section: Evaluation
ROUGE Evaluation. We evaluate our models with the widely used ROUGE) toolkit. We compare our system's results with the results of state-of-the-art neural summarization approaches reported in recent papers, including both abstractive models and extractive models. The extractive models include SummaRuNNer ( and SummaRuNNer-abs which is similar to SummaRuNNer but is trained directly on the abstractive summaries. The abstractive models include: 1) Seq2seq-baseline, which uses the basic seq2seq encoder-decoder architecture with attention mechanism, and incorporates with copy mechanism () to alleviate the OOV problem. 2) ABS-temp-attn ( , which uses Temporal Attention on the seq2seq architecture to overcome the repetition problem. 3) Point-cov (, which is an extension of the Seq2seq-baseline model by importing word-coverage mechanism to reduce repetitions in summary. 4) Graph-attention (), which length Method Rouge-1 Rouge-2 R.-L < 100
Our M.  uses a graph-ranking based attention mechanism based on a hierarchical architecture to identify important sentences. 5) Hierachical-baseline, which just uses the basic hierarchical encoder-decoder with hybrid attention model proposed in this paper.
Results in show that our model significantly outperforms all the neural abstractive baselines and extractive baselines. An interesting observation is that the performance of the Hierarchical-baseline model are lower than the Seq2seq-baseline model, which demonstrates the difficulty fora traditional model to identify the structural properties of document summarization process. Our model outperforms the Hierarchical-baseline model by more than 4 ROUGE points, which demonstrates that the structural regularization improves the document summarization performance significantly.
To verify the superiority of our model on generating long summaries, we also compare our method with the best seq2seq model by evaluating them on a test set w.r.t. different length of reference summaries. The results are shown in, which demonstrate that our model is better at generating long summary than the seq2seq model. As the summary becomes longer, our system will obtain larger advantages over the baseline (from +0.22 Rouge-1, +0.08 Rouge-2 and +0.39 Rouge-L for summary less than 100 words, rising to +5.00 Rouge-1, +0.54 Rouge-2 and +4.88 Rouge-L for summaries more than 150 words).
Human Evaluation. In addition to the ROUGE evaluation, we also conducted human evaluation on 50 random samples from CNN/DailyMail test set and compared the summaries generated by our method with the outputs of. Three data annotators were asked to compare the generated summaries   with the human summaries, and assess each summary from four independent perspectives: (1) Informative: How informative the summary is? Concise: How concise the summary is? (3) Coherent: How coherent (between sentences) the summary is? (4) Fluent: How fluent, grammatical the sentences of a summary are? Each property is assessed with a score from 1(worst) to 5(best). The average results are presented in.
The results show that our model consistently outperforms the Seq2seq-baseline model and the previous state-of-the-art method Point-cov. As shown in, the summary generated by Seq2Seq-Baseline usually contains repetition of sentences or phrases, which seriously affects its informativeness, conciseness as well as coherence.
The Point-cov model effectively alleviates the information repetition problem, however, it usually loses some salient information and mainly copies original sentences directly from the input document. The summaries generated by our method obviously contains more salient information and are more concise through sentences compression, which shows the effectiveness of the structural regularization in our model. The results also show that the sentence-level modeling of document and summary in our model makes the generated summaries achieve better inter-sentence coherence.
section: Discussion
section: Model Validation
To verify the effectiveness of each component in our model, we conduct several ablation experiments. Based on the Hierarchical-baseline model, several different structural regularizations are added one by one: +strCom indi- cates adding structural-compression regularization during model learning, +strCov indicates adding structural-coverage regularization during model learning, +hierD indicates using the hierarchical decoding algorithm with both structuralcompression and structural-coverage regularizations during inference. Results on the test set are shown in. Our method much outperforms all the compared systems, which verifies the effectiveness of each component of our model. Note that, both the structural-compression and structural-coverage regularization significantly affect the summarization performance. The higher structuralcompression and structural-coverage scores will lead to higher ROUGE scores. Therefore, we can conclude that the structural-compression and structural-coverage regularization based on our hierarchical model have significant contributions to the increase of ROUGE scores.
section: Structural Properties Analysis
We further compare the ability of different models in capturing the structural-compression and structural-coverage properties of document summarization. shows the comparison results of 4000 document-summary pairs with 14771 reference-summary sentences sampled from CNN/Daily Mail dataset. shows that most samples (over 95%) fall into the righttop area in human-made summaries, which indicates high structural-compression and structural- coverage scores. However, and show that in both the Seq2seq-baseline model and the Hierarchical-baseline model, most samples fall into the left-bottom area (low structuralcompression and structural-coverage), and only about 13% and 7% samples fall into the righttop area, respectively. shows that our system with structural regularization achieves similar behaviors to human-made summaries (over 80% samples fall into the right-top area). The results demonstrate that the structural-compression and structural-coverage properties are common in document summarization, but both the seq2seq models and the basic hierarchical encoder-decoder models are not yet able to capture them properly.
section: Effects of Structural Regularization
The structural regularization based on our hierarchical encoder-decoder with hybrid attention model improves the quality of summaries from two aspects: (1) The summary covers more salient information and contains very few repetitions, which can be seen both qualitatively and) and quantitatively. The model has the ability to shorten along sentence to generate a more concise one or compress several different sentences to generate a more informative one by merging the information from them. shows several examples of abstractive summaries produced by sentence compression in our model.
section: Related Work
Recently some work explored the seq2seq models on document summarization, which exhibit some undesirable behaviors, such as inaccurately reproducing factual details, OOVs and repetitions. To alleviate these issues, copying mechanism () has been incorporated into the encoderdecoder architecture to help generate information correctly. Distraction-based attention model Original Text: luke lazarus , a 23-year-old former private schoolboy , was jailed for at least three years on march 27 for raping an 18-year-old virgin in an alleyway outside his father 's soho nightclub in kings cross , inner sydney in may 2013 .(...) Summary: luke lazarus was jailed for at least three years on march 27 for raping an 18-year-old virgin in an alleyway outside his father 's soho nightclub in may 2013 . Original Text: (...) amy wilkinson , 28 , claimed housing benefit and council tax benefit even though she was living in a home owned by her mother and her partner , who was also working .wilkinson , who was a british airways cabin crew attendant , was ordered to payback a total of 17,604 that she claimed over two years when she appeared at south and east cheshire magistrates court last week . (...) Summary: amy wilkinson , 28 , claimed housing benefit and council tax benefit even though she was living in a home owned by her mother and her partner . she was ordered to payback a total of 17,604 that she claimed over two years when she appeared at south and east cheshire magistrates court last week . Original Text: (...) a grand jury charged durst with possession of a firearm by a felon , and possession of both a firearm and an illegal drug : 5 ounces of marijuana , said assistant district attorney chris bowman , spokesman for the district attorney . millionaire real estate heir robert durst was indicted wednesday on the two weapons charges that have kept him in new orleans even though his lawyers say he wants to go to los angeles as soon as possible to face a murder charge there . his arrest related to those charges has kept durst from being extradited to los angeles , where he 's charged in the december 2000 death of longtime friend susan berman .(...) Summary: durst entered his plea during an arraignment in anew orleans court on weapons charges that accused him of possessing both a firearm and an illegal drug , marijuana . the weapons arrest has kept durst in new orleans even though he is charged in the december 2000 death of a longtime friend .: Examples of sentences compression or fusion by our model. The link-through denotes deleting the non-salient part of the original text. The italic denotes novel words or sentences generated by sentences fusion or compression.
() and word-level coverage mechanism () have also been investigated to alleviate the repetition problem. Reinforcement learning has also been studied to improve the document summarization performance from global sequence level ().
Hierarchical Encoder-Decoder architecture is first proposed by  to train an auto-encoder to reconstruct multi-sentence paragraphs. In summarization field, hierarchical encoder has first been used to alleviate the long dependency problem for long inputs. also propose to use a hierarchical encoder to encode multiple summaries produced by several extractive summarization methods, and then decode them into a headline. However, these models don't model the decoding process hierarchically. first use the hierarchical encoder-decoder architecture on generating multisentences summaries. They mainly focus on incorporating sentence ranking into abstractive document summarization to help detect important sentences. Different from that, our work mainly tends to verify the necessity of leveraging document structure in document summarization and studies how to properly capture the structural properties of document summarization based on the hierarchical architecture to improve the performance of document summarization.
section: Conclusions
In this paper we analyze and verify the necessity of leveraging document structure in document summarization, and explore the effectiveness of capturing structural properties of document summarization by importing both structuralcompression and structural-coverage regularization based on the proposed hierarchical encoderdecoder with hybrid attention model. Experimental results demonstrate that the structural regularization enables our model to generate more informative and concise summaries by enhancing sentences compression and coverage. Our model achieves considerable improvement over state-ofthe-art seq2seq-based abstractive methods, especially on long document with long summary.
