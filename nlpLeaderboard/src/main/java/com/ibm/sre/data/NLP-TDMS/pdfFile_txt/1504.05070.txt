section: title
Self-Adaptive Hierarchical Sentence Model
section: abstract
The ability to accurately model a sentence at varying stages (e.g., word-phrase-sentence) plays a central role in natural language processing. As an effort towards this goal we propose a self-adaptive hierarchical sentence model (AdaSent). AdaSent effectively forms a hierarchy of representations from words to phrases and then to sentences through recursive gated local composition of adjacent segments. We design a competitive mechanism (through gating networks) to allow the representations of the same sentence to be engaged in a particular learning task (e.g., classification), therefore effectively mitigating the gradient vanishing problem persistent in other recursive models. Both qualitative and quantitative analysis shows that AdaSent can automatically form and select the representations suitable for the task at hand during training, yielding superior classification performance over competitor models on 5 benchmark data sets.
section: Introduction
The goal of sentence modeling is to represent the meaning of a sentence so that it can be used as input for other tasks. Previously, this task was often cast as semantic parsing, which aims to find a logical form that can describe the sentence. With recent advances in distributed representations and deep neural networks, it is now common practice to find a vectorial representation of sentences, which turns out to be quite effective for tasks of classification, machine translation, and semantic matching.
Perhaps the simplest method in this direction is the continuous Bag-of-Words (cBoW), where the representations of sentences are obtained by global pooling (e.g, averagepooling or max-pooling) over their word-vectors. The wordvectors, also known as word-embedding, can be determined in either supervised or unsupervised fashion. cBoW, although effective at capturing the topics of sentences, does not consider the sequential nature of words, and therefore has difficulty capturing the structure of sentences. There has been a surge of sentence models with the order of words incorporated, mostly based on neural networks of various forms, including recursive neural networks, recurrent neural network, and convolution neural network. These works apply levels of non-linear transformations to model interactions between words and the structure of these interactions can also be learned on the fly through gated networks. However these models output a fixed length continuous vector that does not retain intermediate information obtained during the composition process, which maybe valuable depending on the task at hand.
In this paper, we propose a self-adaptive hierarchical sentence model (AdaSent). Instead of maintaining a fixedlength continuous vectorial representation, our model forms a multi-scale hierarchical representation. AdaSent is inspired from the gated recursive convolutional neural network (grConv) in the sense that the information flow forms a pyramid with a directed acyclic graph structure where local words are gradually composed to form intermediate representations of phrases. Unlike cBoW, recurrent and recursive neural networks with fixed structures, the gated nature of AdaSent allows the information flow to vary with each task (i.e., no need fora pre-defined parse tree). Unlike grConv, which outputs a fixed-length representation of the sentence at the top of the pyramid, AdaSent uses the intermediate representations at each level of the pyramid to form a multiscale summarization. A convex combination of the representations at each level is used to adaptively give more weight to some levels depending on the sentence and the task. illustrates the architecture of AdaSent and compares it to cBoW, recurrent neural networks and recursive neural networks.
Our contributions can be summarized as follows. First, we propose a novel architecture for short sequence modeling which explores anew direction to use a hierarchical multiscale representation rather than a flat, fixed-length representation. Second, we qualitatively show that our model is able to automatically learn the representation which is suitable for the task at hand through proper training. Third, we conduct extensive empirical studies on 5 benchmark data sets to quantitatively show the superiority of our model over previous approaches. . Flows with green and blue colors act as special cases for recurrent neural networks and recursive neural networks respectively (see more details in Sec. 3.2). Each level of the pyramid is pooled and the whole pyramid reduces into a hierarchy H, which is then fed to a gating network and a classifier to form an ensemble.
section: Background
Let x 1:T denote the input sequence with length T . Each token x t ∈ x 1:T is a V dimensional one-hot binary vector to encode the ith word, where V is the size of the vocabulary. We use U ∈ R d×V to denote the word embedding matrix, in which the jth column is the d-dimensional distributed representation of the jth word in the vocabulary. Hence the word vectors for the sequence x 1:T is obtained by h 0 1:T = U x 1:T . In the cBoW sentence model, the representation ¯ h for x 1:T is obtained by global pooling, either average pooling (Eq. 1) or max pooling (Eq. 2), overall the word vectors:
It is clear that cBoW is insensitive to the ordering of words and also the length of a sentence, hence it is likely for two different sentences with different semantic meanings to be embedded into the same vector representation. Recurrent neural networks area class of neural networks where recurrent connections between input units and hidden units are formed through time. The sequential nature of recurrent neural networks makes them applicable to various sequential generation tasks, e.g., language modeling and machine translation [.
Given a sequence of word vectors h 0 1:T , the hidden layer vector ht at time step t is computed from a non-linear transformation of the current input vector h 0 t and the hidden vector at the previous time step h t−1 . Let W be the input-hidden connection matrix, H be the recurrent hidden-hidden connection matrix and b be the bias vector. Let f (·) be the component-wise non-linear transformation function. The dynamics of recurrent neural networks can be described by the following equations:
The sentence representation ¯ h is then the hidden vector obtained at the last time step, h T , which summarizes all the past words. The composition dynamics in recurrent neural networks can be described by a chain as in.  Recursive neural networks build on the idea of composing along a pre-defined binary parsing tree. The leaves of the parsing tree correspond to words, which are initialized by their word vectors. Non-linear transformations are recursively applied bottom-up to generate the hidden representation of a parent node given the hidden representations of its two children. The composition dynamics in a recursive neural network can be described
where h is the hidden representation fora parent node in the parsing tree and h l , hr are the hidden representations for the left and right child of the parent node, respectively. W L , W R are left and right recursive connection matrices. Like in recurrent neural networks, all the parameters in recursive neural networks are shared globally. The representation for the whole sentence is then the hidden vector obtained at the root of the binary parsing tree. An example is shown in.
Although the composition process is nonlinear in recursive neural network, it is pre-defined by a given binary parsing tree. Gated recursive convolutional neural network (grConv) extends recursive neural network through a gating mechanism to allow it to learn the structure of recursive composition on the fly. If we consider the composition structure in a recurrent neural network as a linear chain and the composition structure in a recursive neural network as a binary tree, then the composition structure in a grConv can be described as a pyramid, where word representations are locally combined until we reach the top of the pyramid, which gives us the global representation of a whole sentence. We refer interested readers to for more details about grConv.
section: Self-Adaptive Hierarchical Sentence Model
AdaSent is inspired and built based on grConv. AdaSent differs from grConv and other neural sentence models that try to obtain a fixed-length vector representation by forming a hierarchy of abstractions of the input sentence and by feeding the hierarchy as a multi-scale summarization into the following classifier, combined with a gating network to decide the weight of each level in the final consensus, as illustrated in.
section: Structure
The structure of AdaSent is a directed acyclic graph as shown in. For an input sequence of length T , AdaSent is a pyramid of T levels. Let the bottom level be the first level and the top level be the T th level. Define the scope of each unit in the first layer to be the corresponding word, i.e., scope(h 1 j ) = {x j }, ∀j ∈ 1 : T and for any t ≥ 2, define scope(h t j ) = scope(h t−1 j ) ∪ scope(h t−1 j+1 ). Then the tth level in AdaSent contains a layer of T − t + 1 units where each unit has a scope of size t. More specifically, the scope of ht j is {x j:j+t−1 }. Intuitively, for the sub-pyramid rooted at ht j , we can interpret ht j as atop level summarization of the phrase x j:j+t−1 in the original sentence. For example, h 3 4 in can be viewed as a summarization of the phrase on the mat. In general, units at the tth level are intermediate hidden representations of all the consecutive phrases of length tin the original sentence (see the scopes of units at the 3rd level in for an example). There are two extreme cases in AdaSent: the first level contains word vectors and the top level is a global summarization of the whole sentence.
Before the pre-trained word vectors enter into the first level of the pyramid, we apply a linear transformation to map word vectors from Rd to RD with D ≥ d. That way we can allow phrases and sentences to be in a space of higher dimension than words for their richer structures. More specifically, the hidden representation h 1 1:T at the first level of the pyramid is
where U ∈ R D×d is the linear transformation matrix in AdaSent and U ∈ R d×V is the word-embedding matrix trained with a large unlabeled corpus. Equivalently, one can view U U U ∈ R D×V as anew word-embedding matrix tailored for AdaSent. This factorization of the wordembedding matrix also helps to reduce the effective number of parameters in our model when d D.
section: Local Composition and Level Pooling
The recursive local composition in the pyramid works in the following way
where j ranges from 1 to T − t + 1 and t ranges from 2 to T . W L , W R ∈ R D×D are the hidden-hidden combination matrices, dubbed recurrent matrices, and b W ∈ RD is a bias vector. ω l , ω rand ω care the gating coefficients which satisfy ω l , ω r , ω c ≥ 0 and ω l + ω r + ω c = 1. Eq. 6 provides away to compose the hidden representation of a phrase of length t from the hidden representation of its left t − 1 prefix and its right t − 1 suffix. The composition in Eq. 6 includes a non-linear transformation, which allows a flexible hidden representation to be formed. The fundamental assumption behind the structure of AdaSent is then encoded in Eq. 5: the semantic meaning of a phrase of length t is a convex combination of the semantic meanings of its t − 1 prefix, t − 1 suffix and the composition of these two. For example, we expect the meaning of the phrase the cat to be expressed by the word cat since the is only a definite article, which does not have a direct meaning. On the other hand, we also hope the meaning of the phrase not happy to consider both the functionality of not and also the meaning of happy. We design the local composition in AdaSent to make it flexible enough to catch the above variations in language while letting the gating mechanism (the way to obtain ω l , ω rand ω c ) adaptively decide the most appropriate composition from the current context.
Technically, when computing ht j , ω l , ω c and ω rare parametrized functions of h t−1 j and h t−1 j+1 such that they can decide whether to compose these two children by a non-linear transformation or simply to forward the children's representations for future composition. For the purpose of illustration, we use the softmax function to implement the gating mechanism during the local composition in Eq. 7. But note that we are not limited to a specific choice of gating mechanism. One can adopt more complex systems, e.g., MLP, to implement the local gating mechanism as long as the output of the system is a multinomial distribution over 3 categories.
G L , G R ∈ R 3×D and b G ∈ R 3 are shared globally inside the pyramid. The softmax function over a vector is defined as:
Local compositions are recursively applied until we reach the top of the pyramid.
It is worth noting that the recursive local composition in AdaSent implicitly forms a weighted model average such that each unit at layer t corresponds to a convex combination of all possible sub-structures along which the composition process is applied over the phrase of length t. This implicit weighted model averaging makes AdaSent more robust to local noises and deteriorations than recurrent nets and recursive nets where the composition structure is unique and rigid. shows an example when t = 3. ) =: The hidden vector obtained at the top can be decomposed into a convex combination of all possible hidden vectors composed along the corresponding sub-structures.
Once the pyramid has been built, we apply a pooling operation, either average pooling or max pooling, to the tth level, t ∈ 1 : T , of the pyramid to obtain a summarization of all consecutive phrases of length tin the original sentence, denoted by ¯ ht (see an example illustrated in for the global level pooling applied to the 3rd level in the pyramid). It is straightforward to verify that ¯ h 1 corresponds to the representation returned by applying cBoW to the whole sentence.
T then forms the hierarchy in which lower level summarization in the hierarchy pays more attention to local words or short phrases while higher level summarization focuses more on the global interaction of different parts in the sentence.
section: Gating Network
Suppose we are interested in a classification problem, one can easily extend our approach to other problems of interests. Let g(·) be a discriminative classifier that takes ¯ ht ∈ RD as input and outputs the probabilities for different classes. Let w(·) be a gating network that takes ¯ ht ∈ RD , t = 1, . . . , T as input and outputs a belief score 0 ≤ γ t ≤ 1. Intuitively, the belief score γ t depicts how confident the tth level summarization in the hierarchy is suitable to be used as a proper representation of the current input instance for the task at hand. We require γ t ≥ 0, ∀t and T t=1 γ t = 1. Let C denote the categorical random variable corresponding to the class label. The consensus of the whole system is reached by taking a mixture of decisions made by levels of summarizations from the hierarchy:
where each g(·) is the classifier and w(·) corresponds to the gating network in.
section: Back Propagation through Structure
We use back propagation through structure (BPTS) to compute the partial derivatives of the objective function with respect to the model parameters. Let L(·) be our scalar objective function. The goal is to derive the partial derivative of L with respect to the model parameters in AdaSent, i.e., two recurrent matrices, W L , W Rand two local composition matrices G L , G R (and their corresponding bias vectors):
where I is the identity matrix and diag(f ) is a diagonal matrix spanned by the vector f , which is the derivative off (·) with respect to its input. The identity matrix in Eq. 12 and Eq. 13 plays the same role as the linear unit recurrent connection in the memory block of LSTM to allow the constant error carousel to effectively prevent the gradient vanishing problem that commonly exists in recurrent neural nets and recursive neural nets. Also, the local composition weights ω l , ω rand ω c in Eq. 12 and Eq. 13 have the same effect as the forgetting gate in LSTM] by allowing more flexible credit assignments during the back propagation process.
section: Experimental Setting
Statistics about the data sets used in this paper are listed in. We describe each data set in detail below: 1. MR. Movie reviews 1 data set where each instance is a sentence. The objective is to classify each review by its overall sentiment polarity, either positive or negative. 2. CR. Annotated customer reviews of 14 products obtained from Amazon [Hu and 2 . The task is to classify each customer review into positive and negative categories. 3. SUBJ. Subjectivity data set where the goal is to classify each instance (snippet) as being subjective or objective. 4. MPQA. Phrase level opinion polarity detection subtask of the MPQA data set] 3 . 5. TREC. Question data set, in which the goal is to classify an instance (question) into 6 different types and Matrix-vector recursive neural network. In these two models, words are gradually composed into phrases and sentence along a binary parse tree. 3. CNN and. Convolutional neural network for sentence modeling. In DCNN, the author applies dynamic k-max pooling overtime to generalize the original max pooling in traditional CNN. 4. P.V.. Paragraph Vector is an unsupervised model to learn distributed representations of words and paragraphs. We use the public implemen-tation of P.V. and use logistic regression on top of the pre-trained paragraph vectors for prediction. 5. cBoW. Continuous Bag-of-Words model. As discussed above, we use average pooling or max pooling as the global pooling mechanism to compose a phrase/sentence vector from a set of word vectors. 6. RNN, BRNN. Recurrent neural networks and bidirectional recurrent neural networks. For bidirectional recurrent neural networks, the reader is referred to for more details. 7. GrConv. Gated recursive convolutional neural network shares the pyramid structure with AdaSent and uses the top node in the pyramid as a fixed length vector representation of the whole sentence.
section: Training
The difficulty of training recurrent neural networks is largely due to the notorious gradient exploding and gradient vanishing problem. As analyzed and discussed before, the DAG structure combined with the local gating composition mechanism of AdaSent naturally help to avoid the gradient vanishing problem. However, the gradient exploding problem still exists as we observe in our experiments. In this section, we discuss our implementation details to mitigate the gradient exploding problem and we give some practical tricks to improve the performance in the experiments.
section: Regularization of Recurrent Matrix
The root of the gradient exploding problem in recurrent neural networks and other related models lies in the large spectral norm of the recurrent matrix as shown in Eq. 12 and Eq. 13. Suppose the spectral norm of W Land W R 1, then the recursive application of Eq. 12 and Eq. 13 in the back propagation process will cause the norm of the gradient vector to explode. To alleviate this problem, we propose to penalize the Frobenius norm of the recurrent matrix, which acts as a surrogate (upper bound) of the corresponding spectral norm, since 1) it is computationally expensive to compute the exact value of spectral norm and 2) it is hard to establish a direct connection between the spectral norm and the model parameters to incorporate it into our objective function. Let L(·, ·) be our objective function to minimize. For example, when L is the negative log-likelihood in the classification setting, our optimization can be formulated as
where xi is the training sequence and y i is the label. The value of the regularization coefficient λ is problem dependent. In our experiments, typical values of λ range from 0.01 to 5 × 10 −5 . For all our experiments, we use minibatch AdaGrad with the norm-clipping technique to optimize the objective function in Eq. 14.
section: Implementation Details
Throughout our experiments, we use a 50-dimensional word embedding trained using word2vec on the Wikipedia corpus (∼1B words). The vocabulary size is about 300,000. For all the tasks, we fine-tune the word embeddings during training to improve the performance. We use the hyperbolic tangent function as the activation function in the composition process as the rectified linear units are more prone to the gradient exploding problem in recurrent neural networks and its related variants. We use an MLP to implement the classifier on top of the hierarchy and use a softmax function to implement the gating network. We also tried using MLP to implement the gating network, but this does not improve the performance significantly.
section: Experiment Results
Model  The classification accuracy of AdaSent compared with other models is shown in. AdaSent consistently outperforms P.V., cBoW, RNN, BRNN and GrConv by a large margin while achieving comparable results to the state-ofthe-art and using much fewer parameters: the number of parameters in our models range from 10K to 100K while in CNN the number of parameters is about 400K 6 . AdaSent outperforms all the other models on the MPQA data set, which consists of short phrases (the average length of each instance in MPQA is 3). We attribute the success of AdaSent on MPQA to its power in modeling short phrases since long range dependencies are hard to detect and represent.
Compared with BRNN, the level-wise global pooling in AdaSent helps to explicitly model phrases of different lengths while in BRNN the summarization process is more sensitive to a small range of nearby words. Hence, AdaSent consistently outperforms BRNN on all data sets. Also, AdaSent significantly outperforms GrConv on all the data sets, which indicates that the variable length multi-scale representation is key to its success. As a comparison, GrConv does not perform well because it fails to keep the intermediate representations.
More results on using GrConv as a fixed-length sequence encoder for machine translation and related tasks can be found in. cBoW is quite effective on some tasks (e.g., SUBJ). We think this is due to the language regularities encoded in the word vectors and also the characteristics of the data itself. It is surprising that P.V. performs worse than other methods on the MPQA data set. This maybe due to the fact that the average length of instances in MPQA is small, which limits the number of context windows when training P.V.. by running each of the models on every data set 10 times using different settings of hyper-parameters and random initializations. We report the mean classification accuracy and also the standard deviation of the 10 runs on each of the data set. Again, AdaSent consistently outperforms all the other competitor models on all the data sets.
section: Model
To study how the multi-scale hierarchy is combined by AdaSent in the final consensus, for each data set, we sample two sentences with a pre-specified length and compute their corresponding belief scores. We visualize the belief scores of 10 sentences by a matrix shown in. As illustrated in, the distribution of belief scores varies among different input sentences and also different data sets. The gating network is trained to adaptively select the most appropriate representation in the hierarchy by giving it the largest belief score. We also give a concrete example from MR to show both the predictions computed from each level and their corresponding belief scores given by the gating network in. The first row in shows the belief scores Pr(H x = t|x 1:T ), ∀t and the second row shows the probability Pr(y = 1|H x = t), ∀t predicted from each level in the hierarchy. In this example, although the classifier predicts incorrectly for higher level representations, the gating network assigns the first level with the largest belief score, leading to a correct final consensus. The flexibility of multiscale representation combined with a gating network allows AdaSent to generalize GrConv in the sense that GrConv corresponds to the case where the belief score at the root node is 1.0.  If the movie were all comedy it might work better but it has an ambition to say something about its subjects but not willingness.
To show that AdaSent is able to automatically learn the appropriate representation for the task at hand, we visualize the first two principal components (obtained by PCA) of the vector with the largest weight in the hierarchicy for each sentence in the dataset. shows the projected features from AdaSent (left column) and cBoW (right column) for SUBJ (1st row), MPQA (2nd row) and TREC (3rd row). During training, the model implicitly learns a data representation that enables better prediction. This property of AdaSent is very interesting since we do not explicitly add any separation constraint into our objective function to achieve this.
section: Conclusion
In this paper, we propose AdaSent as anew hierarchical sequence modeling approach. AdaSent explores anew direction to represent a sequence by a multi-scale hierarchy instead of a flat, fixed-length, continuous vector representation. The analysis and the empirical results demonstrate the effectiveness and robustness of AdaSent in short sequence modeling. Qualitative results show that AdaSent can learn to represent input sequences depending on the task at hand.
